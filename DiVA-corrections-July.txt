In diva2:873936 there is an error in the title  'arambient' should be 'Ar ambient' - as can be see on the cover of the thesis.

Additionally, the abstract should be:

"We grow epitaxial graphene on 6H-SiC (0001) substrates in Ar background and intercalate hydrogen to reform the carbon buffer layer into single and multilayered graphene. We will analyze the graphene using a combination of techniques including optical microscopy, micro-Raman spectroscopy, Atomic Force Microscopy (AFM), and reflectance mapping and contactless measurements of sheet carrier density and charge carrier mobility. We have studied in detail, the influence of growth parameters and in-situ surface preparation of substrate on the thickness uniformity and surface morphology of graphene. Additionally, as-grown graphene layers were intercalated with H to obtain quasi-free standing layers of graphene with enhanced charge carrier mobility."
----------------------------------------------------------------------
In diva2:1216876 a number of words are merged, the abstract should be:

The aim of this research project was to create a conversational interface for retrieving information from rulebooks. This conversational interface takes the shape of an assistant named OLGA (short for Open Legend Game Assistant) to whom you can give enquiries about the rules of any game loaded into the program. We tuned and designed the assistant around a specific type of board games called TRPGs (tabletop role playing games), hence the conversational interface is focused around game rulebooks. By giving the assistant the rules for a game in the form of a raw text document the program can extract key concepts and words from the rules which we call entities. The process of extracting entities and all other functions of the assistant were calibrated on the TRPG called Open Legend, hence the name Open Legend Game Assistant. When the user sends a query to the assistant it is first sent to the web service Dialogflow for interpretation. In Dialogflow we enter our extracted entities to assist the service in recognizing key words and concepts in the queries. Dialogflow then returns an object with information telling the assistant what the intent of the user’s query was and any additional information provided. The assistant then responds to the query. The standard response for a request for information about an entity is what we call a streak search. The assistant locates parts of the rules that contain the entity and sorts them by a relevance score, then the results are presented in order of relevance. When testing on people with no prior knowledge of the game it was concluded that the assistant indeed could be helpful in finding answers to rule questions in the limited amount of time provided. Generalization being one of our goals the program was also applied on another rule system in the TRPG genre, Pathfinder, applied on this system the assistant worked as intended without altering any algorithm.
----------------------------------------------------------------------
In  diva2:1818546 \emph{outliers} indicated the word 'outliers' should be emphasize in italics or bold.

In diva2:1871154 'MITRAC/TC1500TM' should be 'MITRAC/TC1500™', i.e, the trademark character rather than "TM"

In diva2:742615,  in the abstract there are multiple spaces between words, some words are merged such as "andtoday"while others have been split apart, such as "per    cent"- the "full text" is not the PDF of the thesis, but rather a PDF containing images of files on someone's desktop! Additionally, there is no examiner listed in the DiVA entry.

In diva2:847246 in the abstract a number of words are merged together.
----------------------------------------------------------------------
In diva2:1295396 many words have been run together and the paragraphs are not kept. The abstract should be:

 <p>Uplink power control is a resource management function that controls the signal’s transmit power from a user device, i.e. mobile phone, to a base-station tower. It is used to maximize the data-rates while reducing the generated interference.</p><p>Reinforcement learning is a powerful learning technique that has the capability not only to teach an artificial agent how to act, but also to create the possibility for the agent to learn through its own experiences by interacting with an environment.</p><p>In this thesis we have applied reinforcement learning on uplink power control, enabling an intelligent software agent to dynamically adjust the user devices’ transmit powers. The agent learns to find suitable transmit power levels for the user devices by choosing a value for the closed-loop correction signal in uplink. The purpose is to investigate whether or not reinforcement learning can improve the uplink power control in the new 5G communication system.</p><p>The problem was formulated as a multi-armed bandit at first, and then extended to a contextual bandit. We implemented three different reinforcement learning algorithms for the agent to solve the problem. The performance of the agent using each of the three algorithms was evaluated by comparing the performance of the uplink power control with and without the agent. With this approach we could discover whether the agent is improving the performance or not. From simulations, it was found out that the agent is in fact able to find a value for the correction signal that improves the data-rate, or throughput measured in Mbps, of the user devices average connections. However, it was also found that the agent does not have a significant contribution regarding the interference.</p>

Similarly for diva2:1188687, the abstract should be:

<p>This degree project, conducted at Volvo Cars, investigates whether closed-loop re-simulation (CLR) methods can provide a safety proof for the autonomous driving (AD) functions based on previously collected driving data. The elements under study for this closed loop approach are model-in-loop based Simulation Platform Active Safety (SPAS) environment and Active Safety (AS) software.</p><p>The prerequisites for securing the closed loop re-simulation environment are performing open-loop simulations with AS software under test and preparing a validated vehicle model constituting the sensors and actuators. The validated vehicle model against a set of physical data ensures high confidence in the CAE environment. This results in high correlation between physical and simulated data for the closed loop tests performed for testing the Active Safety algorithms.</p><p>This thesis work focuses on preparing the vehicle model in SPAS with the emphasison performance of auto-brake functionality in CLR. The vehicle model in SPAS was prepared by tuning the brake model focusing on the EuNCAP cases in which CLR environment was subsequently tested with respect to Eu-NCAP scenarios.</p<<p>In the procedure of securing CLR methods, it was crucial to design the scenarios in virtual test environment as close as possible to field test conditions to make reliable comparison with the reality. Therefore, the verification of CLR environment was carried out by subjecting the CAE Environment to EuNCAP braking scenarios with dry surfaces, host vehicle velocities up to 80 km/h and target vehicle deceleration levels being 2m/s<sup>2</sup> and 6m/s<sup>2</sup>.</p><p>As a result of all these virtual tests, it was empirically verified that CLR environment can be used to predict braking behaviour of the vehicle in certain traffic scenarios for the verification of autonomous driving functions.</p>


----------------------------------------------------------------------
 in diva2:1271491, merged words and a Roman numeral page number The  abstract should be:

 <p>There is currently a high demand for electric power from renewable sources. One source that remains relatively untapped is the motion of ocean waves. Anders Hagnestål has been developing a uniquely efficient and simplified design for a point-absorb buoy generator by converting its linear motion directly into alternating electric power using a linear PM engine. To test this method, a smaller prototype is built. Its characteristics present some unusual challenges in the design and construction of its winding.</p><p>Devices of this type typically use relatively low voltage (690V typically for a wind turbine, compared to the 10kV range of traditional power plants). To achieve high power, they need high current, which in turn requires splitting the conductors in the winding into isolated parallel strands to avoid losses due to eddy currents and current crowding. However, new losses from circulating currents can then arise. In order to reduce said losses, the parallel conductors should be transposed in such a way that the aggregate electromotive force the circuits that each pair of them forms is minimized.</p><p>This research and prototyping was performed in absence of advanced industrial means of construction, with limited space, budget, materials, manpower, know-how, and technology. Manual ingenuity and empirical experimentation were required to find a practical implementation for: laying the cables, fixing them in place, transferring them to the machine, stripping their coating at the ends and establishing a reliable connection to the current source.</p><p>Using theoretical derivations and FEM simulation, a sufficiently good transposition scheme is proposed for the specific machine that the winding is built for. A bobbin replicating the shape of the engine core is built to lay down the strands.</p><p>The  parallel strands are then organized each into their respective bobbin, with a bobbin rack and conductor funneling device being designed and constructed to gather them together into a strictly-organized bundle. An adhesive is found to set the cables in place.</p><p>Problems with maintaining the orientation and configuration of the cables in the face of repeated torsion are met and solved. A chemical solution is used to strip the ends of the conductors, and a reliable connection is established by crimping the conductors into a bi-metal Cu-Al lug.</p><p>In conclusion, the ideal transposition schemes required to cancel out circulating currents due to magnetic flux leakage are impossible to put in practice without appropriate technological means. The feasible transposition scheme turns out to be a simple mirroring of conductors’ positions, implemented by building each half of the winding separately around replicas of the core and then connecting them using crimping lugs.</p>
Note that diva2:654273 is one of the many theses that does not have full text in DiVA, but rather therre is a link to http://www.csc.kth.se/utbildning/kandidatexjobb/datateknik/2011/rapport/hassel_olle_OCH_janse_petter_K11049.pdf
 - it is likely that the www.csc.kth.se server will go away and this this and other full text will be lots (much the same as happened when the nada server went away).
----------------------------------------------------------------------

In diva2:860672, there were merged words and missing ligatures, the abstract should be:

<p>The efficiency of today's after treatment system relies on the exhaust gas temperature.The Selective Catalytic Reduction system, which is a part of the after treatment system has an operating temperature window which starts at approximately 200C in order to work as efficiently as possible. Rolling downhill without burning fuel could cause the exhaust after treatment system to cool down below this window, which gives higher emissions when burning fuel again.This thesis focuses on investigating the potential of using the exhaust brake, a throttle between the turbine and the exhaust after treatment system, in slopes and investigates and evaluates possible strategies. The exhaust brake is controlled using Model Predictive Control with models of acceleration and temperature with respect to exhaust brake pressure, mass and road incline. Different strategies are investigated and it is concluded that controlling the exhaust brake to keep constant velocity down the slope gives the highest temperature at the exhaust after treatment system. It is also concluded that there is no strategy which could guarantee the exhaust after treatment temperature to stay within the temperature window just using the exhaust brake.</p>

In diva2:1485487, merged words and missing subscript, the abstract should be: 

<p>This thesis considers stabilization of constant power loads (CPLs) fed by a dc power source through an input filter, using model predictive control (MPC). Train propulsion systems generally utilize electrical motors whose output torque is tightly regulated by power converters. Often, these systems behave as CPLs. When a CPL is coupled with an input filter it can lead to a stability problem known as the negative impedance instability problem. Current state of the art regulators deal with this problem using classical frequency domain optimization-based controllers, such as H<sub>∞</sub>. This thesis instead proposes a linear parameter-varying model predictive controller (LPV-MPC). This advanced control method solves the negative impedance instability problem while also being capable of explicitly addressing signal constraints, which often exist in power converter applications. The regulator is evaluated in MATLAB/Simulink as well as in a software-in-the-loop (SIL) simulator. It has furthermore been realized in a real-time hardware-in-the-loop (HIL) simulator and tested in a power laboratory. Theoretical results show improved performance over conventional H<sub>∞</sub> controllers, in terms of damping and control input use, under certain operating conditions where the control input is limited. The results can be used as a benchmark of theoretical performance limits for design of other regulators.</p>

In diva2:1414451, 'coreferencere': should be 'coreference', so the abstract would be:

: <p>This degree project examines and evaluates the performance of various ways of improving contextualization of text span representations within a general multi-task learning framework for named entity recognition, coreference solution and relation extraction. A span-based approach is used in which all possible text spans are enumerated, iteratively refined and finally scored. This work examines which ways of contextualizing the span representations are beneficial when using the text embedder BERT. Furthermore, I evaluate to what degree graph propagations can be used together with BERT to enhance performance further, and observe F1-score improvements over previous work. The architecture sets new state-of-the-art results on four datasets from different domains - SciERC, ACE2005, GENIA and WLPC. Qualitative examples are provided to highlight model behaviour and reasons for the improvements are discussed.</p>
----------------------------------------------------------------------
----------------------------------------------------------------------
In diva2:818797 each line of the abstract had been treated as a paragraph, but there are only three paragraphs. There are also some words merged. The abstract should be::

<p>The objective of this thesis is to derive an analytical model representing a reduced form of a mine hoist hydraulic braking system. Based primarily on fluid mechanical and mechanical physical modeling, along with a number of simplifying assumptions, the analytical model will be derived and expressed in the form of a system of differential equations including a set of static functions. The obtained model will be suitable for basic simulation and analysis of system dynamics, with the aim to capture the fundamentals regarding feedback control of the brake system pressure.</p><p>The thesis will mainly cover hydraulic servo valve and brake caliper modeling including static modeling of brake lining stress-strain and disc spring deflection force characteristics. Nonlinearities such as servo valve hysteresis, saturation, effects of under- or overlapping spool geometry, flow forces, velocity limitations and brake caliper frictional forces have intentionally been excluded in order not to make the model overly complex. The hydraulic braking system will be described in detail and basic theory that is needed regarding fluid properties and fluid mechanics will also be covered so as to facilitate the reader in his understanding of the material presented in this work.</p><p>Overall, the scope of this thesis is broad and more work remains in order to complement the model of the system both qualitatively and quantitatively. Although not complete in its simplified form and with known nonlinearities aside, the validity of the model in the lower frequency domain is confirmed by results given in form of measurements and dynamic simulation. Static analysis of the brake caliper model is also verified to be essentially correct when comparing calculated characteristics against actual measurements, as is also the case for the static models of the brake lining and disc-spring characteristics.</p>

In diva2:1298737 there were merged words and missing ligatures, the abstract should be:

 <p>The recent increase of electric cars adoption will influence the electricity demand in the distribution networks which risks to be higher than the maximum power available in the grid, if not well planned. For this reason, it is on the DSOs and TSOs's interest to plan carefully coordinated charging of a bulk of EVs as well as assess the possibility of EVs acting as energy storages with the Vehicle-to-Grid (V2G) or Vehicle-to-Building (V2B) capability. When parked and plugged into the electric grid, EVs will absorb energy and store it, being also able to deliver electricity back to the grid/building (V2G/B system). This can be an optimized process, performed by an aggregator, gathering multiple EVs that discharge the battery into the grid at peak time and charge when there is low demand i.e. overnight and off-peak hours.</p><p>Numerous studies have investigated the possibility of aggregating multiple EVs and optimizing their charging and discharging schedules for peak load reduction or energy arbitrage with participation in the electricity market. However, no study was found for optimizing a shared fleet of EVs with daily reservations for different users trying to perform V2B. In this study an optimization modelling algorithm (mixed integer linear problem - MILP) that manages the possible reservations of the shared fleet of EVs, coordinates the charging and discharging schedules, and provides V2B (Vehicle-to-Building), with the objective of minimizing energy costs and accounting with battery ageing has been developed. A case study with real data for a building is carried out modelling different number of EVs for two different days in year 2017, one in March and other in June.</p><p>Results show that the profits are higher for all cases when introducing V2B as compared to a no optimization scenario: V2B with battery degradation (50 ore/kWh) has decreased daily variable electricity costs between 54 and 59% in March and 60 and 63% for June when compared without smart charging. Integration of battery degradation cost in V2B applications is necessary and influences significantly the charging and discharging strategies adopted by EV and finally the total daily costs: The total daily cost increase by maximal 10% for the day in March and 13% for the day in June when comparing the scenario that has stationary battery and uses only-charging model for EVs with the scenario applying V2B mode considering a degradation cost of 80 ore/kWh.</p>

In diva2:489848, there were merged words and the "ff" ligature. The abstract should be: 

<p>The sliding contact between catenary and pantograph has to transfer a large amount of current and power to the locomotive reliably. Sometimes detachment and attachment occur between the contact wire and the pantograph.This is related to the speed of the train as well as the lifting force of the pantograph. It leads to the creation of arcs whose visibility is also increased by the current level and their probability of appearance by the presence of a layer of ice (under harsh winter conditions) that could be formed on the bottom of the contact wire, which creates a separation between pantograph and catenary. The ﬁrst goal of the work is to create a model of the arc impedance, based on previously done experimental investigations. The model aims to simulate arc voltages (output) from experimental arc currents (input) which differentiate from each other by the RMS value, the voltage level, the powerfactor or the speed of the train. A strong link between the arc current waveform characteristics and the corresponding arc voltage has been brought to light, leading to very good simulations: for each arc period, the voltage is strongly related to the extreme value of the current over the same period. Once the model has been built and validated, it has been found that the simulated arc voltages were quite similar to the experimental voltages, and thus that the main part of the information about the arc phenomenon is contained in the arc current itself. The transient overvoltages as well as the average values are well modeled and approximated. Then, the modeled arc voltages have been studied in order to see how the impedance model behaves when the arc currents have different characteristics. Both thermal and dielectric reignitions (breakdowns) are correctly recreated, and the presence of a net DC voltage component (originating from transients and asymmetrically distorted waveforms) was also conﬁrmed for all the test runs. Finally, an electric circuit model is elaborated. Starting from a simple frequency analysis, an RLC circuit has been simulated and the encouraging results lead the way to a further study.</p>

In diva2:930984 there was a "ff" and "fi" ligature, the abstract should be:

 <p>Handsets have experienced a tremendous growth over the past few years, with users have more and more options in applications. The push notification has replaced the email grown in importance as a new bridge for application service provider to engage with users. The essential purpose of push notifications is to enable an app to inform its users that it has something for them when the app is not running in the foreground. For example, a message or an upcoming appointment [1]. However, in practice, the notification channel would get ﬂooded by commercials regardless of which platform that was used. Aimless push notifications would only result in pushing the users away from the applications. This leads to the rise of the concept of contextual notifications, sending notifications at the right place, right time, regarding the right piece of information to the targeted audience. The concept of contextual notifications is not entirely new, yet the power of contextual notifications has not been fully developed, especially for video on demand applications. By sending more personal, contextual notifications to the user, it could prevent users from not opening the application or even deleting the application. Moreover, since users nowadays are used to having a large range of options in browsing videos over different platforms as well, contextual notifications for cross platforms are also of interest. Users can use their companion mobile devices to interact with the video that is being played on the ﬁrst screen, even the two devices are different platforms, the companion device still can receive the relative contextual notification about the video on the ﬁrst screen.</p><p>The thesis work presents the result of the exploratory study and design development, then a discussion and conclusion will is presented based on the result. The development consists of three main parts. First of all, a high-level design and a framework are proposed based on pre-study of Accedo Appgrid product and the other existing system. Secondly, an implementation on proof of concept of contextual notifications server is performed. Thirdly, a demonstration application is implemented in order to test the proof of concept contextual notification server.</p><p>In the end, the functional evaluation is presented and a conclusion can be made that the contextual notification solution is achievable.</p>

In diva2:1263117 there was an "ff" ligature, a missing italics,  and the paragrpahs were all merged, the abstract should be:

 <p>The recent emergence of a distributed technology named blockchain, clearly created a new point of view in the data storing and data distribution fields. If on one hand blockchain is mainly known for Bitcoin (an auto-regulated decentralized digital currency), on the other hand it has the potential to set up an auto regulated economy.</p><p>In this thesis, the blockchain technology will be analyzed and described starting from P2P architecture and its origin in 2009 Satoshi Nakamoto’s whitepaper, and leading to the most up to date blockchains. The advantages and disadvantages of such architecture will be pointed out keeping in mind the security, speed and cost of such infrastructure.</p><p>While Real Estate companies have often anticipated the technological innovations, land registries, instead, derive and keep a working manner which is extremely old and out of date: made of unclear procedures and wet signatures. The market needs and legislation will be researched mainly referring to other works and integrated with a technical point of view with particular focus on the decentralization of such systems.</p><p>After analyzing the flow, problems and flaws of the current system, a new proposal will be researched, in particular trying to minimize the dead time in between the different steps of the mortgage, increase transparency, as well as reducing dependence on the central authorities, leading to more convenient interactions among the properties’ stakeholders. An attractive low capitalization decentralized financial product will also be proposed and implemented able to lower the interest rate and create a profitable investment with low risk, low interest and durable in time.</p><p>Secure and <em>ad-hoc</em> algorithms will be presented and, in a later section, analyzed in combination with different blockchain technologies. Scalability and performance will also be evaluated, taking into account all the current technology limitations and the near future opportunities.</p>

In diva2:1368334 there was an "ff" ligature, a missing italics. The abstract should be:

 <p>In the last years, the demands on different models using deep learning to generate textual data conditionally have increased, where one would like to control what textual data to generate from a deep learning model. For this purpose, a couple of models have been developed and achieved state-of-art performance in the ﬁeld of generating textual data conditionally. Therefore, the purpose of this study was to develop a new model that could outperform the relevant baseline models with respect to the BLEU metric. The alternative model combined some of the properties from the state-of-art models and was given the name the <em>Variational Attribute-to-Sequence decoder model</em> (shortened to the V-Att2Seq model) that paraphrases the name of one of the state-of-art models and "variational" refers to its application of variational recurrent autoencoders (VRAE). The data set used in this study contained drug reviews that were written by patients to express their opinion about the drug that they have used to treat a certain condition. The drug review texts were accompanied by the following attributes: the (name of the) drug, the condition, and the rating that the patient has given to the drug. The results in this study show that the V-Att2Seq model did not outperform all the baseline models, which concluded that the V-Att2Seq model did not satisfy the requirements imposed on the model itself. However, there are some future work that is suggested by this study to hopefully improve the performance of the V-Att2Seq model in the future such as including other mechanisms that are present in the state-of-art models, testing with e.g. other sizes and settings of the V-Att2Seq model, and testing different strategies forgenerating sequences since there is still potential that has been observed in the model that should be further investigated to improve its performance.</p>

In diva2:1586228 there was an "ff" and "fi" ligatures the paragrpahs were all merged, the abstract should be:

 <p>Databases of user generated data can quickly become unmanageable. Klarna faced this issue, with a database of around 700,000 customer reviews. Ideally, the database would be cleaned of uninteresting reviews and the remaining reviews categorized. Without knowing what categories might emerge, the idea was to use an unsupervised clustering algorithm to find categories.</p><p>This thesis describes the work carried out to solve this problem, and proposes a solution for Klarna that involves artificial neural networks rather than unsupervised clustering. The implementation done by us is able to categorize reviews as either interesting or uninteresting. We propose a workﬂow that would create means to categorize reviews not only in these two categories, but in multiple.</p><p>The method revolved around experimentation with clustering algorithms and neural networks. Previous research shows that texts can be clustered, however, the datasets used seem to be vastly different from the Klarna dataset. The Klarna dataset consists of short reviews and contain a large amount of uninteresting reviews.</p><p>Using unsupervised clustering yielded unsatisfactory results, as no discernible categories could be found. In some cases, the technique created clusters of uninteresting reviews. These clusters were used as training data for an artificial neural network, together with manually labeled interesting reviews. The results from this artificial neural network was satisfactory; it can with an accuracy of around 86% say whether a review is interesting or not. This was achieved using the aforementioned clusters and five feedback loops, where the model’s wrongfully predicted reviews from an evaluation dataset was fed back to it as training data.</p><p>We argue that the main reason behind why unsupervised clustering failed is that the length of the reviews are too short. In comparison, other researchers have successfully clustered text data with an average length in the hundreds. These items pack much more features than the short reviews in the Klarna dataset. We show that an artificial neural network is able to detect these features despite the short length, through its intrinsic design.</p><p>Further research in feature extraction of short text strings could provide means to cluster this kind of data. If features can be extracted, the clustering can thus be done on the features rather than the actual words. Our artificial neural network shows that the arbitrary features interesting and uninteresting can be extracted, so we are hopeful that future researchers will find ways of extracting more features from short text strings. In theory, this should mean that text of all lengths can be clustered unsupervised. </p>

In diva2:1205396 there was a "ff" ligrautre, the abstract should be:

 <p>Nowadays it is more clear that the Internet of things (IoT) is not a transient trend but a completely new industry. The internet of things has the capability to enhance current industries (Industry 4.0), as well as to help protecting the environment and people. The latter is the case with the system developed and described in this thesis.</p><p>The possibilities that IoT brings are due to the interconnection of heterogeneous embedded devices to the internet. This thesis focus on LPWANs (Low Power Wide Area Networks), which is a new set of technologies specifically design for the needs of IoT devices.Due to the recent deploy of NB-IoT (Narrow Band IoT) networks it has become more diﬃcult to know what LPWAN is best for a certain application. Thus, the first half of this thesis involves the comparative study of NB-IoT and LoRaWAN LPWANs. This comparison required an in depth study of each technology, specially on the physical and datalink layers. The comparison briefly displays the main characteristics of each technology and explain the main conclusions in a concise manner.</p><p>The second part of the thesis describes the development of a GNSS tracker. This tracker will be used on train wagons carrying goods that are dangerous for people and the environment. This thesis report describes the different steps taken, from the requirement specification to the partial development of the software.</p>

In diva2:931627  there was a "ff" ligrautre and merged paragraphs, the abstract should be

<p>Autonomous driving vehicles introduce challenging research areas combining different disciplines. One challenge is the detection of obstacles with different sensors and the combination of information to generate a comprehensive representation of the environment, which can be used for path planning and decision making.</p><p>The sensor fusion is demonstrated using two Velodyne multi beam laser scanners, but it is possible to extend the proposed sensor fusion framework for different sensor types. Sensor fusion methods are highly dependent on an accurate pose estimate, which can not be guaranteed in any case. A fault tolerant sensor fusion based on the Dempster Shafer theory to take the uncertainty of the pose estimate into account is discussed and compared using an example, although not implemented on the test vehicle.</p><p>Based on the fused occupancy grid map, dynamic obstacles are tracked to give a velocity estimate without the need of any object or track association methods. Experiments are carried out on real world data as well as on simulated measurements, for which a ground truth reference is provided.</p><p>The occupancy grid mapping algorithm runs on central- and graphical-processing units, which allows to give a comparison between the two approaches and to stress out which approach is preferably used, depending on the application.</p>
----------------------------------------------------------------------
In diva2:1590753 there was a "ff" ligature and the paragraphs were merged, the abstract should be:

 <p>In general, electric propulsion offers very high eﬃciency but relatively low thrust. To remedy this, several ion engines can be assembled in a clustered configuration and operated in parallel. This requires the careful design of a frame to accommodate the individual propulsion systems. This frame must be modular to be used in different cluster sizes, and verify thermal and mechanical requirements to ensure the nominal operation of the thrusters.</p<<p>The present report aims to show the design process of such a frame, from preliminary modelling to the experimental study of a prototype. This document features an overview of the iterative design process driven by thermal simulations rendered on COMSOL Multiphysics. This process led to the conception of a 2-thruster and 4-thruster cluster frame. A lumped-parameter model of the electric propulsion system was also created to model its complex thermal behaviour. In addition, the 2-thruster frame was studied mechanically with analytical calculations and simulations of simple load cases on SolidWorks. Lastly, a prototype based on the 2-thruster frame model was assembled. The prototype was used to conduct temperature measurements while hosting two operating thrusters inside a vacuum chamber. The temperature distribution in the cluster was measured, and compared to simulation results.</p<<p>Thermal simulations of the 2-thruster and 4-thruster frame showed promising results, while mechanical simulations of the 2-thruster version met all requirements. Moreover, experimental results largely agreed with thermal simulations of the prototype. Finally, the lumped-element model proved instrumental in calibrating the models, with its high flexibility and quick computation time. </p>

In diva2:1046844:

 <p>Diabetes mellitus is a disease that leads to an unstable blood glucose concentration, with oscillations and peaks higher than the normal range. It is a worldwide problem that is currently affecting 387 million people. To avoid further complications associated to the disease, patients have to monitor their blood glucose level multiple times per day, making this compound the most commonly tested analyte. Self-monitoring blood glucose strips (SMBG strips) and continuous glucose monitoring systems (CGMS) are the most widespread glucose monitoring devices nowadays. CGMS, detecting glucose in interstitial fluid (ISF), even if more advantageous than traditional devices, are still relatively invasive and painful, due to their size and needle-based insertion mechanism.</p><p>For the correct functioning of amperometric glucose sensors, having a reference electrode (RE) which is stable in the physiological environment is a crucial requirement. In this work the development of stable and selective miniaturized electrodes to be integrated in biosensors for ISF glucose monitoring is presented. In particular, a method to produce stable miniaturized iridium/iridium oxide (Ir/IrOx) quasi-reference electrode (quasi-RE) was firstly developed. Cyclic voltammetry was used for this purpose, and the process allowed the realization of miniaturized electrodes, stable for several days under physiological conditions. By using this technology it is possible to solve some of the common problems arising from the miniaturization of traditional RE.</p><p>Secondly, miniaturized working electrodes (WE) for selective glucose detection in presence of interfering species have been developed. In fact, to detect the analyte, amperometric glucose sensors need the application of a potential, which also oxidizes interfering species and results in an overestimation of glucose reading. To solve the problem, anti-interference membranes based on different materials and deposition techniques have been developed. In particular, in this study three different membranes have been studied. The performed techniques allowed to realize reproducible films on top of miniaturized WE. As a future step, the realized electrodes should be integrated and tested as complete miniaturized glucose sensors, to prove the feasibility of further miniaturized CGMS.</p>


In diva2:931267 "ff" , "fl", and "ffi" ligature and missing italics and the note was missing (the URL is currently invalid), the abstract should be:

 <p>Digital technologies have increased the influence of technology in business, even changing business models and strategies of organisations. This influence, called <em>Digital Transformation of Business (DT)</em>, happens when there is an increase of the number of digital connections, information and interactions. This phenomena has been defined as <em>Digital Density (DD)</em> and aims to provide an assessment of the digitalization status of an organization. With the concept of <em>DD</em> we pro-pose the <em>DD Framework</em>, that has two parts: qualitative and quantitative. In order to support both parts we simulate the effect that a platform for smart-phones called Waze has on the traffic flow of a city. For the qualitative side, we redefine the concept of <em>DD</em> and with Business Model CANVAS we compare the changes introduced by Waze in the business model. The quantitative part of the simulation is performed with MATLAB and its goal is to understand how an agent chooses the path to drive from point <em>A</em> to point <em>B</em>. As expected, we prove that with more information agents can estimate better the costs of the different paths. Despite, even if our simulation seems to prove that the more information the agents have the better the system works, we introduce an analogy with the <em>Braess Paradox</em> to find potential problems and provide solutions for them. We conclude with a proposal of future work.</p><p><em>Note: This report, the presentation of this master thesis and the Matlab files used for the simulation can be found in next hyperlink:<br>
<a href="www.kth.se/profile/188107/page/master-thesis-digital-density-as-the-d/">www.kth.se/profile/188107/page/master-thesis-digital-density-as-the-d/</a></em></p>


In diva2:1040704 there were ligatures, the abstract should be

: <p>The objective of this report is to investigate the fringing flux around the air gap of a high frequency reactor and what correlation it has with losses, air gap length and frequency. A computer model is made using a finite element analysis software and a prototype reactor is built and tested on to verify the model. Variables such as air gap length, ripple frequency and current are changed in order to investigate different relationships. Results show that the computer model is sufficiently valid, and that trends regarding core losses versus frequency and air gap length correlates with theory. From test and simulation results conclusions are made for making designers aware of different measures for mitigating unwanted fringing flux effects.</p>


In diva2:1247549 there were ligatures and the delta character was missing, the abstract should be:

 <p>In order to accurate calculate currents during unsymmetrical faults in the electric power grid information about all components positive, negative and zero sequence impedances are important. Transformers are one key component in the power grid and accurate information about sequence impedances for power transformers are therefore essential. However, to analytically derive the zero sequence impedance is extremely hard. Therefore are the transformers tested before they are connected.‌</p><p>The Swedish TSO (Transmission System Operator), Svenska kraftnät, Svk, are responsible for operating and balancing the Swedish transmission grid. For Svk it is crucial that the grid is operated in a reliable and safe way so that the society always have available electricity at the same time as no persons working on the grid are exposed to unnecessary risks. Having accurate current calculations during unsymmetrical faults is one crucial part. Unfortunate, for some transformers in Svk area of responsibility the information about zero sequence impedance is missing and must therefore be estimated.</p><p>This work propose, by applying a case study on a existing group of transformers (three limb design with no &Delta; windings), a method for estimating the zero sequence impedance. The aim of the the analysis is to investigate if there are any relationship between the zero sequence impedance and some more basic operating data for the transformers. From an initial literature study it was concluded that the following parameters would affect the zero sequence impedance: rated power and voltage, no-load power and currents and information about winding configuration, manufacture and production year.</p><p>In the case study linear relationships was found based on a least square method and was tuned using a robust linear square method called Bisquare. The results showed that several of the tested quantities showed a growing trend when they were correlated with the zero sequence impedance. The no-load impedance was identified as one interesting quantity that was investigated further.</p><p>The tested transformers were divided into different groups based on number of windings and winding configuration. In the next step the transformers were ordered based on rated power. New linear relationships were derived for every group and the final results were compared between the different transformer groups.</p><p>The reliability of the proposed algorithm is dependent on the size of the tested transformer group and also on the data quality. The data quality was investigated by trying to identify transformer with abnormal values. The results showed that abnormal behaviour was very rare in the tested group of transformers.</p><p>The results from the case study indicates that a relationship between the no-load impedance and the zero sequence impedance exists for the common types of transformers, the YNyn0, YNauto0 and YNyn0yn0. The results shows that these relationships differ depending on the rated power of the high voltage winding. The case study also concludes that it is possible to estimate a winding’s zero sequence impedance if the zero sequence impedance for a second winding is known as well as the rated voltage of the two windings. The presented relationships have all some uncertainty related to them, for some relationships the uncertainty is relatively high whereas for others it is very low. The relationship with the lowest uncertainty found was the function describing a winding’s zero sequence impedance as a function of another windings zero sequence impedance and the rated voltages</p>

In diva2:1095893:

<p>Today the Internet of Things (IoT) lacks universal standards for communication and interaction between devices. There is a wide collection of diverse software architectures for IoT applications on the market, where smart devices from different manufacturers are often unable to interact with each other.</p><p>A standards organization within IoT gaining recognition is the Open Connectivity Foundation (OCF), an industry group delivering an IoT software framework specification and a product certification program. Open Connectivity Foundation (OCF) is funding an open source reference implementation of the specification called <em>IoTivity</em>, which runs as middleware intended to be portable to all operating systems and connectivity platforms. The goal of the OCF is to enable interoperability between IoT devices regardless of manufacturer, operating system, chipset or physical transport.</p><p>Through a literature review, the key functional and non-functional requirements for IoT middleware architectures were found. Functionality requirements identified were data management, resource management, resource discovery, and context-awareness. The quality attributes were found to be interoperability, adaptability, scalability, security, and real-time behavior.In this thesis project, IoTivity was evaluated with respect to these requirements with the scenario-based Method for Evaluating Middleware Architectures (MEMS). As a part of MEMS, a case study of implementing a building management system (BMS) with IoTivity was conducted.</p><p>Results showed that, within the framework of the case study, IoTivity complied with three out of four functional requirements, and three out of five quality requirements identified for IoT middleware architectures. One of the quality requirements, security, was not evaluated in this study.</p>


In diva2:855986 ligrature(s), should be

: <p>This thesis deals with the task of identifying implicit citations between scientific publications. Apart from being useful knowledge on their own, the citations may be used as input to other problems such as determining an author’s sentiment towards a reference, or summarizing a paper based on what others have written about it. We extend two recently proposed methods, a Machine Learning classifier and an iterative Belief Propagation algorithm. Both are implemented and evaluated on a common pre-annotated dataset. Several changes to the algorithms are then presented, incorporating new sentence features, different semantic text similarity measures as well as combining the methods into a single classifier. Our main finding is that the introduction of new sentence features yield significantly improved F-scores for both approaches.</p>

In diva2:576409 ligature(s) and some werhed words, should be:

 <p>Similar to many technological developments, wireless sensor networks have emerged from military needs and found its way into civil applications. Today, wireless sensor networks has become a key technology for different types of ”smart environments”, and an intense research effort is currently underway to enable the application of wireless sensor networks for a wide range of industrial problems. Wireless networks are of particular importance when a large number of sensor nodes have to be deployed, and/or in hazardous situations.</p><p>Localization is important when there is an uncertainty of the exact location of some fixed or mobile devices. One example has been in the supervision of humidity and temperature in forests and/or fields, where thousands of sensors are deployed by a plane, giving the operator little or no possibility to influence the precise location of each node. An effective localization algorithm can then use all the available information from the wireless sensor nodes to infer the position of the individual devices. Another application is the positioning of a mobile robot based on received signal strength from a set of radio beacons placed at known locations on the factory floor.</p><p>This thesis work is carried out on the wireless automation testbed at the S3. Focusing on localization processes, we will first give an overview of the state of the art in this area. From the various techniques, one idea was found to have significant bearing for the development of a new algorithm. We present analysis and simulations of the algorithms, demonstrating improved accuracy compared to other schemes although the accuracy is probably not good enough for some high-end applications. A third aspect of the work concerns the feasibility of approaches based on received signal strength indication (RSSI). Multiple measurement series have been collected in the lab with the MoteIV wireless sensor node platform. The measurement campaign indicates significant fluctuations in the RSSI values due to interference and limited repeatability of experiments, which may limit the reliability of many localization schemes, especially in an indoor environment.</p>


In diva2:576407 ligatures and merged words, should be:

 <p>Look ahead cruise control is a relatively new concept. It deals with the possibility of making use of recorded road slope data in combination with GPS, in order to improve vehicle cruise control. This thesis explores the possibility of estimating road slope as well as investigating the sensitivity of two look ahead controllers, with respect to errors in estimation of mass, wheel radius and road slope.</p><p>A filter using GPS and standard vehicle sensors is used for estimation of road slope. The filter is robust to losses in data since redundant information is available. Possible errors in estimation caused by the filter are identified.</p><p>Two previously published look ahead controllers using different strategies to control a heavy vehicle are investigated. A description of controller behaviour in perfect conditions is presented. Sensitivity analysis is performed identifying erroneous control behaviour inflicted by errors in the used vehicle model and road slope. Further, effects on controllers caused by errors in road slope estimation estimation are detected. Conclusions about the two look ahead strategies are drawn.</p>

In diva2:812042 ff ligrature , should be:

 <p>In this paper we compare various Sudoku solving algorithms in order to determine what kind of run-time improvements different optimizations can give. We will also examine what kind of effect the existence of multiple solutions in the puzzles has on our result.</p>

In diva2:1400696, ff ligature, should be:

 <p>Neurala nätverk är kända för att memorera delar av sina träningsset. När känslig information är inblandad kan därför offentliggörandet av ett tränat nätverk innebära att sekretessen bryts. I detta examensarbete använder vi differential privacy för att träna neurala nätverk som bevisligen skyddar deltagarnas identitet. I synnerhet tar vi upp de problem som uppstår inom området bildsegmentering. Här har tidigare metoder behövt lägga till orimligt höga brusnivåer för att skydda sekretessen på grund av den höga dimensionaliteten. Vi använder dimensionalitetsreduktion för att sänka den nödvändiga brusnivån, vilket resulterar i en bättre avvägning mellan sekretessbevarande och användbarhet. Vi bevisar integritetsgarantin formellt och utvärderar den prediktiva prestandan empiriskt på ett syntetiskt dataset.</p>


In diva2:1049369, ff ligrature, should be:

 <p>Wave power is currently a hot topic of research, and has shown great potential as a renewable energy source. There have been lot of progress made in developing cost effective Wave Energy Converters (WECs) that can compete with other sources of energy in regard to price and electrical power. Theoretical studies has shown that optimal control can increase the generated power for idealized WECs. This thesis is done in collaboration with CorPower Ocean, and investigates the use of economic Model Predictive Control (MPC) to control the generator torque in a light, point-absorbing, heaving WEC that is currently under development. The objective is to optimize the generator torque, such that the average generated power is maximized while maintaining a small ratio between maximum and average generated power. This results in a nonconvex cost function. Due to the highly nonlinear and nonsmooth dynamics of the WEC, two controllers are proposed. The first controller consists of a system of linear MPCs, and the second controller is a nonlinear MPC. Relevant forces acting on the WEC are identified and the system dynamics are modelled from a force perspective. The models are discretized and the controllers are implemented in Simulink. The WEC, together with the controllers, is simulated in an extensive Simulink model developed by CorPower Ocean. Several different types of ocean waves are considered, such as its energy content and its regularity. In the majority of cases, the controllers do not increase the performance of the WEC compared to a simple, well tuned controller previously developed by CorPower Ocean. Finally, possible improvements of how to reduce existing model errors are proposed.</p>

In diva2:576430, ff  and flligature, should be:

 <p>Chemical reactors are a part of modern industry and the catalytic tubular fixed bed reactor examined in this work is an important reactor for chemicals production.</p><p>In this work two different types of models for the reactor are studied; a pseudohomogeneous model and a heterogeneous model. The goal is to find differences in behaviour between these two types of reactor models and explain these.</p><p>In a real reactor there exists two phases, a solid catalyst and a fluid reactant. Both these phases are in the pseudohomogeneous model treated as a single phase, a pseudofluid. In the heterogeneous model the two phases are treated separately.</p><p>When comparing these types of models a few structural differences exist, void fraction, heat exchange between two phases, and heat dispersion in the phases, and all of these will affect the behaviour of the models differently.</p><p>The models are studied using bifurcation analysis and linear analysis. Bifurcation theory is used to find and track different solutions depending on a certain parameter and to get a good overall picture of a system’s solutions and their type, steady state or sustained oscillation.</p><p>Linear analysis is used to study linearization around a specific solution and to determine stability and frequency dependency.</p><p>It is found that the concept of void fraction in the reactor model affects the behaviour only as a time scaling, while the concept of interfacial heat exchange affects the stability. The distribution of heat dispersion between phases has a significant impact on the reaction behaviour. Feedback is determined as the main cause for instabilities and oscillative solutions.</p>

In diva2:65431 ff ligature1

: <p>This report evaluates possible performance differences between Java and native C on the operating system Android, by developing tests and analyzing the execution. The ambition is that each test should evaluate the performance of a certain task, such as memory access or arithmetic operations of different data types. The results were in some cases unexpected and show that the executed implementations were faster on C compared to Java on one of the test devices, but not the other. The conclusion partly opposes earlier research and this is probably partly due to the fact that the Java Virtual Machine has been improved vastly in the latest versions of Android.</p>

The full text is at:

https://www.csc.kth.se/utbildning/kandidatexjobb/datateknik/2011/rapport/ulvesand_andreas_OCH_eriksson_daniel_K11009.pdf

In diva2:918152 ff and ffi ligature, should be:

 <p>This Thesis details the research on Machine Learning techniques that are central in performing Anomaly and Masquerade attack detection. The main focus is put on Web Applications because of their immense popularity and ubiquity. This popularity has led to an increase in attacks, making them the most targeted entry point to violate a system. Specifically, a group of attacks that range from identity theft using social engineering to cross site scripting attacks, aim at exploiting and masquerading users. Masquerading attacks are even harder to detect due to their resemblance with normal sessions, thus posing an additional burden.</p><p>Concerning prevention, the diversity and complexity of those systems makes it harder to define reliable protection mechanisms. Additionally, new and emerging attack patterns make manually configured and Signature based systems less effective with the need to continuously update them with new rules and signatures. This leads to a situation where they eventually become obsolete if left unmanaged. Finally the huge amount of traffic makes manual inspection of attacks and False alarms an impossible task. To tackle those issues, Anomaly Detection systems are proposed using powerful and proven Machine Learning algorithms.</p><p>Gravitating around the context of Anomaly Detection and Machine Learning, this Thesis initially defines several basic definitions such as user behavior, normality and normal and anomalous behavior. Those definitions aim at setting the context in which the proposed method is targeted and at defining the theoretical premises. To ease the transition into the implementation phase, the underlying methodology is also explained in detail.</p><p>Naturally, the implementation is also presented, where, starting from server logs, a method is described on how to pre-process the data into a form suitable for classification. This preprocessing phase was constructed from several statistical analyses and normalization methods (Univariate Selection, ANOVA) to clear and transform the given logs and perform feature selection. Furthermore, given that the proposed detection method is based on the source and1request URLs, a method of aggregation is proposed to limit the user privacy and classifier over-fitting issues. Subsequently, two popular classification algorithms (Multinomial Naive Bayes and Support Vector Machines) have been tested and compared to define which one performs better in our given situations.</p><p>Each of the implementation steps (pre-processing and classification) requires a number of different parameters to be set and thus a method called Hyper-parameter optimization is defined. This method searches for the parameters that improve the classification results. Moreover, the training and testing methodology is also outlined alongside the experimental setup. The Hyper-parameter optimization and the training phases are the most computationally intensive steps, especially given a large number of samples/users. To overcome this obstacle, a scaling methodology is also defined and evaluated to demonstrate its ability to handle larger data sets.</p><p>To complete this framework, several other options have been also evaluated and compared to each other to challenge the method and implementation decisions. An example of this, is the "Transitions-vs-Pages" dilemma, the block restriction effect, the DR usefulness and the classification parameters optimization. Moreover, a Survivability Analysis is performed to demonstrate how the produced alarms could be correlated affecting the resulting detection rates and interval times.</p><p>The implementation of the proposed detection method and outlined experimental setup lead to interesting results. Even so, the data-set that has been used to produce this evaluation is also provided online to promote further investigation and research on this field.</p>

In diva2:576413 ff ligature, should be:

 <p>The fully automated milking system VMS has different functions which complements the actual milking of cows. This master thesis presents a method to improve the calculation of milk yield in dairy cows for the VMS. This report also investigates if it is possible to improve the algorithm for finding cows with mastitis (udder inflammation). The correctness of the prediction of milk yield is important for a couple of actions in the VMS. For example, valuable time can be saved if teatcups are attached first to high yielding teats. Only cows with an attained minimum level of predicted yield should be allowed to enter the VMS and get milked. Milking has traditionally been an event to monitor the condition of the cows. Therefore methods that determine the condition are demanded for any automatic milking systems. Mastitis is a costly illness and a working test for ill cows should be implemented in the VMS in order to know which cows that are ill.</p><p>The goal of this thesis work is to develop two new algorithms for the VMS. First, an improved algorithm for the prediction of secretion rate is presented. The improved algorithm uses a Kalman-filter to update the secretion-rate. The improved method has a lower total prediction in most cases. The Kalman-filter was tested and developed for five farms and was verified on one farm.</p><p>Second, this report investigates if a cusum test can be used to detect ill cows. The method turns out to be slightly better than the current algorithm. A test for cows which are milked on three or two teats is evaluated. In this test the number of milkings with high conductivity and low secretion rate are weighted together. This algorithm is slightly better than the current algorithm used for detection of ill cows.</p>


In diva2:576428, ff ligrature and merged words, should be:

 <p>As part of design of a personal robot for operation in an everyday environment there is need to endow the system with facilities to track a person as it is taken on a tour of the environment.</p><p>In the literature a number of different methods based on laser tracking and computer vision have been presented. However, most of these methods are not particularly robust and in many cases the methods do not operate in realtime.</p><p>In this master thesis a system for people detecting and tracking has been implemented using laser and vision. The information given by both scanners are used for two different purposes, laser range data are used to detect persons and the images grabbed by the camera are used to confirm the hypotheses made by the laser.</p><p>Methods for people tracking based on laser and vision have two main problems. The ones based on laser are not very robust and the ones based on vision hardly ever operate in real time.</p><p>This project is aimed at taking the main advantages of both methods:</p><p><strong>Laser</strong></p><p><em>Advantage: </em>High Speed =&gt; Works in real time</p><p><em>Disadvantage:</em> Not robust =&gt; Not reliable 100%</p><p></p><p><strong>Vision</strong><strong>:</strong></p><p><em>Advantage: </em>Robust =&gt; More reliable</p><p><em>Disadvantage: </em>Low speed =&gt; Does not work in real time</p>

In diva2:654196, ff ligature and hyphenation being kept, should be:

 <p>In this master thesis report a new method for simulating waters surface waves is presented. The method is well adapted for real-time applications and has been developed with computer games in mind. By simulating the water surface at several different resolutions simultaneously using a construction similar to Laplacian Pyramids dispersion is handled approximately resulting in a complex behaviour.</p><p>The simulation is also extended with a dynamic level of detail method and phenomenological models for boundaries and high frequency waves. This method is pro-totyped inside the Frostbite™ engine developed at EA™ DICE™ and runs at 3 ms per time step on a single core of a Intel™ Xeon™ processor with high quality results.</p>

The NADA link is no longer valid: https://www.nada.kth.se/utbildning/grukth/exjobb/rapportlistor/2011/rapporter11/ottosson_bjorn_11105.pdf

In diva2:1164152 ff ligature,should be:

 <p>The Miniature Student Satellite (MIST) is a project at the Royal Institute of Technology in Stockholm, Sweden. The goal of the project is to launch a satellite designed and constructed by different student teams. The satellite carries seven scientific experiments that continuously collects experiment data as the satellite orbits the Earth. When the satellite is launched, the communication link between the satellite and the ground station is an essential part. This communication consists of a radio link between the on-board computer through a radio module and the ground station on Earth. Satellite communication is not a new field and there exist predefined communication standards and protocols. These standards and protocols are quite extensive and need to be tailored for the specific mission.</p><p>Once a satellite is launched, it is out of reach for further development. This makes it crucial that the software running on the on-board computer are well tested and correctly integrated with the mission control system (MCS) that are used the send commands to control the satellite from Earth.</p><p>Since satellite radio equipment is expensive, this bachelor thesis describes how to set up, implement and test an end-to-end communication chain between the satellite on-board computer and the MCS using a hardware simulator. The simulator both mimics the functionality of an on-board radio and replaces the ground station and radio link. Communication standards and protocols are studied and investigated, alongside with on-board pre-implemented subsystem libraries and an MCS named Elveit from Solenix. As the simulator also replaces the radio link, data transfer errors such as data loss, data corruption and, connection time windows can be simulated and tested.</p><p>The simulator development results in a feasible end-to-end communication chain between the on-board computer and the MCS. This includes mimicking and acting as a radio module against the on-board computer, simulation of the radio link with the possibility to add transmission errors and, acting as a ground station against the MCS. To ensure that the simulator performs as the on-board radio module, the simulator performance is tested against the on-board computer. These results can be compared with on-board radio module performance to make sure that the behavior is similar.</p>


In diva2:1046447 ff ligature, hyphen kept,  and merged paragraphs, should be:

 <p>Anomaly detectors in control systems are used to detect system faults and they are typically based on an analytical system model, which generates residual signals to find a fault. The detectors are designed to detect randomly occurring faults but not coordinated malicious attacks on the system.</p><p>Therefore three different anomaly detectors, namely a detector solely based on the last residual, a multivariate exponentially weighted moving average filter and a cumulative sum, are investigated to determine which detector yields the smallest worst-case impact of a time-limited data injection attack.</p><p>For this reason optimal control problems are formulated to characterize the worst-case attack under different anomaly detectors, which lead to non-convex optimization problems. Relaxations to convex problems are proposed and solved numerically and in special cases also analytically.</p><p>The detectors are compared by solving the optimal control problems for a simple simulation example as well as a quadruple-tank process. Simulations and experiments show that the cumulative sum seems to be the detector to choose, if one wants to limit the worst-case attack impact.</p>

In diva2:1421282, ff ligature, merged words,  and unnecessary spaces, should be:

 <p>Attitude Determination and Control System (ADCS) is often a complex system on-board any satellite which needs validation and testing to prove its operability and verify its software compatibility with hardware and other subsystems. One failure in orbit is extremely expensive in terms of cost and time due to payload preparation and launch. The ideal test bench would be the one that perfectly simulates the space environment and all its main factors such as weightlessness, Earth’s Magnetic Field (EMF), vacuum, neutral particles, plasma and radiation, among others. The target in this case was the Earth’s Magnetic Field (EMF), solved with a Helmholtz Cage in a Merritt Configuration, and weightlessness, not implemented but analysed in detail where different alternatives are proposed, similar to market solutions.As derived from literature and simulations executed along this M. Sc. Thesis, the Merritt Cage seems beneficial against any other configuration in terms of magnetic field uniformity and effective volume. After the design and assembly of the test bench, both properties were verified and successfully achieved, despite the lack of calibration, not executed because of time limitation, and tiny issues encountered along the full evolution of the project.</p>

In diva2:721641, ff ligature and missing superscripts:

 <p>Two implementations of the backtracking algorithm for solving Sudoku puzzles as well as their dependence on the representations of the problem have been studied in order to ascertain pros and cons of different approaches. For each backtracking step, empty cells could be assigned numbers sequentially or, by using a greedy heuristic, by the probability that guessed numbers were more likely to be correct. Representations of the Sudoku puzzles varied from a n<sup>2</sup> matrix to a n<sup>3</sup> matrix, as well as a combination of both. This study shows that (1) a sequential approach has better best case times but poor worst case behaviour, and a n<sup>3</sup> representation does not benefit over a n<sup>2</sup> representation; (2) a greedy heuristic approach has superior worst case times but worse best case, and n<sup>3</sup> representations sees great benefits over n<sup>2</sup> representations. A combination of n<sup>2</sup> and n<sup>3</sup> representations grants the best overall performance with both approaches.</p>

In diva2:1038731 ff ligature and hyphens left, should be:

 <p>With the coming growth in Internet of Things (IoT) applications, we can expect environments with many independent networks operating in nearby locations. Wireless Sensor Networks (WSN), which have become popular during the last few years, are the main type of networks used in IoT. The IEEE 802.15.4 protocol designed for low-rate wireless personal area networks has been widely adopted for this kind of network. Together with ZigBee, this protocol is gaining increasing interest from the industry, as they are considered a universal solution for low-cost, low-power, wireless connected monitoring and control devices. Internetwork interference issues in IEEE 802.15.4 networks can be a major problem because of the extensive use of wireless channels. In this thesis, an in-depth simulation study of the internetwork interferences is performed using Castalia, a widely used network simulator. We focus on the beacon collision problem, as it has been proved to be the main cause of performance degradation for coexisting networks. We carry out a prestudy of the main node simulation parameters to setup the different scenarios. Then, we evaluate how the overlap of the active periods and the location of the nodes affect the network performance. We continue with a network coexistence analysis to study the inter-action of two networks of two nodes and their performance regarding the beacon reception rate. We show that there are significantly different operation regions, depending on the network location. Following this, a probabilistic analysis is carried out in order to obtain an average beacon reception rate depending on the size of the area considered. Finally, we discuss available beacon collisions avoidance methods, taking into account the detailed simulation results. Our conclusions have theoretical and practical implications for the design of wireless sensor networks, and for the evaluation of beacon collisions avoidance schemes.</p>
----------------------------------------------------------------------
In diva2:1421238, unnecessary dashes from hyphenation and a missing space, should be:

 <p>In this thesis, we study the active estimation of the attention level of an approaching human driver from the perspective of an autonomous vehicle.   The  focus  is  on  how to  plan  information  gathering  actions for probing the human driver. The responses to these actions are then used for the actual model-based estimation of the attention level.In the first part of this thesis, a non-cooperative game framework is proposed for modelling the interaction between a human driver and an autonomous vehicle.  The best response algorithm is employed to find the equilibrium strategy of the game.  The equilibrium strategy is then used by the autonomous vehicle to actively learn the attention level of the human driver.In the second part of this thesis, the active learning of driver’s attention level is studied in a cooperative adaptive cruise control (CACC) scenario.  A novel CACC protocol is proposed which allows the platoon of vehicles to simultaneously achieve the platooning and learning objectives.In both cases for validating the proposed approach some simulations are performed using a simulated human driver which behaves as an attentive or distracted driver.  Then some real-time case studies are  performed  with  a  real  human  driving  the  car  in  the simulation. Both simulations and real-time case studies endorse the effectiveness of the proposed framework for the active learning of human driver’s attention level.</p>


In diva2:1821985 there were merged words, the abstract should be:

<p>The goal of this project is to examine how a higher penetration of power electronics-based forms of power production affects system stability and damping. The project covers how power system stability can be improved by using a Thyristor Controlled Series Capacitor (TCSC) with Power Oscillation Damping (POD). In order to reach these aims, a small Kundur's two-area power system is analysed. A fault is implemented in a system with four synchronous generators, this is done using the simulation software SIMPOW. By replacing portions of the synchronously generated power with power electronics based generation, the impacts of renewable energy sources on power system stability can be evaluated. The results of the project show that introducing power electronics based generation to a system generally worsens the stability of said system. Furthermore, it is discovered that the oscillations in the system response can be efficiently damped by using a TCSC. On the other hand, when a certain amount of renewable energy is integrated into the system the method used in this project is no longer sufficient.</p>

In diva2:1499079 there are merged words, merged paragraphs, and missing italics/emphesis, the abstract should be:

 <p>The dielectric antenna is an interesting alternative to a metallic antenna. This is mainly due to its low manufacturing cost and the possibility to fabricate complex antenna geometry with the aid of additive manufacturing (AM). Sophisticated AM technology provides new degrees of freedom in shaping the outer and inner geometry of antennas. This feature can be utilized to optimize various properties of antenna, such as its bandwidth, radiation pattern etc[.], while maintaining a compact geometry.</p><p>This master thesis investigates the possibility of improving the bandwidth of a compact dielectric antenna by modifying its geometry. Specifically, dielectric resonator antennas (DRAs) have been considered here. In this connection, two embedded cylindrical DRAs operating within 8 GHz-17 GHz frequency band have been designed and simulated using <em>Ansys HFSS</em>. For the first design (<em>Design-1</em>), a bandwidth (corresponding to reflection coefficient ≤ -10dB) of approximately 63% has been obtained and the second design (<em>Design-2</em>) has a bandwidth (corresponding to reflection coefficient ≤ -10dB) of about 57%. However, in terms of radiation characteristics, the performance of <em>Design-2</em> has been found to be superior compared to <em>Design-1</em>, mainly due to its symmetrical geometry. Furthermore, the two designs have been compared to an existing compact rectangular embedded DRA. It has been found that both <em>Design-1</em> and <em>Design-2</em> have comparatively wider bandwidth. With respect to the radiation characteristics, the performance of the reference antenna and <em>Design-2</em> are similar. While, the radiation performance of the reference antenna is found to be better than<em> Design-1</em>.</p>

In diva2:1635738 merged words and merged paragraphs, abstract should be ('enterpriseLang' is a special purpose language):

 <p>As the number of digital systems grows yearly, there is a need for good cyber security. Lack of such security can be attributed to the demand on resources or even knowledge. To fill this gap, tools such as enterpriseLang can be used by the enduser to find flaws within his system, which he can revise. This allows a user with inadequate knowledge of cyber security to create safer IT architecture. The authors of this paper took part in the development of enterpriseLang and its improvement. This was done by suggesting improvements based on certain design guidelines, as well as attempting to achieve 100% attack coverage and improving the defense coverage.</p><p>The results show a coverage increase of 0.6% for a specific model’s attack steps. Further more, we find that nearly 84.6%of the compiled guidelines are met, followed by 7.7% that were not fully met and a similar amount that were non-applicable to enterpriseLang. As the language is still in development, there remains much work that can improve it. A few suggestions would be to increase the attack coverage by 100%, increasing the defense coverage and improving enterpriseLang to fulfill thed esign guidelines, which would ultimately ease future projects within this domain.</p>

In diva2:497435 there was some incorrect formatting and a right quote mark (rather than a double quote mark), the abstract should be:

 <p>Today the extent and value of electronic data is constantly growing. Dealing across the internet depends on how secure consumers believe their personal data are. And therefore, information security becomes essential to any business with any form of web strategy, from simple business-to-consumer, or business-to-business to the use of extranets, e-mail and instants messaging. It matters too any organization that depends on computers for its daily existence.</p>
<p>This master thesis has its focus on Information Security Governance. The goal of this thesis was to study different Information Security processes within the five objectives for Information Security Governance in order to identify which processes that organizations should prioritize in order to reduce negative consequences on the data, information and software of a business from security incidents. By surveying IT experts, it was possible to gather their relative opinion regarding the relationship between Information Security Governance processes and security incidents.</p>
<p>By studying the five desired objectives for Information Security Governance,
<em>Strategic Alignment</em>, <em>Risk Management</em>, <em>Resource Management</em>, <em>Performance Measurement</em>
and
<em>Value Delivery</em>
the result indicated that some processes within <em>Performance Measurement<em>s have a difference in relation to other processes. For those processes a conclusion can be made that they are not as important as the processes which they were compared to. A reason for this can be that the processes within performance measurement are different in such a way that they measure an incident after it has actually happened.
While other processes within the objectives for ISG are processes which needs to be fulfilled in order to prevent that an incident happens. This could obviously explain why the expert&#8217;s choose to value the processes within performance measurement less important compared to other processes.</p>
<p>However, this conclusion cannot be generalized, since the total amount of completed responses where less than expected. More respondents would have made the result more reliable. The majority of the respondents were academicals and their opinion and experience may be different from the IT experts within the industry, which have a better understanding of how it actually works in reality within an organization.</p> 
----------------------------------------------------------------------
In diva2:1635763, there were merged words, the abstract should be:

 <p>For the future implementation of high speed communication, safety remains one of the main concerns. To ensure the safety of new applications, specifically the new 5G antennas, it is crucial to know that they will not cause any harm to the human body. There are a few ways to test how safe a system using high frequency radiation is but the industry standard is by using the Specific Absorption Rate (SAR). The SAR is directly correlated to the initial temperature rise in the volume exposed to radiation which is what the method used in this report is based on. The temperature rise in a skin-like phantom due to 5 GHz exposure was recorded using an IR-camera, which in turn was used to calculate the SAR. The purpose of this report was to test if this method is a valid way of obtaining the peak surface SAR. It was concluded that the method is valid but there are some uncertainties in regards to abstracting the method to far-field exposure for our considered frequency. The SAR value that is achieved in this report is 333.4 W/Kg which is high in relation to the SAR-limits in IEEE guidelines, although the set up is not supposed to reflect a realistic use of the antenna. This is due to the fact that the waveguide in the setup is close to measurement sample, and has a higher intensity than is to be expected from real world applications. The method may be applicable for far-field exposure with a higher frequency as that would concentrate the measurable heat to the surface of the measurement sample and would also carry more energy by default.</p>


In diva2:1822765 there were merged word, the abstract should be:

<p>Experimentally, it has been difficult to identify the exact physical processes at work in the magnetosphere that drive the strong currents that later give rise to auroras. The aim of this report has been to address this question by analyzing data gathered from Cluster-, MMS-, and DMSP satellite missions. The focus has specifically been on finding time intervals between the years 2018 and 2023 where Cluster and MMS had magnetic field-line conjunctions that occurred within the plasma sheet. Multiple conjunctions were found, and the most promising event, occurring on 2018-06-24, was further investigated. The field-aligned current was calculated using the curlometer method. The current parallel to the magnetic field was related to the shear velocity of the magnetic field and the ion bulk velocity. The experimental findings of this report confirmed the theoretical framework regarding the generation of field-aligned currents. In addition, during the investigated time interval, auroral kilometric radiation was observed while DMSP registered auroral acitivity. In conclusion, it has been shown that there is an experimental connection between field-aligned currents and auroral formation.</p>

In diva2:675977 there were unnecessary paragraph divisions and a missing "-" in 'field-of-view', the abstract should be:

 <p>This essay details a 3D simulation of a number of control methods used for maneuvering of teleoperated USAR robots. The implementation was produced in the Unity3D engine. The simulation implemented different variations on field-of-view angle, turning algorithms, and camera view perspectives. An evaluation using volunteer test operators was conducted and discussed. The sample size was too small to draw any definitive conclusions. Further testing is advised.</p>

Note also that there is a space missing in the title, it should be: "Efficiency Evaluation of Simulated USAR Control Methods"

Also note that the Swedish abstract also has unnecessary paragraph divisions and should be:

<p>Denna uppsats behandlar en 3D-simulering samt användartester av flera olika kontrollmetoder som används vid fjärrstyrning av obemannade räddningsrobotar. Implementationen skapades med Unity3D-plattformen. De styrmetoder som testades var olika stora synfältsvinklar på kameran, olika algoritmer för att styra robotens svängning, samt olika kameraperspektiv. Användartester med frivilliga testförare genomfördes och diskuteras. Provstorleken var för liten för att kunna dra några definitiva slutsatser. Ytterligare tester rekommenderas.</p>

In diva2:547622 unnecessary paragraph divisions and missing subscripts, abstract should be:

 <p>Heating, Ventilation and Air Conditioning (HVAC) systems consist of all the equipment that control the conditions and distribution of indoor air. Indoor air must be confortable and healthy for the occupants to maximize their productivity. Moreover, HVAC energy consumption is between 20% and 40% of the total energy consumption in developed countries and accounts around 33% of the global CO<sub>2</sub> emissions. So the study of HVAC systems plays an important role in building science.</p><p>The aim of this project is to identify mathematical models that will be employed by intelligent control algorithms which guarantee human comfort indoors, energy saving and less CO<sub>2</sub> emissions at the same time. Three models, based on first-principle physical knowledge, are proposed for CO<sub>2</sub> concentration, temperature and humidity, respectively, for a room in the Q-building at KTH. Thermodynamic equations and an original estimation of the number of the occupiers of the room are employed.</p><p>Validation shows that models have really good performances, even with a short training dataset. Discussions on the obtained results are given and some ideas for future work are proposed.</p>

Note the word 'confortable' is misspell in the original abstract, so I have kept it in the above.

In diva2:616632, there are spaces missing in the title, it should be: "Computations on the French transmission grid in order to improve the voltage security assessment"'

The abstract had merged words and should be:

diva2:616632: <p>As the electric consumption increases and the investments are hard to make, electricity networks are operated closer to their limits. In such conditions, a generator or a transmission line outage can have tremendous impact, leaving a great number of people without electricity. It is therefore a matter of prime importance to ensure power system security and in particular voltage stability. Static criteria used in on-line simulations as well as protection and defense devices such as load-shedding devices play a critical role for voltage stability and are thus crucial for the network security. The core of this project is to determine efficient tools to detect undesirable conditions in operational context and to determine a pertinent activation level for an automatic load-shedding device used for the system protection against voltage instability.</p><p>In the first part of this report, theoretical background regarding voltage stability is presented, followed by the software and methodologies used during the Master’s thesis work.</p><p>The second part of this report focuses on case studies conducted for the French power system. From an initial objective of updating static criteria, the results have actually led to the withdrawal of these criteria and a switch to dynamic simulations for the North-East and East areas, as well as to the improvement of Astre software database. Simulations on the most stressed conditions from last winter allowed the updating of the activation level for the automatic load-shedding device. These changes have been validated and will be applied for voltage security assessment of the French network in the future.</p>

In diva2:1168320: there were merged words and paragraphs, the abstract should be 

<p>With the continuous progress on autonomous vehicle and remote driving techniques, connection quality demands are changing compared with conventional quality of service. Vehicle to everything communication, as the connectivity basis for these applications, has been built up on Long Term Evolution basis, but due to various ethical and environmental issues, few implementations have been made in reality. Therefore simulation approaches are believed to provide valuable insights.</p><p>To fully model an LTE vehicular network, in this work we first provide a comparison study to select the preferable LTE simulator. Aiming to integrate communication nodes with mobility, a solution for simulation framework is developed based on a state-of-art comparison study on the existing simulator frameworks. We then further develop the network simulator, and complement it with hybrid wireless channel modeling, channel and quality of service aware scheduler, and admission control strategies. In terms of instant optimization of the network, real-time access is emulated for external devices to communicate with the simulator. In this thesis, the evaluation of the framework performance considers two aspects: the performance of the simulator in LTE V2X use case and the feasibility of the service, specifically, remote driving, under realistic network capacity. For our framework, the results indicate that it is feasible to realize remote driving in an LTE urban scenario, but, as an example, we show that for an area of Kista, five vehicles could be hold by a base-station with guaranteed service at most.</p>


In diva2:1479353 there were merged words and paragraphs, the abstract should be:

 <p>As the size and complexity of the internet increased dramatically in recent years,the burden of network service management also became heavier. The need for an intelligent way for data analysis and forecasting becomes urgent. The wide implementation of machine learning and data analysis methods provides a new way to analyze large amounts of data.</p><p>In this project, I study and evaluate data forecasting methods using machine learning techniques and time series analysis methods on data collected from the KTH testbed. Comparing different methods with respect to accuracy and computing overhead I propose the best method for data forecasting for different scenarios.</p><p>The results show that machine learning techniques using regression can achieve better performance with higher accuracy and smaller computing overhead. Time series data analysis methods have relatively lower accuracy, and the computing overhead is much higher than machine learning techniques on the datasets evaluated in this project.</p>


In diva2:1331911 there were merged words and paragraphs, the abstract should be:

 <p>Anticipating the future positions of the surrounding vehicles is a crucial task for an autonomous vehicle in order to drive safely. To foresee complex manoeuvres for longer time horizons, a framework that relies on high-level properties of motion and is able to incorporate, e.g. contextual features, is needed. In this thesis, the problem of predicting the trajectories of the surrounding vehicles on a highway is tackled by using machine learning. The objective is to evaluate the performance of recurrent neural networks for trajectory prediction, specifically long-short term memory neural networks. Moreover, the goal is to investigate if contextual features can improve the predictions.</p><p>The problem of predicting future trajectories is solved by using two different approaches, which are compared by using the same framework. The first approach is based on the vehicle states of the surrounding vehicles relative to the ego-vehicle, where the reference system is in the ego-vehicle. The second approach is based on the velocities of the vehicles relative to the ground, where the reference system is in the ground. The results show that, with the proposed architecture, the latter approach results in a lower RMSE in the longitudinal direction compared with the former approach. The results also show that the proposed models, overall, outperform a simple model, which is based on polynomial fitting, particularly in the lateral direction where the proposed models are significantly better than the polynomial models. Furthermore, contextual features do not improve the predictions significantly. However, the results indicate that contextual information has a positive impact on the predictions in specific scenarios.</p>

In diva2:1548712 there were merged words, the abstract should be:

 <p>One of the main focus of robotics is to integrate robotic tasks and motion planning, which has an increased significance due to their growing number of application fields in transportation, navigation, warehouse management and much more. A crucial step towards this direction is to have robots automatically plan its trajectory to accomplish the given task. In this project a multi-layered approach was implemented to accomplish it. Our framework consists of a discrete high-level planning layer that is designed for planning, and a continuous low-level search layer that uses a sampling-based method for the trajectory searching. The layers will interact with each other during the search for a solution. In order to coordinate for multi-agent system, velocity tuning is used to avoid collisions, and different priority are assigned to each robot to avoid deadlocks. As a result, the framework trades off completeness for efficiency. The main aim of this project is to study and learn about high-level motion planning and multi-agent system, as an introduction to robotics and computer science.</p>

In diva2:1634504 there were merged words, the abstract should be:

 <p>With the rise of global sustainability energy initiatives,the implementation of renewable energy sources in future electrical grids is increasing. Many of the renewable energy sources are however intermittent, meaning they provide varying levels of power. As grids meet the demand of larger loads of intermittent renewable energy sources, small signal instability arises as result of the power oscillations. Small signal instability occurs when a system cannot return to steady state after being exposed to small disturbances. One method to damp power oscillations in an unstable system is by using a Power System Stabilizer (PSS). The goal of this project is to tune a PSS or PSSs required to successfully damp out the power oscillations in a system which is small signal unstable without any PSSs connected. The PSSs are tuned through a trial and error approach, and the system is a Kundur two-area four-machine MATLAB Simulink model. Overall, the trial and error method is successful in tuning PSSs, which damp out the system’s power oscillations. Other methods of tuning are discussed and compared in terms of efficiency to damp out power oscillations.</p>
----------------------------------------------------------------------
In diva2:1723189 words were merged, the abstract should be:

<p>In this report, the effect that a higher penetration of renewable energy sources has on electric power grid stability is evaluated. The report also compares different methods of stabilizing an unstable grid. The model used is a two-area four-machine system and the main objective is to stabilize the synchronous generators such that they revert back to synchronism after being subjugated to a small signal disturbance. The stabilization methods consists of supplementary Power System Stabilizers (PSSs) complementing the exciter systems of the synchronous machines, as well as two types of converter-based controllers in the renewable energy source: Grid-Following (GFL) converters and Grid-Forming (GFM) converters. The results show that a system with renewable energy sources is more sensitive to disturbances and has a larger rotor angle deviation from a steady state when using only GFLs compared to the conventional grid without PSSs. It is also found that a conventional grid requires supplementary PSSs to be stable. This is also the case for a system with renewable energy controlled by GFL. The system with GFM controllers does however not need supplementary PSS to be stable. This leads to the conclusion that GFM is more preferable than GFL to control a grid with a higher penetration of renewable energy.</p>
----------------------------------------------------------------------
In diva2:1619556 words and paragraphs were merged, the abstract should be:

<p>The automotive industry is evolving with the advent and development of autonomous cars. For the operation of these vehicles, sensors are needed, and utilising radar technology is a good solution. Slotted waveguide arrays in gap waveguide technology provide a good solution for the radar antennas; their low losses, high performance while keeping a compact design and its ease and cost-efficient manufacturing are the main features that make this antenna powerful.</p><p>This work aims to facilitate the design process of the antenna through a synthesis method that manages to find an antenna with desired characteristics more efficiently. These characteristics include central frequency, impedance bandwidth, sidelobe level and beamwidth. The synthesis method is implemented and verified throughout the work in three waveguide structures, rectangular, ridge, and ridged gap waveguide. The implementation is done in <em>Matlab</em> and the simulations are carried on <em>CST Microwave Studio</em>.</p><p>Two four-slots waveguide arrays in gap waveguide technology with their central frequencies at 28 GHz are designed and manufactured. The difference is in the SLL, which is -15 dB for one and -25 dB for the other. The objective is to verify that the synthesis method matches the simulations and measurements. </p>
----------------------------------------------------------------------
In diva2:1391129, 'framework1' should be 'framework<sup>1</sup>'
where 1 is for a footnote saying "<sup>1</sup>https://yukaichou.com/gamification-examples/octalysis-complete-gamificationframework/"

Note: The cover is in Swedish, but the thesis in in English.
----------------------------------------------------------------------

In diva2:1380818 words were merged, the abstract should be:

<p>This thesis examines the problem of downlink power allocation in dense 5G networks, and attempts to develop a data-driven solution by employing deep reinforcement learning. We train and test multiple reinforcement learningagents using the deep Q-networks (DQN) algorithm, and the so-called Rainbow extensions of DQN. The performance of each agent is tested on 5G Urban Macro simulation scenarios, and is benchmarked against a fixed power allocation approach. Our test results show that the DQN models are successful at improving data rates at cell-edge, while generalizing well to previously unseen simulation scenarios. In addition, the agents induce throughput balancing effects, i.e., achieve fairness among users, in networks with full-downlink-buffer traffic by properly designing the reward signal.</p>
----------------------------------------------------------------------

In diva2:680026 words and paragraphs were merged, the abstract should be:

<p>The giant ever increasing demand for higher data rates and better Quality of Service (QoS) is rapidly growing and operators’ main concern is to support the growth of mobile data traffic and address the users’ expectations while at the same time keeping the costs of services reasonable [1], [2], [3]. This is more vital in residential and dense urban areas where the reception of themacro signal level becomes weak [4]. Therefore, the implementation of ultra dense networks becomes a promising approach which is expected to provide good indoor coverage and higher capacity in residential areas. Nevertheless, the potential degradation of network performance due to severe interference originated from nearby networks should be deeply studied prior to full-scale implementation of ultra dense networks. The main concern of this thesis work is to investigate the coexistence between two operators in Time Division Duplex (TDD) system which are using adjacent frequency channels and implemented in the same geographical area. For this purpose, the system level simulation based on Monte Carlo method is performed to reveal the impact of critical parameters including <em>Adjacent Channel Interference power Ratio (ACIR)</em>, <em>Uplink-Downlink synchronization between operators<em>, <em>Base-Stations positioning</em>, and <em>Internal walls existence</em> on the system performance. Afterwards, the effect of densification on the previous findings is studied.</p><p>Results show that in downlink and uplink, approximately 30 dB and 55 dB of ACIR is required, respectively, in order to eliminate the impact of adjacent channel interference. Furthermore, in uplink, synchronization is necessary when base stations of operators are collocated. In downlink, however, synchronization and collocation is beneficial when signal quality is poor. On the other hand, it is shown that densification is feasible provided base stations employ adjustable transmission power model. Moreover, internal walls can improve system performance due to attenuation of interferences originated from surrounding cells.</p>
----------------------------------------------------------------------
In diva2:1748023 the paragraphs were merged and there were missing ff ligatures, the abstract should be:
<p>Block partitioning is a computationally heavy step in the video coding process. Previously, this stage has been done using a full-search-esque algorithm. Recently, Artificial Neural Networks (ANN) approaches to speed-up block partitioning in encoders compliant to the Versatile Video Coding (VVC) standard have shown to significantly decrease the time needed for block partitioning.</p><p>In this degree project, a state of the art Convolutional Neural Network (CNN) was ported to VTM16. It was ablated into 7 new models which were trained and tested. The eects of the ablations were compared and discussed with respect to the number of Multiply-Accumulate operations (MAC) a model required, the speed-up in the encoding stage as well as the quality of the encoding.</p><p>The results show that the number of MACs can be substantially decreased from that of the state of the art model while having low negative eects on the quality of the encoding. Furthermore, the results show that the two tested approaches of reducing the computational complexity of the model were effective. Those were: 1) reducing the image’s resolution earlier in the model. 2) reducing the number of features in the beginning layers. The results point towards the first approach being more effective.</p>
----------------------------------------------------------------------

In diva2:860991 missing italics and bold face, the abstract should be:
<p>Nashorn is a JavaScript engine that compiles JavaScript source code to Java bytecode and executes it on a Java Virtual Machine. The new bytecode instruction <strong>invokedynamic</strong> that was introduced in Java 7 to make it easier for dynamic languages to handle linking at runtime is used frequently by Nashorn. Nashorn also has a type system that optimizes the code by using primitive bytecode instructions where possible. They are known to be the fastest implementations for particular operations.</p><p>Either types are proved statically or a method called <em>optimistic type guessing</em> is used. That means that expressions are assumed to have an <strong>int</strong> value, the narrowest and fastest possible type, until that assumption proves to be wrong. When that happens, the code is deoptimized to use types that can hold the current value.</p><p>In this thesis a new architecture for Nashorn is presented that makes Nashorn’s type system reusable to other dynamic language implementations. The solution is an intermediate representation very similar to bytecode but with untyped instructions. It is referred to as Nashorn bytecode in this thesis.</p><p>A TypeScript front-end has been implemented on top of Nashorn’s cur-rent architecture. TypeScript is a language that is very similar to JavaScript with the main difference being that it has type annotations. Performance measurements which show that the type annotations can be used to improve the performance of the type system are also presented in this thesis. The results show that it indeed has an impact but that it is not as big as anticipated.</p>
----------------------------------------------------------------------
In diva2:1660097 there were instances of "grid- -", the abstract should be:
<p>The modern power system is aiming to progress away from conventional synchronous machine  based power generation towards converter dominated system that leads to extensively high penetration of renewable energy such as wind and PV. This transition of modern power system toward converter based renewable energy comes with new challenges as the conventional synchronous generation is being replaced by converter based power system (CBPS). The converter is commonly interfaced to the power system with Phase Locked Loop (PLL) technique to synchronize the converter with the grid voltage angle and inject the current at the right angle. Therefore, this approach is called grid-following converter; this type of configuration of converters may lead to some power system instabilities (e.g., voltage instability, frequency instability, synchronous and sub­synchronous instabilities). In order to overcome the limitation of the grid-following converters, another converter control concept become present in the literature as a grid-forming converter where the synchronizing method to the grid eliminates the need for PLL .In this thesis, a grid-forming controlled power converter is implemented with an energy storage system to emulate the inertia of the synchronous generator through the VSM control concept. An electromagnetic transient (EMT) simulation has been modeled in the PSCAD simulation environment. The model is the well­ known four-­machine two-­area power system. The model has been tested by incrementally replacing the synchronous machines with wind farms connected through power converters; this weakens the grid and may lead to frequency instability during a disturbing event. An Energy Storage System (ESS) has been implemented and added to the system to mitigate the loss of the kinetic energy of the rotating masses of the synchronous generators. The ESS is integrated with a grid-forming converter that is controlled to mimic the dynamic behavior of a synchronous generator. Thus, the ESS is synchronized to the system based on the swing equation of the synchronous generator. The results show significant improvements in the frequency stability of the system under study.</p>
----------------------------------------------------------------------
In diva2:1600236 the paragraphs were merged, the abstract should be:
<p>Mobile manipulators are changing the way companies and industries complete their work. Untrained end users risk facing unfunctional and nonuser- friendly Graphical User Interfaces. Recently, there has been shortages of people and talent in the heathcare industry where these applications would benefit in being used to accomplish easy and low level tasks. All these reasons contribute to the need of finding functional robot-user ways of communicating that allow the expansion of mobile manipulation applications. This thesis addresses the problem of finding an intuitive way to deploy a mobile manipulator in a laboratory environment.</p><p>This thesis has analyzed whether it is possible to permit the user to work with a manipulator efficiently and without too much effort via a functional graphical user interface. Creating a modular interface based on user needs is the innovation value of this work. It allows the expansion of mobile manipulator applications that increases the number of possible users. To accomplish this purpose a Graphical User Interface application is proposed using an explanatory research strategy. First, user data was acquired using an ad hoc research survey and mixed with literature implementations to create the right application design. Then, an iterative implementation based on code-creation and tests was used to design a valuable solution. Finally, the results from an observational user study with non-roboticist programmers are presented.</p><p>The results were validated with the help of 10 potential end users and a validation matrix. This demonstrated how the system is both functional and user-friendly for novices, but also expressive for experts. </p>

Note that in the 2rd sentence 'healthcare' is misspelled, so it perhaps could be 'hea[l]thcare' - to reflect that the error is in the original abstract in the thesis.
----------------------------------------------------------------------
In diva2:1443829 there were merged words and paragraphs, the abstract should be:

<p>With increasing demand for autonomous systems and self-driving heavy-duty vehicles there is an even more increasing demand for safety. In order to achieve desired safety level on the public roads, engineers have to tackle many technical issues, like decision making, object detection and perception. In order to detect an object or to have an understanding of its surroundings, autonomous heavy-duty vehicles are equipped with different types of sensors. These sensors are placed on different parts of the autonomous truck. The fact that some parts of the truck are highly dynamical introduces additional disturbances to the signals coming from onboard sensors. One of the most dynamic parts of every truck is its cabin. Moving cabin may induce additional disturbances into data coming from sensors attached to it. This corrupted data may lead the autonomous trucks to make wrong decisions. In the worst case, such decisions may be fatal.</p><p>This thesis uses a data driven modeling approach for creating a mathematical description of cabin movements based on data from onboard sensors. For that purpose, tools from system identification field are used. The resulting models are aimed to be used for implementation of real-time estimation algorithm for the cabin dynamics, which in turn can be used for real-time compensation of the disturbances.</p>
----------------------------------------------------------------------
In diva2:1076208 there were merged words, the abstract should be:
<p>Comets are a key to understanding the early stages of the solar system. They were here at its formation and have not evolved ever since, which means they are our best shot at learning the processes that led to the formation of the solar system as we know it today. Yet, our knowledge about these bodies is very limited. They are far from the Earth and small, which makes it complicated and expensive to reach them. But the study of the chemistry and geology of comets is not the only goal of the scientific community. The plasma environment of these astronomical bodies could also give answers to many questions regarding the science of plasma physics, such as the interaction of the solar wind with plasmas. Answering some of these questions was an objective of the Rosetta Mission. Before its launch, only three space missions out of eight targetting comets had plasma instruments onboard. Rosetta carried several instruments designed to analyse the plasma environment of comet 67P/Churyumov-Gerasimenko. We were able to perform a statistical analysis of the electric field spectrum in the vicinity of comet 67P/Churyumov-Gerasimenko. This allowed us to determine two regions of high spectral activity using the two probes of the LAP instrument and to propose several theories about the physical processes that were active.</p>
----------------------------------------------------------------------
In diva2:789016 the title and abstract are missing spaces, the title should be:
"Evaluation of IEC 61175 for semantic interpretation in OPAL-RT reference distribution network and Jess-OPC"

the abstract should be:
diva2:789016: <p>In the past few decades the electric power system has seen tremendous growth in terms technology such as integration of DG and advanced power electronics devices. Such developments mainly include renewable energy, increased use of HVDC transmission, state of the art communication systems and adaptation of information modeling standards applied to power systems. In this thesis work a RBTS distribution grid network model has been implemented in OPAL-RT to integrate automation and ICT with more devices. These motivate use of standardization like IEC 61850 and IEC 61175. This network model is used for verification of earlier developed monitoring and control methods at the ICS department. The verification setup involves interfacing of the network models in OPAL-RT with Jess rule engine via OPC. The focus of this master project work is evaluation of the use of IEC 61175 for consistent signal naming in the network model. It is done through detailed signal modeling in the Simulink model for use in the real time simulation and implementation of a graphical user interface to show all the signals along with their measured value in a tree view. Second part of the work focuses on utilization of the signal modeling in high-level applications. The applications can utilize the consistent signal naming done by using IEC 61175 to implement a semantic reasoning model for interpretation of the signals and thereby improve the monitoring and control functions. An interface has been implemented to generate an XML file of the mapped signals along with the last measured value for the parser implemented in Java. After parsing the XML file all the data is used for the Jess rule engine to fire the particular rule in a specific situation. Some rules are implemented for monitoring the system voltage and for finding out the signals associated with distributed generation unit to calculate their actual generation in the network. In this project an ontology based approach is also performed in JADE to get a clear idea of integration of information sources. Results of the work show that proper signal designation in a network according the standard IEC 61175 can potentially help reduce the complexity and improve reliability of operations.</p>
----------------------------------------------------------------------
In diva2:1248837 there were merged words and a ligature, the abstract should be:

<p>The worldwide automotive market is constantly evolving due to digitalization. Vehicles manufacturers introduce more and more digitalized human-machines interfaces by adding screens in cars. Nowadays a majority of cars have two screens: one used for navigation and another one to display technical parameters such as speed or gas consumption. When the same image has to be displayed on both screens it is not possible to generate two images adapted for both screens due to hardware limitations. This implies that only one image adapted for a specific screen is generated; to be displayed on the other screen this image has to be rescaled. This resizing is often source of image distortion and reduced user perceived image quality. The aim of this Master Thesis is to use sub-pixel rendering in resizing algorithms to improve perceived quality. Sub-pixel rendering takes the human visual system and the screen properties into account to improve image perceived quality after resizing. This thesis adapts the sub-pixel rendering for resizing and compares different existing upscaling algorithms with the sub-pixel rendering upscaling algorithms in terms of perceived quality. The perceived image quality after resizing is assessed both by a reference less image quality algorithm and a sample-group subjective evaluation composed of automotive engineers and designers. The evaluation highlights an improvement of perceived quality when the sub-pixel rendering algorithms is used for resizing.</p>
----------------------------------------------------------------------
In diva2:1142915 merged words, the abstract should be:

<p>In this bachelor thesis project, the problem of image classification with convolutional neural networks is considered. In several fields of biology, automatized cell detection is a helpful tool for facilitating the process of cellular analysis. This report answers the question whether a computer program can tell if an image contains muscle stem cells or not. Analogously to the neurons of the human brain, the creation of such a program involves training thousands of mathematically modeled artificial neurons to maximize the likelihood of producing correct classifications.This report covers how such a network is implemented and shows how its performance dependens on the network’s dimensions. It is revealed that a neural network indeed can replace and speed up the manual process of classifying images. With an image dataset of cells, the best performing networks manage to classify images with an accuracy of up to 90%.</p>
----------------------------------------------------------------------
In diva2:1135693 merged words, the abstract should be:

<p>In this paper, we propose a novel active learning algorithm for short-text (Chinese) classification applied to a deep learning architecture. This topic thus belongs to a cross research area between active learning and deep learning. One of the bottlenecks of deep learning for classification is that it relies on large number of labeled samples, which is expensive and time consuming to obtain. Active learning aims to overcome this disadvantage through asking the most useful queries in the form of unlabeled samples to be labeled. In other words, active learning intends to achieve precise classification accuracy using as few labeled samples as possible. Such ideas have been investigated in conventional machine learning algorithms, such as support vector machine (SVM) for image classification, and in deep neural networks, including convolutional neural networks (CNN) and deep belief networks (DBN) for image classification. Yet the research on combining active learning with recurrent neural networks (RNNs) for short-text classification is rare. We demonstrate results for short-text classification on datasets from Zhuiyi Inc. Importantly, to achieve better classification accuracy with less computational overhead,the proposed algorithm shows large reductions in the number of labeled training samples compared to random sampling. Moreover, the proposed algorithm is a little bit better than the conventional sampling method, uncertainty sampling. The proposed active learning algorithm dramatically decreases the amount of labeled samples without significantly influencing the test classification accuracy of the original RNNs classifier, trained on the whole data set. In some cases, the proposed algorithm even achieves better classification accuracy than the original RNNs classifier.</p>
----------------------------------------------------------------------
In diva2:853249: "importanceof" should be "importance of"
----------------------------------------------------------------------
In diva2:1638177 merged paragraphs and one case of merged words, the abstract should be:

<p>Along with the growth and popularity of mobile networks, users enjoy more convenient connection and communication. However, exposure of user presence in mobile networks is becoming a major concern and motivated a plethora of LPPM Location Privacy Protection Mechanisms (LPPMs) have been proposed and analysed, notably considering powerful adversaries with rich data at their disposal, e.g., mobile network service providers or Location Based Services (LBS). In this thesis, we consider a complementary challenge: exposure of users to their peers or other nearby devices. In other words, we are concerned with devices in the vicinity that happen to eavesdrop (or learn in the context of a peer-to-peer protocol execution) MAC/IP addresses or Bluetooth device names, to link user activities over a large area (e.g., a city), and especially when a small subset of the mobile network devices parasitically logged such encounters, even scattered in space and time, and collaboratively breach user privacy. The eavesdroppers can be honest-but-curious network infrastructures such as wireless routers, base stations, or adversaries equipped with Bluetooth or WiFi sniffers. The goal of this thesis is to simulate location privacy attacks for mobile network and measure the location privacy exposure under these attacks. We consider adversaries with varying capabilities, e.g., number of deployable eavesdroppers in the network and coverage of eavesdropper, and evaluate the effect of such adversarial capabilities on privacy exposure of mobile users.</p><p>We evaluate privacy exposure with two different metrics, i.e., Exposure Degree and Average Displacement Error (ADE).We use Exposure Degree as a preliminary metric to measure the general coverage of deployed eavesdroppers in the considered area. ADE is used to measure the average distance between user’s actual trace points and user’s trajectory predictions. We simulate three attack cases in our scheme. In the first case, we assume the attacker only acquires the collected data from users. We vary the number of receivers to test attack capacity. Exposure Degree is used to evaluate location privacy in this case. For the second and third cases, we assume the attacker also has some knowledge about users’ history traces. Thus, the attacker can utilize machine learning models to make prediction about user’s trace. We leverage Long Short-Term Memory (LSTM) neural network and Hidden Markov Model (HMM) to conduct real-time prediction and Heuristic LSTM to reconstruct more precise user trajectories. ADE is used to evaluate the degree of location privacy exposure in this cases. The experiment results show that LSTM performs better than HMM on trace prediction in our scheme. Higher number of eavesdroppers would decrease the ADE of LSTM model (increase user location privacy exposure). The increase of communication range of receiver can decrease ADE but will incur ADE increase if communication range successively increases. The Heuristic LSTM model performs better than LSTM to abuse user location privacy under the situation that the attacker reconstructs more precise users trajectories based on the incomplete observed trace sequence. </p>
----------------------------------------------------------------------
In diva2:1582903 merged paragraphs and "in-sight" should be "insight", the abstract should be:

<p>This study examines the correlation between self-regulated learning (SRL) and psychological well-being (PWB) in students in the online learning environment. Previous research suggests that these concepts are positively correlated, i.e. that application of SRL contributes to better PWB or vice versa. However, most studies on this relation have been performed with the traditional/in-person learning environment as context. Therefore, there is a lack of insight into how this relation might behave in the online learning environment, which is currently employed by many universities due to the ongoing Covid-19 pandemic. Thus, in this study, SRL and PWB were measured across 6 subscales each in students at KTH in order to perform a correlation analysis between the subscales of the two concepts. The aim is to gain insight into the relationship between SRL and PWB, which could be useful for teachers in assisting their students’ utilization of SRL and in turn improve their PWB.</p><p>Data was gathered through an online survey which was administered to students at KTH through a course and was published on an online forum for KTH students on Facebook in the year 2021. The survey received 103 responses. The data was analyzed using Spearman rank correlation analysis, which revealed mostly statistically insignificant correlations, or statistically significant but weak correlations between the subscales within SRL and PWB. The results show an overall weaker correlation between SRL and PWB compared to the results of previous studies. The large number of insignificant correlations might indicate that the sample size was insufficient for the method and tools used.</p><p>In conclusion, the results of this study did not reveal much meaningful information on the relation between SRL and PWB in students in the online learning environment. </p>
----------------------------------------------------------------------
In diva2:618087 merged words, the abstract should be:

<p>Mobile communication network is consuming 0.5 percent of the global energy supply alone these days. While the unrelenting increase in user capacity requirement, which is expected to grow 1000 times in 10 years, will lead to even more energy consumption in this filed. On the one hand, the energy cost which has covered half of the operators’ operation fee will continually increase; On the other hand, global warming problem may be still more severe due to the rising amount of carbon emissions caused by the increasing energy consumption. Although the system spectrum efficiency is improved significantly by the introduction of latest cellular network standards 3G and 4G, and the incremental enhancements in electronics and signal processing are bringing the energy consumption down in base stations, these improvements are still not enough to match the huge increase in energy consumption which is realted to the surging increase demands for more capacity. It is clear that solutions have to be found at the architectural layer. Besides, more and more capacity requirement is generating from indoor environment, it has been a vital concern for operators to consider how to provide enough service coverage with the existing macro-only network due to the complex indoor environment and high wall penetration loss. All these cause urgent demand for more energy efficient cellular networks to deal with the capacity challenges. One of the promising technologies to this situation is macro-femto heterogeneous networks by offloading indoor traffic to femtocells. Considering the shorten distance between indoor users and base stations and spacial frequency reuse advantages, as well as the drawbacks on introduced interference and extra power consumption by femto base stations, this report proposes a simulation framework to study whether this network topology could enhance the system energy efficiency or not. The results suggest that the introduction of femtocells may be a promising method to save energy consumption in the future and meet the increasing user data rate access requirements, especially in high user capacity demand networks, macro-femto deployment could save more energy consumption.</p>
----------------------------------------------------------------------
In diva2:1635560 merged words, the abstract should be:

p>Real-time monitoring of vehicle data during testing can drastically cut down on test times as well as improve the quality of testing by facilitating the implementation of run-time compliance verification with expected model behavior, along with anomaly detection in both hardware and software. By providing a wireless communication link between vehicles and a monitoring base station, this project aims to build the groundwork for more sophisticated testing proceedings in the future. The wireless communication system implemented in this project mirrors data from the two CAN data busses on the vehicle and transmits them via a licence-free 868 MHz ISM band. The receiver is connected to a computer where the data can be visualized and analyzed in real-time. The project goals were exceeded in both throughput and range. Early testing has shown that data rates of 150 kbit/s and ranges 1.2 km and beyond are achievable. The project has set a solid foundation upon which wireless testing routines can now be developed. Hardware and software developed in this project can be built upon and optimized further in future revisions to achieve even higher data rates and longer ranges.</p>
----------------------------------------------------------------------
In diva2:1836839, in the English keywords, one should remove "keywords here: "
----------------------------------------------------------------------
In diva2:1634343 merged words, the abstract should be:

<p>Systems consisting of multiple robots are traditionally difficult to optimize. This project considers such a system in a simulated warehouse setting, where the robots are to deliver boxes while avoiding collisions. Adding such collision constraints complicates the problem. For dynamical multi-agent systems as these, reinforcement learning algorithms are often appropriate. We explore and implement a reinforcement learning algorithm, called multi-agent rollout, that allows for re-planning during operation. The algorithm is paired with a base policy of following the shortest path. Simulation results with up to 10 robots indicates that the algorithm is promising for large-scale multi-robot systems. We have also discussed the possibility of using neural networks and partitioning to further increase performance.</p>
----------------------------------------------------------------------
In diva2:1453604 merged words and paragraphs as well as missing ligatures, the abstract should be:

<p>For a ship sailing between destinations on a schedule, having a speed controller that can sail the ship according to a speed reference offers extended capabilities for the crew. It is also a necessary tool for sailing the ship, according to an optimally planned speed reference. A method used for speed control of large sea-going vessels today is PID-control. This solution, however, tends to offer poor performance: the speed of the vessel tends to overshoot the reference during reference changes and the controller has a hard time tracking the reference under disturbances from waves and wind.</p><p>This thesis investigates the possibility of using Model Predictive Control to achieve better control performance while satisfying the constraints of the ship's propulsion system. The solution presented in this thesis is to use a Model Predictive Controller that uses a linear model obtained at each sampling instance from a ship's nonlinear surge dynamics. The required thrust, calculated by the controller, is then mapped to a fuel consumption setpoint for the ship's main engines. The results show that the MPC can in simulations achieve offset free tracking without overshoots or undershoots. Full scale test on a ship with a length of 171 [<em>m</em>] shows that the controller achieves good performance at speeds over 13 [<em>kn</em>] and in steady-state operation under modest disturbances, but is unable to achieve satisfactory tracking during accelerations or decelerations at speeds less than 13 [<em>kn</em>].</p>
----------------------------------------------------------------------
In diva2:705687 unnecessary hyphen spaces, the abstract should be:

<p>The unmanned helicopter Skeldar relies on model based control to per form its tasks.  System identification is an integral component  in the process  of  deriving models of helicopter dynamics.  This thesis aims to investigate how nonparametric methods of system identification can support the current  modelling  and system identification  practices at Saab.</p><p>Nonparametric system identification does not require a pre-defined model structure. Models estimated with this methodology may be used to validate parametric models, which are necessary for the implementation of the model based control system. This thesis examines several nonparametric methods of system identification in both the frequency and time domains. The theory of these methods is presented and their performance is analyzed on data from flight tests as well as from simulated systems.</p><p>Analysis of the results shows that models are highly dependent on the choice of input signal spectrum.  To best take advantage of nonparametric system identification in this application, experiments should be designed with special regard to the system properties sought to be modelled. Nonparametric system identification can then be used to provide a good understanding of the system properties in the excited frequency region.</p><p>In the specific  case of helicopter dynamics, of which the principles are very well understood at Saab, it can be concluded  that the existing system identification process is sufficient to provide well performing models. However, a nonparametric model could be estimated and used as a tool for comparison and validation in the process of identifying a paramteric model.</p>
----------------------------------------------------------------------
In diva2:690824 merged words, the abstract should be:

<p>With the dynamic changes in technological advancements, wireless communication technologies has made a tremendous progress from simple to complex systems that are able to communicate across multiple networks platform. As these systems continue to prove their proficiency and benefits, it is strongly asserted that wireless technologies will continue to play an even more critical and vital role compared to wired connections in the future.</p><p>The most relevant question now regarding the future of wireless technologies is whether it going to dominate the wireline transmission or be a complementary to wireline where it’s difficult for any reason to have wireline like wireless backhaul. With today’s wireless data rate speeds it would be difficult to imagine it replacing wireline in the near future, but technically speaking it is feasible to achieve those data rates with use of wider spectrum.</p><p>Moreover, wireless communication technologies particularly that of wireless mobile phone technology, is continuously more preferred in communication today, making it the first priority of modern day lifestyle. Modern communication system standards have therefore been subjected to evaluation and analysis to establish a more profound understanding of these various technologies.The proposed study presents an overview of various wireless communication systems such as: Global system for mobile communications (GSM), high-speed packet access (HSPA), long-term evolution (LTE), mobile WiMAX, ultra wideband (UWB) technology, ultra mobile broadband (UMB), wireless local area network (WLAN), Bluetooth wireless technology, and, Wi-Fi.</p>
----------------------------------------------------------------------
In diva2:1453632 merged words and bulleted paragraphs, the abstract should be:

<p>This degree project work has been carried out in the context of in the Hybrid Innovative Powertrain (HIP) project presently executed by the company Altran Technologies S.A.. Its purpose is to provide clients with a modular platform to model and simulate their vehicle as accurately and as fast as possible. The idea is to build a custom Simulink library with a block for each major component found in any road vehicle. The platform must enable clients to model battery electric vehicle as well as plug-in hybrid or fuel cell vehicles with different powertrain topologies. It must provides component blocks for each layer of control, from high-level energy efficiency algorithms embedded in high authority hardware to low-level component control solutions. The work presented here contributes to the development of the modular powertrain electric part with the electric motor, the inverter and the chopper. It is composed of three main steps:
<ul>
<li>Building an electric powertrain model with its proper control schemes in order to enable fast simulations. The model must be split into blocks that become part of the project library.</li>
<li>Building a more realistic model that takes into account the discontinuous switching dynamics and the discrete nature of the actual controllers, in order to verify and validate the control effectiveness.</li>
<li>Designing a diagnostic method for inverter faults.</li>
</ul>
The first part of this work is dedicated to a literature review of automotive electric drives solutions as well as the research in progress in the domain, notably in fault detection schemes. The mathematical models and controller designs used for the simulations are developed in a second part. The last part presents the software implementation aspects and the analysed simulation results.</p>

I suspect that there are also some merged paragraphs, but since there is no full text in DiVA, I cannot be sure.
----------------------------------------------------------------------
In diva2:869163 merged words and paragraphs, the abstract should be:

<p>Testing of information systems is an essential part of the system development process to minimize errors and improve the reliability of systems. Trafikverket IT unit had a structured testing in the test phase high-level, however, they had not a structured testing in the development phase, low-level tests. We were assigned to examine methods and working methods in low-level test. We also would compare systems that had undergone a structured testing in low- and high-level test against systems that had undergone an unstructured low-level test and structured high-level test.</p><zp>The goal of the thesis was to propose appropriate method/methods in low-level test for Trafikverket IT unit. The goal was also to make a recommendation if a structured testing in low- and high-level were to be recommended in comparison with systems that had undergone unstructured low-level test and structured high-level test. Through literary studies and interviews with Trafikverket employees we reached our result.</p><zp>Our recommendation for Trafikverket IT is that they should use test-driven development because developers were unsure of what should be tested and the method would make this clear. The developers also wanted to have options and guidelines that would give them a definite work structure. We also recommend an adaptation of the Self-Governance framework from where activities can be selected from each project manager (Scrum Master) that determines which activities will be performed in individual- and group level for each project.</p>
----------------------------------------------------------------------
In diva2:1324244 merged words and paragraphs, the abstract should be:

<p>The interlock Beam Position Monitor (BPM) system in the Large Hadron Collider(LHC) is responsible for monitoring the particle beam position at the point of the beam dump kicker magnets and is part of the machine protection system. The current interlock BPM system has some limitations and because of this, an upgrade project has been initiated. This master thesis describes the development of the analog front end electronics of this system, consisting mainly of two parts: A delay line based microwave filter and a high isolation and highly balanced power combiner circuit.</p<<p>The filter has been validated with real LHC beam measurements and is found to work as expected. More work however needs to be done to ensure the effect that the filter itself has on the beam measurements as the filter could introduce some ringing effects on the signal. The highly balanced high isolation power combiner has been tested through lab measurements and also shows promising results but long-term tests need to be conducted to ensure the reliability of the component as it will need to endure very high signal levels over long periods of time.</p>
----------------------------------------------------------------------
In diva2:771132 the title is missing a space, it should be:
"Challenges With Session to Session Management in Brain Computer Interfaces"


In diva2:771132 there were merged words, the abstract should be:

<p>Brain computer interfaces (BCIs) enable communication between a brain and a computer, without the need for any motor actions. Electroencephalography (EEG) signals can be used as input for a BCI, but they need to go through a number of steps in the BCI to create useable output. One of the most critical steps is the classification algorithm, which is the step that is investigated in this report. A linear and a nonlinear Support Vector Machine (SVM), together with a Linear Discriminant Analysis (LDA), are investigated in how well they can handle session to session performance when classifying EEG data from three different recording sessions of three different test subjects. The results show that the average performance of the classifiers are in most cases similar, slightly above 60%. The performance of the investigated algorithms differed depending on subject and session. The sometime slow performance of the classification algorithms may be due to the low signal-to-noise ratio in the EEG signals, or possibly even due to bad performance in producing recognizable EEG patterns by the test subjects. The conclusion drawn from the project is that data from different sessions can vary quite extensively, and in this project it was handled best by the nonlinear SVM with RBF kernel, with the highest average classification accuracy.</p>
----------------------------------------------------------------------
In diva2:1273039 unnecessary hyphens and missing hyphens, the abstract should be:

<p>Equation-based object-oriented modeling languages represent a highly composable class of modeling languages. In these languages models are expressed as differential-algebraic equations with no explicit causal relation between variables. Modeling of structurally varying systems in such languages is typically done by defining modes that describe the continuous evolution of the system, coupled with mode-switches describing structural changes. Specifically, structural changes can give rise to discontinuities and impulses, which can result in additional changes to the system. This thesis formalizes semantics for the treatment of structurally varying systems in such languages, including automatic handling of discontinuities and impulses from the theory of non-linear circuits. The semantics are implemented as part of an equation-based modeling language, where the treatment of impulses is based on backwards-Euler. The expressiveness of the implementation is evaluated on a number of structurally varying systems, both in the electrical and mechanical domains. We conclude that the semantics are expressive enough to describe some structurally varying systems, but are sensitive to numerical errors. Furthermore, more work is needed to allow the semantics to express inelastic collision in a satisfactory manner.</p>
----------------------------------------------------------------------
In diva2:809641 merged words and unnecessary paragraphs, the abstract should be:

<p>Given the demand for mobility in our society, the cost of building additional infrastructures and the increasing concerns about the sustainability of the traffic system, traffic managers have to come up with new tools to optimize the traffic conditions within the existing infrastructure. This study considered to optimize the durations of the green light phases in order to improve several criteria such as the ability of the network to deal with important demands or the total pollutant emissions.</p><p>Because the modeling of the problem is difficult and computationally demanding, a stochastic micro-simulator called ’Simulation of Urban MObility’ (SUMO) has been used with a stochastic optimization process, namely a Genetic Algorithm (GA).</p><p>The research objective of the study was to create a computational framework based on the integration of SUMO and a Multi-Objective Genetic-Algorithm (MOGA). The proposed framework was demonstrated on a medium-size network corresponding to a part of the town of Rouen, France. This network is composed of 11 intersections, 168 traffic lights and 40 possible turning movements. The network is monitored with 20 sensors, spread over the network. The MOGA considered in this study is based on NSGA-II. Several aspects have been investigated during the course of this thesis.</p><p>An initial study shows that the proposed MOGA is successful in optimizing the signal control strategies for a medium-sized network within a reasonable amount of time.</p><p>A second study has been conducted to optimize the demand-related model of SUMO in order to ensure that the behavior in the simulated environment is close to the real one. The study shows that a hybrid algorithm composed of a gradient searchalgorithm combined with a GA achieved a satisfactory behavior for a medium-size network within a reasonable time. The demand is defined as the number of cars</p>
----------------------------------------------------------------------
diva2:809641 and diva2:816306 seem to be duplicates with the latter having more downloads
----------------------------------------------------------------------
In diva2:1272223 merged words, the abstract should be:

<p>Rising penetration of renewable energy sources in electric power grids is both a challenge and an opportunity to optimally utilize the potential of either wind or PV energy sources, to stabilize operation of future power systems. Bi-directional flows between distribution and transmission system operators cause signicant problems with keeping the voltages in the grid within admissible limits. This paper contains description of Oland's island medium- and low-voltage electric power grid, ranging from 0.4 kV to 130 kV in the purpose of quasi-static analysis of active and reactive power flows in the system. Goal of the analysis is to optimize reactive power exchange at the point of connection with the mainland grid. In the analyzed grid system, there is an enormous, 190% penetration of wind sources. Capacity of the wind parks connected to dedicated buses totals to 136.1 MW, that supply up to 90.5 MW of load. With industry-wise reactive power capability limits, total contribution of wind parks reaches almost 66 MVAr, enabling to compensate deficits and extra surpluses of the reactive power in the grid. Presented system is connected to the mainland's grid through one point of connection, which is simulated as Thevenin equivalent circuit. Main objective of the thesis is to test and analyze viable solutions to minimize reactive power exchange at the point of connection at Stavlo substation connecting Oland's and Sweden's electric grid keeping valid all necessary contingencies enforced by current grid codes applied in Sweden as well as thermal limits of the lines and voltage limits of the system. Furthermore, state of the art of current reactive power compensation methodologies and most promising techniques to eciently and effectively control reactive power flow are outlined. Droop control methodologies, with focus on global and local objectives, and smartgrid solutions opportunities are being tested and modeled by the authors and are comprehensively presented in this paper. Moreover, economic costs of control methods are compared. Analysis of active power losses in the system as well as cost of implementation of alternative solutions is presented, where most financially viable solutions are outlined, giving brief outlook into future perspectives and challenges of electric power systems. It is shown that controllability of reactive power support by wind turbine generators can enhance operation of electric power grids, by keeping the reactive power ow minimized at the boundary between grids of distribution and transmission system operators. Furthermore, results indicate that extra reactive power support by wind turbine generators can lead to diminishment of active power losses in the system. Presented system is being modeled in the PSS/E software dedicated for power system engineers with use of Python programming languages. Analysis of data was done either in Python or R related environments. Thesis was written with cooperation between KTH and E.On Energidistribution AB.</p>
----------------------------------------------------------------------

The above were all sent on or before 2024-08-01
======================================================================
In diva2:868398 merged words and paragraphs, as well as a missing ligature, along with the page number "i", the abstract should be:

<p>Manufactures of power system products face an increased pressure to reduce the time to market of their development process without compromising quality. Moreover the operation of power systems needs to be performed in a secure and reliable manner. One of the key systems to guarantee those stringent requirements is the protection system. The objective of this Master’s thesis is the development of a protection system, which solidly relies on Commercial-off-the-Shelf (COTS) components as well as on the developed protection functions. Thereby it is shown that the tight cost requirements can be fulfilled without jeopardising the reliability and security performance.</p><p>This project comprises the development of a definite-time directional overcurrent and earth fault protection. The applied development process is based on a model-based-design approach, which comprises the definition of the requirements, the design phase, the implementation on the target system and the test phase. As part of this thesis each stage is described and executed. Moreover MATLAB/Simulink was used as development environment, since it perfectly supports the model-based-design approach. The considered functional requirements are mostly based on the standard IEC 60255-151. The developed protection algorithm runs on a realtime linux system and the interface to the process is based on the EtherCAT protocol and their corresponding I/O modules. Lastly, the test phase is based on a functional performance test, a type test according to IEC 60255-151, a long term test and an evaluation of the EMC performance of the used I/O modules.</p><p>The results of the type tests showed that a IEC 60255-151 compliant solution is yield. Moreover the functional performance test proofed that the developed protection function operate as intended for various fault scenarios. Lastly, the realtime performance of protection system has to be further analysed and adapted in order to achieve satisfactory behaviour.</p>
----------------------------------------------------------------------
In diva2:1110818 the title should be: "Electromagnetic modelling and testing of a Thomson coil based actuator"
----------------------------------------------------------------------
In diva2:1034057 merged words, the abstract should be: <p>Software Defined Networks (SDN) constitute the new communication paradigm of programmable computer networks. By decoupling the control and date plane the network management is easier and more flexible. However, the new architecture is vulnerable to a number of security threats, which are able to harm the network. Network monitoring systems are pivotal in order to protect the network. To this end, the evaluation of a network monitoring system is crucial before the deployment of it in the real environment. Network simulators are the complementary part of the process as they are necessary during the evaluation of the new system’s performance at the design time.</p><p>This work focuses on providing a complete simulation framework which is able to (i) support SDN architectures and the OpenFlow protocol, (ii) reproduce the impact of cyber and physical attacks against the network and (iii) provide detection and mitigation techniques to address Denial-of-Service (DoS) attacks. The performance of the designed monitoring system will be evaluated in terms of accuracy, reactiveness and effectiveness.The work is an extension of INET framework of OMNeT++ network simulator.</p>----------------------------------------------------------------------
In diva2:874540, missing spaces, the abstract should be

<p>Recently, unique combination of electro- and magneto-optic, piezoelectric, and coupled acousto-optic properties makes complex metal oxide films to be important players in emerging heterogeneous integration technologies. Nowadays, bulk-single crystal- quality thin film heterostructures can be fabricated directly on a semiconductor platform. Integration of dissimilar classes of high performance functional materials into a single system enables optical and electrical signal processing and storage: e.g., waferlevel integration of tunable narrowband lasing sources, electro-optic modulators, magneto-optic isolators, logic and memory devices. As a result, well-known photonic materials own new functionalities. In addition, interesting effect of so called resistive switching is observed in several transition metal oxide films might be also considered for the application in novel optical devices. Functional properties of transition metal oxide films can be tailored in a wide range. Standard plasma dry etching technique can be employed to pattern these films to fabricate optical waveguides and optical integrated circuits. Therefore, a complex research program that includes films processing, characterization, theoretical modeling and device simulation is required for material validation and cost efficiency and should precede further practical exploration.</p><p>Three kinds of dielectric oxide films, Er<sub>2</sub>O<sub>3</sub>, WO<sub>3</sub>, and Ta<sub>2</sub>O<sub>5</sub> were deposited on a transparent borosilicate glass substrate via radio-frequency magnetron sputtering. Optical properties of grown films were characterized by ellipsometer technique within the range 1.5-6 eV (850 -200 nm) and using optical transmissionspectrometer 1.1 -6 eV (200-1100nm). Inherent transition peaks at 379 and 524nm caused by a single trivalent Er<sup>3+</sup> ion is observed in transmission spectrum of Er<sub>2</sub>O<sub>3</sub> film. Two different approaches, computation using the ellipsometer DeltaPsi 2 software and Matlab simulation based on the microscopic electric dipole theory were employed to obtain dispersion relations of refractive index and extinction coefficient. We found and explained several mismatches of refractive indexes and extinction coefficients obtained by these two methods.</p><p>Optical characteristics were obtained with a reasonable precision and were set into optical waveguide simulations as input parameters. Modeling of light propagation in 2D planar waveguide with different widths yields requirements for an optical insertion loss. We compared the performance of the waveguides made from the fabricated functional oxides with currently achieved optical properties. Variation of processing parameters leads to the variation of film stoichiometry, critically influences film refractive index and strongly effects a light confinement. With Comsol multiphysics we modeled C-bandlight amplification in 2D waveguides made from Er-doped Ta<sub>2</sub>O<sub>5</sub> and WO<sub>3</sub>.The threshold values of the fractional population of 4I<sup>13/2</sup> level of Er<sup>3+</sup>-ions were found for currently achieved Ta<sub>2</sub>O<sub>5</sub> and WO<sub>3</sub>film properties.</p><p>Advanced device modeling and design should include a tailoring of optical films properties through the optimization of processing conditions furnished with a detailed feedback of comprehensive films characterization.</p>
----------------------------------------------------------------------
In diva2:567676
"model-parameter-invariant" should be
"model-parameter-invariant"
----------------------------------------------------------------------
In diva2:1570274 merged words and merged paragraphs, the abstract should be:

<p>It is a notoriously difficult task to find winning strategies for multi-agent games. Especially if one or multiple agents lack the information required to determine which state the game is in. When this type of uncertainty arises in a game it is referred to as a multi-agent game of imperfect information.</p<<p>In this project we designed and built a tool for strategy synthesis of multi-agent games against nature. The strategy synthesis was knowledge-based and therefore a multi-agent extension of the Knowledge Based Subset Construction, built by a previous project group, was applied to the input games. This construction creates a new knowledge-based game, with reduced uncertainty compared to the initial multi-agent game of imperfect information. We constructed the tool using a forward search heuristic which meant that it would locate all existing winning strategies.</p<<p>We study the performance of the tool by comparing it to a baseline approach relying solely on randomisation. This comparison was performed on five different games. Our tool found every relevant strategy for each game at least 35% faster than the baseline found the same amount of unique winning strategies. If a strategy can win without transitioning through a state, then that state is not relevant and is not part of the strategy. The comparison test for this game shows that the tool is working very well.</p>
----------------------------------------------------------------------
In diva2:615538 merged words and paragraphs, the abstract should be:

<p>The concept of interference alignment has recently become one of the important tools to analyze the capacity of many multiuser communication networks, e.g. K-user interference channel, wireless X networks, multi hop interference networks, etc. The idea is to consolidate the interference into smaller dimensions of signal space at each receiver and use the remaining dimensions to transmit the desired signals. Furthermore, most progress in understanding of the wireless networks capacity has been made on the single hop schemes and multi-hop multi-cast networks. However, there has not been as much progress in multi-hop multi-flow networks where all messages are not required by all destination nodes. One of the basic problems in this area, is the capacity of 2 × 2 × 2 interference channel. It is proved that the upper bound value of 2 degrees of freedom (DoF) for this channel can be achieved using the so called “aligned interference neutralization” method.</p<<p>In the proposed interference alignment schemes for network problems which we mentioned in the above, including 2 × 2 × 2 interference channel, there are some theoretical assumptions which seem to be difficult to apply in practice, e.g. high transmit power, asymptotic symbol extension of the channel, global and perfect channel state information (CSI), etc. Among these assumptions the availability of CSI specially at transmitter, is crucial for performing the interference alignment technique. The CSI at transmitter (CSIT) is usually available through feedback from receiver and it is used to estimate the current channel state, given that the channel coherence-time is long enough. However,it has been shown recently that the delayed CSIT, which is assumed to be independent of current channel state, still can be used to increase DoF of some specific network settings.</p<<p>In this work, we consider the 2 × 2 × 2 interference channel where two source nodes communicate with corresponding destination nodes via two relay nodes. We investigated the degrees of freedom of 2×2×2 interference channel with delayed CSIT and we derived the upper bound on the degrees of freedom of the channel under this condition. Furthermore, we showed that this upper bound can be achieved using interference alignment technique. We also showed that this completely out-of-date information of the channel can still be useful to achieve higher rate compared to the situation where no CSIT is available at the source nodes. Moreover, we observed that using relay nodes in interference channel can improve DoF compared to one hop interference channel where transmitters and receivers directly communicate with each other.</p>
----------------------------------------------------------------------
In diva2:1141375 merged words and paragraphs, the abstract should be:

<p>The ITEA3 OpenCPS (Open Cyber-Physical System Model-Driven Certified Development) project focuses on interoperability between the Modelica/Unified Modeling Language (UML)/Functional Mock-up Interface (FMI) standards, improved (co-)simulation execution speed, and verified code generation. The project aims to develop a modeling and simulation framework for cyber-physical and multi-domain systems. One of the main use cases for the framework, is the multi-domain equation-based modeling and simulation of detailed gas turbine power plants (including the explicit equation-based modeling of turbomachinery dynamics) and the electrical power grid.</p><p>In this work, UML class diagrams based on the Common Information Model (CIM) standard are used to describe the semantics of the electrical power grid. An extension based on the standard ISO 15926 has been proposed to derive the multi-domain semantics required by the models that integrate the electrical power grid with the detailed gas turbine dynamics.</p><p>Furthermore, the multi-domain physical modeling and simulation Modelica language has been employed to create the equation-based models of the use case of this project. A comparative analysis between the Single-Domain and Multi-Domain model responses has been performed both in time and frequency. The results show some interesting differences between the turbine dynamics representation of the commonly used GGOV1 standard model and the less simplified model of a gas turbine.</p><p>Finally, the models from each domain can be exchanged between two different stakeholders by means of Functional Mock-Up Units (FMUs), defined by the FMI standard. Promising test results were obtained with different simulation tools that support the standard, which demonstrates the feasibility of exchanging unambiguous multi-domain models with a detailed gas turbine representation. This shows the potential of the FMI standard for manufacturers to exchange equation-based multi-domain models, while at the same time protecting their intellectual property.</p>
----------------------------------------------------------------------
In diva2:1141736 merged words, the abstract should be:

<p>Over the course of the last few years, the fashion industry has begun to focus more resources on their digital transition. For a fashion e-commerce business, it is essential to know whether or not to invest money and time in building a modern web application. This master's thesis aims at finding practical results on how transitioning from a multi-page website towards a Single Page Application can have an impact on the business performances of the company, as measured by Conversion Rate, Page Views and Gross Sales. In collaboration with the development and product team of a fashion company, this master's thesis is based on the six-months development of a new Single Page Application using the Javascript framework React.js, building on known User-Experience Design principles and Human-Computer Interface heuristics. The live data collection from the website's audience allowed a quantitative analysis of the transition's effect on business performance, which showed positive impact on business performance. A qualitative user survey was then conducted in order to further elaborate on the causes of the aforementioned impact: all respondents praised the Single Page Application as compared to the multi-page website, and noted lower Response Time, efficient filter-and search system and high user interaction as advantages that played in favour of their browsing experience and their will-to buy a product. The impact of lowering Response Time even more was discussed, as well as the different limitations due to the scope of this thesis. A list of user suggestions for further improvements was also compiled.</p>
----------------------------------------------------------------------
In diva2:1412847, 
   multi-tenancy‟s
should be
   multi-tenancy's
----------------------------------------------------------------------
In diva2:618110 merged words, the abstract should be:

<p>Traditional design methods could not cope with the recent development of multiprocessor systems-on-chip (MPSoC). Especially, hard real-time systems that require time-predictability are cumbersome to develop. What is needed, is an efficient, automatic process that abstracts away all the implementation details. ForSyDe, a design methodology developed at KTH, allows this on the system modelling side. The NoC System Generator, another project at KTH, has the ability to create automatically complex systems-on-chip based on a network-on-chip on an FPGA. Both of them support the synchronous model of computation to ensure time-predictability. In this thesis, these two projects are analysed and modelled. Considering the characteristics of the projects and exploiting the properties of the synchronous model of computation, a mapping process to map processes to the processors at the different network nodes of the generated system-on-chip was developed. The mapping process is split into three steps: (1) Binding processes to processors, (2) Placement of the processors on net network nodes, and (3) scheduling of the processes on the nodes. An implementation of the mapping process is described and some synthetic examples were mapped to show the feasibility of algorithms.</p>
----------------------------------------------------------------------
In diva2:703998 merged words and pagragraphs, as well as missing emphasis (italics) and numbered list items, the abstract should be:

<p>IP traffic increase has resulted in a demand for greater capacity of the underlying Ethernet network. As a consequence, not only Internet Service Providers (ISPs) but also telecom operators have migrated their mobile back-haul networks from legacy SONET/SDH circuit-switched equipment to packet-based networks.</p><p>This inevitable shift brings higher throughput efficiency and lower costs; however, the guaranteed QoS and minimal delay and packet delay variation (PDV) that can only be offered by circuit-switched technologies such as SONET/SDH are still essential and are becoming more vital for transport and metro networks, as well as for mobile back-haul networks, as the range and demands of applications increase.</p><p>Fusion network offers "both an Ethernet wavelength transport and the ability to exploit vacant wavelength capacity using statistical multiplexing <em>without interfering</em> with the performance of the wavelength transport" [RVH] by dividing the traffic into two service classes while still using the capacity of the same wavelength in a wavelength routed optical network (WRON) [SBS06]:<ol><li>A <em>Guaranteed Service Transport</em> (GST) service class supporting QoS demands such as no packet loss and fixed low delay for the circuit-switched traffic.</li><li>A <em>statistical multiplexing</em> (SM) service class offering high bandwidth efficiency for the best-effort packet-switched traffic.</li></ol></p><p>Experimentation was carried out using two TransPacket's H1 nodes and the Spirent Test-Center as a packet generator/analyzer with the objective of demonstrating that the fusion technology, using TransPacket's H1 muxponders allow transporting GST traffic with circuit QoS; that is with no packet loss, no PDV and minimum delay independent of the insertion of statistically multiplexed traffic.</p><p>Results indicated that the GST traffic performance is completely independent of the added SM traffic and its load. GST was always given absolute priority and remained with a constant average end-to-end delay of 21.47 μs, no packet loss and a minimum PDV of 50 ns while SM traffic load increased, increasing the overall 10GE lightpath utilization up to 99.5%.</p>
----------------------------------------------------------------------
In diva2:1556726
   naphtha-lene
should be
   naphthalene
----------------------------------------------------------------------
In diva2:1039090 paragraphs were merged, the abstract should be:

<p>The Internet of the 21th century is a different version from the original Internet. The Internet is becoming more and more a huge distribution network for large quantities of data (Photos, Music, and Video) with different types of connections and needs. TCP/IP the work horse for the Internet was intended as a vehicle to transport best effort Connection oriented data where the main focus is about transporting data from point A to point B regardless of the type of data or the nature of path.<7p><p>Information Centric Networking (ICN) is a new paradigm shift in a networking where the focus in networking is shifted from the host address to the content name. The current TCP/IP model for transporting data depends on establishing an end to end connection between client and server. However, in ICN, the client requests the data by name and the request is handled by the network without the need to go each time to a fixed server address as each node in the network can serve data. ICN works on a hop by hop basis where each node have visibility over the content requested enabling it to take more sophisticated decisions in comparison to TCP/IP where the forwarding node take decisions based on the source and destination IP addresses.<7p><p>ICN have different implementations projects with different visions and one of those projects is Named Data Networking (NDN) and that’s what we use for our work. NDN/ICN architecture consists of different layers and one of those layers is the Forwarding Strategy (FS) layer which is responsible for deciding how to forward the coming request/response. In this thesis we implement and simulate three Forwarding Strategies (Best Face Selection, Round Robin, and Weighted Round Robin) and investigate how they can adapt to changes in link bandwidth with variable traffic rate. We performed a number of simulations using the ndnSIMv2.1 simulator. We concluded that Weighted Round Robin offers high throughput and reliability in comparison to the other two strategies. Also, the three strategies offer better reliability than using a single static face and offer lower cost than using the broadcast strategy. We also concluded that there is a need for a dynamic congestion control algorithm that takes into consideration the dynamic nature of ICN. </p>

----------------------------------------------------------------------
In diva2:726393 merged words and missing ligrature, the abstract should be:

<p>A near-field generalization of Friis transmission equation has previously been proposed in the literature. Using this generalization, it is possible to calculate the mutual coupling between two antennas as a weighted integral over the antenna far-fields. In this thesis, a change of variables is used to remove the singularity in the integrand and a normalization of the antenna far-field is suggested to take mismatch and thermal losses into account. The resulting non-singular integral has been implemented in a computer program that can be used to calculate the mutual coupling between two arbitrarily polarized antennas given the antenna far-fields and the geometrical separation between the antennas. The program has several advantages compared to previous programs based on the near-field generalization of Friis transmission equation. Firstly, this program can calculate the mutual coupling between two arbitrarily polarized and oriented antennas whereas previous programs could only be used for linearly polarized and polarization-matched antennas. Secondly, the advantage of the non-singular form is the improved numerical stability. The mutual coupling calculated using this program is demonstrated to agree well with results from full three-dimensional simulations of antennas located in each others near-fields using commercial software. Finally, we investigate for the first time if this integral relation can be used to calculate approximate values of the mutual coupling between antennas on an electrically large vehicle.</p>
----------------------------------------------------------------------
In diva2:1354488 merged words, merged paragraphs, missing emphasis, and missing ligratures, the abstract should be:

<p>To investigate near-field contributions for installed antennas, an in-house code is written to incorporate near-field terms in <em>Shooting and Bouncing Rays</em> (SBR). SBR is a method where rays are launched toward an object and scatter using <em>Geometrical Optics</em> (GO). These rays induce currents on the object, from which the total scattered field can be found.</p><p>To gauge the effect of near-field terms, the in-house code can be set to exclude near-field terms. Due to this characteristic, the method is named <em>SBR Including or Excluding near-field Terms</em> (SIENT). The SIENT implementation is thoroughly described. To make SIENT more flexible, the code works with triangulated meshes of objects. Antennas are represented as near-field sources, allowing complex antennas to be represented by simple surface currents. Further, some implemented optimizations of SIENT are shown.</p><p>To test the implemented method, SIENT is compared to a reference solution and comparable commercial SBR solvers. It is shown that SIENT compares well to the commercial options. Further, it is shown that the inclusion of near-field terms acts as a small correction to the far-field of the installed antenna.</p>

----------------------------------------------------------------------
In diva2:1635607 merged words and paragraphs, the abstract should be:

<p>A modular Recurrent Bayesian Confidence Propagating Neural Networks (BCPNN) with two synaptic time traces is a computational neural network that can serve as a model of biological short term memory. The units in the network are grouped into modules called hypercolumns within which there is a competitive winner-takes-all mechanism.</p><p>In this work, the network’s capacity to store sequential memories is investigated while varying the size of and number of hyperocolumns in the network. The network is trained on sets of temporal sequences where each sequence consist of a set of symbols represented as semi-stable attractor state patterns in the network and evaluated by its ability to later recall the sequences.</p><p>For a given distribution of training sequence the networks’ ability to store and recall sequences was seen to significantly increase with the size of the hypercolumns. As the number of hypercolumns was increased, the storage capacity increased upto a clear level in most cases. After this point it was observed to remain constant and did not improve by adding any more hypercolumns (for a given sequence distribution). The storage capacity was also seen to depend a lot on the distribution of the sequences.</p>
----------------------------------------------------------------------
In diva2:865901 merged paragraphs, the abstract should be:

<p>Openratio offers a mobile platform as a service (mPaaS) to build secure enterprise apps that use legacy systems from vendors such as Microsoft, IBM and SAP. The platform creates native mobile applications and mobile websites using a simple drag and drop interface without writing code.</p><p>Openratio continuously develops their platform in short design cycles, and started a major system upgrade in 2014. The old mobile website was incompatible with the new specifications and upgrading it was time intensive, leading to a 6 months delay behind other system components’ progress.</p><p>This thesis covers the process of analyzing the previous mobile website server and implementing a new mobile website rendering server. The server is developed from scratch using node.js, a javascript platform for developing web applications, and jQuery mobile, an HTML 5 based interface for responsive mobile websites.</p><p>The designed server uses real-time configuration settings to dynamically render each page’s layout, navigation and content with enterprise-grade security. In addition, the server handles user authentication and session management, content fetching from external data sources and uses styling and website templating engines.</p><p>The new implemented server shortened the updating time for each development cycle and decoupling the mobile website rendering server from the main server allowed team members to work on several platform components in parallel.</p>
----------------------------------------------------------------------
In diva2:1375911 missing emphasis, the abstract should be:

<p>Wave surfing is a popular sport that requires minimal financial investment, while it can still be enjoyable from the very first attempt. At the same time, the demand for smart devices that enhance the experience of doing sports by analyzing and evaluating the activities is growing. For surf sport, there are some solutions that are able to collect statistics about activities being done during a surf session, but none of them is able to recognize specific maneuvers that are performed during wave riding.</p><p>The goal of this Master Thesis is to improve a currently existing surf activity monitoring solution by extending it with the ability to identify the two most common surf maneuvers during a wave riding session, namely cutback and snap. The solution is using the user’s smartphone to collect IMU sensor data and feed it to a classification pipeline.</p><p>The implemented algorithm takes raw sensor data as an input, performs various preprocessing steps, splits the input stream into segments, extracts features from these segments and feed them into a hierarchical classification tree. The implemented pipeline is able to classify <em>non-maneuver</em>, <em>cutback</em> and <em>snap segments</em> with 78% accuracy on a self-collected dataset.</p>
----------------------------------------------------------------------
In diva2:1295364 merged words, the abstract should be:

<p>Customized truck is a relevant and high-profit part of Scania’s market. Nowadays designers do not have a self-developed electric control unit for introducing non-standard functionality. This thesis is intended to investigate the specification of a new system and an approach to implement it in current truck’s electrical system. The adopted methods in this research are systems analysis, functional specification, HW &amp; SW research and comparison and validation test of prototype system. The results obtained in this research include the potential functions and corresponding requirement specification, 5 types of hardware alternatives and 2 recommended software platform. Their feasibility is verified by a prototype system with 2 typical functions, control the motor of inward sliding door and combine the communication of different system. 3 recommended schemes and 2 directions for future research are given in the end. Based on investigation results, developers are able to know new system’s specification preliminarily, understand the architectural requirements and suitable tools and materials for implementation. This research will help to improve Scania’s truck’s electrical system and product manufacturing in the future.</p>
----------------------------------------------------------------------
In diva2:606245 merged words, the abstract should be:

<p>The study of dust dynamics in tokamaks has been carried out by means of the DDFTU numerical code solving the coupled equations of motion, charging and heat balance for a dust grain immersed in plasmas with given profiles. The code has been updated to include (i) a non-steady state heat balance model and phase transitions, (ii) geometrical properties of the vessel such as gaps, (iii) realistic boundary conditions for dust-wall collisions. The models for secondary electron emission (SEE), thermionic emission and black body radiation have also been refined, and sensitivity of the results to the SEE strength is demonstrated.</p><p>The DDFTU code has been used for the first time to explore a large range of initial conditions (position, velocity and radius) for dust grains of various tokamak-relevant materials. This study confirmed the impact of the drag force as one of the main factors in dust dynamics and allowed to estimate average lifetimes, to locate preferred sites for dust deposition and to judge the sensitivity to initial conditions. This is a first step towards the use of the code as a predictive tool for devices of importance, such as JET and ITER.</p><p>Preliminary simulations of scenarios relevant for dust injection experiments in TEXTOR have yielded results in remarkable agreement with experimental data.</p><p>These preliminary studies allowed to identify the most crucial issues affecting dust dynamics, lifetime, deposition rate and contribution to impurities, which are to be pursued in future studies.</p>
----------------------------------------------------------------------
In diva2:1332086 merged words, the abstract should be:

<p>The European Space Agency’s Rosetta spacecraft followed the comet 67P/Churyumov-Gerasimenko from August 2014 to September 2016, providing observations of the comet ionosphere at varying heliocentric distances. Measurements from the Rosetta mission have shown a multitude of non-thermal electron distributions in the cometary environment, challenging the previously assumed origin and plasma interaction mechanisms near a cometary nucleus. In this thesis, we discuss electron trapping near a weakly outgassing comet from a fully kinetic (particle-in-cell) perspective which self consistently describe the ambipolar field. Using electromagnetic fields derived from the simulation, we characterize the trajectories of trapped electrons in the potential well surrounding the cometary nucleus and identify the distinguishing features in their respective velocity and pitch angle distributions. In accordance with theoretical findings in space plasma, our analysis allows us to define a clear boundary in velocity phase space between the distributions of trapped and passing electrons.</p>

----------------------------------------------------------------------
In diva2:1453586 merged words and ":" instead of ".", the abstract should be:

<p>Recent advances in 3D printing, with new techniques and materials, are an opportunity for the design of innovative microwave devices, as requirements in electromagnetic discretion and bandwidth are becoming increasingly strict. To this end, an electromagnetic absorber that is omnidirectional, wideband in the super high frequency domain and compact has to be devised. Additionnally, as part of a material by design approach, the ideal properties of future printable materials, dedicated to the design of absorbers, have to be investigated. We design and simulate a hyperbolic metamaterial microwave absorber made of metal-dielectric cavities stacked in a pyramidal fashion, to be fabricated with fused deposition modeling (FDM) materials. It achieves over 90% power absorption in 8.9 – 18.6 GHz for a thickness of 6.3mm. Taking advantage of the flexibility provided by 3D printing, we show that a geometry using stacked parallepipedic cavities with such dimensions that the corresponding absorption peaks are evenly spread in the absorption band yields better results than the existing tapered metamaterial absorbers. Finally, we evidence the need fora high dielectric permittivity to improve performances in oblique incidence. This work opens new perspectives for compact and efficient radiofrequency devices and should drive the development of future materials.</p>
----------------------------------------------------------------------
In diva2:1499100 merged words, the abstract should be:

<p>As the era of Industry 4.0 arises, the number of devices that are connected to a network has increased. The devices continuously generate data that has various information from power consumption to the configuration of the devices. Since the data have the raw information about each local node in the network, the manipulation of the information brings a potential to benefit the network with different methods. However, due to the large amount of non-IID data generated in each node, manual operations to process the data and tune the methods became challenging. To overcome the challenge, there have been attempts to apply automated methods to build accurate machine learning models by a subset of collected data or cluster network nodes by leveraging clustering algorithms and using machine learning models within each cluster. However, the conventional clustering algorithms are imperfect in a distributed and dynamic network due to risk of data privacy, the nondynamic clusters, and the fixed number of clusters. These limitations of the clustering algorithms degrade the performance of the machine learning models because the clusters may become obsolete over time. Therefore, this thesis proposes a three-phase clustering algorithm in dynamic environments by leveraging 1) GAN-based clustering, 2) cluster calibration, and 3) divisive clustering in federated learning. GAN-based clustering preserves data because it eliminates the necessity of sharing raw data in a network to create clusters. Cluster calibration adds dynamics to fixed clusters by continuously updating clusters and benefits methods that manage the network. Moreover, the divisive clustering explores the different number of clusters by iteratively selecting and dividing a cluster into multiple clusters. As a result, we create clusters for dynamic environments and improve the performance of machine learning models within each cluster.</p>
----------------------------------------------------------------------
In diva2:1471509 merged paragraphs, the abstract should be:

<p>An automated AI solution for out-door Telecom equipment segmentation is beneficial to most of the workflow for site survey and engineering performed by human. AI solutions that perform segmentation tasks are today trained with supervised learning which requires manually labeled images. However, labeling images is both time consuming and expensive, which makes semisupervised learning attractive where unlabeled data is used to further improve the performance of models.</p><p>To determine if semi-supervised learning can be used to improve the performance of instance segmentation, the effectiveness of a semi-supervised learning approach called FixMatch was tested for instance segmentation using a custom dataset. The dataset contains 590 labeled and 1000 unlabeled drone-captured images of Telecom equipment. An extension was made to FixMatch where the predicted bounding boxes and masks are augmented like the images, which makes it possible to use FixMatch for instance segmentation. The extension was evaluated with mean Average Precision (mAP) but only achieved 1 point higher mAP than without using the extension. The small improvement in performance shows that this semi-supervised approach is not suitable for instance segmentation of Telecom equipment where the amount of unlabeled data is twice the labeled.</p>


Note "out-door" is in the original, rather than "outdoor".
----------------------------------------------------------------------
In diva2:1230122
   'over-all'
should be
   'overall'
----------------------------------------------------------------------
In diva2:850230 unnecessary hyphens and unnecessary paragraphs, the abstract should be:

<p>Responsive web design is a popular approach to support devices with varying characteristics (viewport size, input mechanisms, media type, etc.) by conditionally style the content of a document by such criteria using CSS media queries. To reduce complexity it is also popular to develop web applications by creating reusable modules. Unfortunately, responsive modules require the user of a module to define the conditional styles since only the user knows the layout of the module. This implies that responsive modules cannot be encapsulated (i.e., that modules cannot perform their task by themselves), which is important for reusability and reduced complexity. This is due to the limitation of CSS media queries that elements can only be conditionally styled by the document root and device properties. In order to create encapsulated responsive modules, elements must be able to be conditionally styled by element property criteria, which is known as element queries.</p><p>Participants of the main international standards organization for the web, the W3C, are interested in solving the problem and possible solutions are being discussed. However, they are still at the initial planning stage so a solution will not be implemented natively in the near future. Additionally, implementing element queries imposes circularity and performance problems, which need to be resolved before writing a standard.</p><p>This thesis presents the issues that element queries impose to layout engines and shows some approaches to overcome the problems. In order to enable developers to create encapsulated responsive modules, while waiting for native support, a third-party element queries JavaScript library named ELQ has been developed. As presented in this thesis, the library provides both performance and usage advantages to other related libraries. An optimized subsystem for detecting resize events of elements has been developed using a leveled batch processor, which is significantly faster than similar systems. As part of the empirical evaluation of the developed library the Bootstrap framework has been altered to use element queries instead of media queries by altering ~50 out of ~8500 lines of style code, which displays one of the advantages of the library.</p>

Note that the Swedish abstract also needs to be corrected:

diva2:850230: <p>Responsiv webbutveckling är ett populärt sätt att stödja enheter med varierande egenskaper (storlek av visninsområdet, inmatningsmekanismer, mediumtyper, etc.) genom att ange olika stilar för ett dokument beroende på enhetens egenskaper med hjälp av CSS media queries. Det är också populärt att utveckla webbapplikationer genom att skapa återanvändbara moduler för minskad komplexitet. Tyvärr kräver responsiva moduler att användaren av en modul definierar de olika responsiva stilarna eftersom endast användaren vet i vilket kontext modulen används. Detta implicerar att responsiva moduler inte är enkapsulerade (alltså att de inte fungerar av sig själva), vilket är viktigt för återanvändning och reduktion av komplexitet. Det beror på CSS media queries begränsningar att det endast går att definiera olika stilar för element beroende på dokumentets rot och enhetens egenskaper. För att kunna skapa enkapsulerade responsiva moduler måste olika stilar kunna definieras för ett element beroende på ett elements egenskaper, vilket är känt som element queries.</p><p>Deltagare av det internationella industrikonsortiet för webbstandardisering, W3C, är intresserade av att lösa problemet och möjliga lösningar diskuteras. De är dock endast i det initiala planeringsstadiet, så det kommer dröja innan en lösning blir implementerad. Dessutom är det problematiskt att implementera element queries eftersom de medför problem gällande cirkularitet samt prestanda, vilket måste lösas innan en standard skapas.</p><p>Denna rapport presenterar de problem för webbläsares renderingsmoterer som element queries medför och visar sätt att övervinna vissa av problemen. För att möjliggöra skapandet av enkapsulerade responsiva moduler, i väntan på webbläsarstöd, har ett tredjepartsbibliotek för element queries namngett ELQ skapats i JavaScript. Biblioteket erbjuder både prestanda- och användningsfördelar jämfört med andra relaterade bibliotek. Ett optimerat delsystem för att detektera förändringar av elements storlekar har utvecklats som använder en nivåuppdelad <em>batch</em>-processerare vilket medför att delsystemet erbjuder signifikant bättre prestanda än relaterade system. Som del av den empiriska utvärderingen har det populära ramverket Bootstrap modifierats att använda element queries istället för media queries genom att ändra ~50 utav ~8500 rader stilkod, vilket visar en av fördelarna med det utvecklade biblioteket.</p>


The title is missing two spaces:
    'ELQ: Extensible Element Queries forModular Responsive WebComponents'
should be:
    'ELQ: Extensible Element Queries for Modular Responsive Web Components'
----------------------------------------------------------------------
In diva2:1095622: 

   peerto-peer
should be:
   peer-to-peer
----------------------------------------------------------------------
In diva2:1620371 merged words and multiple hyphens were missing, the abstract should be:

<p>Building occupancy estimation has become an important topic for sustainable buildings that has attracted more attention during the pandemics. Estimating building occupancy is a considerable problem in computer vision, while computer vision has achieved breakthroughs in recent years. But, machine learning algorithms for computer vision demand large datasets that may contain users’ private information to train reliable models. As privacy issues pose a severe challenge in the field of machine learning, this work aims to develop a privacy-preserved machine learning-based method for people counting using a low-resolution thermal camera with 32 × 24 pixels. The method is applicable for counting people in different scenarios, concretely, counting people in spaces smaller than the field of view (FoV) of the camera, as well as large spaces over the FoV of the camera. In the first scenario, counting people in small spaces, we directly count people within the FoV of the camera by Multiple Object Detection (MOD) techniques. Our MOD method achieves up to 56.8% mean average precision (mAP). In the second scenario, we use Multiple Object Tracking (MOT) techniques to track people entering and exiting the space. We record the number of people who entered and exited, and then calculate the number of people based on the tracking results. The MOT method reaches 47.4% multiple object tracking accuracy (MOTA), 78.2% multiple object tracking precision (MOTP), and 59.6% identification F-Score (IDF1). Apart from the method, we create a novel thermal images dataset containing 1770 thermal images with proper annotation.</p>
----------------------------------------------------------------------
In diva2:1249540 merged words and paragraphs, the abstract should be:

<p>This master thesis aimed to design a permanent magnet assisted synchronous reluctance machine (PMaSynRM) rotor for pump applications which were to be implemented in an existing Induction Machine stator. The machine were to be compared with a similar permanent magnet synchronous machine (PMSM) with similar torque production in terms of cost and performance.</p><p>This thesis goes through the theory of the Synchronous Reluctance Machine and the Permanent Magnet assistance. A rotor was designed by utilizing existing design approaches and simulation of performance by use of finite element analysis. A demagnetization study was conducted on the added permanent magnets in order to investigate the feasiblity of the design.</p><p>The final design of the PMaSynRM was thereafter compared to the equivalent surface-mounted PMSM in terms of performance and cost. The performance parameters was torque production, torque ripple, efficiency and power factor. Due to the lower torque density of the PMaSynRM, for equal torque production the PMSM had a 40% shorter lamination stack than the PMaSynRM.</p><p>The economic evaluation resulted in that when utilizing ferrite magnets in the PMa-SynRM it became slightly cheaper than the PMSM, up to 20%. However, due to the fluctuating prices of NdFeB magnets, there exist breakpoints below which the PMaSynRM is in fact more expensive than the PMSM or where the price reduction of the PMaSynRM is not worth the extra length of the motor. However, it was shown that the PMaSynRM is very insensitive to magnet price fluctuations and thereby proved to be a more secure choice than the PMSM</p>
----------------------------------------------------------------------
In diva2:766162, title is missing a space
   Classify Swedish bank transactions withearly and late fusion techniques
should be:
   Classify Swedish bank transactions with early and late fusion techniques

Missing paragraphs, spaces, and ligratures, the abstract should be:

diva2:766162: <p>Categorising bank transactions to predefined categories are essential for getting a good overview of ones personal finance. Tink provides a mobile app for automatic categorisation of bank transactions. Tink's categorisation approach is a clustering technique with longest prefix match based on merchant.</p><p>This thesis will examine if a machine learning model can learn to classify transactions based on its purchase, what was bought, instead of merchant.</p><p>This thesis classifies bank transactions in a supervised learning setting by exploring early and late fusion schemes on three types of modalities (text, amount, date) found in Swedish bank transactions. Experiments are carried out with Naive Bayes, Support Vector Machines and Decision Trees. The different fusion schemes are compared with no fusion, learned on only one modality, and stacked classification, learning models in a pipe-lined fashion.</p><p>The early fusion concatenation schemes shows all worse performance than no fusion on the text modality. The late fusion experiments on the other hand shows no impact of modality fusion.</p><p>Suggestions are made to change the feedback loop from user, to get more data labeled by users, which would potentially boost the other modalities importance</p>

Note "pipe-lined" is used in the original, although it should be "pipelined".
----------------------------------------------------------------------
In diva2:1713353, missing italics, missing first hyphen in pixel-by-pixel, the degree symbols were centered vertically (rather than the correct position), merged paragraphs, the abstract should be:

<p>Ray tracing comes together with a tremendous computational cost [1]. Therefore, Keller <em>et al.</em>, expressed that possible cost reduction appears when a hybrid rendering pipeline is implemented by combining rasterization and ray tracing, which have already been introduced to the film and game industries. Such a rendering method within Grand Strategy Games (GSG) has been an unexplored task.</p><p>The standard rendering method of GSG has been rasterization. Implementing hybrid rendering for GSG would allow this niche to follow the continuously developing rendering techniques. Therefore, this thesis examined the advantages and disadvantages of hybrid rendering compared to a path traced pipeline. The study measured different camera angles applied to three GSG-inspired scenes by rendering time and quality according to pixel-by-pixel comparison focusing on effects like shadows and reflections. Closeup images have been taken on the rendered scenes to evaluate interesting pieces in the scenes.</p><p>Steady time performance for all angles was the significant advantage of the hybrid pipeline. The angles at lower grades resulted in an increased difference in shadows and reflections for two out of three scenes. Additionally, the entire pixel-by-pixel comparison did not generate more than ten percent difference for any scene and not more than twelve percent difference on closeup images. Still, differences were noticeable to the eye since the path tracer was superior for developing sharp shadows. The hybrid pipeline generated a massive reflection compared to the path tracer. Since the path tracer was defined as the ground truth, this quantity of reflections was not considered positive.</p><p>The thesis concludes that a simple hybrid rendering pipeline could be an exciting future for GSG, especially for angles above 67.25°. Additionally, improving the sharpness of the shadows for the hybrid rendering pipeline could increase the interest in hybrid rendering for GSG even at angles below 67.25°. Some interesting future work is rendering advanced 3D map-based GSG scenes, including more shadows and reflections. Another suggestion is a qualitative analysis of users playing a game with the two rendering pipelines before attending a user study about their possible improved graphical experience and how the game experience has been affected.</p>

Some of the same problems in Swedish abstract, it should be:

<p>Strålspårning kommer tillsammans med en stor beräkningskostnad [1]. Därför har Keller m.fl. uttryckt att kostnaderna kan reduceras genom att implementera en hybridrenderingsmetod baserad på en kombination av rastrering och strålspårning, vilket redan har introducerats till film- och spelindustrin. En sådan renderingsmetod inom Grand Strategy Games (GSG) har dock varit ett outforskat område.</p><p>Standard renderingsmetoden för GSG har varit rastrering. Implementering av hybridrendering för GSG skulle tillåta denna nisch att följa de ständigt utvecklande renderingsteknikerna. Därför undersöker denna avhandling fördelarna och nackdelarna med hybridrendering jämfört med en renderingspipeline baserad på strålspårning. Studien har mätt olika kameravinklar applicerade på GSG-inspirerade scener mätt med renderingstid och kvalitet enligt pixel-för-pixel-jämförelse och med fokus på effekter som skuggor och reflektioner. Närbilder har tagits på de renderade scenerna för att utvärdera intressanta delar i scenerna.</p><p>Stabil tidprestanda av samtliga vinklar var den betydande fördelen med hybridpipelinen. Vinklarna vid lägre grader resulterade i ökad differens av skuggor och reflektioner för två av tre scener. Dessutom resulterade hela pixeljämförelsen inte mer än tio procents skillnad för någon av scenerna och inte mer än tolv procents skillnad på närbilderna. Ändå var skillnaderna märkbara för ögat eftersom strålspårningen var överlägsen för att generera skarpa skuggor. Hybridlösningen genererade en stor andel reflektion jämfört med strålspårningen. Eftersom strålspårningen definierades som målbilden var denna mängd reflektioner inte positiva.</p><p>Avhandlingen drar slutsatsen att en enkel hybridmetod kan vara en spännande framtid för GSG, speciellt för vinklar över 67,25° . Dessutom kan en förbättring av skärpan på skuggorna för hybridrenderingen öka intresset för hybridrendering för GSG även vid vinklar under 67,25° . Intressanta framtida arbeten är rendering av avancerade GSG scener, som inkluderar fler skuggor och reflektioner. Ett till förslag är kvalitativ analys av användare som spelar ett spel med de två renderingsmetoder följt av användarstudie om deras möjliga förbättrade grafiska upplevelse och om spelupplevelsen har drabbats.</p>
----------------------------------------------------------------------
In diva2:1548999 merged words, the abstract should be:

<p>In order to meet the increasing demand of energy in today’s society while at the same time minimizing the environmental impact, renewable energy sources will be required to be integrated into the existing energy mix. Technological advances in high voltage direct current (HVDC) grids play a crucial role in making this possible. Therefore the purpose of this project has been to validate the properties of basic control strategies in terms of how they respond to four different simulation cases. All simulations have been conducted on a simplified version of the CIGR ́E B4 test grid, consisting of four monopolar HVDC converters. After analyzing the results obtained from each control strategy it became evident that provided if the benefits of the redundancy introduced by a multi-terminal grid are to be fully utilized, a distributed voltage control should be used. Moreover, after substituting one of the four internal controllers with an external one, it became clear that simply deciding the droop constants based on results from the simulation model wouldn’t be sufficient for real worl dapplications.</p>
----------------------------------------------------------------------
In diva2:1381398 merged words and merged paragraphs, the abstract should be:

<p>Pitch-shifting lowers or increases the pitch of an audio recording. This technique has been used in recording studios since the 1960s, many Beatles tracks being produced using analog pitch-shifting effects. With the advent of the first digital pitch-shifting hardware in the 1970s, this technique became essential in music production. Nowadays,it is massively used in popular music for pitch correction or other creative purposes. With the improvement of mixing and mastering processes, the recent focus in the audio industry has been placed on the high quality of pitch-shifting tools. As a consequence, current state-of-the-art literature algorithms are often outperformed by the best commercial algorithms. Unfortunately, these commercial algorithms are ”black boxes” which are very complicated to reverse engineer.</p><p>In this master thesis, state-of-the-art pitch-shifting techniques found in the literature are evaluated, attaching great importance to audio quality on musical signals. Time domain and frequency domain methods are studied and tested on a wide range of audio signals. Two offline implementations of the most promising algorithms are proposed with novel features. Pitch Synchronous Overlap and Add (PSOLA), a simple time domain algorithm, is used to create pitch-shifting, formant-shifting, pitch correction and chorus effects on voice and monophonic signals. Phase vocoder, a more complex frequency domain algorithm, is combined with high quality spectral envelope estimation and harmonic-percussive separation to design a polyvalent pitch-shifting and formant-shifting algorithm. Subjective evaluations indicate that the resulting quality is comparable to that of the commercial algorithms.</p>
----------------------------------------------------------------------
In diva2:1130024 merged words, the abstract should be:

<p>A spacecraft needs to simultaneously provide orbital and attitude control but these are in general treated as separate systems. Normally the attitude control is conducted via reaction wheels but can in scenarios with high manoeuvrability demands be handed over to pure thruster control. In specific cases the reaction wheels are removed from the spacecraft to save mass. If both the orbital and attitude control is regulated with thrusters, there is a potential to save fuel in a combined control strategy. Model predictive control has been shown to be a viable method for orbital control with a fuel minimising objective. This thesis investigates a combined orbital and attitude model predictive control strategy. Three test cases are simulated with a specific thruster configuration; maintaining a passive orbit relative to a target, large-angle reorientation and repositioning, and rendezvous. Preliminary results show that including the coupled dynamics lowers the overall fuel consumption while satisfying requirements on position and attitude in scenarios where the timescale of the orbital and attitude control is similar.</p>
----------------------------------------------------------------------
In diva2:1272290 merged words and paragraphs, the abstract should be:

<p>Model predictive control (MPC) is an advanced control technique that requires solving an optimization problem at each sampling instant. Several emerging applications require the use of short sampling times to cope with the fast dynamics of the underlying process. In many cases, these applications also need to be implemented on embedded hardware with limited resources. As a result, the use of model predictive controllers in these application domains remains challenging.</p><p>This work deals with the implementation of an interior point algorithm for use in embedded MPC applications. We propose a modular software design that allows for high solver customization, while still producing compact and fast code. Our interior point method includes an efficient implementation of a novel approach to constraint softening, which has only been tested in high-level languages before. We show that a well conceived low-level implementation of integrated constraint softening adds no significant overhead to the solution time, and hence, constitutes an attractive alternative in embedded MPC solvers.</p>
----------------------------------------------------------------------
In diva2:1354472 merged words, the abstract should be:

<p>Industrial robots are becoming an integral part of the production industry. Efficient operation with respect to fast movements is critical to increase the economic benefits of automating the production line. Facilitating near optimality with regards to time has high computational demands however and multiple frameworks have been suggested to remedy this. In this thesis we consider one of these frameworks, namely the elastic band framework. We investigate how the elastic band time optimal control framework performs regarding computational time for point-to-point movements on a SCARA type robot with three revolute and one prismatic joint. We compare an unconstrained elastic band formulation with a constrained formulation in the open loop, along with simulating performance in the closed-loop. We show that a constrained formulation which considers the sparseness of underlying matrices in the optimization problem has the lowest computational time. Additionally, we show that the unconstrained formulation benefits from early stopping. Finally, we show that a controller implementing this formulation can be used in a model predictive controller, although the computational time is still too high for commercial use on the hardware used in testing.</p>
----------------------------------------------------------------------
In diva2:1140559 merged words and missing ligrature, the abstract should be:

<p>The aim of the project is to design an active safety system for passenger vehicles for mitigating secondary collisions after an initial impact. The control objective is to minimize the lateral deviation from the known original path while achieving a safe heading angle after the initial collision. A hierarchical controller structure is proposed: the higher layer is formulated as a linear time varying model predictive controller that defines the virtual control moment input; the lower layer deploys a rule-based controller that realizes the requested moment. The designed control system is then tested and validated in Simulink as well as in IPG CarMaker, a high delity vehicle dynamics simulator.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-08
======================================================================
In diva2:1472471 merged paragraphs, the abstract should be:

<p>Elevated heart rate is considered to be an indicator of stress. Thus, noticing one’s own heartbeat can have a negative connotation. Yet, the heartbeat is simply a physiological function, neither positive nor negative in itself, that is experienced in diverse contexts, such as medical, athletic, or intimate. This study uses first-person  research through design and soma design to increase awareness of the heartbeat from both an individual and social angle and examines the potential benefits of using external sensory stimuli to convey biofeedback information. It also opens up the design space around the heartbeat and sensory stimuli and reflects upon comfort and relaxation, biofeedback and digital mindfulness, the Sensiks sensory reality pod as a tool and space, and the heartbeat as a spectrum and a way of getting to know people. The study results in four deliverables: a design critique of the Sensiks sensory reality pod, a design fiction publication, a design proposal, and an experience prototype.</p><p>The study proposes the design for the Gallery of Heartbeats – a sensory experience aimed at externalising and sharing the heartbeat of self and others. The Gallery of Heartbeats supports individual reflections, providing the user with real-time numerical, graphical, and auditory biofeedback on their heart rate. It also encourages social communication of this commonly unnoticed physiological feature, allowing users to record and store their heartbeat to an archive and experience the pre-recorded heartbeats of others in a multisensory way.</p><p>The evaluation of the Gallery of Heartbeats prototype shows that the design succeeds in making people more aware of their cardiovascular activity, triggers their curiosity, and increases empathy. However, the Gallery of Heartbeats also makes the users want to control or change their heart rate which goes against the mindfulness principles of presence-in and presence-with the design was inspired by. Sensory stimuli, especially sound and visuals, are assessed as beneficial for creating feelings of immersion, whereas different representations of the biofeedback information have different effects and use cases.</p>

----------------------------------------------------------------------
In diva2:654254 the paragraphs were merged and the text does not match that in tbe thesis, the abstract should be:

<p>When it comes to authenticating a user on the Internet today, the de facto standard is to do so with an alphanumeric password. This method is simple to use both from a user- and from a developers prespective. However there are growing concerns about the security of this scheme.</p><p>There exists researches about alternatives to the alphanumeric scheme, some of these focus on the user's ability to remember and identifying pictures. Such a scheme is reserched and implemented in this thesis and a small study about the useability of a such scheme is presented. The results, though intresting, were to slim to do any statistically secure conclusion.</p>

"to slim" should be "too slim" but "to" is used in the abstract in the thesis.
'prespective' should be 'perspective', but the former is used in the abstract in the thesis.
----------------------------------------------------------------------
In diva2:1823470 merged words, the abstract should be:

<p>As machine learning models affect our lives more strongly every day, developing methods to train these models becomes paramount. In our paper, we focus on the problem of minimizing a sum of functions, which lies at the heart of most - if not all - of these training methods. This problem was formulated in terms of a decentralized consensus optimization, with the terms of the sum belonging to different agents. We examined the efficency and privacy-preserving properties of methods to solve this problem, as well as conducted numerical experiments on several variations of the I-ADMM algorithm. Our results show that utilizing encryption is inefficient compared to PI-ADMM1, while PI-ADMM1 converges at the same speed as I-ADMM.</p>
----------------------------------------------------------------------
In diva2:1835948, the title is missing spaces:
   Membership Inference Attacksagainst GenerativeAdversarial Networks
should be:
   Membership Inference Attacks against Generative Adversarial Networks

In the abstract:
   privacypreserving
should be:
   privacy preserving

----------------------------------------------------------------------
In diva2:654167
   'projet'
should be:
   'project'
This is an error in the original.
----------------------------------------------------------------------
In diva2:1247558
   'projetct'
should be:
   'project'
This is an error in the original.

There should not be a "</p><p>" before MATLAB.
----------------------------------------------------------------------
In diva2:1837771 merged paragraphs and missing emphasis, the abstract should be:

<p>The field of <strong>artificial intelligence</strong> (AI) has grown rapidly in recent years and its applications are becoming more widespread in various fields, including politics. In particular, <em>presidential debates</em> have become a crucial aspect of election campaigns and it is important to analyze the information exchanged in these debates in an objective way to let voters choose without being influenced by biased data. The objective of this project was to create an <strong>automatic analysis tool</strong> for presidential debates using AI.</p><p>The main challenge of the final system was to determine the <em>speaking time</em> of each candidate and to analyze <em>what each candidate said</em>, to detect the <em>topics discussed</em> and to calculate the time spent on each topic. This thesis focus mainly on the <strong>speaker detection</strong> part of this system. In addition, the high <strong>overlap rate</strong> in the debates, where candidates cut each other off, posed a significant challenge for speaker diarization, which aims to determine who speaks when.</p><p>This problem was considered appropriate for a Master’s thesis project, as it involves a combination of advanced techniques in AI and speech processing, making it an important and difficult task. The application to political debates and the accompanying <em>overlapping pathways</em> makes this task both challenging and innovative.</p><p>There are <strong>several ways</strong> to solve the problem of speaker detection. We have implemented <em>classical</em> approaches that involve <strong>segmentation</strong> techniques, <strong>speaker representation</strong> using embeddings such as i-vectors or x-vectors, and <strong>clustering</strong>. Yet, due to speech overlaps, the <em>End-to-end</em> solution was implemented using pyannote-audio (an open-source toolkit written in Python for speaker diarization) and the <strong>diarization error rate</strong> was significantly reduced after refining the model using our own labeled data.</p><p>The results of this project showed that it was possible to create an <strong>automated presidential debate analysis tool</strong> using AI. Specifically, this thesis has established a state of the art of <strong>speaker detection</strong> taking into account the particularities of the politics such as the high speaker <em>overlap rate</em>.</p>

Similiary for the Swedish abstract:

<p><strong>AI-området</strong> (artificiell intelligens) har vuxit snabbt de senaste åren och dess tillämpningar blir alltmer utbredda inom olika områden, inklusive politik. Särskilt <em>presidentdebatter</em> har blivit en viktig aspekt av valkampanjerna och det är viktigt att analysera den information som utbyts i dessa debatter på ett objektivt sätt så att väljarna kan välja utan att påverkas av partiska uppgifter. Målet med detta projekt var att skapa ett <strong>automatiskt analysverktyg</strong> för presidentdebatter med hjälp av AI.</p><p>Den största utmaningen för det slutliga systemet var att bestämma <em>taltid</em> för <em>varje kandidat</em> och att analysera vad varje kandidat sa, att upptäcka <em>diskuterade ämnen</em> och att beräkna den tid som spenderades på varje ämne. Denna avhandling fokuserar huvudsakligen på <strong>detektering av talare</strong> i detta system. Dessutom innebar den höga <strong>överlappningsgraden</strong> i debatterna, där kandidaterna avbröt varandra, en stor utmaning för talardarization, som syftar till att fastställa vem som talar när.</p><p>Detta problem ansågs lämpligt för ett examensarbete, eftersom det omfattar en kombination av avancerade tekniker inom AI och talbehandling, vilket gör det till en viktig och svår uppgift. Tillämpningen på politiska debatter och den åtföljande <em>överlappande vägar</em> gör denna uppgift både utmanande och innovativ.</p><p>Det finns <strong>flera sätt</strong> att lösa problemet med att upptäcka talare. Vi har genomfört <em>klassiska</em> metoder som innefattar <strong>segmentering</strong> tekniker, <strong>representation av talare</strong> med hjälp av inbäddningar som i-vektorer eller x-vektorer och <strong>klustering</strong>. På grund av talöverlappningar implementerades dock <em>Endto-end-lösningen</em> med pyannote-audio (en verktygslåda med öppen källkod skriven i Python för diarisering av talare) och <strong>diariseringsfelprocenten</strong> reducerades avsevärt efter att modellen förfinats med hjälp av våra egna märkta data.</p><p>Resultaten av detta projekt visade att det var möjligt att skapa ett <strong>automatiserat verktyg för analys av presidentdebatten</strong> med hjälp av AI. Specifikt har denna avhandling etablerat en state of the art av <strong>talardetektion</strong> med hänsyn till politikens särdrag såsom den höga <em>överlappningsfrekvensen</em> av talare.</p>

----------------------------------------------------------------------

In diva2:1463843 missing emphasis, the abstract should be:

<p>Creating cloze sentences—contextual fill-in-the-blanks questions—can be time consuming and challenging. While research has been done on automatic question generation, a problem within this area is the high complexity of processing text and constructing relevant questions. <em>word2vec</em> is a new word embedding toolkit, which allows for vectorisation of words. For instance, by creating a vector space from a paragraph, distances between words may be found. This report investigates the use of <em>word2vec</em> in cloze sentence generation by constructing a bare-bones program and comparing the results to MEK questions of the SweSAT. Of 20 survey respondents, 39.6% identified computer generated questions as SweSAT questions. In contrast, 56.8% correctly identified the computer generated questions, and 70% correctly identified the control SweSAT questions. However, due to the low number of candidates partaking in the survey, the results were inconclusive.</p>
----------------------------------------------------------------------
In diva2:1332039 merged words and paragraphs, the abstract should be:

<p>Voltage Impasse Region (VIR) is a phenomenon in power systems whose dynamics are describe by a set of Differential Algebraic Equations (DAE). VIR denotes a state-space area where voltage causality is lost, i.e. the Jacobian of the algebraic part of DAE is singular. In a Time Domain Simulation (TDS) once system trajectories enter VIR, TDS experiences non-convergence of the solution. Then, there is no reason to continue with the simulation. This is why it is important to understand the mechanisms that introduce VIR. It is known that VIR appears in relation to static, non-linear load models. However, it remained unknown what the cumulative effect of several static, non-linear loads would be.</p><p>This master thesis has further expanded the concept of VIR by carrying out a structured study on how the load modelling affects VIR. For this purpose, this thesis proposes a quasi-dynamicm ethodology to map VIR in the relative rotor angle space. The methodology introduces a new discrete index called Voltage Impasse Region Flag (VIR<sub>flag</sub>), which allows to determine if the algebraic equations of DAE are solvable or not and, thus, to locate VIR. A test system is used to test the proposed quasi-dynamic approach.The VIR<sub>flag</sub> was first used to map VIR for various load combinations. Then, the relationship between TDS non-convergence issues and the intersection of a trajectory with VIR is examined to verify the proposed methodology.</p><p>The proposed method has been proved to be efficient in the determination of VIR regardless of the number of non-linear loads in the power system. Among the static exponential load models, the Constant Power (CP) load component has been identified as the one with the largest influence on VIR appearance and shape. The Constant Current (CC) loads induce ”smaller" VIR areas and the Constant Impedance (CI) load can only alter the shape of VIR in the presence of non-linear load models.</p>

Similar changes needed for Swedish abstract:

<p>VIR (Voltage Impasse Regions) är ett fenomen i kraftsystem vars dynamiska förlöp beskrivs av differential-algebraiska ekvationer (DAE). VIR betecknar ett område i tillståndsrummet där går förlorad,dvs Jakobianen av den algebraiska delen av DAE är singulärI tidsdomän-simuleringar (TDS) när en trajektoria träffar VIR, konvergerar TDS inte till en lösning. Då finns ingen anledning att fortsätta med simuleringen. Därför är det viktigt att förstå mekanismerna som introducerar VIR. Det är känt att VIR är relaterade till statiska, icke-linjära lastmodeller. Det var dock okänt vad den kumulativa effekten av flera statiska, icke-linjära belastningar skulle vara.</p><p>Denna uppsats har vidareutvecklat begreppet VIR genom att genomföra en strukturerad studie om hur lastmodellering påverkar VIR. För detta ändamål föreslår denna avhandling en kvasidynamiskmetod för att kartlägga VIR i det relativa rotorvinkelrummet. Metoden introducerar ett nytt diskret index som heter Voltage Impasse Region Flag (VIR<sub>flag</sub>), vilket gör det möjligt att bestämma om den algebraiska delen av DAE är lösbar eller inte och därmed lokalisera VIR. Ett används för att testa det föreslagna kvasi-dynamiska tillvägagångssättet. VIR<sub>flag</sub> användes först för att kartlägga VIR för olika belastningskombinationer. Därefter granskas förhållandet mellan konvergensproblem i TDS och korsningen mellan en trajektoria och VIR för att verifiera den föreslagna metoden.</p><p>Den föreslagna metoden har visat sig vara effektiv vid bestämning av VIR, oberoende av antalet icke-linjära belastningar. Bland de statiska exponentiella belastningsmodellerna har konstanteffekt last(CP) har identifierats som den som har störst inflytande på VIR;s form. Den konstantströmlasten (CC) inducerar mindre "VIR-områden och konstantimpedanslasten (CI) kan endast ändra formen av VIR i närvaro av icke-linjära belastningsmodeller.</p>
----------------------------------------------------------------------
In diva2:820944
   'questionnarie'
should be:
   'questionnaire'

......................................................................
In diva2:958222 merged parapgraphs and missing <li>, the abstract should be:

<p>The older generation now have plenty of time for daily use of their computers. (1) Nordicom´s Mediabarometer from March 2014 shows that among daily users of the Internet, the age group 65-79 look for more information and digital companionship. On the average day in 2014 they spend 101 minutes on the Net, to be compared with 69 minutes in 2011, an increase of 68 %. Statistics available on the web in 2011 showed the 2009 figures relating to the interest of older adults in using (2) Facebook. Only 4 % of the Swedish users were in the interest group 65 plus, and there was no maximum age.</p><p>The study intends to find out how potential Facebook users over 65 on sociala media perceive and use Facebook.<ol>
<li>How, in wich way, does the older generation use Facebook?</li>
<li>With whom and why?</li>
<li>How use they their social interaction?</li>
<li>Covers Facebook user´s daily needs for information and communication on social media?</li>
</ol></p>
<p>To have my questions about Facebook answered, I had a questionnare compiled and laid it out on the Web, hoping for replies from potential users. SeniorNet in Sweden is an association of about 50 clubs across the country with 8 500 members. Through contact with SeniorNet in Stockholm area, who mediated my link to their area members, I received answers to my questions. The link to the questionnarie was available for about a month. Of the 90 induviduals who responded, 60 already used Facebook and 30 were not interested in using Facebook. Of the respondents most were woman; 58 woman (64 %) and 32 men (36 %) between 65 and 90. The most frequent use was watching on pictures and status resports of children and grandchildren, and also chatting with old friends and the family. Being able to get quick information about close relatives abroad in the event of natural disasters and not having to worry about them was very important. Being able to follow development of family members with pictures and status reports for those who were living abroad was also important. Availability and having quick answers from Facebook users was the most positive aspect, according to the study.</p>

If one is to correct the spelling errors in the original, then:
   'questionnarie'
should be:
   'questionnaire'

   'sociala'
should be:
   'social'

   'wich'
should be:
   'which'

----------------------------------------------------------------------
In diva2:1142713 many missing ligatures, missing subscripts, and merged words, the abstract should be:

<p>Huge-scale optimization problems appear in several applications ranging from machine learning over large data sets to distributed model predictive control. Classical optimization algorithms struggle to handle these large-scale computations, and recently, a number of randomized first-order methods that are simple to implement and have small per-iteration cost have been proposed. However, optimal step size selections and corresponding convergence rates of many randomized first-order methods were still unknown. In this thesis, we hence derive convergence rate results for several randomized first-order methods for convex and strongly convex optimization problem, both with and without convex constraints. Furthermore, we have implemented these randomized first-order methods in MATLAB and evaluated their performance on l<sub>2</sub>-regularized least-squares support vector machine (SVM) classication problems. In addition, we have implemented randomized first-order projection methods for constrained convex optimization, derived associated convergence rate bounds, and evaluate the methods on l<sub>2</sub>-regularized least-squares SVM classication problems with Euclidean ball constraints of the weight vector. Based on the implementation experience, we finally discuss how data scaling/normalization and conditioning affect the convergence rates of randomized first-order methods.</p>
----------------------------------------------------------------------
In diva2:806746 the title is misssing a space:
    Environment for Industrial RobotPerformance Evaluation
should be:
    Environment for Industrial Robot Performance Evaluation

The abstract has merged words and paragraphs, the abstract should be:

<p>Industrial robots and robotic systems have already become key components in various industry sectors. After a new industrial robot model is developed, robot manufactures have the responsibility to test it before the new model hits market. In this thesis, several technologies regarding industrial robot test are analysed, and a test environment for industrial robots is developed.</p><p>ISO 9283 is the latest international standard for industrial robot tests. Robot test processes and test principles are described in this thesis. Questions and doubts about the standard are discussed. Laser tracker is one of the most advanced measuring instruments for industrial robot tests. The accuracy of laser tracker is essentially important for test results. Accuracy of laser trackers is analyzed in this thesis. Coordinate system calibration problem is discussed and solutions are shown. Two calibration methods are applied in this thesis for connecting different coordinate systems. Results of different calibration methods are compared. In data analysis, an accuracy verification test of the laser tracker is carried out. A data selection strategy is introduced by analysing raw data sequences.</p><p>To verify functionalities of the test environment, an industrial robot is tested with the test environment. Pose accuracy and pose repeatability test results from the test environment are shown and analysed. Result error sources are discussed. The example test shows that the test environment is able to provide scientific results, which reflect robot performance characteristics.</p>
----------------------------------------------------------------------
In diva2:1078389 mered words, the abstract should be:

<p>FPGA based systems have been heavily used to prototype and test Application Specic Integrated Circuit (ASIC) designs with much lower costs and development time compared to hardwired prototypes. In recent years, thanks to both the latest technology nodes and a change in the architecture of reconfigurable integrated circuits (from traditional Complex Programmable Logic Device (CPLD) to full-CMOS FPGA), FPGAs have become more popular in embedded systems, both as main computation resources and as hardware accelerators. A new era is beginning for FPGA based systems: the partial run-time reconguration of a FPGA is a feature now available in products already on the market and hardware designers and software developers have to exploit this capability. Previous works show that, when designed properly, a system can improve both its power efficiency and its performance taking advantage of a partial run-time reconfigurable architecture. Unfortunately, taking advantage of run-time reconfigurable hardware is very challenging and there are several problems to face: the reconfiguration overhead is not negligible compared to nowadays CPUs performance, the reconfiguration time is not easily predictable, and the software has to be re-though to work with a time-evolving platform.</p><p>This thesis project aims to investigate the performance of a modern run-time reconfigurable SoC (a Xilinx Zynq 7020), focusing on the reconfiguration overhead and its predictability, on the achievable speedup, and the trade-off and limits of this kind of platform. Since it is not always obvious when an application (especially a real-time one) is really able to use at its own advantage a partial run-time reconfigurable platform, the data collected during this project could be a valid help for hardware designers that use reconfigurable computing.</p>

Note that the manuscript has
    're-though'
which should have been
    're-thought'
----------------------------------------------------------------------
In diva2:1779224 merged paragraphs, the abstract should be:

<p>Today, face recognition is becoming increasingly accurate and faster with deep learning methods such as convolutional neural networks (CNNs), and is now widely used in areas such as security and entertainment. Typically, these CNNs are trained using real-face datasets like CASIA-WebFace, which was put together using web-crawling of IMDB. This can, however, lead to privacy and bias issues. Synthetic datasets made up of computer generated pictures, such as DigiFace-1M, created by Microsoft and the University of Cambridge, provide alternatives, offering large volumes of unbiased training data that respect privacy.</p><p>Despite these advances, there’s been little research comparing the performance of models pre-trained on synthetic datasets versus traditional ones. In this study, we address this gap. We tested a ResNet-18 model pre-trained on a subset of real-face images from CASIA-WebFace against one trained on a subset of the synthetic DigiFace-1M dataset. We also compared these results to a model pre-trained on ImageNet, a large, general-purpose, mixed object dataset, and a model without any pre-training. The models were later evaluated on the first 100 identities in CASIA-WebFace.</p><p>The findings showed that while the best performance came from pre-training on real-face datasets, the synthetic dataset also offered a viable option for multiclass face recognition. The synthetic dataset showed slightly better performance than ImageNet and significantly better performance than the model without pre-training, all while avoiding privacy issues linked to web-crawled images. More research is needed to further explore whether classification models pre-trained on larger synthetic datasets like DigiFace-1M can significantly outperform broader datasets like ImageNet, or even improve upon real-face pre-training.</p>
----------------------------------------------------------------------
In diva2:1067587
    'real-time-capability'
should be:
    'real-time capability'

It is correct in the original abstract.
----------------------------------------------------------------------
In diva2:1193593 merged paragraphs and words, the abstract should be:

<p>Over the last years, the growth and development of video on demand (VOD) services has given new possibilities of performing machine learning on large amounts of video history data. A common usage of machine learning for businesses is market segmentation, which is usually addressed with cluster analysis.</p><p>Market segmentation with cluster analysis has been performed for the video streaming service company Viaplay. It was found that K-means with cosine measure performed best of the attempted methods and has been shown to facilitate a useful and interpretable market segmentation based on a set of segment criteria: <em>understandability</em>, <em>homogeneity independence</em>, <em>stability</em> and <em>actionability<em>.</p><p>The thesis also shows an example of how to evaluate clustering of video streaming users. A version of term frequency-inverse document frequency (tf-idf) was introduced, which is called video importance score (VIS). VIS is used to find videos specifically important to a cluster, and has proven to be helpful in interpreting the resulting clusters.</p><p>The results were evaluated within a commonly used market segmentation evaluation framework, which was adapted to the problem at hand. Although the market segmentation strongly indicates to be useful, it still has to be in real-word scenario evaluated by the company before any definitive conclusions can be drawn.</p>

Note that 'real-word' should be 'real-world', but 'real-word' appears in the original abstract.
----------------------------------------------------------------------
In diva2:1039132 and diva2:1597929
Note that 'real-word' should be 'real-world', but 'real-word' appears in the original abstract.
----------------------------------------------------------------------
In diva2:1108935 merged paragraphs, the abstract should be:

<p>In this thesis, the correlation of the code metrics Cyclomatic Complexity, Number of Methods in Class and Depth of Inheritance Tree with the real reasons a codebase got heavily refactored is investigated. Code metrics have long been used in a attempts to indicate the amount of effort needed to change a codebase when new requirements emerge. In the case study we look at a codebase that has been refactored due to the low maintainability, to see to which degree the code metrics imply the problems on which the decision to refactor was based. The goal is to find out whether code metrics can be used as indicators of what should be refactored when you as a consultant join a project with existing quality problems.</p><p>The method used to identify the major problems in the codebase is qualitative interviews and the code metrics values are measured with the tool NDepend. To improve the reproducability and validity of the analysis, the interviews are built around the definition of maintainability presented in ISO/IEC 25010.</p><p>The conclusion is that metrics showing very high values for certain classes or methods often indicate objects involved in major problems in the codebase. In this case study though, the measures seldom indicated objects for the ”right reason”, by which we mean the reason given in the literature. Because of this, the code metric values should not be used as a direct indication of what should be refactored but rather as a direction in which to look while doing a professional assessment. The results are not entirely in line with previous research, where common correlations between code metrics values and maintainability problems have been compiled. This is assumed to be due to the high degree of loose coupling in the studied code.</p>

Note that 'reproducability' should be 'reproducibility', but 'reproducability' has been used the the original abstract.
----------------------------------------------------------------------
In diva2:1259247 merged paragraphs, the abstract should be:

<p>CERN Scalable Analytics Section currently offers shared YARN clusters to its users as monitoring, security and experiment operations. YARN clusters with data in HDFS are difficult to provision, complex to manage and resize. This imposes new data and operational challenges to satisfy future physics data processing requirements. As of 2018, there were over 250 PB of physics data stored in CERN’s mass storage called EOS. Hadoop-XRootD Connector allows to read over network data stored in CERN EOS. CERN’s on-premise private cloud based on OpenStack allows to provision on-demand compute resources. Emergence of technologies as Containers-as-a-Service in Openstack Magnum and support for Kubernetes as native resource scheduler for Apache Spark, give opportunity to increase workflow reproducability on different compute infrastructures with use of containers, reduce operational effort of maintaining computing cluster and increase resource utilization via cloud elastic resource provisioning. This trades-off the operational features with datalocality known from traditional systems as Spark/YARN with data in HDFS.</p><zp>In the proposed architecture of cloud-managed Spark/Kubernetes with data stored in external storage systems as EOS, Ceph S3 or Kafka, physicists and other CERN communities can on-demand spawn and resize Spark/Kubernetes cluster, having fine-grained control of Spark Applications. This work focuses on Kubernetes CRD Operator for idiomatically defining and running Apache Spark applications on Kubernetes, with automated scheduling and on-failure resubmission of long-running applications. Spark Operator was introduced with design principle to allow Spark on Kubernetes to be easy to deploy, scale and maintain with similar usability of Spark/YARN.</p><zp>The analysis of concerns related to non-cluster local persistent storage and memory handling has been performed. The architecture scalability has been evaluated on the use case of sustained workload as physics data reduction, with files in ROOT format being stored in CERN mass-storage called EOS. The series of microbenchmarks has been performed to evaluate the architecture properties compared to state-of-the-art Spark/YARN cluster at CERN. Finally, Spark on Kubernetes workload use-cases have been classified, and possible bottlenecks and requirements identified.</p>

Note that 'reproducability' should be 'reproducibility', but 'reproducability' has been used the the original abstract.
----------------------------------------------------------------------
In diva2:1612469 merged paragraphs, the abstract should be:

<p>The companies’ support phase, as all of business’ functional areas and components, went through a heavy and rapid digitalization which has unlocked the availability of an unprecedented amount of data. Unlike other relevant business areas and components, the support phase seems to have experienced fewer improvements attributable to Data Science and machine learning.</p><p>By focusing on two well-known problems of these two fields, Time Series Analysis and Regression Analysis, this project aims at understanding which techniques are applicable within the support phase and how these can improve the effectiveness and pro-activeness of this area. The goal within this project is to apply them to improve the handling of support tickets, the digital entity used to track issues and requests within support systems. Through the use of Time Series Analysis, we aim at forecasting the volume of tickets to be expected in a near-future time frame. Using Regression Analysis we intend to estimate the resolution time of a newly submitted ticket.</p><p>The results produced by the two tasks were satisfactory. On one hand, the Time Series task produced accurate results and the models could be directly employed and bring some added value to help Elvenite’s support team. On the other hand, while the Regression Analysis results were not as good, they nonetheless proved that the task’s aim is achievable through improvements on both the data used and the models applied. Finally, both tasks successfully showcased how to investigate and evaluate the application of such techniques within the support phase of a business. </p>
----------------------------------------------------------------------
In diva2:1421309 merged paragrpahs and unnecessary hyphens, the abstract should be:

<p>In accordance to the insertion of the regulation "(EU) nr 1227/2011"  (REMIT), responsibilities and duties for participants as well as a People Professionally Arranging Transactions (PPAT) comes to maintain a competitive and fair market.  After recent clarifications of the term PPAT, directives from the Agency for the Cooperation of Energy Regulators (ACER) specify that system operators with responsibilities over a balancing power market should also be considered as PPAT’s. Thus, Svenska kraftnät (Svk) has an obligation to introduce an weekly automated ex post monitoring of the balancing power market to identify potential violations.</p><p>The study proposes a method to detect potential violations on the weekly data. The method is divided into two steps where the first step contains a comparison of two approaches for a screening analysis and the second step is a regression analysis. The two approaches compared in the first step is one where the data are approximated to normal distribution and one non-parametric approach called bootstrap.  The purpose of the first step is to find anomalies in the weekly bids based on the price. The flagged bids are then used in a regression analysis where the actual bids are compared to predicted prices which are generated by using external variables.</p><p>It was concluded that the bidding data that was used could not be approx- imated to normal distribution. The results were more promising for the boot-strap approach in the screening analysis. Further, it was concluded that such a statistical analysis can be applied more efficiently on a market where the submitted bids reflect the companies bidding strategy, that is, by placing bids per regulation object (RO). The market design also had an impact on the regression analysis results.  The R2 value showed that the regression explained the price variations better on a market where the bids were connected to a RO.</p>
----------------------------------------------------------------------
In diva2:919024 unnecessary hyphens, the abstract should be:

<p>The aim of this thesis work is to improve the performance of an already existing information retrieval system that uses relevance feedback for performing query expansion. It is a constant goal to improve this system because the documents that are retrieved are a base for various data analysis tasks. It is therefore important that the precision and recall are high. A user can choose to give relevance feedback when executing a query, meaning the user can mark documents in the search result as relevant or irrelevant and redo the search based on this feedback. The original query will then be expanded based on the user’s feedback. The approach presented in this thesis uses the documents marked as relevant or irrelevant to train a classifier that can classify unknown documents from the search result as either relevant, irrelevant or unknown. The aim is to classify unknown documents and add them to the set of feedback documents that are used for the query expansion. The assumption that this thesis is based on is that the more feedback a user gives, the better the query expansion will perform. The system developed in this thesis is evaluated for the English language. The results in this thesis show that integrating the classifier in the existing system improved the performance in three out of four use cases. The existing system already has a good performance, but small improvements are important. It would therefore be beneficial to integrate it into the existing system. </p>
----------------------------------------------------------------------
In diva2:1247859 merged paragraphs and words, the abstract should be:

<p>The purpose of the thesis work is to investigate methods for closed loop control of hydraulic pressure in transmissions to make them be more precise. This is desirable since it decreases the fuel consumption as well as emissions, and improves the driving performance.</p><p>To be able to study the behaviour of the transmission, a Simulink model is designed with the parts relevant to the problem, and from this a linear model is obtained. Three different controllers are designed and implemented in the Simulink model, to compare and analyze different solutions. The controllers implemented are a PI controller, a PID controller and a LQR controller.</p><p>The results from the simulation with the different controllers show step responses to be able to evaluate their individual performance. The results show that all of the controllers meet the requirements for a step reponse under better conditions, but under worse ones the LQR controller performs best of the three. The LQR controller is therefore the most suitable of the three controllers for this particular problem.</p>
----------------------------------------------------------------------
In diva2:1247868 merged paragraphs and words, the abstract should be:

<p>The consumption has increased drastically over the years, where consumers have high demands on the quality of the products, the time it takes to receive the products and the personalization options. Factories try to scale with the consumers demands by removing human labour and deploying automation devices that can produce products more rapidly and with higher precision.</p><p>Wireless communication in the factories would help to achieve this goal, by enabling mobility as well as reducing cable reconfiguration/troubleshooting and increasing the utilization of the factories resources.</p><p>This report is investigating if it is possible to achieve beneficial wireless communication in a production line, where the evolved Node B scheduler can prioritize important cyclic Real-Time and alarm packets by using machine learning based classification models. This new prioritization technique would allow important factory applications to have high priority and it would make sure that important packets gets served. We found several useful application classification models for factory environments, but demonstrated that the best model may depend on the factory setup. Therefore, the report introduces as well the idea of automated deep learning model construction, which allows for model improvements by time.</p>
----------------------------------------------------------------------
In diva2:852897 merged words, the abstract should be:

<p>Healthcare practices are changing as focus shifts from treating acute illnesses to chronic diseases. The responsibility of managing the treatment has shifted from healthcare providers to the individual in a higher degree. To achieve good treatment the patients need to be empowered so that they understand their condition and can make informed choices throughout their self-care. A research through design approach was used to investigate how to design a personalized empowering application for heart failure patients. Aside from information relating to the condition, the themes of physical activity, dieting and social connectedness were identified as central to address for the empowerment of this group. Patients, partners and healthcare providers contributed with different perspectives throughout the design process. As a result five personas, representing potential users, were developed. Based on the personas and knowledge of the domain, user scenarios in current- and preferred state were constructed in order to guide the design of the empowering application called ‘The Heart Companion’. It is a tablet application catering to the different needs of the personas that also addresses the three themes relevant for empowerment. The purpose of the application is to facilitate better understanding, a feeling of safety and a more active empowered life for the patient. The application enables personalization of the content by providing bookmarking and addresses empowerment of physical activity by enabling various guided exercise sessions, personalized feedback, the possibility of reflection and construction of personalized exercise sessions.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-12
======================================================================
In diva2:1140173 merged paragraphs. merged words, and missing ligatures, the abstract should be:

<p>Learning to control an uncertain system is a problem with a plethora of applications in various engineering fields. In the majority of practical scenarios, one wishes that the learning process terminates quickly and does not violate safety limits on key variables. It is particularly appealing to learn the control policy directly from experiments, since this eliminates the need to first derive an accurate physical model of the system. The main challenge when using such an approach is to ensure safety constraints during the learning process.</p><p>This thesis investigates an approach to safe learning that relies on a partly known state-space model of the system and regards the unknown dynamics as an additive bounded disturbance. Based on an initial conservative disturbance estimate, a safe set and the corresponding safe control are calculated using a Hamilton-Jacobi-Isaacs reachability analysis. Within the computed safe set a variant of the celebrated Q-learning algorithm, which systematically explores the uncertain areas of the state space, is employed to learn a control policy. Whenever the system state hits the boundary of the safe set, a safety-preserving control is applied to bring the system back to safety. The initial disturbance range is updated on-line using Gaussian Process regression based on the measured data. This less conservative disturbance estimate is used to increase the size of the safe set. To the best of our knowledge, this thesis provides the first attempt towards combining these theoretical tools from reinforcement learning and reachability analysis to safe learning.</p><p>We evaluate our approach on an inverted pendulum system. The proposedvalgorithm manages to learn a policy that does not violate the pre-specied safety constraints. We observe that performance is signicantly improved when we incorporate systematic exploration to make sure that an optimal policy is learned everywhere in the safe set. Finally, we outline some promising directions for future research beyond the scope of this thesis.</p>
----------------------------------------------------------------------
In diva2:1884899 missing subscripts and superscripts, the abstract should be:

<p>The formation of hydrogen sulfide (H<sub>2</sub>S) in wastewater and sewage systems poses significant challenges to wastewater infrastructure and public health. A deeper understanding of H<sub>2</sub>S prediction could be beneficial for implementing preventive measures, ultimately reducing the impact of hydrogen sulfide. This paper compares the predictive capabilities of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models for predicting H<sub>2</sub>S levels using operational data from pump stations, including temperature, concentration, water age, and flow. Using statistical measures such as R<sup>2</sup> and various custom metrics, the study found that the CNN model outperformed the LSTM model. The CNN achieved an accuracy of 94.2%, compared to LSTM’s 89.8%, within a defined margin of error. While LSTM effectively captured peak characteristics, it struggled with generalization. Despite CNN’s superior performance, both models showed strong predictive capabilities based solely on operational data, without incorporating chemical features such as oxygen levels, nitrates, or pH. Additionally, the study examined strategic factors essential for integrating H<sub>2</sub>S predictions into enterprise offerings. The findings highlight considerations such as financial, capability, market strategy, and implementation risks.</p>
----------------------------------------------------------------------
In diva2:1524918 merged paragraphs and missing ligratures, the abstract should be:

<p>The space electronics sector is shifting towards the New-Space paradigm, in which traditional space-qualified and expensive components and payloads are replaced by commercial off-the-shelf (COTS) alternatives. This change in mentality is accompanied by the development of inexpensive cubesats, lowering the entry barrier in terms of cost, enabling an increase in scientific research in space. However, also well-established and resourceful spacecraft manufacturers are adopting this trend that allows them to become more competitive in the market.</p><p>Following this trend, Thales Alenia Space is developing R&amp;D activities using COTS components. One example is the SpaceFibre In-Orbit Demonstrator, a digital board integrated in a cubesat payload that aims to test two Intellectual Property blocks implementing the new ECSS standard for high-speed onboard communication.</p><p>This thesis presents the necessary steps that were taken to integrate the firmware for the demonstrator's Field-Programmable Gate Array (FPGA) that constitutes the main processing and control unit for the board. The activity is centered around the development of a Leon3 System-on-Chip in VHDL used to manage the components in the board and test the SpaceFibre technology.</p><p>Moreover, it also addresses the main problem of using COTS components in the space environment: their sensitivity to radiation, that, for a FPGA results in Single-Event Upsets causing the implementation to malfunction, and a potential failure of the mission if they are not addressed. To accomplish the task, a SEU-emulation methodology based in partial reconfiguration and integrating the state of the art techniques is elaborated and applied to test the reliability of the SpaceFibre technology.</p><p>Finally, results show that the mean time between failures of the SpaceFibre Intellectual Property Block using a COTS FPGA is of 170 days for Low Earth Orbit (LEO) and 2278 days for Geostationary Orbit (GEO) if configuration memory scrubbing is included in the design, enabling its usage in short LEO missions for data transmission. Moreover, tailored mitigation techniques based on the information gathered from applying the proposed methodology are presented to improve the figures. </p>
----------------------------------------------------------------------
In diva2:1548984 merged words, merged paragraphs, and some problems with hyphens, the abstract should be: 

<p>Reinforcement learning methods allows self-learning agents to play video- and board games autonomously. This project aims to study the efficiency of the reinforcement learning algorithms Q-learning and deep Q-learning for dynamical multi-agent problems. The goal is to train robots to optimally navigate through a warehouse without colliding.</p><p>A virtual environment was created, in which the learning algorithms were tested by simulating moving agents. The algorithms’ efficiency was evaluated by how fast the agents learned to perform predetermined tasks.</p><p>The results show that Q-learning excels in simple problems with few agents, quickly solving systems with two active agents. Deep Q-learning proved to be better suited for complex systems containing several agents, though cases of sub-optimal movement were still possible. Both algorithms showed great potential for their respective areas however improvements still need to be made for any real-world use.</p>
----------------------------------------------------------------------
In diva2:1635576 merged words and paragraphs, the abstract should be:

<p>Researchers within digital pathology are endeavoring to develop machine-learning tools to support dentists when making a diagnosis. The purpose of this study was to investigate how applying colour normalisation (CN) algorithms on an oral, histopathological dataset would impact both machine-learning models and ensembles of models when classifying cell types.</p><p>The dataset was run through four different CN algorithms by using a stain normalisation toolbox. The now five datasets (1 + 4) were then fed separately into a pipeline to create machine-learning models, specifically convolutional neural networks with EfficientNet architecture. Two different ensembles were studied, one that used all the models and one that used the three models with the highest test accuracy. Each model gave a cell type prediction of each cell. The ensembles super positioned their models’ predictions of the same cell and used the results as their own predictions.</p><p>The models based on datasets created by two of the CN algorithms had a weighted, average accuracy of ca. four percentage points higher than the model based on the unnormalised dataset. Unexpectedly, the models based on the colour-normalised datasets had a larger standard deviation than the model based on the unnormalised dataset. All the models were generally bad at classifying two of the four cell types. Both the ensembles had a weighted, average accuracy of ca. ten percentage points higher than the model based on the unnormalised dataset, as well as a larger standard deviation. The increase in accuracy is significant and could move forward the timeline for when machine-leaning tools can be implemented into dentists’ and pathologists’ workflow.</p>

Similarly for the Swedish abstract:
diva2:1635576: <p>Forskare inom digital patologi strävar efter att utveckla maskininlärnings-verktyg som stödjer tandläkarenär de ställer diagnoser. Syftet med denna studie är att utreda hur tillämpning av färgnormaliserande algoritmer (CN algoritmer) på ett oralt, histopatologiskt dataset påverkar hur både maskininlärningsmodeller och ensembler av modeller klassificerar celltyper.</p><p>Datasetet kördes igenom fyra olika CN algoritmer med hjälpav en färgnormaliserings-verktygslåda. De nu fem dataseten (1 + 4) matades separat in i en ”pipeline” för att skapa maskininlärningsmodeller, specifikt djupa neurala nätverk med EfficientNet arkitektur. Två olika ensembler skapades, en som använde alla modeller och en som endast använde de tre som hade högst noggrannhet på testsettet. Varje modell uppskattade celltypen för varje cell. Ensemblerna superpositionerade deras modellers uppskattningar för varje cell och använde resultaten som sina egna uppskattningar.</p><p>Modellerna som tränats på två av de färgnormaliseraden dataseten ökade i viktad, snitt-noggrannhet med fyra procentenheteri förhållande till modeller tränade på det ursprungliga datasetet. Förvånansvärt nog så ökade även standardavvikelsen hos modeller tränade på de färgnormaliserade dataseten. Alla modeller var generellt dåliga på att klassificera två av de fyra celltyperna. Ensemblen uppnådde en viktad snitt-noggrannhet på ca. tio procentenheter mer än modeller tränade på det ursprungliga datasetet. Noggrannhetens signifikanta ökning kan leda till en tidigare implementering av maskininlärnings-verktyg i tandläkares och patologers arbetsflöde.</p>
----------------------------------------------------------------------
In diva2:912685 merged words and paragraphs, as well as a missing subscript and incorrect handling of combining dieresis, the abstract should be:

<p>This thesis was aimed at studying the existing methods for origin-destination (OD) estimation problem and developing a new algorithm which provides higher promise.</p><p>The performance was evaluated on a simulated data-set for Stockholm city. Data for this study were obtained with the help of G. Flötteröd from Department of Transport Science in KTH. Information minimizing approach and entropy maximizing approach, which are the state-of-art methods in transport field were modified to implement. Several existing algorithms in signal processing field, such as BP/BPDN, OMP and SP, were implemented and analyzed. A recently proposed algorithm calld OMP + was described. Then a more effective method SP<sub>+</sub> with better reconstruction performance in sparse signal processing area was proposed in this report.</p><p>By numerical experiments, it was concluded that the methods in signal processing field could deal with OD estimation problem well. Hopefully this thesis could make a contribution to opening the door to another field and introducing methods of that universe, as well as developing a new algorithm with robust results and small computation cost.</p>
----------------------------------------------------------------------
In diva2:1142920 merged words, the abstract should be:

<p>Transmission of visual data can be quite demanding in terms of energy and bandwidth. Therefore, it is important that all the sensors in a Wireless Visual Sensor Network get a good signal quality for their transmissions when they come online for the first time. The purpose of this report is to design, simulate and evaluate an algorithm that the sensors can use to perform this initialization, i.e. bootstrapping. First the bootstrapping process is modeled as an NP-hard optimization problem. Then, by taking into account what information is directly available to a sensor, the algorithm is designed to be distributed in order to save energy and achieve scalability of the network. To avoid excessive computing times due to the NP-hardness of the problem, the algorithm is designed to approximate the theoretical solution. The algorithm is implemented and tested in a software simulation environment that was built as part of the project. The tests show that the algorithm performs close to optimal for small networks and retains a good approximation ratio for medium to large networks.</p>
----------------------------------------------------------------------
In diva2:1635585 merged words, the abstract should be:

<p>Many problems involving decision making with imperfect information can be modeled as extensive games. One family of state-of-the-art algorithms for computing optimal play in such games is Counterfactual Regret Minimization (CFR). The purpose of this paper is to explore the viability of CFR algorithms on the board game Stratego. We compare different algorithms within the family and evaluate the heuristic method “imperfect recall” for game abstraction. Our experiments show that the Monte-Carlo variant External CFR and use of game tree pruning greatly reduce training time. Further, we show that imperfect recall can reduce the memory requirements with only a minor drop in player performance. These results show that CFR is suitable for strategic decision making. However, solutions to the long computation time in high complexity games need to be explored.</p>

Similar for the Swedish abstract:

diva2:1635585: <p>Många beslutsproblem med dold information kan modelleras som spel på omfattande form. En familj av ledande algoritmer för att beräkna optimal strategi i sådana spelär Counterfactual Regret Minimization (CFR). Syftet med denna rapport är att undersöka effektiviteten för CFR-algoritmer i brädspelet Stratego. Vi jämför olika algoritmer inom familjen och utvärderar den heuristiska metoden “imperfekt minne” för spelabstraktion. Våra experiment visar att Monte-Carlo-varianten External CFR och användning av trimning av spelträd kraftigt minskar träningstiden. Vidare visar vi att imperfekt minne kan minska algoritmens lagringskrav med bara en mindre förlust i spelstyrka. Dessa resultat visar att CFR är lämplig för strategiskt beslutsfattande. Lösningar på den långa beräkningstiden i spel med hög komplexitet måste dock undersökas.</p>
----------------------------------------------------------------------
In diva2:714894 merged words, the abstract should be:

<p>With the increasing share of variable renewable generation, balancing electric power systems could become a major concern for system operators because of their variable and hardly predictable nature. However, gas technologies appear as a solution to provide this flexibility, but the impacts on the gas power system have hardly been investigated.</p><p>In this thesis, consulting reports on the subject matter, regulator suggestions and gas-electricity interaction models in scientific literature are studied and four sources are identified to be used for balancing: linepack, storage facilities, liquefied natural gas and intraday gas supply from adjacent areas. Then, a gas-electricity model for flexibility supply is designed and three case studies are simulated in order to analyze both gas and electric power systems’ behaviors. In these case studies, electricity generation, contribution of gas sources and costs are analysed.</p><p>The study concludes that critical situations on gas market that can occur, e.g. in cases of large variation in the net electricity demand and limited availability of linepack and storage facilities, the need of intraday modulation can exceed the possibilities to provide for it. Then, gas cannot be supplied to power plants during peak periods, and more gas than necessary is used during off-peak periods. The case studies also show that day-ahead forecast errors in variable renewable generation can be handled much easier than variations by the gas system but leads to higher costs.</p>
----------------------------------------------------------------------
In diva2:713473 merged words and missing ligatures, the abstract should be:

<p>Performance of many P2P systems depends on the ability to construct a random overlay network among the nodes. Current state-of-the-art techniques for constructing random overlays have an implicit requirement that any two nodes in the system should always be able to communicate and establish a link between them. However, this is not the case in some of the environments where distributed systems are required to be deployed, e.g, Decentralized Online Social Networks, Wireless networks, or networks with limited connectivity because of NATs/firewalls, etc. In such restricted networks, every node is able to communicate with only a predefined set of nodes and thus, the existing solutions for constructing random overlays are not applicable. In this thesis we propose a gossip based peer sampling service capable of running on top of such restricted networks and producing an on-the-fly random overlay. The service provides every participating node with a set of uniform random nodes from the network, as well as efficient routing paths for reaching those nodes via the restricted network. We perform extensive experiments on four real-world networks and show that the resulting overlays rapidly converge to random overlays. The results also exhibit that the constructed random overlays have self healing behaviour under churn and catastrophic failures.</p>

Note the "e.g," is an error in the original abstract.
----------------------------------------------------------------------
In diva2:1821993 merged words, the abstract should be:

<p>As the world moves away from fossil fuels, microgrids have garnered attention for their ability to aid the deployment of sustainable energy alternatives. Residential microgrid technology has proven to be a useful aid in the design of the power grid of the future. As society moves towards more renewable energy, more sophisticated energy solutions are required. This paper proposes a modeling approach for a residential hybrid AC/DC microgrid using Matlab and Simulink. The system consists of a photovoltaic (PV) system for DC generation, battery storage, and the potential for grid connection to enable reliable and efficient power. The paper discusses the modeling process and performance during various operating conditions. Simulation results showthe system's performance in terms of power generation, and storage as well as the impact of factors such as solar irradiation levels. The proposed model could be a valuable tool for further study and optimization for residential hybrid AC/DC microgrid systems.</p>
----------------------------------------------------------------------
In diva2:1248299 merged words and unnecessary hyphens, the abstract should be:

<p>This thesis attempts to model homeostatic regulation, a behavioural phenomenon ubiquitous in animals, in the domain of reinforcement learning. We specifically look at multi-objective reinforcement learning that can facilitate multi-variate regulation. When multiple objectives are to be handled, the current framework of Multi-objective Reinforcement Learning proves to be unsuitable without information on some preference over the objectives. We therefore model homeostatic regulation as a motivational process, that selectively activates some objectives over others, and implements cognitive control. In doing so, we utilize cognitive control not as behavioural principle, but as a control mechanism that arises as a natural necessity for homeostatic regulation.</p><p>We utilize a recent framework for drive reduction theory of reinforcement learning, and attempt to provide a normative account of arbitration of objectives from drives. We show that a purely reactive agent can face difficulties in achieving this regulation, and would require a persistence-flexibility mechanism. This could be handled effectively in our model by incorporating a progress metric. We attempt to build this model with the intention of acting as a natural extension to the current reinforcement learning framework, while also showing appropriate behavioural properties.</p>
----------------------------------------------------------------------
In diva2:1570300 merged words, the abstract should be:

<p>The strategy game Risk is a very popular board game, requiring little effort to learn but lots of skill to master. The aim of this project is to explore the fortification phase of the game, where the player’s troops are moved between territories. Our method is based on adapting Monte Carlo tree search (MCTS) to Risk. To improve the troop movements, we propose two techniques, hierarchical search and progressive bias. These methods, combined with other extensions of MCTS are then compared against a baseline player of the game. Our results show that hierarchical search improved the MCTS agent’s playing power and the progressive bias have potential to improve the agent but needs further investigation.</p>
----------------------------------------------------------------------
In diva2:1112609 merged words and paragraphs as well as a missing ligature, the abstract should be:

<p>With the rapid growth of data volume, data storage has attracted more and more research interests in recent years. Distributed storage systems play important roles of meeting the demand for data storage in large amounts. That is, data are stored by multiple storage nodes which are connected together with various network topologies. The main merits of such distributed storage are faster response, higher reliability and better scalability. However, due to network failure, link outage or buffer overflow, the updated data might not be received by all storage nodes, resulting in the coexistence of multiple versions of the file in the system. Thus, the major challenge is consistency, which means that the latest version of the file is accessible to any read request. We aim to study multi-version storage and code design in distributed storage systems, where the latest version of the file or a version close to the latest version is recoverable. Moreover, compared to previous studies, higher availability can be achieved in our system model, namely, at least one version of the file can be obtained.</p><p>On the other hand, both storage nodes and links are vulnerable to fail in storage systems. For the sake of reliability demand, the lost data is supposed to be reconstructed. In this thesis, additional storage nodes dedicated to repair (DR storage nodes) are introduced in the repair process. The results show that optimal repair bandwidth with minimal additional storage space can be achieved by introducing a certain number of DR storage nodes. Subsequently, linear combinations are provided to reduce the communication cost of repair where the link cost is high. Last but not the least, we show that the cooperation among surviving nodes and DR storage nodes suffices to complete the repair process successfully even with link failure.</p>
----------------------------------------------------------------------
In diva2:1587035 merged paragraphs and merged words, the abstract should be:

<p>Object detection is a research area within computer vision that consists of both localising and classifying objects in images. The applications of this kind of research in society are many, ranging from facial recognition to self driving cars. Some of these use cases requires the detection of objects in motion and are therefore considered to be in a separate category of object detection, commonly referred to as real time object detection.</p><p>The goal of this thesis is to shed further light on the area of real time object detection by investigating the effectiveness of successful object detection techniques when applied to objects of smaller sizes. More specifically, the task of detecting small objects is described by the community as a difficult problem. This is also an area that has not been extensively researched before and the results could thus be used by the research community at large and/or for real life applications. This paper is a comparative study between the effectiveness of two different deep learning techniques within real time object detection, namely RetinaNet and YOLOv3. The objects used are small characters and digits that are engraved onto ball bearings. Ball bearings have been photographed while traveling on a production line, and a collection of such images are what constitutes the dataset used in this study. The goal is to classify as many characters and digits as possible on each bearing, with as low inference time as possible.</p><p>The two deep learning models were implemented and then evaluated on their performance, measured in terms of precision and average inference time. The evaluation was performed on labeled bearings not previously seen by the two models.</p><p>The results show that RetinaNet vastly outperforms YOLOv3 when it comes to real-time object detection of small objects in terms of mAP@50. However, when it comes to average inference time YOLOv3 performed twice as fast as RetinaNet. In conclusion it can be noted that YOLOv3 struggles when it comes to smaller objects whereas RetinaNet excels in this area. It can also be concluded, from previous research, that an increase in mAP and average inference time is most likely limited by the hardware used during training. The verification of this could be a potential further investigation of this thesis </p>
----------------------------------------------------------------------
In diva2:1632729 merged words and a missing hyphen, the abstract should be:

<p>In recent years, the usage of 3D deep learning techniques has seen a surge, mainly driven by advancements in autonomous driving and medical applications. This thesis investigates the applicability of existing state-of-the-art 3D deep learning network architectures to dense voxel grids from single photon counting 3D LiDAR. This work also examine the choice of loss function as a means of dealing with extreme data imbalance, in order to segment people and vehicles in outdoor forest scenes. Due to data similarities with volumetric medical data, such as computer tomography scans, this thesis investigates if a model for 3D deep learning used for medical applications, the commonly used 3D U-Net, can be used for photon counting data. The results show that segmentation of people and vehicles is possible in this type of data but that performance depends on the segmentation task, light conditions, and the loss function. For people segmentation the final models are able to predict all targets, but with a significant amount of false positives, something that is likely caused by similar LiDAR responses between people and tree trunks. For vehicle detection, the results are more inconsistent and varies greatly between different loss functions as well as the position and orientation of the vehicles. Overall, we consider the 3D U-Net model a successful proof-of-concept regarding the applicability of 3D deep learning techniques to this kind of data.</p>
----------------------------------------------------------------------
In diva2:1142974 merged words, the abstract should be:

<p>Wave power is not an energy source taken advantage of partly due to the lack of effective generators for slow speeds. There is an ongoing project at the Royal Institute of Technology in Sweden where a transversive flux machine specialised for slow speeds is being developed. This paper aims to design the core of this machine to achieve high efficiency and low cost. The basic design is presented along with the approach to the different aspects of the manufacturing. After examining possible losses these have been by passed or minimised using various methods. If this is not done properly, the losses will be too severe forthe machine to prove useful. A study of the results show that a very high efficiency will be achieved, way superior to that of currently existing wave power generators.</p>
----------------------------------------------------------------------
In diva2:1601440 merged words, the abstract should be:

<p>A denoising autoencoder is a type of neural network which excels at removing noise from noisy input data. In this project, a denoising autoencoder is optimized for removing noise from mobile positioning data. The mobile positioning data with noise is generated specifically for this project. In order to generate realistic noise, a study in how real world noise looks like is carried out. The project aims to answer the question: can a denoising autoencoder be used to remove noise from mobile positioning data? The results show that using this method can effectively cut the noise in half. In this report it is mainly analyzed how the amount of hidden layers and respective sizes affected the performance. It was concluded that the most optimal design forthe autoencoder was a single hidden layer model with multiple more nodes in the hidden layer than the input and output layer.</p>
----------------------------------------------------------------------
In diva2:1470639  'single-viewDI'
should be:
'single-view DI'
----------------------------------------------------------------------
In diva2:1871789  'singlestain'
should be:
'single-stain'
----------------------------------------------------------------------
In diva2:874367 merged paragraphs and words, the abstract should be:

<p>Articulated Funiculator is a new and innovative concept developed by Tyréns for achieving a more efficient vertical transportation with a higher space utilization. Having a variety of merits, i.e.: simple construction, direct electromagnetic thrust propulsion, and high safety and reliability in contrast to rotary induction motor, linear induction motor (LIM) is considered to be one of the cases as the propulsion system for Articulated Funiculator. The thesis is then carried out with the purpose of determining the feasibility of this study case by designing the LIMs meeting some specific requirements. The detailed requirements include: a set of identical LIMs are required to jointly produce the thrust that is sufficient to vertically raise the moving system up to 2 m/s<sup>2</sup>; the size of the LIMs cannot exceed the specification of the funiculator; the maximum flux density in the air gap for each LIM is kept slightly below 0.6 T; no iron saturation of any part of the LIMs is allowed.</p><p>In this thesis report, an introduction of LIM is firstly presented. Following the introduction, relevant literature has been reviewed for a strengthened theoretical fundamentals and a better understanding of LIM’s history and applications. A general classification of LIMs is subsequently introduced. In addtion, an analytical model of the single-sided linear induction motor (SLIM) has been built based on an approximate equivalent circuit, and the preliminary geometry of the SLIM is thereby obtained. In order to acquire a more comprehensive understanding of the machine characteristics and a more precise SLIM design, a two-dimensional finite element method (2D-FEM) analysis is performed initially according to the preliminary geometry. The results, unfortunately, turn out to be iron severely saturated in the teeth and yoke, and a excessive maximum value of air-gap flux density. Specific to the problems, different parameters of the SLIM are marginally adjusted and a series of design scenarios are run in Flux2D for 8-pole and 6-pole SLIM. The comparisons between the results are conducted and the final solution is lastly chosen among them.</p>
----------------------------------------------------------------------
In diva2:1458904 unnecessary "- ", the abstract should be:

<p>Contract management is a sensitive and crucial task for companies. Contracts and other legal documents have to be fully searched and understood to define the obligations of the companies towards other actors in the company business. It is common for such companies to hire lawyers to analyze their contracts for them. These lawyers spend tremendous amounts of time doing tedious and repetitive tasks in order to extract all of the relevant information from the documents such as relationships between different entities. To address these issues, Hyperlex offers a software solution based on Natural Language Processing (NLP).</p><p>Relation Extraction is an active area in NLP research which aims to extract and classify the relationship between two entities. Legal documents contain many such entities and relationships which are specific to the judicial domain and provide capital information about obligation definitions. This master thesis proposes novel approaches to deal with these issues and improve Hyperlex’s relation extraction and classification tools.</p>
----------------------------------------------------------------------
In diva2:1214412 unnecessary "- ", the abstract should be:

<p>Sentiment analysis is the process of letting a computer guess the sentiment of someone towards something based on a text. This can among other things be useful in marketing, for example in the case of the computer figuring out that a certain person likes a certain product it can present ads for similar products to the person. Sentiment analysis in social media is when the texts analyzed are from a social media context like comments or posts on Twitter, Facebook, etc. One problematic aspect of these texts is sarcasm. People tend to be sarcastic very often in social media, with sarcasm being something that can be hard to detect even for a human this does cause problems for the computer. This study was conducted with the intention of investigating how sarcasm detection can be performed in social media texts with the help of machine learning. For this purpose Google’s machine learning framework for Python, TensorFlow, was utilized. The machine learning model created was a deep neural network with two hidden layers containing ten nodes each. As for the input a dataset of 4692 texts were used with a 80/20 training/testing split. For preprocessing the texts into a more suitable form for TensorFlow the methods Bag of Words, Bigrams and a naive method here refered to as Char for Char were considered. However due to time constraints proper results from the more advanced approaches (Bigrams and Bag of Words) were not achieved. It was at least found that the rather simple approach was better than expected, with results notably better than 50% that would be highly unlikely to achieve through sheer luck.</p>
----------------------------------------------------------------------
In diva2:1499098 unnecessary "- ", the abstract should be:

<p>One of the biggest challenges of the automotive industry at the moment is the idea of autonomous vehicles and the huge amount of data that they require due to the main technology they use, Deep Learning. Often, collecting enough data is very expensive and time-consuming, causing the industry to start adopting technologies such as Scenario Cloning, where previously recorded sequences are used to digitally reconstruct the scenario. At its time, within this field, one of the most relevant tasks is Simultaneous Localization and Mapping. This thesis presents a series of improvements based on Deep Learning that can be introduced in current feature-based Visual Simultaneous Localization and Mapping systems to overcome some of the most recurrent problems, such as dealing with highly dynamic environments. The main focus of the thesis is to take an existing state-of-the-art Visual Simultaneous Localization and Mapping method and combine it with Deep Learning-based semantic segmentation. The resulting system successfully avoids placing features on dynamic objects and other regions that tend to decrease the performance of the system, thus improving substantially the overall performance on dynamic environments. Additionally, the system uses the information provided by the Deep Learning model to assign semantic information to each of the points forming the sparse map, resulting in a more complete tool and opening the door for new opportunities in tasks such as obstacle avoidance or planning.</p>
----------------------------------------------------------------------
In diva2:1699284 missing superscripts, the abstract should be:

<p>Unique layered structure with excellent electrical, mechanical, thermal, and optical properties gives graphene widespread application. Graphene based materials are extensively studied in the field of energy storage such as batteries, hydrogen storage and supercapacitors (SC’s). High surface area, electrical conductivity and mechanical flexibility are notable properties for the materials used in energy conversion systems. Porous spaced graphene oxide (PGO) structures were synthesized by hydrothermal and solvothermal reaction between GO and various pillaring molecules include Tetrakis (4-aminophenyl) methane (TKAm), Ethylenediamine (EDA), 2-Amino-5-diethylaminopentane (ADAP) and 2-Aminoethyl trimethylammonium chloride hydrochloride (ATA). Pristine GO shows interlayer distance of 7.2 Å. Characterisation techniques such as XRD, SEM, FTIR, BET and TGA were used understand the properties of these PGO. In contrast, these pillared structures show interlayer distance greater than of the pristine GO. Notably, GO/TKAm show interlayer distance of 14.30 Å. These pillared structures are considered to solve the restacking and aggregation issues found in 2D porous structures. Since these pillaring molecules help to achieve 3D porous network. Pristine GO shows only surface area of 14 m<sup>2</sup>/g whereas these materials also show excellent surface area as well. GO/TKAm shows high surface area of 450 m<sup>2</sup>/g. Followed it GO/ATA shows surface area of 106 m<sup>2</sup>/g. GO/pillared structures show low sheet resistance which means good electrical conductivity. Ultimately, these pillared structures not only solve the issues in 2D porous systems but also improve the surface area, mechanical stability, and electrical conductivity of those systems by means of 3D porous interconnected structures. All these excellent properties make them a great candidate for the energy conversion systems.</p>
----------------------------------------------------------------------
In diva2:742569
   'start-upsoperating'
should be:
   'start-ups operating'
----------------------------------------------------------------------
In diva2:1479310 merged paragraphs and words, the abstract should be:

<p>Massive multiple-input and multiple-output (MIMO) is a method to improve the performance of wireless communication systems by having a large number of antennas at both the transmitter and the receiver. In the fifth-generation (5G) mobile communication system, Massive MIMO is a key technology to face the increasing number of mobile users and satisfy user demands. At the same time, recovering the transmitted information in a massive MIMO uplink receiver requires more computational complexity when the number of transmitters increases. Indeed, the optimal maximum likelihood (ML) detector has a complexity exponentially increasing with the number of transmitters. Therefore, one of the main challenges in the field is to find the best sub-optimal MIMO detection algorithm according to the performance/complexity tradeoff. In this work, all the algorithms are empirically evaluated for large MIMO systems and higher-order modulations.</p><p>Firstly, we show how MIMO detection can be represented by a Markov Random Field (MRF) and addressed by the loopy belief propagation (LBP) algorithm to approximately solve the equivalent MAP (maximum a posteriori) inference problem. Then, we propose a novel algorithm (BP-MMSE) that starts from the minimum mean square error (MMSE) solution and updates the prior in each iteration with the LBP belief. To avoid the complexity of computing MMSE, we use Graph Neural Networks (GNNs) to learn a message-passing algorithm that solves the inference task on the same graph.</p><p>To further reduce the complexity of message-passing algorithms, we recall how in the large system limit, approximate message passing (AMP), a low complexity iterative algorithm, can be derived from LBP to solve MIMO detection for i.i.d. Gaussian channels. Then, we show numerically how AMP with damping (DAMP) can be robust to low/medium correlation among the channels. To conclude, we propose a low complexity deep neural iterative scheme (Pseudo-MMNet) for solving MIMO detection in the presence of highly correlated channels at the expense of online training for each channel realization. Pseudo-MMNet is based on MMNet algorithm presented in [24] (in turn based on AMP) and it significantly reduces the online training complexity that makes MMNet far from realistic implementations.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-16
======================================================================
In diva2:824967 merged words, the abstract should be:

<p>In this report we investigate the technique of stereoscopic 3D. This report investigates the steps needed to create a game adapted for an improved stereoscopic 3D effect. Furthermore we investigate what improvements one should make to avoid the beholder to experience any discomfort due to the effect. The report talks about technical aspects one needs to consider when using stereoscopic 3D, as well as performance issues we might need to take into consideration. The process of developing the prototype of the game Kodo using anaglyph stereoscopic 3D and OpenGL is described in this report. The prototype was then used for testing and analyzing the stereoscopic 3D effects.</p>
----------------------------------------------------------------------
In diva2:1548792 merged words, the abstract should be:

<p>Many machine learning approaches have been used for financial forecasting to estimate stock trends in the future. The focus of this project is to implement a Support Vector Machine with price and news analysis for companies within the technology sector as inputs to predict if the price of the stock is going to rise or fall in the coming days and to observe the impact on the prediction accuracy by adding news to the technical analysis. The price analysis is compiled of 9 different financial indicators used to indicate changes in price, and the news analysis uses the bag-of-words method to rate headlines as positive or negative. There is a slight indication of the news improving the results if the validation data is randomly sampled the testing accuracy increases. When testing on the last fifth of the data of each company, there was only a small difference in the results when adding news to the calculation and such no clear correlation can be seen. The resulting program has a mean and median testing accuracy over 50 % for almost all settings. Complications when using SVM for the purpose of price forecasting in the stock market is also discussed.</p>
----------------------------------------------------------------------
In diva2:753928 the title is missing a space:
"Mobile traffic dataset comparisons throughcluster analysis of radio network event sequences"
should be
"Mobile traffic dataset comparisons through cluster analysis of radio network event sequences"

----------------------------------------------------------------------
In diva2:1586062 the title is missing a space:
"Penetration Testinga Saia Unit: A Control System for Water, Ventilation, and Heating in Smart Buildings"
should be
"Penetration Testing a Saia Unit: A Control System for Water, Ventilation, and Heating in Smart Buildings"
----------------------------------------------------------------------
In diva2:677075 space missing in title:
"social´sschedule"
should be
"social´s schedule"
----------------------------------------------------------------------
In diva2:883329 unnecessary "- ", the abstract should be:

<p>In this thesis, the possibility to use event correlation techniques in the C2 monitor systems is analyzed. C2 is a monitoring software built by Ericsson Broadcast &amp; Media Services for their television broadcasting center in Stockholm. C2 enables the operators in playout to easily monitor events and alarms generated by devices responsible for producing each channel.</p><p>Previously archived events that are saved in the database was analyzed to achieve an understanding regarding what events that can occur within the system. C2 utilizes a compression correlation technique for events to limit the number of records in the database. This means however that information regarding the time aspect for each event often is lost which made the problematic to draw any conclusions from the data.</p><p>The most common correlations techniques today are compared and analyzed in comparison to the requirements set up for an implementation into C2. The dependency graph technique was chosen as the most promising technique. A prototype with a depth first search and a limit on depth level was then tested. The results indicate that this technique was not efficient enough to correlate all events in real time. The main reason for this is because C2 is built on Node.js, which is an single threaded framework. When events can occur within a few milliseconds between each other, extensive computations would make it impossible to handle new events quickly enough.</p><p>After discussions with operators a none real-time correlation implementation was done. This makes it possible for them to quickly find devices with corresponding events that are masked out and hidden from the current view. </p>
----------------------------------------------------------------------
In diva2:643020 an unnecessary "- " and an empty paragraph, the abstract should be:

<p>This report presents a layer-optimized streaming technique for delivering video content over the Internet using quality-scalable motion-compensated orthogonal video. We use Motion-Compensated Orthogonal Transforms (MCOT) to remove temporal and spatial redundancy. The resulting subbands are quantized and entropy coded by Embedded Block Coding with Optimized Truncations (EBCOT). Therefore, we are able to encode the input video into multiple quality layers with sequential decoding dependency. A layer-distortion model is constructed to measure the trade-off between expected streaming layer and expected distortion. Due to the sequential dependency among streaming layers, we build a cost function with concave properties. With that, we develop a fast algorithm to find the optimal transmission policy at low computational complexity. The experiments demonstrate the advantages of expected distortion and computational complexity for challenging streaming scenarios.</p>
----------------------------------------------------------------------
In diva2:1094877 unnecessary "-", the abstract should be:

<p>This Master's Thesis focuses on the recent Cortical Learning Algorithm (CLA), designed for temporal anomaly detection. It is here applied to the problem of anomaly detection in user behavior of web services, which is getting more and more important in a network security context.</p><p>CLA is here compared to more traditional state-of-the-art algorithms of anomaly detection: Hidden Markov Models (HMMs) and t-stide (an N-gram-based anomaly detector), which are among the few algorithms compatible with the online processing constraint of this problem.</p><p>It is observed that on the synthetic dataset used for this comparison, CLA performs significantly better than the other two algorithms in terms of precision of the detection. The two other algorithms don't seem to be able to handle this task at all. It appears that this anomaly detection problem (outlier detection in short sequences over a large alphabet) is considerably different from what has been extensively studied up to now.</p>
----------------------------------------------------------------------
In diva2:1548799 merged words and an unnecessary hyphen, the abstract should be:

<p>Using the powerful methods developed in the field of reinforcement learning requires an understanding of the advantages and drawbacks of different methods as well as the effects of the different adjustable parameters. This paper highlights the differences in performance and applicability between three different Q-learning methods: Q-table, deep Q-network and double deep Q-network where Q refers to the value assigned to a given state-action pair. The performance of these algorithms is evaluated on the two OpenAI gym environments MountainCar-v0 and CartPole-v0. The implementations are done in Python using the Tensorflow toolkit with Keras. The results show that the Q-table was the best to use in the Mountain car environment because it was the easiest to implement and was much faster to compute, but it was also shown that the network methods required far less training data. No significant difference in performance was found between the deep Q-network and the double deep Q-network. In the end, there is a trade-off between the number of episodes required and the computation time for each episode. The network parameters were also harder to tune since much more time was needed to compute and visualize the result.</p>

In the Swedish abstract "Q- network" should be "Q-network".
----------------------------------------------------------------------
In diva2:1129343 many missing ligatures, the abstract should be:

<p>The aim of this thesis was to systematically develop a tool used for the visualization of real-time flight data during suborbital mission, on behalf of the Swedish Space Corporation. By focusing on requirements, restrictions, and trade-offs regarding functionality, communications technologies, geospatial visualization platforms, and many other aspects, an interactive product visualizing the position, attitude, speed, G-loads and angular rates of the rockets could be developed, ensuring a visualization as close to real-time as possible. The resulting product was tested during the flight of the MAXUS 9 rocket, Europe's largest sounding rocket, and was subsequently refined based on the results of that flight.</p>
----------------------------------------------------------------------
In diva2:815221 missing ligratures and unnecessary paragraph divisions, the abstract should be:

<p>The aim of this thesis is to investigate the possibility to improve the separation of <em>HCl</em> and <em>SO<sub>2</sub></em> in the dry flue-gas treatment for boiler #3 at Fortum's thermal power plant in Hogdalen; by using a model predictive control instead of a PID controller to govern the slaked lime injection.</p><p>To achieve this an ARMAX model was derived using MATLAB's System Identification Toolbox and measurements of the incoming and outgoing levels of <em>HCl</em>, <em>SO<sub>2</sub></em> and the speed of the injection motor. The ARMAX model was then converted to a state space model which will be used as the internal model for the MPC predictions. The cost function was a quadratic problem which included the error between the output and the set points, the change rate of the input and the inputs deviation from a default value. The MPC uses both a feedforward and a feedback loop to estimate the error over the prediction horizon. The controller also utilizes the ability to set constraints and tuning of the cost function weights.</p><p>In conclusion, the thesis shows that a MPC controlled lime injection is possible and would offer some unique possibilities such as: natural constraints handling, more intuitive live tuning for the operator and prioritized input control. However the dry scrubber still struggles to suppress high amounts of incoming <em>SO<sub>2</sub></em> and since the project lacked a measuring unit for incoming <em>HCl<z/em> concentration the results showing an improvements in pollution separation was not conclusive.</p>
----------------------------------------------------------------------
In diva2:1046438 some unnecessary hyphen, the abstract should be:

<p>End-to-end encryption is becoming a standard feature in popular mobile chat applications (apps) with millions of users. In the two years a number of leading chat apps have added end-end encryption features including LINE, KakaoTalk, Viber, Facebook Messenger, and WhatsApp.</p><p>However, most of these apps are closed-source and there is little to no independent veriﬁcation of their end-to-end encryption system design. These implementations may be a major concern as proprietary chat apps may make use of non-standard cryptographic algorithms that may not follow cryptography and security best practices. In addition, governments authorities may force chat app providers to add easily decryptable export-grade cryptography to their products. Further, mainstream apps have a large attack surface as they oﬀer a variety of features. As a result, there may be software vulnerabilities that could be exploited by an attacker in order to compromise user’s end-to-end privacy. Another problem is that, despite being closed-source software, providers often market their apps as being so secure that even the provider is not able to decrypt messages. These marketing claims may be potentially misleading as most users do not have the technical knowledge to verify them.</p><p>In this Master’s thesis we use KakaoTalk – the most popular chat app in South Korea – as a case study to perform a security and privacy assessment and audit of its “Secure Chat” opt-in end-to-end encryption feature. Also, we examine KakaoTalk’s Terms of Service policies to verify claims such as “[. . . ] Kakao’s server is unable to decrypt the encryption [. . . ]” from a technical perspective.</p><p>The main goal of this work is to show how various issues in a product can add up to the potential for serious attack vectors against end-to-end privacy despite there being multiple layers of security. In particular, we show how a central public-key directory server makes the end-to-end encryption system vulnerable to well-known operator-site man-in-the-middle attacks. While this naive attack may seem obvious, we argue that (KakaoTalk) users should know about the strength and weaknesses of a particular design in order to make an informed decision whether to trust the security of a chat app or not.</p>

----------------------------------------------------------------------
*** correction there was also a space missing before "e.g."

In diva2:1368334 there was an "ff" ligature, a missing italics. The abstract should be:

 <p>In the last years, the demands on different models using deep learning to generate textual data conditionally have increased, where one would like to control what textual data to generate from a deep learning model. For this purpose, a couple of models have been developed and achieved state-of-art performance in the ﬁeld of generating textual data conditionally. Therefore, the purpose of this study was to develop a new model that could outperform the relevant baseline models with respect to the BLEU metric. The alternative model combined some of the properties from the state-of-art models and was given the name the <em>Variational Attribute-to-Sequence decoder model</em> (shortened to the V-Att2Seq model) that paraphrases the name of one of the state-of-art models and "variational" refers to its application of variational recurrent autoencoders (VRAE). The data set used in this study contained drug reviews that were written by patients to express their opinion about the drug that they have used to treat a certain condition. The drug review texts were accompanied by the following attributes: the (name of the) drug, the condition, and the rating that the patient has given to the drug. The results in this study show that the V-Att2Seq model did not outperform all the baseline models, which concluded that the V-Att2Seq model did not satisfy the requirements imposed on the model itself. However, there are some future work that is suggested by this study to hopefully improve the performance of the V-Att2Seq model in the future such as including other mechanisms that are present in the state-of-art models, testing with e.g. other sizes and settings of the V-Att2Seq model, and testing different strategies forgenerating sequences since there is still potential that has been observed in the model that should be further investigated to improve its performance.</p>
----------------------------------------------------------------------
In diva2:1381341 merged words and paragraphs, the abstract should be:

<p>Combined Cycle Power Plant (CCPP)s play a key role in modern power system due to their lesser investment cost, lower project execution time, and higher operational flexibility compared to other conventional generating assets. The nature of generation system is changing with ever increasing penetration of the renewable energy resources. What was once a clearly defined generation, transmission, and distribution flow is shifting towards fluctuating distribution generation. Because of variation in energy production from the renewable energy resources, CCPP are increasingly required to vary their load levels to keep balance between supply and demand within the system. CCPP are facing more number of start cycles. This induces more stress on the gas turbine and as a result, maintenance intervals are affected.</p><p>The aim of this master thesis project is to develop a dispatch algorithm for the short-term operation planning for a combined cycle power plant which also includes the long-term constraints. The long-term constraints govern the maintenance interval of the gas turbines. These long-term constraints are defined over number of Equivalent Operating Hours (EOH) and Equivalent Operating Cycles (EOC) for the Gas Turbine (GT) under consideration. CCPP is operating in the open electricity market. It consists of two SGT-800 GT and one SST-600 Steam Turbine (ST). The primary goal of this thesis is to maximize the overall profit of CCPP under consideration. The secondary goal of this thesis it to develop the meta models to estimate consumed EOH and EOC during the planning period.</p><p>Siemens Industrial Turbo-machinery AB (SIT AB) has installed sensors that collects the data from the GT. Machine learning techniques have been applied to sensor data from the plant to construct Input-Output (I/O) curves to estimate heat input and exhaust heat. Results show potential saving in the fuel consumption for the limit on Cumulative Equivalent Operating Hours (CEOH) and Cumulative Equivalent Operating Cycles (CEOC) for the planning period. However, it also highlighted some crucial areas of improvement before this economic dispatch algorithm can be commercialized.</p>
----------------------------------------------------------------------
In diva2:1249005 merged words and paragraphs, the abstract should be:

<p>This paper focuses on improving the accuracy of detecting on-road objects, including cars, trucks, pedestrians, and cyclists. To meet the requirements of the embedded vision system and maintain a high speed of detection in the advanced driving assistance system (ADAS) domain, the neural network model is designed based on single channel images as input from a monocular camera.</p><p>In the past few decades, forward collision avoidance system, a sub-system of ADAS, has been widely adopted in vehicular safety systems for its great contribution in reducing accidents. Deep neural networks, as the the-state-of-art object detection techniques, can be achieved in this embedded vision system with efficient computation on FPGA and high inference speed. Aimed at detecting on-road objects at a high accuracy, this paper applies an advanced end-to-end neural network, single-shot multi-box detector (SSD).</p><p>In this thesis work, several experiments are carried out on how to enhance the accuracy performance of SSD models with grayscale input. By adding proper extra default boxes in high-layer feature maps and adjust the entire scale range, the detection AP over all classes has been efficiently improved around 20%, with the mAP of SSD300 model increased from 45.1% to initially 76.8% and the mAPof SSD512 model increased from 58.5% to 78.8% on KITTI dataset. Besides, it has been verified that without color information, the model performance will not degrade in both speed and accuracy. Experimental results were evaluated using Nvidia Tesla P100 GPU on KITTI Vision Benchmark Suite, Udacity annotated dataset and a short video recorded on one street in Stockholm.</p>
----------------------------------------------------------------------
In diva2:1609020 an unnecessary hyphen, the abstract should be:

<p>This thesis investigates whether Computer Vision can be a useful tool in interpreting the behaviors of monitored horses. In recent years, research in the field of Computer Vision has primarily focused on people, where pose estimation and action recognition are popular research areas. The thesis presents a pose classification network, where input features are described by estimated 2D keypoints of horse body parts. The network output classifies three poses: ’Head above the wither’, ’Head aligned with the wither’ and ’Head below the wither’. The 2D reconstructions of keypoints are obtained using DeepLabCut applied to raw video surveillance data of a single horse. The estimated keypoints are then fed into a Multi-layer preceptron, which is trained to classify the mentioned classes. The network shows promising results with good performance. We found label noise when we spot-checked random samples of predicted poses and comparing them to the ground truth, as some of the labeled data consisted of false ground truth samples. Despite this fact, the conclusion is that satisfactory results are achieved with our method. Particularly, the keypoint estimates were sufficient enough for these poses for the model to succeed to classify a hold-out set of poses. </p>
----------------------------------------------------------------------
In diva2:1723072 merged words, the abstract should be:

<p>This report presents an application of reinforcement learning to the problem of controlling multiple robots performing the task of moving boxes in a warehouse environment. The robots make autonomous decisions individually and avoid colliding with each other and the walls of the warehouse. The problem is defined as a dynamical multi-agent system and a solution is reached by applying the DQN algorithm. The solution is designed for achieving scalability, meaning that the trained robots are flexible enough to be deployed in simulated environments of different sizes and alongside a different number of robots. This was successfully achieved by feature engineering.</p>
----------------------------------------------------------------------
In diva2:860599 missing space in title:
"System Integration Testing ofAdvanced Driver Assistance Systems"
should be
"System Integration Testing of Advanced Driver Assistance Systems"

There were merged words and missing ligatures, the abstract should be:

<p>A key factor to further improve road safety is the development and implementation of Advanced Driver Assistance Systems (ADAS) in vehicles. Common aspects of the investigated ADAS' are their abilities of detecting and avoiding hazardous traffic situations by using sensor data and vehicle states in order to control the movement. As more complex and safety critical ADAS are developed, new test methods have to be considered. This thesis investigate how to test new ADAS from a complete vehicle level by considering aspects such as suitable test environments and traffic scenarios, and thereafter compare the results with existing testing methods. Different classifications of ADAS have been investigated and combined with own classifications considering complexity and traffic safety aspects, have made it possible to conclude and propose general test strategies for different ADAS.</p>
----------------------------------------------------------------------
In diva2:1477486 merged paragraphs & words and missing ligatures, the abstract should be:

<p>In this work, a methodology to visualize and mitigate antenna coupling in a systematic way is proposed, which can be used to increase the isolation between different antenna systems installed on a common platform. Formulations for visualizing the coupling in space, based on reciprocity are derived from the reaction theorem and the different formulations are compared with each other through numerical simulations in CST Microwave Studio and by post-processing in Matlab. The derived formulations are also compared witha more established method based on the Poynting vector.</p><p>The ability of the derived formulations to predict appropriate positions for placing absorbers is compared by introducing surface wave absorbing materials (SWAM), based on the visualization results. This is done for two simplified platforms with simple antennas to compare the different methods. A realistic model consisting of a ground plane with cavities for integrating the antennas that could represent an integrated mast for naval ships is studied with the target to reduce the mutual coupling using the visualization methods.A cavity-backed spiral antenna and a body-of-revolution (BoR) array antenna are used as sources.</p><p>One of the formulations derived from the reaction theorem performs well as a predictor for absorber placement and at several points better than existing methods for the simplified platforms. This formulation can predict areas where placing an absorber will increase or decrease the coupling respectively. Furthermore, it is possible to estimate the impact of introducing an absorber in one region, relative to another. In the realistic case, it is demonstrated how this formulation can be used to reduce the mutual coupling in a systematic and efficient way. It is shown that the isolation is increased with 4-5 dB by placing SWAM at positions based on the visualization results, compared to covering the entire surface with SWAM.</p>
----------------------------------------------------------------------
In diva2:1847107 merged paragraphs and a missing dash after "y" in "y-dimensions", the abstract should be:

<p>Cancer is one of the most common causes of death worldwide. When given optimal treatment, however, the risk of severe illness may greatly be reduced. Determining optimal treatment in turn requires evaluation of disease progression and response to potential, previous treatment.</p><p>Analysis of perfusion, a physiological property that describes how well different tissues are supplied with blood, has been shown useful for revealing important tumor characteristics. By performing a contrast agent-enhanced, non-invasive medical imaging procedure, quantitative parameters of perfusion can be obtained by fitting the image data to mathematical models. These parameters may then provide valuable insights into tumor properties, useful for purposes such as diagnostics and treatment response evaluation. Varieties of parameter calculation frameworks and perfusion models may however lead to a wide range of possible parameter values, which negatively impacts reproducibility and confidence in results.</p><p>The aim of this thesis project was to explore how different implementation choices in a perfusion parameter calculations framework, as well as image data noise and filtering, affected the parameter estimations. Image data of nine brain-tumor patients and a physical phantom was used for calculating perfusion parameters after systematically applying changes to the default calculations framework. The results showed that the choice of optimization method for parameter estimations could provide a significant difference in parameter estimations. A semi-automated method for obtaining a venous input function was evaluated and shown to be robust with respect to simulated user inputs. Generation of a T1 map, used when performing the parameter calculations, was explored for the variable flip-angle method and from this investigation it was concluded that a few combinations of flip-angles generated unrealistic T1 maps. Finally, a Gaussian image filter applied in the x- and y-dimensions of the image data was found to provide a noticeable reduction of applied noise. The outcome of the experiments exemplified how calculation framework setup affected parameter estimations, which was discussed to be of importance for other areas of research as well. Future work could encompass exploration of other, more complex perfusion models, and performing similar analysis for tumors in other body-parts.</p>
----------------------------------------------------------------------
In diva2:1216324 merged words and unnecessary hyphens, the abstract should be:

<p>Voxel based cone tracing is a promising approach to approximate global illumination for real-time applications. This technique utilizes a voxel field approximating the original scene to retrieve the necessary radiance information during sampling. The simplest approach to creating a voxel field is to use a 3D texture. Since this requires too much GPU memory for larger scenes alternative data structures are necessary. This thesis compares two seemingly suitable data structures <em>3D-Clipmaps</em> and <em>Sparse voxel octrees</em>. To compare the two structures we implement them using OpenGL and C++. We then use the improved Sponza model with additional dynamic objects to benchmark the differences between the two approaches. Both data structures has its pros and cons. Our conclusion is that Clipmaps seems to be the most practical approach for real-world purposes.</p>
----------------------------------------------------------------------
In diva2:927736 unnecessary hyphens, the abstract should be:

<p>Unsupervised pre-training has recently emerged as a method for initializing supervised machine learning methods. Foremost it has been applied to artificial neural networks (ANN). Previous work has found unsupervised pre-training to increase accuracy and be an effective method of initialization for ANNs[2].</p><p>This report studies the effect of unsupervised pre-training when detecting Twitter trends. A Twitter trend is defined as a topic gaining popularity.</p><p>Previous work has studied several machine learning methods to analyse Twitter trends. However, this thesis studies the efficiency of using a multi-layer perceptron classifier (MLPC) with and without Bernoulli restricted Boltzmann machine (BRBM) as an unsupervised pre-training method. Two relevant factors studied are the number of hidden layers in the MLPC and the size of the available dataset for training the methods.</p><p>This thesis has implemented a MLPC that can detect trends at an accuracy of 85%. However, the experiments conducted to test the effect of unsupervised pre-training were inconclusive. No benefit could be concluded when using BRBM pre-training for the Twitter time series data. </p>
----------------------------------------------------------------------
In diva2:1556731 merged words, the abstract should be:

<p>Field observations has shown that wind turbines are especially exposed to lightning strikes. The probability for lightning strikes to offshore wind turbines has been analysed in a previous article. In this project the probability for upward self-initiated lightning strikes to onshore wind turbines and a lightning protection tower was analysed. This was done by collecting elevation data and recreating the site topography in COMSOL Mutliphysics 5.5, and also by collecting weather data which were analysed in MATLAB. The probability for the critical electrostatic field was then calculated and analysed. The result shows that the risk of lightning strike is correlated to the topography and cloud height.</p>
----------------------------------------------------------------------
In diva2:1453588 merged words, the abstract should be:

<p>Non-dispersive infrared (NDIR) gas sensing is one of the best gas measurement method for air quality monitor. The major advantage of NDIR gas sensors is their long lifespan. However, all sensors drift over time, NDIR sensors are not excluded due to sensor aging and environmental factors. Due to this, research on calibration for long-term accuracy becomes necessary. In particular, self-calibration methods have been subject of many studies but the problem still remains a challenge. Today, the most common used self-calibration method for NDIR sensors is the ABC technology (Automatic Baseline Correction). It has been shown that the ABC technology is efficient, easy to implement and has been widely used in commercial gas sensors. However, this method cannot work well in an environment where the sensors are never exposed to fresh air. In order to build smart networks of infrared gas sensors for continuous self-calibration, accurately quantifying the relationship between the sensor’s drift and external factors becomes more and more important. In this work, we aim to model the sensor’s drift by taking advantage of statistical learning algorithms. Firstly, we investigate the provided dataset and apply the ABC technology for estimating the sensor drift. Secondly, the random walk model and Gaussian process models are adopted to describe the drift with respect to sensor aging and ambient temperature changes. Both the random walk and Gaussian processes can be used to provide a predictive probability distribution on the drift for each measurement of the sensors. Experiments have been carried out on different inputs and kernels for modeling the drift as Gaussian processes. Comparisons of the test prediction performance have also been made between different models. The results demonstrate that Gaussian processes, with the advantage of providing an estimate of the prediction of the drift together with an estimate of the uncertainty in the prediction, can be used to model the sensor drift and provide good prediction performance.</p>
----------------------------------------------------------------------
In diva2:1793576 merged words, the abstract should be:

<p>Text mining has gained considerable attention due to the extensive usage of electronic documents. The significant increase in electronic document usage has created a necessity to process and analyze them effectively. Rule-based systems have traditionally been used to evaluate short pieces of text, but they have limitations, including the need for significant manual effort to create and maintain rules and a high risk of complex bugs. As a result, text classification has emerged as a promising solution for extracting meaning from short texts, which are defined as texts limited by a specific character count or word count. This study investigates the feasibility and effectiveness of text classification in classifying short pieces of text according to their appropriate text properties, based on users’ intentions in the text. The study focuses on comparing two transformer models, GPT-2 and BERT, in their ability to classify short texts. While other studies have compared these models in intention classification of text, this study is unique in its examination of their performance on short pieces of text in this specific context. This study uses user-labelled data to fine-tune the models, which are then tested on a test dataset from the same source. The comparative analysis of the models indicates that BERT generally outperforms GPT-2 in classifying users’ intentions based on the appropriate text properties, with an F1-score of 0.68 compared to GPT-2’s F1-score of 0.51. However, GPT-2 performed better on certain closely related classes, suggesting that both models capture interesting features of these classes. Furthermore, the results demonstrated that some classes were accurately classified despite being context-dependent and positioned within longer sentences, indicating that the models likely capture features of these classes and facilitate their classification. Both models show promising potential as classification models for short texts based on users’ intentions and their associated text properties. However, further research may be necessary to improve their accuracy. Suggestions for enhancing their performance include utilizing more recent versions of GPT, such as GPT-3 or GPT-4, optimizing hyperparameters, adjusting preprocessing methods, and adopting alternative approaches to handle data imbalance. Additionally, testing the models on datasets from diverse domains with more intricate contexts could provide greater insight into their limitations.</p>
----------------------------------------------------------------------
In diva2:1331871 merged paragraphs and words, the abstract should be:

<p>The nature of the power system is changing; what was once a clearly defined generation-transmission-distribution-consumer power flow is now shifting towards a distributed infrastructure, with great amounts of variable renewable sources in the system. The penetration of alternative energies like solar and wind have represented a game changer for the electric power industry, diminishing the traditional dominance of fossil fuel based sources, and moving towards a more renewable mixture. All this was made possible due to severe climate regulations, both in the European and global framework, as well as ever decreasing installation, operation and maintenance costs for these novel technologies.</p><p>However, the penetration of renewable energies brings challenges to the reliability and stability of the power system, which must be tackled accordingly. Fortunately, the tools are there, standards like the IEC61850 and IEC61499 were independently designed, and for different purposes. The IEC61850 standard strives for inter-operability and vendor-independence within the substation design field, specifically in regards to the data objects exchanged between the devices. On the other hand, the IEC61499 standard is used for the design of industrial distributed systems.</p><p>This report aims to answer the question: is it possible to generate an IEC61499 description which complements an IEC61850 substation specification? To this end, an IEC61850-IEC61499 interface was created, which takes an IEC61850 substation specification description, as well as a series of configuration files (rules, connections, allocation, parameters) and generates extended descriptions of the substation, as well as a fully operational IEC61499 system specification. This can be directly imported in the 4DIAC tool, and executed to evaluate the performance of the designed protection systems, in a network of distributed or centralized devices.</p><p>A series of test cases were evaluated, and the obtained results demonstrate that it is possible to bridge the gap between IEC61850 and IEC61499, thus enabling power system engineers to assess the performance of a certain protection scheme, before actually implementing it in the substation. This potentially reduces human errors and development times. The interface was implemented in Java p.l. and is distributed as an open source project.</p>
----------------------------------------------------------------------
In diva2:933512 there were unnecessary "- " occurrences, the abstract should be:

<p>A study was performed on Naive-Bayes and Label Spreading methods applied in a spam filter as classifiers. In the testing procedure their ability to predict was observed and the results were compared in a McNemar test; leading to the discovery of the strengths and weaknesses of the chosen methods in a environment of varying training data. Though the results were inconclusive due to resource restrictions, the theory is discussed from various angles in order to provide a better understanding of the conditions that can lead to potentially different results between the chosen methods; opening up for improvement and further studies. The conclusion made of this study is that a significant difference exists in terms of ability to predict labels between the two classifiers. On a secondary note it is recommended to choose a classifier depending on available training data and computational power. </p>
----------------------------------------------------------------------
In diva2:724013 there were unnecessary "- " occurrences, the abstract should be:

<p>Speech synthesis is an area of computer science with many practical uses, such as enabling people with visual impairments to take part of text and to provide more human-like feedback from information systems. A similar area of research is text-to-song, where systems comparable to those used in text-to-speech provide mappings from text to melodic units of song. This paper discusses how a text-to-song algorithm can be developed and what parameters affect what emotion is communicated. Fifty participants listened to music generated with our algorithm. Results show that tempo and mode both heavily account for what emotion is communicated; a melody performed with a tempo of 250 bpm was perceived as significantly more happy than a performance with a tempo of 120 bpm, and a melody in major tonality was perceived as significantly more happy than a melody in minor tonality. Combined, these parameters gave even more significant results. A fast tempo combined with major tonality produced a performance that was perceived as even more happy. The opposite was observed when a slow tempo was combined with minor tonality. When a fast tempo was combined with a minor tonality the average answer was neutral with answers distributed over the whole spectrum from sad to happy. A slow tempo combined with a major tonality gave almost identical results. We concluded that generating emotionally expressive song with the use of an algorithm is definitely possible, but that the methodology can be improved in order to convey emotions even more clearly.</p>
----------------------------------------------------------------------
In diva2:618745
   'webuser-interface'
should be:
   'web user-interface'
----------------------------------------------------------------------
In diva2:470725 merged paragraphs and an space and 0x2010 dash instead of "-", the abstract should be:
<p>The netback value is a price mechanism approach for determining prices in long-term contracts. This method consists in ensuring that gas remains competitive with competing fuels by setting the border in each long-term sale contract below the weighted-average price of the cheapest alternative fuels across all sectors, including industry, residential tertiary, transportation and electricity generation. This price mechanism is used in Continental Europe because the gas does not have captive uses and it can be replaced by other energy in all sectors such as the coal in the electricity generation sector.</p><p>This price is then indexed under long-term contracts to the main competing fuel. This pattern of indexation allows avoiding volatility and the use of market power but some actors really want to change this method of indexation. Indeed, the power sector represents the main source of incremental demand growth for gas in the recent years and therefore, indexation to coal is conceivable. Moreover, the economic crisis contributes to lower the gas market price, opening a gap between the spot and the long-term contract prices. The spot price and the long-term price are currently at nearly the same level but during the winter, the long-term contract price should be normally lower. More and more Combine Cycle Gas and Turbines (CCGT) are built in Europe and the investors would like to include the gas value of the electricity generation in the calculation of the weighted Netback value. Some key factors such as the construction of the coal plant cost, the fuel cost and the CO2 cost have a significant impact on this Netback value for electricity generation.</p><p>The CO2 Capture and Storage (CCS) is also a promising technology regarding the share of fossil fuel in the energy mix. However, the netback value found with the CCS coal plants must be viewed with some caution regarding the uncertainties around the capture costs, the transport costs, the distribution of storage sites and the learning rate.</p>


Oddly, there is only an abstract!

----------------------------------------------------------------------
In diva2:1232315 missing the footnote and missing the emphasis, the abstract should be:

<p>On the internet today, information to expose books is generated manually. That includes information such as genre, author, places, and summary. The full text of books are not publicly available on the Internet due to copyright law, and for this reason it is not possible to generate this type of information automatically. One solution is to construct a prototype that processes the original book and automatically generates information that can be exposed to the Internet, without exposing the entire book. In this report, three different algorithms that deal with processing books are compared: <em>stemming</em>, <em>filtering of stop words</em> and <em>scrambling of sentences within paragraphs</em>. The algorithms are compared by generating relevant information to the services: search engines, automatic metadata, smart ads and text analysis. Search engines allows a user to search for e.g. the title or a sentence from the book. Automatic metadata automatically breaks out descriptive information from the book. Smart ads can use descriptive information to recommend and promote books. Text analysis can be used to automatically create a brief descriptive summary. The information stored from the books should only be relevant information for the services and the information should not have any literal value<sup>2</sup> for a human to read. The result of the work shows that the combinations <em>scrambling of sentences→filtering of stop words</em> and <em>filtering of stop words→scramlbing of sentences</em> are optimal in terms of searchability. It is also recommended to add stemming as an additional step in the processing of the original book, as it generates more relevant automatic metadata to the book.</p>

2 is a footnote:
<p>No literal value means that the content of the book is revised so that it is no longer possible for a person to understand the context and action. Revising the contents of a book can be done in different ways, for example, by changing the structure of the sentences and deleting conjunctions in the text.</p>
----------------------------------------------------------------------
In diva2:1248819 "<7p>" should be "</p>", the abstract should be:

<p>Vatten är en av de viktigaste resurserna i världen. Det har direkt inverkan på mänsklighetens dagliga liv och samhällets hällbara utveckling. Vattenkvaliteten påverkar det biologiska livet och måste följa strikta föreskrifter. Traditionella metoder för vattenkvalitetssäkring, som används idag, innefattar manuell provtagning foljt av laboratorieanalys. Denna process är dyr på grund av höga arbetskostnader för provtagning och laboratoriearbete. Dessutom saknar den realtidsanalys som är väsentlig för att minimera förorening.</p><p>Avhandlingen syftar till att hitta en lösning på detta problem med hjälp av IoT-sensorer och maskinlärningsteknik för att upptäcka avvikelser i vattenkvaliteten. Den spatiala skalbarheten är ett viktigt krav vid val av överföringsprotokoll, eftersom sensorer kan spridas runt vattennätverket. Vi diskuterar lösningar som är lättillgängliga eller snart ska vara på marknaden. De viktigaste LPWAN-teknikerna som studerats är: SigFox, LoRaWAN och NB-IoT. Generellt har dessa protokoll många egenskaper som är nödvändiga för övervakning av färskvatten, som lång batterilivslängd och lång räckvidd, men de har många begränsningar vad gäller överföringshastighet och arbetscykel. Det är därför viktigt att hitta en lösning som skulle hitta anomalier vid högt säkerhet men samtidigt överensstämmer med begränsade överförings- och bearbetningskapaciteter hos sensorerna och de ovan nämnda protokoll.</p><p>En försökssensor finns redan på plats i Lake Mälaren och dess avläsningar används för denna studie. Övervakade maskininlärningsalgoritmer, såsom Logistic Regression, Artificial Neural Network, Decision Tree, One Class K-NN and Support Vector Machine (SVM) studeras och diskuteras beträffande tillgängliga data. SVM väljs sedan, implementeras och optimeras för att uppfylla IoTs begränsningarna. Balansen mellan falska avvikelser och falska normala avläsningar diskuteras också.</p>
----------------------------------------------------------------------
In diva2:706748 merged words and paragraphs, the abstract should be:

<p>Genome-wide association studies (GWAS) has been in the heart of medical research for the last 5 years. These studies seek for common variants in the genome that are linked to risk for common complex diseases (CCDs). Although GWAS has defined a number of interesting genetic loci for a range of CCDs, the current GWAS analysis has limitation such as investigating the DNA variants one-by-one focusing on the most significant DNA variants. As a consequence, most risk variants for CCDs are, in my belief, still hidden in the GWAS data. Herein, I use a method of GWAS analysis that considers risk-enrichment for groups of functionally associated genes defined by for example gene networks, believed to play a role in CCDs.</p><p>In this method, a set of expression SNP (single nucleotide polymorphism) was selected from genes which are known to be related to coronary artery disease (CAD) in a way that a singlee SNP was chosen for each gene. Then using the data available from the International HapMap Project and a GWAS data available, it is possible to find SNPs which are in strong linkage with the initial set, which we call it expanded set. Depending on the association of the initial set to the CAD, expanded set can show an enrichment score greater or smaller compared to the null distribution set of SNPs with same properties of the expanded set.</p><p>In conclusions, CCDs are not a consequence of isolated genetic variants/genes in isolated pathways but instead sets of genetic variants/genes acting in conjunction, cause CAD. Genetic risk enrichment analysis is a fairly simple and straightforward method to determine to what extent a group of functionally associated genetic variants/genes are enriched for a given CCD. In addition, this analysis can perhaps help to decipher some of the 90-85% of risk variation in populations that remains unaccounted.</p>
----------------------------------------------------------------------
In diva2:808731 unnecessary paragraphs, the abstract should be:

<p>With the rapid development of new technologies during the last decades, the demand of complex algorithms to work in real-time applications has increased considerably. To achieve the real time expectations and to assure the stability and accuracy of the systems, the application of numerical methods and matrix decompositions have been studied as a trade-off between complexity, stability and accuracy. In the first part of this thesis, a survey of state-of-the-art QR Decomposition methods applied to matrix inversion is done. Stability and accuracy of these methods are analyzed analytically and the complexity is studied in terms of operations and level of parallelism. Besides, a new method called Modified Gaussian Elimination (MGE) is proposed. This method is shown to have better accuracy and less complexity than the previous methods while keeping good stability in real time applications. In the second part of this thesis, different techniques of extended Kalman Filter implementations are discussed. The EKF is known to be numerically unstable and various methods have been proposed in the literature to improve the performance of the filter. These methods include square-root and unscented versions of the filter that make use of numerical methods such as QR, LDL and Cholesky Decomposition. At the end of the analysis, the audience/reader will get some idea about best implementation of the filter given some specifications.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-19
======================================================================
In diva2:511132 a "c" was inserted at the end of the first line and the paragraphs were merged, the abstract should be:

<p>The Spinning QUad Ionospheric Deployer (SQUID) is a sounding rocket experiment developed to test and verify a novel mechanism to deploy wire booms. The SQUID consists of the Rocket Mounted Unit (RMU) and the Free Flying Unit (FFU), the former is attached to the rocket and the latter is ejected. The FFU carries the electronics box (eBox) that controls the system and the boom deployment system known as SCALE. The FFU needs to be independent when has been ejected from the rocket.</p><p>This thesis work covers the design and manufacture of the SQUID electronics system to control the functionality of the experiment. The control is implemented in a Field Programmable Gate Array (FPGA) using the VHDL language. The integration, testing and validation of software and hardware also is presented here.</p><p>The SQUID experiment was launched onboard the REXUS-10 rocket from ESRANGE the 23rd February 2011.</p>
----------------------------------------------------------------------
In diva2:1040724 merged paragraphs, missing ligatures, and unnecessaru hyphens, the abstract should be:

<p>Autonomous vehicles have been the subject of intense research, resulting in many of the latest cars being at least partly self driving. Cooperative driving extends this to a group of vehicles called a platoon, relying on communication between the vehicles in order to increase safety and improve the flow of traffic. This thesis is partly done in context of Grand Cooperative Driving Challenge (GCDC) 2016 where KTH has participated with a Scania truck and the Research Concept Vehicle (RCV), an electric prototype car.</p>p>Trajectory planning is investigated for the longitudinal control of both the truck and the RCV. This planner is to ensure that the vehicles reached a position in a given time and a desired velocity. This is done using Pontryagin's minimum principle and interpolation.</p>p>A more advanced planner based on Model Predictive Control (MPC) is used to avoid collisions in two different scenarios. One considers obstacle avoidance in the form of an overtake and the other a lane change scenario were the vehicle needs to decide how to position itself relative to the other vehicles.</p>p>Simulations of the longitudinal control and planning of the truck did show that it could time the position and speed with a position error of less than 2m and speed error less than 0.2 m/s, assuming a distance of 120-200 m, a time interval of 40s and goal speed of 7m/s. The same simulation for the RCV had a distance error of less than 0.3m and a speed error below 0.2m.</p>p>Simulations of the RCV using MPC planners showed that overtaking and lane changes could be performed. When performing the lane change the RCV managed to maintain a longitudinal distance of at least 1m, even if the other vehicles are slowing down or increasing their speed. The overtaking could also be successfully performed although with small margins, having a lateral distance of 0.5 m to the vehicle being overtaken.</p>
----------------------------------------------------------------------
In diva2:706726 the "*" marks a footnote:

For the English abstract:
This master thesis also considers and describes other security specifications such as British Standard Institution (BSI) 10012, and International Standards Organization (ISO) and International Electrotechnical Commission (IEC) standard (ISO/IEC 27001) to provide a complete picture of the rules and regulations concerning personal data protection.


For the Swedish:
Examensarbetet tar också hänsyn till och beskriver andra säkerhetspecifikationer som British Standard Institution (BSI) 10012, och International Standards Organization (ISO) och International Electrotechnical Commission (IEC) standard (ISO/IEC 27001) för att ge en fullständig bild av regler och förordningar angående säkrandet av personlig data.
----------------------------------------------------------------------
In diva2:789372 merged paragraphs and words, the abstract should be:

<p>Video conferencing services are dependent on many other underlying devices, network services and infrastructure and TCP/IP services before they can provide seamless, reliable and good quality video meeting services to end users. Providing fully automated video conferencing services at Skiptrip AB requires engagement of even more variant and complex set of TCP/IP services and devices that has made its network a heterogeneous one consisting of hundreds of modern and legacy systems along with the high definition and bandwidth sensitive video conferencing systems. In this thesis the process of designing and implementing a secure network module forseparating and transferring non-production (management) network traffic flow of all network equipment via establishing and fine-tuning virtual IP-sec tunnels among edge routers or firewalls of each video station in this enterprise-scale network has been conducted in order to make sure that the network traffic flow belonging to the management module is treated separately and securely thanks to the encryption mechanisms of IPsec protocol on the header and payload of IP packets.</p><p>After getting inspired by studying some well-known network design and architecture methodologies and industry best practices like Cisco SAFE, characterizing the existing network is done in the early stages of this thesis with a focus on security measures such as the utilization of Access Control Lists on different router interfaces which were utilized to provide perimeter network security to some extent. Afterwards, a new network design is proposed where the management flow is separated from the production traffic flow and is transferred through the secure IPsec tunnels in a semi-mesh topology which form a virtual network module for the management traffic of the whole internetwork. The new network module is then given a new IP addressing scheme based on the private range of IPv4 addresses and, after relevant discussions, a certain way of implementation of static routing in combination with classless interdomain routing and variable length subnetmasking is introduced to provide, implemented and tested in order to provide route-redundancy in IP connectivity level of management network module in a similar-to dynamic routing protocol manner.</p><p>Innate sensitivity of high definition video conferencing protocols like H.323 and SIP to quality of the underlying network infrastructure which is usually defined in terms of packet loss and jitter as well as the bandwidth limitation of costly Internet links in each video station and the heterogeneity of the internetwork were amongst the main technical challenges of this thesis and shaped the outcome of proposed design and also the evaluation mechanisms which are done at the end of this project.</p>
----------------------------------------------------------------------
In diva2:1453627 merged paragraphs and merged words, the abstract should be:

<p>The most common fault type in electric power systems is the line to ground fault. In this type of faults, an electrical arc is usually developed. The thesis presents a mathematical model that describes the behavior of the arc during a fault. The arc model has been verified based on real and simulated tests that were conducted on a system that has resonant earthing coil.</p><p>In addition, two studies have been conducted on the same verified system. The first studied was implemented to see the effect of detuning the resonant earthing coil at different levels. It was noted that detuning the coil affected AC and the DC components in the arc faults. Also, the detuning affected the arc extinction.</p><p>The second study has been looking at the effects of implementing a parallel resistor to the resonant earthing coil. The tests have been conducted using different set values of the resistor. In some of the studied cases and during the testing period, the resistor has affected the self-extinguish behavior of the arc.</p>
----------------------------------------------------------------------
In diva2:1148538 merged paragraphs and words, the abstract should be:

<p>Unmanned Aerial Vehicles (UAV), in particular the four-rotor quadrotor, are gaining wide popularity in research as well as in commercial and hobbyist applications. Maneuverability, low cost, and small size make quadrotors an attractive option to full-scale, manned helicopters while opening up new possibilities. These include applications where full-scale helicopters are unsuitable, such as cooperative tasks or operating indoors.</p><p>Many UAV systems use the Global Positioning System (GPS), IMU (Inertial Measurement Unit) sensors, and camera sensors to observe the UAV’s state. Depending on the application, different methods for observing the states are suitable. Outdoors, GPS is available and widely used and in cluttered environments on-board cameras can be the best choice. Controlled lab environments often use external cameras to track the quadrotor. Most applications make use of the IMU in their implementations, most commonly the gyroscope for attitude estimation.</p><p>In this thesis, several external ultra-wideband (UWB) radio sensors are used to measure the distance between individual sensors and a quadrotor. The range measurements are fused with acceleration and angular velocity measurements from an Inertial Measurement Unit to estimate the quadrotors position and attitude. An ultra-wideband sensor is cheap and does not require line-of-sight or heavy equipment mounted on the quadrotor. The drawback of UWB-based positioning is that it requires the assumption of known sensor locations in order to calculate the distance between sensor and UAV. This thesis aims to remove this assumption by estimating the quadrotor’s and the sensors’ position simultaneosly using the Smoothing and Mapping (SAM) technique.</p><p>The Georgia Tech Smoothing and Mapping (GTSAM) framework provides the incremental Smoothing and Mapping implementation, used for estimation of both the quadrotor’s position and attitude, and the sensors’ position. The Inertial Measurement Unit is handled by the state of the art IMU factor, included in GTSAM.</p><p>The system is evaluated with and without prior knowledge of the sensor positions, using recorded flight data from a Crazyflie quadrotor and six Loco Positioning Node sensors. The results show that the system is able to track the UAV’s position and attitude with acceptable errors. The error in estimated sensor position is too large to be satisfactory, Based on the results several topics for future work are proposed.</p>
----------------------------------------------------------------------
In diva2:881082 merged paragraphs & words, and unnecessary hyphen, the abstract should be:

<p>Both data traffic and number of subscriptions have enormously increased in mobile network in recent years. Moreover, there will be an even faster growth in the future. A promising way to satisfy a significantly increasing demand in future radio access network is by using so called Ultra Dense Networks (UDNs) which deploy a large number of base stations compared to the number of active users.</p><p>Radio spectrum is a finite resource and therefore has to be shared by multiple users. This sharing of radio spectrum inevitably causes interference between the users. In this study, the interference management performance of different resource allocation schemes in different network density is studied, which is from a traditional network density to ultra dense network.</p><p>Except for traditional frequency reuse scheme and reuse partitioning scheme, Coordinated Multi Point (CoMP) schemes have been chosen in the work. Different CoMP techniques such as the universal frequency reuse (UFR) and cooperative frequency reuse (CFR) are tested to find the best network performance in terms of average users data throughput and cell rate.</p><p>Besides, after measuring these CoMP schemes which are designed for high base station density, the optimal scheme is found to be a potential method adopted by ultra-dense network.</p>
----------------------------------------------------------------------
In diva2:1556753 merged paragraphs & words, the abstract should be:

<p>Internet of Things devices, such as smartphones and smartwatches, are currently becoming widely accessible and progressively advanced. As the use of these devices steadily increases, so does the access to large amounts of sensory data. In this project, we developed a system that recognizes certain activities by applying a linear classifier machine learning model to a data set consisting of examples extracted from accelerometer sensor data. We obtained the data set by collecting data from a mobile device while performing commonplace everyday activities.These activities include walking, standing, driving, and riding the subway. The raw accelerometer data was then aggregated into data points, consisting of several informative features. The complete data set was subsequently split into 80% training data and 20% test data. A machine learning algorithm, in this case, a support vector machine, was presented with the training dataset and finally classified all test data with a precision higher than 90%. Hence, meeting our set objective to build a service with a correct classification score of over 90%.</p><p>Human activity recognition has a large area of application, including improved health-related recommendations and a more efficiently engineered system for public transportation.</p>
----------------------------------------------------------------------
In diva2:680540 merged paragraphs & words, the abstract should be:

<p>In the context of military field, more and more international coalitions among allied forces have taken place. Information from heterogeneous systems needs to be exchanged without misinterpretation so the involved participating actors can share a common situational awareness regarding certain data and/or messages. This, in turn, requires the preservation of the intended meaning not only on the syntax, language, and representation level, but on a semantic level as well.</p><p>The application domain of the Business Object Reference Ontology Program (BORO) method focuses on the development of ontological or semantic models for large complex operational applications, especially in the military context. It is chosen by FOI, the Swedish Defense Research Agency in the field of Information Systems, to apply to their Semantic Interoperability (SI) project.</p><p>The goal of this thesis is to investigate how BORO method can be implemented for aligning the data and/or messages between the Swedish Armed Forces and other military organizations on a semantic level for the FOI SI project. To achieve this goal the design science research methodology is conducted through a series of steps. The analysis regarding the usability of BORO method for FOI to obtain semantic interoperability in its project will be demonstrated as the result of this thesis, which can also be utilized as a reference for other military organizations when conducting activities of information exchange.</p>
----------------------------------------------------------------------
In diva2:678507 merged words and an unnecessary space, the abstract should be:

<p>The abundance of high-dimensional datasets provides scientists with a strong foundation in their research. With high-performance computing platforms becoming increasingly available and more powerful, large-scale data processing represents an important step toward modeling and understanding the underlying processes behind such data.</p><p>In this thesis, we propose a general cortex-inspired information processing network architecture capable of capturing spatio-temporal correlations in data and forming distributed representations as cortical activation patterns. The proposed architecture has a modular and multi-layered organization which is efficiently parallelized to allow large-scale computations. The network allows unsupervised processing of multivariate stochastic time series, irregardless of the data source, producing a sparse de-correlated representation of the input features expanded by time delays.</p><p>The features extracted by the architecture are then used for supervised learning with Bayesian confidence propagation neural networks and evaluated on speech classification and recognition tasks. Due to their rich temporal dynamics, we exploited auditory signals for speech recognition as an use case for performance evaluation. In terms of classification performance, the proposed architecture outperforms modern machine-learning methods such as support vector machines and obtains results comparable to other state-of-the-art speech recognition methods. The potential of the proposed scalable cortex-inspired approach to capture meaningful multivariate temporal correlations and provide insight into the model-free high-dimensional data decomposition basis is expected to be of particular use in the analysis of large brain signal datasets such as EEG or MEG.</p>
----------------------------------------------------------------------
In diva2:789371 merged paragraphs & words, the abstract should be:

<p>There are different ways to store and process large amount of data. Hadoop is widely used, one of the most popular platforms to store huge amount of data and process them in parallel. While storing sensitive data, security plays an important role to keep it safe. Security was not that much considered when Hadoop was initially designed. The initial use of Hadoop was managing large amount of public web data so confidentiality of the stored data was not an issue. Initially users and services in Hadoop were not authenticated; Hadoop is designed to run code on a distributed cluster of machines so without proper authentication anyone could submit code and it would be executed. Different projects have started to improve the security of Hadoop. Two of these projects are called project Rhino and Project Sentry [1].</p><p>Project Rhino implements splittable crypto codec to provide encryption for the data that is stored in Hadoop distributed file system. It also develops the centralized authentication by implementing Hadoop single sign on which prevents repeated authentication of the users accessing the same services many times. From the authorization point of view Project Rhino provides cell-based authorization for Hbase [2].</p><p>Project Sentry provides fine-grained access control by supporting role-based authorization which different services can be bound to it to provide authorization for their users [3].</p><p>It is possible to combine security enhancements which have been done in the Project Rhino and Project Sentry to further improve the performance and provide better mechanisms to secure Hadoop.</p><p>In this thesis, the security of the system in Hadoop version 1 and Hadoop version 2 is evaluated and different security enhancements are proposed, considering security improvements made by the two aforementioned projects, Project Rhino and Project Sentry, in terms of encryption, authentication, and authorization. This thesis suggests some high-level security improvements on the Centralized authentication system (Hadoop Single Sign on) implementation made by Project Rhino.</p>
----------------------------------------------------------------------
In diva2:1634378 merged words, the abstract should be:

<p>This project investigates the applicability of the original version of Markowitz’s mean-variance model for portfolio optimization to real-world modern actively managed portfolios. The method measures the mean-variance model’s capability to accurately capture the riskiness of given portfolios, by inverting the mathematical formulation of the model. The inversion of the model is carried out both for fabricated data and real-world data and shows that in the cases of real-world data the model lacks certain accuracy for estimating risk averseness. The method has certain errors which both originate from the proposed estimation methods of input variables and invalid assumptions of investors.</p>
----------------------------------------------------------------------
In diva2:762845 merged words, the abstract should be:

<p>This report details our research done on web sandboxes with a focus on two different implementations, Google Caja and ADsafe. Detailing their differences, their soundness, and their suitability for isolation of untrusted JavaScript in a specific multi-module web platform. The report also contains our results from implementing a prototype of a tool to automatically test an implementation of an ADsafesandbox. We present our motivation for this research as the security issues with running non-isolated and unchecked JavaScript, and the specific risks related to Multisoft’s Softadmin platform</p>

I think that the last sentence should end with a period, but there is no full text in DiVA.
----------------------------------------------------------------------
In diva2:1085518 merged words, the abstract should be:

<p>The load response to voltage and frequency changes has a considerable impact on the behaviour of the power system. Thus, the selection of a load model structure and its corresponding parameters is an important task in order to study and predict the system behaviour. Currently, the Nordic Transmission System Operators (TSO) use the ZIP load model, as it provides an easy and flexible way of representing the load. The main goal of the thesis has been to test two approaches for deriving ZIP model parameters, namely the component-based and measurement-based approaches. The former approach uses predefined parameter values, and information on the loads electricity consumption, whereas the latter uses measurement data and curve-fitting techniques. In order to evaluate the methodology, a case study has been performed, where the two aggregation approaches were applied on an evaluation point. It was found that the aggregation by means of the component-based approach may result in ZIP parameters lacking physical significance. ZIP parameters without physical significance pose a challenge for system planners, who may have difficulties in accepting these values as they are less intuitive than physically significant ones. Furthermore, the results of the measurement-based approach indicate that the ZIP model has some limitation when it comes to the sudden load changes that it can accommodate. This has been the case with the measured reactive power in the case study. Based on the results of applying the methodology, it can be concluded that the component-based and measurement-based approaches provide useful information when understanding power system loads.</p>
----------------------------------------------------------------------
In diva2:1453320 merged paragraphs & words, the abstract should be:

<p>Sustainable transport has lead to recent technological advancements for electric vehicles. A weak component of the electric vehicles is the energy storage units and their efficient operation. A battery management system is usually employed to ensure the safe and efficient operation of the batteries. State of charge (SoC), parameter and capacity estimation are vital functions of such a device. However, the development and performance evaluation of these processes is difficult.</p><p>In this thesis, capacity estimation algorithms are developed and tested under various scenarios, like parameter initialisation and SoC error compensation. The investigated algorithms are based on recursive version of the least squares method. The validation of the algorithms is performed on data, provided by <em>Scania AB</em>.</p><p>The experimental results proved that the errors-in-variables (EIV) solution performs overall better than the ordinary recursive least squares in terms of bias compensation and convergence. However, an improved identification of the battery model will eliminate considerable inaccuracies.The proposed estimator achieves similar accuracy to the EIV method in terms of bias and outlier removal. However, its convergence speed is undoubtedly moderate. The validation and testing are substantial obstacles in the development of such algorithms due to huge amount of data and long simulations. Further investigation is required in terms of different battery temperatures.</p>
----------------------------------------------------------------------
In diva2:1272232 merged words, the abstract should be:

<p>Sensorless-controlled drives represent a solution drawing an increasingly important attention for the benefits they entail. At the cost ofa slightly lower dynamics compared with the traditional drives, they involve a reduction of the system costs and complexity and improved noise immunity and reliability. Besides the traditional signal injection methods, involving a limitation of the machine voltage margin, higher iron losses, torque ripple and acoustic noise, a new method has been proposed in 2010 by professors Bolognani, Faggion and Sgarbossa. This algorithm, which has been defined "intrinsic injection" method, makes used of the harmonic content deriving from the PWM modulation.</p><p>In this work, the intrinsic injection sensorless algorithm and its implementation in a MATLAB/Simulink environment is the object of study. Its theoretical foundation is deeply analysed together with the phenomena and the operating conditions that might affect its performance.The drive model is described and three different alternatives for the estimator have been proposed. Simulations have been run with the estimator operating both in open-loop and in closed-loop. The influences of the sampling frequency, of the motor speed, of the load torque, of the implemented modulation strategy and of the DC-link voltage amplitude have been analysed. Lastly, the drive has been simulated with regard to a fan or pump application case.</p>
----------------------------------------------------------------------
IN diva2:888124 merged paragraphs & words:

<p>The established use of IT systems has increased the use of information in modern enterprises. From this information use, the concept of Business Intelligence has developed to enable more efficient and informed decision-making. As the business’ requirements of Business Intelligence reports changes rapidly due to changes of the business’ needs and more analytical organisations, traditional Business Intelligence development faces problems of ad-hoc analyses due to the inefficient adaption to changing needs.</p><p>This Master Thesis serves the purpose of deepen the understanding of the establishment of an agile development program of Self-service BI, addressing the concerns of more effectively meeting the changing requirements of traditional Business Intelligence development. This study explores enablers through a qualitative case study, conducted at a Swedish bank, consisting of four group interviews discussing the establishment of such program in Organisational, Processes, Technical and External dimensions, respectively. The qualitative case study was then followed by a discussion of governance of such program for alignment to enablers.</p><p>The qualitative case study resulted in 15 enablers of an agile development program of Self-Service BI, considering further enablers compared to more general literature of BI success factors, addressing the perspective of both an agile development program and Self-Service BI applications. The discussion of governance of the program then identified eight governance mechanisms, which might align the program to the enablers, for successful establishment and development of applications.</p><p>The findings of the study can be considered to culminate into a structure of an agile development program of Self-Service BI. The Thesis presents, from the findings, a framework for structuring such program, consisting of three development phases; Ordering process, Agile development, and Maintenance/Support and Training, and with the discussed governance for steering the development.</p>
----------------------------------------------------------------------
In diva2:484516 merged words and a typo, the abstract should be:

<p>UWB is a promising technology for short-range high-rate wireless applications. It is able to provide maximal 480Mbps data-rate at a distance of 2 meters in realistic indoor multi-path environments. UWB technology is widely applied to the next generation WPAN as well as the wireless access of consumer electronics at home. Recently, Multi-Band OFDM based UWB technology proposed by WiMedia has been selected as the international standard by ISO. In China, a new transmission architecture based on Dual-Carrier OFDM technology is adopted as UWB standard draft. Comparing to MB-OFDM based UWB system, DC-OFDM based UWB system has multiple advantages, like more spectrum resource, lower requirements on devices, etc. Besides, it is compatible with existing MB-OFDM based UWB technology. Therefore, DC-OFDM based UWB is more flexible.</p>
<p>Synchronization is the first step at the receiver digital baseband, which is of tremendous importance in any wireless communication systems. The performance of synchronization directly determines whether the receiver can pick up radio signals correctly or not, whether the baseband modules can fulfill the digital signal processing effectively or not. The synchronization process in OFDM system can be briefly divided into two parts: symbol timing and frequency synchronization. Symbol timing serves to judge the starting position of OFDM symbols after considering the impact of multi-path fading channel. While the frequency synchronization estimates the multiple imperfections in analog front-end signal processing and make proper compensation.</p>
<p>This thesis puts the emphasis on synchronization issues in DC-OFDM based UWB systems. We are the first to analyze the synchronization algorithm as well as the hardware implementation method tailored for DC-OFDM based UWB system. We also present the VLSI implementation result for synchronization module. The thesis consists of symbol timing and frequency synchronization.</p>
<p>Regarding on the symbol timing, we analyze the impact of several synchronization errors in OFDM system. After that, we divide the synchronization process into four modules by functionality: packet detection, coarse timing, TFC detection and fine timing. The internal parameters in each module are determined by system simulations. In the aspect of algorithm development, we adopt the joint auto-correlation and cross-correlation method to meet the requirements of UWB system in different indoor multi-path environments, and therefore achieve the robustness. In the aspect of hardware implementation, we put the attention on the structure of some key modules in symbol timing and their VLSI implementation result, such as auto-correlator, cross-correlator, real-number divider, etc.</p>
<p>Regarding on the frequency synchronization, we first investigate the multiple analog front-end imperfections in OFDM system, like CFO, SFO and I/Q imbalance, and present their mathematics models respectively in DC-OFDM based UWB system. After that, we analyze the performance degradation in OFDM system due to these non-ideal effects by the metric of EVM. RF designer can build the connection between mismatching parameters and performance degradation by referring to the analysis. Hence, the RF designer is able to traceout the outline of system design. In the aspect of algorithm development, we explore the intrinsic character of I/Q imbalance which causes the image interference. Then, we design a set of new training sequences based on phase rotation and give the corresponding estimation algorithm. The simulation result shows that the new training sequence is able to obtain the diversity message introduced by I/Q imbalance and therefore achieve the diversity gain during demodulation process. In order to deal with the challenging situation where multiple analog front-end imperfections co-exist, we propose a joint estimation and compensation scheme. In the aspect of hardware implementation, we present the hardware structure of CFO estimation and compensation module catered for DC-OFDM based UWB system, with the emphasis on CORDIC unit that is responsible for triangle calculations. The VLSI implementation result shows that the proposed CFO estimation and compensation module satisfies the timing and resource requirements in DC-OFDM based UWB system.</p>
<p>In the last, we present the prospective research area in 60-GHz applications. It includes multiple non-ideal impairments, like phase noise, non-linear power amplification, DC offset, ADCs mismatch, etc. It is even more challenging to develop joint estimation and compensation scheme for these non-ideal effects.</p>
----------------------------------------------------------------------
In diva2:576280 merged paragraphs & words, the abstract should be:

<p>This diploma work is mainly focused on developing the control strategy for a  variable speed drive as an alternative solution to a micro-hydro power plant. The detailed mathematical model for a micro-hydro system including a Kaplan turbine, mechanical shaft and electrical machines is presented and validated through simulations. A control strategy for an autonomous operation of a doubly-fed induction machine-based drive is developed for a wide range of speed. The drive can operate at a unity power factor.</p><p>The possible applications of the analyzed system are also presented. As a positive side of the system, it is found that the direct interaction between the power electronic converters and the utility grid can be avoided by exploiting the proposed topology, which might lead to a better quality of the produced power in terms of harmonics. This could also lead to removal or reduction of the size of the harmonic filters that are being used in conventional doubly-fed induction generator installations.</p><p>As regards to the drawbacks of the system, a comparison of converter and generator ratings between the analyzed solution and the conventional solution was performed. While the converters rating remain the same, there is one more electrical machine and the doubly-fed generator rating is slightly increased. Losses are also slightly larger due to the presence of the second machine.</p>
----------------------------------------------------------------------
In diva2:1071824 merged paragraphs & words, and missing ligatures, the abstract should be:

<p>The emerging field of mobile microrobotics has been suggestive of a plethora of applications, from intelligent micromanipulation to diversifying the prospects of minimally invasive surgeries. Several designs of microrobots have been proposed and are being notoriously studied to enhance the knowledge of their behavior in different environments to cater such applications. At present, control and surveillance of these microrobots have been aided by the convergence of various technologies like magnetic actuation, microscopy and computer vision. The quintessential knowledge of their maneuverability could provide interesting implications of these microrobots in their targeted applications.</p><p>In context of widening this understanding of their behavior, several dexterous methods have been proposed to study and control these microrobots, employing microscopy in stereo vision for visual surveillance. The intuitive drawback of microscopy, jeopardizing its focusing accuracy against field of imaging, has limited these studies to smaller observable volumes. Addressing limitations of conventional microscopy, holography has been explored as a potential candidate for imaging and spatial tracking of these microrobots by means of reconstructing 2-D information in the images. However, the resolution of depth estimation and processing cost incurred in reconstruction posed drawbacks on its applicability.</p><p>The novel method proposed in this report, employed the holographic imaging in stereo vision, overcoming the limitations of both conventional microscopy and holographic reconstruction. The cost of processing this image information at a much lower processing speeds further benchmarked its candidature for spatial tracking of microrobots. The inventive setup design proposed in the report was inspired by conventional stereo vision. It also entails the tracking procedure of a class of flagellated microrobots called Artificial Bacterial Flagella (ABF) based on retrieval of its two diffraction based holograms, and estimating its 3-D position based on lateral positions in the two stereo projections. It further suggests feasible design metrics for fabrication of these ABFs based on Fresnel dirffaction. The central idea behind this 3-D estimation proposed here is based on processing diffraction pattern produced by the ABFs using an image segmentation algorithm and further projecting the lateral coordinates so obtained to real space, achieving much more finesse over the depth resolution in either direction.</p>

----------------------------------------------------------------------
In diva2:1531638 merged words, the abstract should be:

<p>The thesis tackles the problem of data association for monocular object-based SLAM, which gets often omitted in related works. A method for estimating ellipsoid object landmark representations is implemented. This method uses bounding box multi-view object detections from 2D images with the help of YOLOv3 object detector and ORB-SLAM2 for camera pose estimation. The online data association uses SIFT image feature matching and landmark backprojection matching against bounding box detections to associate these object detections. This combination and its evaluation is the main contribution of the thesis. The overall algorithm is tested on several datasets, both real-world and computer rendered. The association algorithm manages well on the tested sequences and it is shown that matching with the backprojections of the ellipsoid landmarks improves the robustness of the approach. It is shown that with some implementation changes, the algorithm can run at real-time. The landmark estimation part works satisfactory for landmark initialization. Based on the findings future work is proposed.</p>
----------------------------------------------------------------------
In diva2:1413113 some missing spaces, the abstract should be:

<p>Automation of the forest industry has for over 30 years been an important subject of research, which could reduce the human workload and costs significantly. However, there are still many problems to be solved, such as enabling the communication between the heavy machinery in a forest and a remote base. High speed and reliable communication is the key to automated operations and remote control of machinery. This thesis investigates the feasibility and performance of IEEE 802.11n/ac WiFi hardware to provide high-bandwidth connection in a forest. In this project, the propagation of WiFi signals in the 2.4 GHz and 5 GHz bands in a typical Nordic forest environment has been simulated using specialized radio propagation software employing ray-tracing and different diffraction models to evaluate the path loss and signal strength. The simulations show that the idea is feasible if high-gain directional antennas are employed, as connections of sufficiently high speed (400+ Mbps for the 5 GHz band) can potentially be established for typical working distances, i.e. 300m. We then designed a directional antenna system and evaluated it in a real Nordic forest environment. We found that by manually aligning the antennas in a forest, reliable connections could be achieved up to 50 m without line-of-sight, however higher distances result in significantly lower speeds (13.3 Mbps at 80 m and 1.21 Mbps at 100 m) due to antenna misalignment. It is however possible to construct a more accurate, automated alignment system, which could replicate the simulation results and fully solve the problem of communication.</p>
----------------------------------------------------------------------
In diva2:618115 missing missing some spaces, missing ligatures, and missing emphasis, the abstract should be:

<p>Tangible Interface Objects underpin the interactions between users and a SAR environment. When utilizing SAR for rapid-prototyping work flows, particularly when the subject of the prototyping is a user-input centric design, the role of the Tangible Interface Objects is crucial. A Tangible Interface Object with form or functionality that does not reflect that of its real-world counterpart is detrimental to the prototyping workflow, where realism in prototypes is highly sought after. Moving from the use of `dumb' input controls with SAR-emulated functionality to `intelligent', state-aware input controls can greatly aid the rapid-prototyping work flow, and SAR environments generally. This research examines two areas: integrating sensors into input controls to enhance both the <em>self-awareness</em> and the <em>local environmental-awareness</em> of the input control, and increasing state-awareness of traditional input controls such as switches and radial dials. This second area has a focus on input controls which do not require a traditional power source. The results from both these areas demonstrate that `intelligent' Tangible Interface Objects are viable, providing numerous benefits to SAR scenes, particularly in the realm of rapid-prototyping.</p>
----------------------------------------------------------------------
In diva2:1249024 merged paragraphs & words and missing ligatures, the abstract should be:

<p>In the near future, the decommissioning of large power plants is planned in the Nordic electric power system, due to environmental and market reasons. This will be countered by an increase in the wind power installed capacity, as well as by significant investments in the transmission system. In such a context, characterized by several changes, the Nordic power system might face reliability challenges.</p><p>This thesis aims to calculate the risk of power shortage in the different price areas which constitute the Nordic power system, for three different scenarios: a base scenario 2015, scenario 2020, and scenario 2025. Different case studies, focusing on the Nordic power system and on some of its subsystems, are investigated. The reliability evaluation which is carried out follows a probabilistic approach, by means of Monte Carlo simulations. Crude Monte Carlo, as well as an advanced variance reduction technique – namely Cross-Entropy based Importance Sampling (CEIS) – are applied and compared. An alternative sampling method based on stratified sampling is presented too.</p><p>The starting point of this thesis is Viktor Terrier's 2017 Master thesis, “North European Power Systems Reliability" [1]. Model-wise, among the other improvements, load and wind power are sampled in a different way to account for the correlation between them. Data-wise, more realistic assumptions are made and more accurate data are used, thanks also to the collaboration with Sweco Energuide AB, Department of Energy Markets.</p><p>From the model perspective, it is concluded that CEIS outperforms crude Monte Carlo when simulating small to medium size systems, but it cannot be successfully applied when simulating large and very reliable systems like the Nordic system as a whole. The presented alternative sampling method can however be used for such cases. From the numerical-results perspective, the drawn conclusion is that the Nordic power system is estimated to become more reliable by years 2020 and 2025. Even if partly intermittent, more generation capacity is expected to be available, and thanks to the significant investments which are planned in the transmission system, it will be possible to effectively transmit more power where needed, regardless of the area where it has been generated.</p><p>The thesis is carried out at KTH Royal Institute of Technology, Department of Electric Power and Energy Systems, in collaboration with Sweco Energuide AB, Department of Energy Markets, within the frame of the North European Energy Perspectives Project (NEPP).</p>
----------------------------------------------------------------------
In diva2:1498908 merged words, the abstract should be:

<p>The ion acceleration process in Bipolar High Power Impulse Magnetron Sputtering is investigated for use in a novel space propulsion system - the BP-HiPIMS thruster. The interest for BP-HiPIMS has recently been growing within the area of thin film deposition due to the theorised acceleration of target ions caused by the reversed pulse following the regular HiPIMS pulse. This same acceleration could be used to produce thrust in a space propulsion system, where the lack of physical grids and temporal separation of ionisation and acceleration are attractive benefits of the suggested system. In this paper the physical processes and parameters of importance are experimentally investigated to gain understanding of the ion acceleration process with the goal of verifying the theory of BP-HiPIMS thusters. Through plasma potential measurements a beneficial potential structure between the magnetic trap and bulk of the plasma which could potentially accelerate ions is found at certain discharge conditions and some acceleration of ions is confirmed in mass spectrometer measurements. The results are promising for a thruster application but further research is needed to evaluate the viability of the proposed system.</p>

Note: 'thuster' should be 'thruster' but the error is in the original thesis
----------------------------------------------------------------------
In diva2:872371 merged paragraphs & words and missing ligatures, the abstract should be:

<p>Shaft current protection in hydro and turbo generators is an important generator protection issue. Currents flowing in the generator shaft might damage generator bearings which, in turn, could reduce operating time and cause large financial losses. Therefore, it is important to prevent operation of the generator under conditions of high shaft currents.</p><p>In this project, task was to develop measurement and protection system that is able to operate under certain conditions. Measurement device has to be able to accurately measure currents lower than 1 A in a generator shaft that can vary in diameter from 16 cm up to 3 m. Also, those currents might appear in frequencies equal to multiples of line frequency. Device is to be located in a limited space and in a proximity of the generator. Thus, stray flux is expected which might influence measurements. Furthermore, since currents that have to be measured are low, output of a measurement device is usually a low level signal. Such signal had to be catered for and adapted in a way that it can be used with numerical relay.</p><p>After literature review and overview of possible solutions, Rogowski coil was chosen as the measurement device which will be further analysed. Two other current transformers were considered which served as a good comparison with Rogowski coil. Several different tests and measurements were made on mentioned measurement devices. Also, it was investigated how IEC61850-9-2 and Merging Unit (MU) could be used in this application. Upon this investigation, complete protection systems were assembled in the laboratory and they were tested. To asses the behaviour of different systems in the real environment, test installation was built in the hydro power plant, Hallstahammar. This installation included traditional systems, with measurement signals connected to the relaya, and the one which utilized concepts of Process Bus and Merging Unit. Measurements and tests that were made there served as a final proof of successfulness of protection systems. Results showed that Rogowski coil was a suitable choice for a measurement device due to its beneficial mechanical and electrical properties. Also, tests made with actual shaft current showed advantages of using Rogowski coil in pair with Merging Unit and process bus over traditional protection systems. Nevertheless,it was confirmed that both types of systems satisfy project requirements.</p>


Note "relayå" is in the original abstract.
----------------------------------------------------------------------
In diva2:1537890 many unnecessary spaces and merged words, the abstract should be:

<p>N̈ar automatiserade fordon introduceras i trafiken och beḧover interagera med m̈anskliga f̈orare ̈ar det viktigt att kunna f̈orutsp̊a m̈anskligt beteende. Detta f̈or att kunna erh̊alla en s̈akrare trafiksituation. I denna studie har en modellsom estimerar m̈anskligt beteende utvecklats. Estimeringarna ̈ar baserade p̊a en Hidden Markov Model d̈ar observationer anv̈ands f̈or att besẗamma k̈orstil hos omgivande fordon i trafiken. Modellen tr̈anas med tv̊a olika metoder: Baum Welch tr̈aning och Viterbi tr̈aning f̈or att f̈orb̈attra modellens prestanda. Tr̈aningsmetoderna utv̈arderas sedan genom att analysera deras tidskomplexitet och konvergens. Modellen ̈ar implementerad medoch utan tr̈aning och testad f̈or olika k̈orstilar. Erh̊allna resultat visar att tr̈aning ̈ar viktigt f̈or att kunna f̈orutsp̊a m̈anskligt beteende korrekt. Viterbi tr̈aning ̈ar snabbare men mer k̈anslig f̈or brus i j̈amf̈orelse med Baum Welch tr̈aning. Viterbi tr̈aningger ̈aven en bra estimering i de fall d̊a observerad tr̈aningsdata avspeglar f̈orarens k̈orstil, vilket inte alltid ̈ar fallet. Baum Welch tr̈aning ̈ar mer robust i s̊adana situationer. Slutligen rekommenderas en estimeringsmodell implementerad med Baum Welch tr̈aning f̈or att erh̊alla en s̈aker k̈orning d̊a automatiserade fordon introduceras i trafiken</p>
----------------------------------------------------------------------

The above were all sent on or before 2024-08-23
======================================================================
In diva2:800428 merged paragraphs & words and missing hyphen, the abstract should be:

<p>Moore’s law is the observation that over the years, the transistor density will increase, allowing billions of transistors to be integrated on a single chip. Over the last two decades, Moore’s law has enabled the implementation of complex systems on a single chip (SoCs). The challenge of the System-on-Chip (SoC) era was the demand of an efficient communication mechanism between the growing number of processing cores on the chip. The outcome established an new interconnection scheme (among others, like crossbars, rings, buses) based on the telecommunication networks and the Network-on-Chip (NoC) appeared on the scene.</p><p>The NoC has been developed not only to support systems embedded into a single processor, but also to support a set of processors embedded on a single chip. Therefore, the Multi-Processors System on Chip (MPSoC) has arisen, which incorporate processing elements, memories and I/O with a fixed interconnection infrastructure in a complete integrated system. In such systems, the NoC constitutes the backbone of the communication architecture that targets future SoC composed by hundred of processing elements. Besides that, together with the deep sub-micron technology progress, some drawbacks have arisen. The communication efficiency and the reliability of the systems rely on the proper functionality of NoC for on-chip data communication. A NoC must deal with the susceptibility of transistors to failure that indicates the demand for a fault tolerant communication infrastructure. A mechanism that can deal with the existence of different classes of faults (transient,intermittent and permanent [11]) which can occur in the communication network.I</p><p>n this thesis, different algorithms are investigated that implement fault tolerant techniques for permanent faults in the NoC. The outcome would be to deliver a fault tolerant mechanism for the NoC System Generator Tool [29] which is a research in Network-on-Chip carried out at the Royal Institute of Technology. It will be explicitly described the fault tolerant algorithm that is implemented in the switch in order to achieve packet rerouting around the faulty communication links.</p>
----------------------------------------------------------------------
Correction in:
In diva2:814393. merged words and missing paragraph separation, abstract should be:

<p>Stock trading is increasingly done pseudo-automatically or fully automatically, using algorithms which make day-to-day or even moment-to-moment decisions.</p><p>This report investigates the possibility of creating a virtual stock trader, using a method used in Artificial Intelligence, called Neural Networks, to make intelligent decisions on when to buy and sell stocks on the stock market.</p><p>We found that it might be possible to earn money over a longer period of time, although the profit is less than the average stock index. However, the method also performed well in situations where the stock index is going down.</p>


I missed the hyphen in "day-to-day"
----------------------------------------------------------------------
In diva2:1223867 merged paragraphs and a missing ligature, the abstract should be:

<p>Alzheimer’s disease (AD) was discovered 111 years ago by Alois Alzheimer. Today, it is the leading cause of dementia in elderly, and incidence is expected to increase with life expectancy. By 2050, the number of affected individuals is predicted to reach 10 million [1]. There have been numerous attempts to describe AD by its primary hallmarks, including amyloid plaques, amyloid beta (Aβ) oligomers, and tau tangles. However, despite several decades of intense research, the cause of AD remains unknown.</p><p>Recently, there has been a focus on the inflammatory components of AD. There is an extensive activation of the immune system within the CNS of AD patients, but neither its cause nor its role in AD is known. However, there are strong indications that the inflammation has an autoimmune character. Considering this, there is an imperative need to examine autoimmunity within AD. In the present study, a proteomic approach was used to determine the autoantibody profiles within plasma and cerebrospinal fluid (CSF) within AD patients and healthy controls.</p><p>Paired plasma and CSF samples from 23 healthy controls and 49 patients were included in the present study. In addition, 2 plasma samples and 18 CSF samples from patients were included (not paired). One 380-plex and one 314-plex targeted suspension bead array (SBA), each consisting of color-coded magnetic microspheres with immobilized antigens, were used to analyze autoantibody profiles in all samples. The resulting data revealed an increased autoantibody response towards anti-gens SLC17A6 (Solute Carrier Family 17 Member 6), MAP1A (Microtubule Associated Protein 1A), and MAP2 (Microtubule Associated Protein 2) in patients compared to healthy controls. However, as these antigens have displayed wide reactivities in previous, unpublished studies, they require further investigation to determine their role in AD.</p><p>Furthermore, the paired CSF and plasma samples were used to investigate the correlation of autoantibody profiles within patients. The correlation was found to follow a normal distribution, with correlation being higher in antigens displaying stronger autoantibody reactivity. This work represents one of the first large-scale studies on the correlation of autoantibody profiles in plasma and CSF.</p>
----------------------------------------------------------------------
The abstract for diva2:1147592 does not match the thesis! This abstract is for
some other document.
----------------------------------------------------------------------
The above were all sent on or before 2024-08-31
======================================================================
diva2:778828 contains equations that use the Unicode Supplementary Private Use Area-B, for example the character U+100D46. However, as there is no full text in DiVA, I cannot make sense of these equations.

Additionally, the abstract seems to have had each line of text entered as a paragraph.
----------------------------------------------------------------------
In diva2:952512 missing "E" in "measurement" and merged paragraphs, the abstract should be:

<p>Wireless microphone systems are set up in schools to improve the sound quality for students with hearing difficulties. Karolinska Universitetssjukhuset is responsible for which resources to use and they purchase systems from different providers. The clients at Karolinska want the ability to check that the specifications of the systems are correct. Aims and objectives of the controls are to: 1. create a basis for questioning the providers’ specifications, 2. create a basis for improving communication with providers, 3. examining and comparing different microphone system for future investments and 4. detect individual defects. The object of this project is to find a suitable measurement equipment and to develop a measurement method to fulfill the aims 1-4.</p><p> In order to satisfy these aims, a number of interviews with experts in audio and microphone systems has been done. Based on the interviews CLIO Pocket was purchased and a measurement method has been developed. The measurement method describes how to use CLIO Pocket in order to check the systems’ bandwidth, noise, dynamic range, and total harmonic distorsion. Aims 2, 3 and 4 have been fulfilled, however Aim 1 was not fulfilled due to inadequate funding which lead to deficiencies in equipment.</p>
----------------------------------------------------------------------
Duplicates: diva2:635950 has no full text, while the version at diva2:646620 does
----------------------------------------------------------------------
In diva2:460449
"<p>[1]Förekomst av industriellt spillvärme vid låga temperaturer,  Ingrid Nyström, Per-Åke Franck, Industriell Energianalys AB, 2002-04-15</p>"

should be

"<p lang='sv'>[1]Förekomst av industriellt spillvärme vid låga temperaturer,  Ingrid Nyström, Per-Åke Franck, Industriell Energianalys AB, 2002-04-15</p>"
----------------------------------------------------------------------
The above were all sent on or before 2024-09-04
======================================================================
possible duplicates: diva2:751100 and diva2:751697
	 	     diva2:721862 and diva2:776949 - note diva2:721862 does not have full text, where as the other does
----------------------------------------------------------------------
The full text to diva2:1770888 does not currespond to the thesis. It actually
points to the text for diva2:1770883
----------------------------------------------------------------------
diva2:893748 missing space after "utav" in title
----------------------------------------------------------------------
In diva2:1229795, the title is missing a space:
"Integrating membrane filtration forwater reuse in tissue mill"
should be:
"Integrating membrane filtration for water reuse in tissue mill"
----------------------------------------------------------------------
In diva2:1219371 should 'ClickMucins' be 'Click Mucins'?
There is not full text in DiVA. But the two words seems to be used in the scientific literature.
----------------------------------------------------------------------
In diva2:756805 the word "Hope" should be "Hops" - the error is in the original thesis
----------------------------------------------------------------------
In diva2:1352888
There title is "Performance monitoring of systems for airpuricationAuthor:Anders"
should be:     "Performance monitoring of systems for air purification"
----------------------------------------------------------------------
In diva2:1159711, there are no fonts - it seems to just be a bit scan of the document.
PDF is images of the pages
----------------------------------------------------------------------
In diva2:1741440, there are no fonts - it seems to just be a bit scan of the document.
----------------------------------------------------------------------
In diva2:802843 'WU' is likely to be 'EU' - no full text in DiVA
----------------------------------------------------------------------
In diva2:722717 each line set as a paragraph!
----------------------------------------------------------------------
In diva2:730268 there is a strange character coding of text, so one cannot easily search for a string
----------------------------------------------------------------------
The above were all sent on or before 2024-09-19
======================================================================
In diva2:1154619 "triglycer ides" shouold be "triglycerides"
this will make it match the Swedish abstract.
----------------------------------------------------------------------
In diva2:1741633 - the PDF is just images of the pages
----------------------------------------------------------------------
The equation n page 28 of https://kth.diva-portal.org/smash/get/diva2:848284/FULLTEXT01.pdf seems to be odd.
It looks like a CAPTCHA test!
-------------------------------------------------------------------------------
In https://kth.diva-portal.org/smash/get/diva2:757292/FULLTEXT01.pdf there are only the two abstracts, there is no additional text.
----------------------------------------------------------------------
In diva2:1577129 the full text has a strange character mapping.
----------------------------------------------------------------------
In diva2:1412560 - as space missing in title: "Automated Intro Detection ForTV Series"
                                    should be "Automated Intro Detection For TV Series"
----------------------------------------------------------------------
In diva2:1785530 - the PDF is just pictures of pages - it is not searchable.
----------------------------------------------------------------------
----------------------------------------------------------------------
Abstract corrections - starting with the one that has the most merged words
----------------------------------------------------------------------
In diva2:1219373 abstract is: <p>The prevalence of antibiotic resistantmicroorganisms is rising and has become one ofthe biggest threats to global health. As a result ofthe decrease of antibiotic effectiveness it hasbecome desirable to develop new technologiesto circumvent or reduce antibiotics usage.Particularly, in surgeries where implants andprosthetic devices are incorporated into the bodyantibiotics are critical to prevent infections. Analternative strategy to prevent extensive use ofantibiotics in this field is to create antimicrobialsurfaces. In this report, antimicrobial surfaceswere created by coupling antibacterial and antibiofilmenzymes to the recombinant spider silk4RepCT. The coupled enzyme were theendolysins Sal-1 and PlysS2, the catalyticdomain of Sal-1 called CHAP and Dispersin B.The coupling was obtained by Sortase Amediated protein conjugation, and investigatedwith SDS-PAGE analysis for coupling reactionin solution and with Octet-analysis for couplingreaction with 4RepCT in coating format.Furthermore, the enzymatic activity of theenzyme coupled spider silk was investigatedwith Turbidity Reduction Assay, where thereduction in OD600 of Staphylococcus aureusbacteria was measured. For Sal-1 and DispersinB an activity test with a substrate addition assaywas also performed. The coupling reaction insolution showed successful coupling for CHAPand Sal-1. Coupling reactions in solution withDispersin B and PlySs2 did not clearly indicatesuccessful coupling. However, the Octetanalysisindicated successful coupling with allenzymes. Enzymatic activity could bedemonstrated for Sal-1 coupled spider silk in theTRA and a substrate addition assay withFluorescein-Di-b-D-Galactopyranoside (FDG).In the TRA, Sal-1 coupled spider silk coatings,prepared by using 17.3 μM of 4RepCT forcoatings and 17.3 μM of Sal-1 for coupling3(17.3/17.3-coating), were shown to reduceOD600 with 62 % after 100 minutes. For Sal-1coupled spider silk coatings, prepared by using17.3 μM Sal-1 and 4.3 μM of 4RepCT(17.3/4.3-coating), the OD600 reduction was 48% after 100 minutes. The OD600 reduction of17.3/17.3-coatings and 17.3/4.3-coatings wereconsidered comparable with 25 nM soluble Sal-1 and 12.5 nM soluble Sal-1 respectively.</p>

corrected abstract: 
<p>The prevalence of antibiotic resistant microorganisms is rising and has become one of the biggest threats to global health. As a result of the decrease of antibiotic effectiveness it has become desirable to develop new technologies to circumvent or reduce antibiotics usage. Particularly, in surgeries where implants and prosthetic devices are incorporated into the body antibiotics are critical to prevent infections. An alternative strategy to prevent extensive use of antibiotics in this field is to create antimicrobialsurfaces. In this report, antimicrobial surfaces were created by coupling antibacterial and antibiofilmenzymes to the recombin ant spider silk 4RepCT. The coupled enzyme were the endolysins Sal-1 and PlySs2, the catalytic domain of Sal-1 called CHAP and Dispersin B. The coupling was obtained by Sortase A mediated protein conjugation, and investigated with SDS-PAGE analysis for coupling reaction in solution and with Octet-analysis for coupling reaction with 4RepCT in coating format. Furthermore, the enzymatic activity of the enzyme coupled spider silk was investigated with Turbidity Reduction Assay, where the reduction in OD600 of Staphylococcus aureusbacteria was measured. For Sal-1 and DispersinB an activity test with a substrate addition assay was also performed. The coupling reaction in solution showed successful coupling for CHAP and Sal-1. Coupling reactions in solution with Dispersin B and PlySs2 did not clearly indicate successful coupling. However, the Octet-analysis is indicated successful coupling with all enzymes. Enzymatic activity could be demonstrated for Sal-1 coupled spider silk in the TRA and a substrate addition assay with Fluorescein-Di-b-D-Galactopyranoside (FDG). In the TRA, Sal-1 coupled spider silk coatings, prepared by using 17.3 μM of 4RepCT for coatings and 17.3 μM of Sal-1 for coupling 3(17.3/17.3-coating), were shown to reduce OD600 with 62 % after 100 minutes. For Sal-1 coupled spider silk coatings, prepared by using 17.3 μM Sal-1 and 4.3 μM of 4RepCT(17.3/4.3-coating), the OD600 reduction was 48% after 100 minutes. The OD600 reduction of 17.3/17.3-coatings and 17.3/4.3-coatings were considered comparable with 25 nM soluble Sal-1 and 12.5 nM soluble Sal-1 respectively.</p>
----------------------------------------------------------------------
In diva2:826712 abstract is: <p>Telge Nät is the current owner and operator for Södertäljes district heating system. In this project a part of this system which includes Scania industial area as well as the residential areas of Pershagenand Värdsholmen will be investigated. The heating for this system is provided partly by thecogeneration plant of Igelsta as well as from heat production and recovery within the area of Scania.The main purpose of this project is to investigate whether or not these unusual operationcircumstances result in any temperature or pressure fluctuations which could lead to advancedfatigue on the system. Temperature fluctuations have been analyzed for main pipe as well as forB006, B210 and Clab which are all larger complexes within the area.Pressure fluctuations have been analyzed for the complex of B210 where hot water as well assuperheated water is used. This leads to intense pump and valve adjustments which is likely to causea lot of pressure fluctuations. The pressure is also measured at the main pipe to investigate if thefluctuations from B210 spread to other parts of the system.Analysis of the temperature fluctuations data from 2012 showed a correlation between the outdoorstemperature and the temperature within the district heating system. Following investigationsmeasured the amount of temperature cycles at the different complexes at several differentmagnitudes. The Temperature cycles were thereafter converted into full temperature cycleequivalents at 110°C using Palmgren-Miners hypothesis. These results were used to maketemperature fatigue estimations for the different complexes. The estimations showed that Clab wasa relatively stable system and that the fluctuations were kept within an acceptable range. Thetemperature of the feed pipe at B006 was proven to be quite unstable; this could be a direct result ofthe overall instabilities on the main pipe caused by heat production and heat recovery within Clabsand B210.The temperature at B210 was very unstable for the superheated water return pipe and extremelyunstable for the hot water return pipe. This instability could in the long run lead to a severelydecreased lifetime for the whole system. The instability is likely to be caused by a constant feed ofsuperheated water for heating at a paint shop which is located within B210, water that is fedregardless if there is need for heating in the paint shop or not. Excess superheated water which is notused for heating the paint shop is redirected to heat areas and pipes where hot water is normally theheat source, something which leads to large fluctuations on the system. By reducing the excess feedof superheated water to the paint shop a large portion of the problems with the systems could besolved.The pressure fluctuations at B210 were analyzed with a measure interval of 0.1 seconds and 30seconds between the measurements. This was done to determine whether or not the intervalbetween the measurements had a big influence on the registered pressure transient. Analysisindicated that pressure transients where registered as slightly bigger and relatively faster when themeasure interval of 0.1 seconds was used. It is however known that even a measure interval of 0.1seconds is far too slow to provide any results on the actual magnitude of the pressure transient. Theresult of this analyze should therefore not be considered as definitive.The largest pressure transients at B210 were registered at the startup of the paint shop. These werehowever still at a level where no damage is expected to occur on the system. Measurements at themain pipe showed no traces of the pressure transients from B210. This leads to the concussion thatthe analyzed pressure fluctuations are kept within an acceptable range.</p>

w='industial' val={'c': 'industrial', 's': 'diva2:826712', 'n': 'error in original'}

corrected abstract:
<p>Telge Nät is the current owner and operator for Södertäljes district heating system. In this project apart of this system which includes Scania industial area as well as the residential areas of Pershagen and Värdsholmen will be investigated. The heating for this system is provided partly by the cogeneration plant of Igelsta as well as from heat production and recovery within the area of Scania. The main purpose of this project is to investigate whether or not these unusual operation circumstances result in any temperature or pressure fluctuations which could lead to advanced fatigue on the system. Temperature fluctuations have been analyzed for main pipe as well as for B006, B210 and Clab which are all larger complexes within the area. Pressure fluctuations have been analyzed for the complex of B210 where hot water as well as superheated water is used. This leads to intense pump and valve adjustments which is likely to cause a lot of pressure fluctuations. The pressure is also measured at the main pipe to investigate if the fluctuations from B210 spread to other parts of the system. Analysis of the temperature fluctuations data from 2012 showed a correlation between the outdoors temperature and the temperature within the district heating system. Following investigations measured the amount of temperature cycles at the different complexes at several different magnitudes. The Temperature cycles were thereafter converted into full temperature cycle equivalents at 110°C using Palmgren-Miners hypothesis. These results were used to make temperature fatigue estimations for the different complexes. The estimations showed that Clab wasa relatively stable system and that the fluctuations were kept within an acceptable range. The temperature of the feed pipe at B006 was proven to be quite unstable; this could be a direct result of the overall instabilities on the main pipe caused by heat production and heat recovery within Clabs and B210. The temperature at B210 was very unstable for the superheated water return pipe and extremely unstable for the hot water return pipe. This instability could in the long run lead to a severely decreased lifetime for the whole system. The instability is likely to be caused by a constant feed of superheated water for heating at a paint shop which is located within B210, water that is fed regardless if there is need for heating in the paint shop or not. Excess superheated water which is not used for heating the paint shop is redirected to heat areas and pipes where hot water is normally the heat source, something which leads to large fluctuations on the system. By reducing the excess feed of superheated water to the paint shop a large portion of the problems with the systems could be solved. The pressure fluctuations at B210 were analyzed with a measure interval of 0.1 seconds and 30 seconds between the measurements. This was done to determine whether or not the interval between the measurements had a big influence on the registered pressure transient. Analysis indicated that pressure transients where registered as slightly bigger and relatively faster when the measure interval of 0.1 seconds was used. It is however known that even a measure interval of 0.1 seconds is far too slow to provide any results on the actual magnitude of the pressure transient. The result of this analyze should therefore not be considered as definitive. The largest pressure transients at B210 were registered at the startup of the paint shop. These were however still at a level where no damage is expected to occur on the system. Measurements at the main pipe showed no traces of the pressure transients from B210. This leads to the concussion that the analyzed pressure fluctuations are kept within an acceptable range.</p>
----------------------------------------------------------------------
In diva2:1447055 abstract is: <p>In the recent years microplastics in the marine environment has been recognized as a potentiallyimportant environmental issue. Today there are microplastics spread in the waterbodies all overthe world, from the equator to the poles in south and north. In 2016 artificial turf was labeled thesecond largest source of microplastics to the marine environment in Sweden [1]. Football is thenational sport of Sweden and accounts for the majority of the activity hours among the youth inSweden. The artificial turf has made it possible for more children to play football and for them toget more hours on the field. Today about 90 % of the football players play on artificial turf [2].The microplastics pathways to the nature and the marine environment were studied andtreatment methods were developed. One of these methods is the so called granule trap, a filterbag which is placed in a stormwater drainage well to catch the rubber granulates and the artificialturf fibers which can be spread from the artificial field to the drainage system. The aim of thisstudy was to optimize the granule trap for possible waterflows to the stormwater drainage welland its efficiency to catch microplastics. This was researched through field studies of the efficiencyof the granule trap at two artificial turfs in Stockholm and the development of a waterflow modelof an artificial turf with varying construction.The rainfall which was used in the waterflow model was the 10-year storm with a duration of 10minutes. This to find the maximum waterflow the granuletraps must manage. The waterflows tothe stormwater drainage well were dependent on the number of wells placed around the artificialturf, in which area of Sweden the football field was placed, in other words the amount of rain thatfell, and the infiltration capacity of the artificial turf. The waterflow model works as a templatefor possible waterflows at an artificial turf with a certain construction and at a certain location inSweden.The artificial turfs which were examined in the field studies were Skytteholms IP in Solna andSpånga IP in Stockholm. At each football field 6 granuletraps were placed, each loaded with twofilter bags, the inner with larger sized mesh and the outer with smaller sized mesh. The mesh sizecombinations were 200 μm with 100 μm, 200 μm with 50 μm and 100 μm with 50 μm. atSkytteholms IP a total amount of 10.3 kg microplastics were caught and at Spånga IP a total of 1.5kg microplastics were caught during the 49 days the granuletraps were placed at the footballfields. Out of the total amount of microplastics in each granuletrap at least 99 % by mass was inthe inner filter bag and maximum 1 % by mass was in the outer filter bag, in the size fractionbetween the outer and the inner filter bag..In conclusion this study shows that the waterflow to the stormwater drainage wells placed aroundthe artificial turfs vary a lot depending on the construction of the artificial turf. Foremost itdepends on the infiltration capacity of the artificial turf and the number of stormwater drainagewells around the field. With regards to the waterflows from the waterflow model and the resultsfrom the field studies the recommended mesh size for the filter bags is 200 μm. This since at least99 % by mass of the microplastics, which were larger than 50 μm, that reached the granule trapsIVwere trapped in the inner filter bag and the elevated risk of clogging and biofilm growth on thefilter bags with smaller mesh size. Further studies should be conducted on the waterflow throughthe granuletraps over time, microplastics smaller than 50 μm, other pathways for themicroplastics away from the artificial turf, improved constructions of artificial turfs and improvedmaintenance on the artificial turfs to reduce the risk of spreading of microplastics from artificialturfs.</p>

w='trapsIVwere' val={'c': 'traps were', 's': 'diva2:1447055', 'n': 'The "IV" was from the page number!'}

corrected abstract:
<p>In the recent years microplastics in the marine environment has been recognized as a potentially important environmental issue. Today there are microplastics spread in the waterbodies all over the world, from the equator to the poles in south and north. In 2016 artificial turf was labeled the second largest source of microplastics to the marine environment in Sweden [1]. Football is the national sport of Sweden and accounts for the majority of the activity hours among the youth in Sweden. The artificial turf has made it possible for more children to play football and for them to get more hours on the field. Today about 90 % of the football players play on artificial turf [2].</p><p>The microplastics pathways to the nature and the marine environment were studied and treatment methods were developed. One of these methods is the so called granule trap, a filter bag which is placed in a stormwater drainage well to catch the rubber granulates and the artificial turf fibers which can be spread from the artificial field to the drainage system. The aim of this study was to optimize the granule trap for possible waterflows to the stormwater drainage well and its efficiency to catch microplastics. This was researched through field studies of the efficiency of the granule trap at two artificial turfs in Stockholm and the development of a waterflow model of an artificial turf with varying construction.</p><p>The rainfall which was used in the waterflow model was the 10-year storm with a duration of 10 minutes. This to find the maximum waterflow the granuletraps must manage. The waterflows to the stormwater drainage well were dependent on the number of wells placed around the artificial turf, in which area of Sweden the football field was placed, in other words the amount of rain that fell, and the infiltration capacity of the artificial turf. The waterflow model works as a template for possible waterflows at an artificial turf with a certain construction and at a certain location in Sweden.</p><p>The artificial turfs which were examined in the field studies were Skytteholms IP in Solna and Spånga IP in Stockholm. At each football field 6 granuletraps were placed, each loaded with two filter bags, the inner with larger sized mesh and the outer with smaller sized mesh. The mesh size combinations were 200 μm with 100 μm, 200 μm with 50 μm and 100 μm with 50 μm. at Skytteholms IP a total amount of 10.3 kg microplastics were caught and at Spånga IP a total of 1.5 kg microplastics were caught during the 49 days the granuletraps were placed at the football fields. Out of the total amount of microplastics in each granuletrap at least 99 % by mass was in the inner filter bag and maximum 1 % by mass was in the outer filter bag, in the size fraction between the outer and the inner filter bag..</p><p>In conclusion this study shows that the waterflow to the stormwater drainage wells placed around the artificial turfs vary a lot depending on the construction of the artificial turf. Foremost it depends on the infiltration capacity of the artificial turf and the number of stormwater drainage wells around the field. With regards to the waterflows from the waterflow model and the results from the field studies the recommended mesh size for the filter bags is 200 μm. This since at least 99 % by mass of the microplastics, which were larger than 50 μm, that reached the granuletraps were trapped in the inner filter bag and the elevated risk of clogging and biofilm growth on the filter bags with smaller mesh size. Further studies should be conducted on the waterflow through the granuletraps over time, microplastics smaller than 50 μm, other pathways for the microplastics away from the artificial turf, improved constructions of artificial turfs and improved maintenance on the artificial turfs to reduce the risk of spreading of microplastics from artificial turfs.</p>
----------------------------------------------------------------------
In diva2:1770583 abstract is: <p>One of the biggest challenges in healthcare is Emergency Department (ED)crowding which creates high constraints on the whole healthcare system aswell as the resources within and can be the cause of many adverse events.Is is a well known problem were a lot of research has been done and a lotof solutions has been proposed, yet the problem still stands unsolved. Byanalysing Real-World Data (RWD), complex problems like ED crowding couldbe better understood. Currently very few applications of survival analysis hasbeen adopted for the use of production data in order to analyze the complexityof logistical problems. The aims for this thesis was to apply survival analysisthrough advanced Machine Learning (ML) models to RWD collected at aSwedish hospital too see how the Length Of Stay (LOS) until admission ordischarge were affected by different factors. This was done by formulating thecrowding in the ED for survival analysis through the use of the LOS as thetime and the decision regarding admission or discharge as the event in order tounfold the clinical complexity of the system and help impact clinical practiceand decision making.By formulating the research as time-to-event in combination with ML, thecomplexity and non linearity of the logistics in the ED is viewed from a timeperspective with the LOS acting as a Key Performance Indicator (KPI). Thisenables the researcher to look at the problem from a system perspective andshows how different features affect the time that the patient are processedin the ED, highlighting eventual problems and can therefore be useful forimproving clinical decision making.</p><p>Five models: Cox Proportional Hazards(CPH), Random Survival Forests (RSF), Gradient Boosting (GB), ExtremeGradient Boosting (XGB) and DeepSurv were used and evaluated using theConcordance index (C-index) were GB were the best performing model witha C-index of 0.7825 showing that the ML models can perform better than thecommonly used CPH model. The models were then explained using SHapleyAdaptive exPlanations (SHAP) values were the importance of the featureswere shown together with how the different features impacted the LOS. TheSHAP also showed how the GB handled the non linearity of the features betterthan the CPH model. The five most important features impacting the LOS wereif the patient received a scan at the ED, if the visited and emergency room,age, triage level and the label indicating what type of medical team seemsmost fit for the patient. This is clinical information that could be implementedto reduce the crowding through correct decision making. These results show that ML based survival analysis models can be used for further investigationregarding the logistic challenges that healthcare faces and could be furtherused for data analysis with production data in similar cases. The ML survivalanalysis pipeline can also be used for further analysis and can act as a first stepin order to pinpoint important information in the data that could be interestingfor deeper data analysis, making the process more efficient.</p>

corrected abstract:
<p>One of the biggest challenges in healthcare is Emergency Department (ED) crowding which creates high constraints on the whole healthcare system as well as the resources within and can be the cause of many adverse events. Is is a well known problem were a lot of research has been done and a lot of solutions has been proposed, yet the problem still stands unsolved. By analysing Real-World Data (RWD), complex problems like ED crowding could be better understood. Currently very few applications of survival analysis has been adopted for the use of production data in order to analyze the complexity of logistical problems. The aims for this thesis was to apply survival analysis through advanced Machine Learning (ML) models to RWD collected at a Swedish hospital too see how the Length Of Stay (LOS) until admission or discharge were affected by different factors. This was done by formulating the crowding in the ED for survival analysis through the use of the LOS as the time and the decision regarding admission or discharge as the event in order to unfold the clinical complexity of the system and help impact clinical practice and decision making.</p><p>By formulating the research as time-to-event in combination with ML, the complexity and non linearity of the logistics in the ED is viewed from a time perspective with the LOS acting as a Key Performance Indicator (KPI). This enables the researcher to look at the problem from a system perspective and shows how different features affect the time that the patient are processed in the ED, highlighting eventual problems and can therefore be useful for improving clinical decision making. Five models: Cox Proportional Hazards (CPH), Random Survival Forests (RSF), Gradient Boosting (GB), Extreme Gradient Boosting (XGB) and DeepSurv were used and evaluated using the Concordance index (C-index) were GB were the best performing model with a C-index of 0.7825 showing that the ML models can perform better than the commonly used CPH model. The models were then explained using SHapley Adaptive exPlanations (SHAP) values were the importance of the features were shown together with how the different features impacted the LOS. The SHAP also showed how the GB handled the non linearity of the features better than the CPH model. The five most important features impacting the LOS were if the patient received a scan at the ED, if the visited and emergency room, age, triage level and the label indicating what type of medical team seems most fit for the patient. This is clinical information that could be implemented to reduce the crowding through correct decision making. These results show that ML based survival analysis models can be used for further investigation regarding the logistic challenges that healthcare faces and could be further used for data analysis with production data in similar cases. The ML survival analysis pipeline can also be used for further analysis and can act as a first step in order to pinpoint important information in the data that could be interesting for deeper data analysis, making the process more efficient.</p>
----------------------------------------------------------------------
In diva2:573478 abstract is: <p><strong>Background: </strong>Since1994, the EU NickelDirective is limiting the release of nickel from objects that are in prolongedcontact with skin. Despite this, nickel is still the most common cause of contactallergy in industrialized countries. This could implicate that other productswith short contact to the skin, which are not restricted by EU´s legislation inREACH (Registration, Evaluation, Authorisation and Restriction of Chemicalsubstances), could be a source of nickel or cobalt exposure. There is no limitfor cobalt release within REACH. To determine if there is cobalt or nickelpresent on the surface of a material and if metal can be released in contactwith the skin, there are quantitative and qualitative methods, such as the DMG(dimethylglyoxime)-test, the cobalt spot-test and a method for releasedetermination according to EN1811.<strong>Objectives: </strong>Laptops are not included in the REACH legislation. The purpose of thisstudy was to investigate whether nickel or cobalt can be found on computersurfaces and if nickel and cobalt can be released from surfaces that are incontact with skin, during work with laptops. Are there any variations betweendifferent models or different brands of laptops (two years old or less) andwhat levels of nickel and cobalt are released from different surfaces on a specificHP computer, that gave positive results for nickel and cobalt in the spot- tests? <strong>Methodand materials: </strong>In this study 30laptops (7 Hewlett Packard (HP), 18 Dell, 3 Sony Vaio, 1Fujitsu and 1 Macbook), weretested by using DMG-test and cobalt spot-test. To measure the release of nickeland cobalt on one specific laptop (HP2560p), the standard method EN1811 and amodified version of the method, were used. <strong>Results: </strong>Laptopsof different models and manufacturers were tested for nickel by using theDMG-test. The test turned out positive for all tested laptops from HP and 4 of18 of the laptops from Dell. For cobalt, onlyone laptop (HP) of the total 30 computers, gave a positive result. The resultsfrom the release test of a specific laptop, was lower than the limit thresholdvalue within REACH (0.5µg/cm<sup>2</sup>/week). The highest amounts of nickel,0.1μg/cm<sup>2</sup>/week, were measured for the computer components thatwere derived from the palm rest. All the cobalt levels, except one, were belowthe detection limit for the analysis. <strong>Conclusions: </strong>Our study indicatesthat laptops are a potential source for nickel deposition onto skin. In this study, computers from five differentmanufacturers have been tested, and it turned out that the tested models from HPand Dell released nickel. Cobalt was only released in very low concentrations,at a level near the LOD for the analysis.</p>

corrected abstract:
<p><strong>Background: </strong>Since 1994, the EU Nickel Directive is limiting the release of nickel from objects that are in prolonged contact with skin. Despite this, nickel is still the most common cause of contact allergy in industrialized countries. This could implicate that other products with short contact to the skin, which are not restricted by EU´s legislation in REACH (Registration, Evaluation, Authorisation and Restriction of Chemical substances), could be a source of nickel or cobalt exposure. There is no limit for cobalt release within REACH. To determine if there is cobalt or nickel present on the surface of a material and if metal can be released in contact with the skin, there are quantitative and qualitative methods, such as the DMG (dimethylglyoxime)-test, the cobalt spot-test and a method for release determination according to EN1811.<strong>Objectives: </strong>Laptops are not included in the REACH legislation. The purpose of this study was to investigate whether nickel or cobalt can be found on computer surfaces and if nickel and cobalt can be released from surfaces that are in contact with skin, during work with laptops. Are there any variations between different models or different brands of laptops (two years old or less) and what levels of nickel and cobalt are released from different surfaces on a specific HP computer, that gave positive results for nickel and cobalt in the spot- tests? <strong>Method and materials: </strong>In this study 30 laptops (7 Hewlett Packard (HP), 18 Dell, 3 Sony Vaio, 1 Fujitsu and 1 Macbook), were tested by using DMG-test and cobalt spot-test. To measure the release of nickel and cobalt on one specific laptop (HP2560p), the standard method EN1811 and a modified version of the method, were used. <strong>Results: </strong>Laptops of different models and manufacturers were tested for nickel by using the DMG-test. The test turned out positive for all tested laptops from HP and 4 of 18 of the laptops from Dell. For cobalt, only one laptop (HP) of the total 30 computers, gave a positive result. The results from the release test of a specific laptop, was lower than the limit threshold value within REACH (0.5µg/cm<sup>2</sup>/week). The highest amounts of nickel, 0.1μg/cm<sup>2</sup>/week, were measured for the computer components that were derived from the palm rest. All the cobalt levels, except one, were below the detection limit for the analysis. <strong>Conclusions: </strong>Our study indicates that laptops are a potential source for nickel deposition onto skin. In this study, computers from five different manufacturers have been tested, and it turned out that the tested models from HP and Dell released nickel. Cobalt was only released in very low concentrations, at a level near the LOD for the analysis.</p>
----------------------------------------------------------------------
In diva2:1170378 abstract is: <p>Nitrogen reduction is an important process that many of the larger treatment plants in Swedenuse since nitrogen affects the environment and the global warming. An efficient method to getrid of nitrogen is to use a process called denitrification. In this process a carbon source is usuallyused and has a main purpose to reduce the nitrogen to more stable and less reactive molecules.There are many parameters to take into consideration when choosing the most suitable carbonsource for the specific treatment plant. The reason for this is that the carbon sources that areavailable are more or less effective in different environments which depends on parameters suchas pH levels and temperatures.</p><p>SÖRAB use Brenntaplus as carbon source in their denitrification process to achieve a reductionof nitrogren. They have recently looked at alternative carbon sources that could replace parts ortheir current one. The primary aim with this project was to investigate if the glycol that SÖRABreceive as waste from the public could be used as carbon source for the leachate, or if it containstoo much dangerous chemicals and heavy metals that will affect the wastewater. In addition tothe glycol, there was some focus was on comparing some other carbon sources to each other inan economic and environmental perspective.</p><p>To determine wheter the incoming glycol is suitable to use as a carbon source or not, severalsamplings were made on different batches of the incoming glycol. The purpose of this was tosee if there was a variety in the composition of the glycol from batch to batch. To determine ifthe content of the glycol would affect the wastewater by raising the concentration of dangerouschemicals to such a high values that the wastewater would not meet the emission requirementsthat SÖRAB has set, a calculation were made to see how much the glycol would affect theleachate.</p><p>When reducing nitrogen in the presence of a carbon source it is important to acknowledge thatthe amount of COD in the carbon source determines how much nitrogen that can be reduced.The problem that occurred is the lack of practical testing in the process makes it hard to give aprecise number of how much carbon source that is needed to give the desired nitrogen reduction.</p><p>The carbon sources that were investigated were ethanol, methanol, glycerol, acetic acid and thecurrent one, Brenntaplus. From an economical perspective the price for respective carbon sourcewere the same if looking one the amount needed in consideration to the price per kilo for eachone. The carbon sources work in different conditions and need different requirements to workfunctionally. Some of the carbon sources are an explosive risk and need special handling andsome sources have a very high freezing temperature which requires special storage.</p><p>The conclusion that can be drawn from this master thesis is that every individual treatment plantreacts different to different carbon sources and it is really hard to theoretically estimate whatamount that needs to be dosed. Theoretical a reference value can be determined and in regard tothis the dosage can be adjusted with several tests until an amount of carbon source that give thewanted nitrogen reduction is found. The deposited glycol that SÖRAB receives from theirrecyclingstations is considered to not be suitable for dosage because the composition differsfrom batch to batch.</p>

w='nitrogren' val={'c': 'nitrogen', 's': 'diva2:1170378', 'n': 'error in original'}
note "recyclingstations" set as one word in the original


corrected abstract: 
<p>Nitrogen reduction is an important process that many of the larger treatment plants in Sweden use since nitrogen affects the environment and the global warming. An efficient method to get rid of nitrogen is to use a process called denitrification. In this process a carbon source is usually used and has a main purpose to reduce the nitrogen to more stable and less reactive molecules. There are many parameters to take into consideration when choosing the most suitable carbon source for the specific treatment plant. The reason for this is that the carbon sources that are available are more or less effective in different environments which depends on parameters such as pH levels and temperatures.</p><p>SÖRAB use Brenntaplus as carbon source in their denitrification process to achieve a reduction of nitrogren. They have recently looked at alternative carbon sources that could replace parts or their current one. The primary aim with this project was to investigate if the glycol that SÖRAB receive as waste from the public could be used as carbon source for the leachate, or if it contains too much dangerous chemicals and heavy metals that will affect the wastewater. In addition to the glycol, there was some focus was on comparing some other carbon sources to each other in an economic and environmental perspective.</p><p>To determine wheter the incoming glycol is suitable to use as a carbon source or not, several samplings were made on different batches of the incoming glycol. The purpose of this was to see if there was a variety in the composition of the glycol from batch to batch. To determine if the content of the glycol would affect the wastewater by raising the concentration of dangerous chemicals to such a high values that the wastewater would not meet the emission requirements that SÖRAB has set, a calculation were made to see how much the glycol would affect the leachate.</p><p>When reducing nitrogen in the presence of a carbon source it is important to acknowledge that the amount of COD in the carbon source determines how much nitrogen that can be reduced. The problem that occurred is the lack of practical testing in the process makes it hard to give a precise number of how much carbon source that is needed to give the desired nitrogen reduction.</p><p>The carbon sources that were investigated were ethanol, methanol, glycerol, acetic acid and the current one, Brenntaplus. From an economical perspective the price for respective carbon source were the same if looking one the amount needed in consideration to the price per kilo for each one. The carbon sources work in different conditions and need different requirements to work functionally. Some of the carbon sources are an explosive risk and need special handling and some sources have a very high freezing temperature which requires special storage.</p><p>The conclusion that can be drawn from this master thesis is that every individual treatment plant reacts different to different carbon sources and it is really hard to theoretically estimate what amount that needs to be dosed. Theoretical a reference value can be determined and in regard to this the dosage can be adjusted with several tests until an amount of carbon source that give the wanted nitrogen reduction is found. The deposited glycol that SÖRAB receives from their recyclingstations is considered to not be suitable for dosage because the composition differs from batch to batch.</p>
----------------------------------------------------------------------
In diva2:1217827 abstract is: <p>In kraft pulping, one of the main issues is the extensive wood losses. With increasing prices ofwoody biomass an incentive towards minimizing the wood losses exists. Amongst the variousprocess steps, the impregnation of wood chips has shown to enhance the cooking by providinga homogeneous distribution of chemicals inside the chips. It is proven that a more proficientimpregnation phase can improve the overall yield in kraft pulping. However, there is a lack ofscientific research comparing different impregnation techniques for hardwood. Hence, thisthesis will attempt to clarify the impregnation of hardwood.The impregnation efficiency was studied by comparing three different impregnation methods:High Alkali Impregnation (HAI), Extended Impregnation (EI) using a low alkali level and aReference Impregnation (REF) to enable a comparison to the industrially establishedconditions. The cases were compared by analysing the yield, selectivity and homogeneity. Thecomparison was also made under cooking conditions with the objective to understand theimpact of impregnation on the subsequent cooking phase. The cooking procedure was assessedby analysing the degree of delignification, yield and reject content.In impregnation, most chemical consuming reactions occurred within the first 10-30 minutes,mainly contributed by deacetylation. HAI obtained the fastest homogeneous distribution of OH-(~60 min), but the fastest dissolution of wood. The effect was contributed by the high [OH-],providing fast diffusion of ions and rapid dissolution of xylan. In the contrary, EI attained thehighest impregnation yield after a given impregnation time but required a prolonged durationto obtain a chemical equilibrium between the free and bound liquor (~120 min). REF showeda higher yield than HAI and similar chemical equilibrium as EI. The hydrosulphide sorption inimpregnation was highest for EI due to the high initial sulphidity charge and similar for REFand HAI. For impregnations at 115°C, the HS- sorption was significantly increased for all cases,resulting from delignification. In the subsequent cooking phase, it was prevalent that impregnation of chips under EIconditions were easier delignified, leading to a reduced cooking time to reach the defibrationpoint. Birch was more prone to delignification than eucalyptus. In turn, eucalyptus also obtaineda higher defibration point. Highest total cooking yield at similar kappa numbers was achievedwith REF conditions, followed by HAI and lastly the EI conditions. The high yield of REF incontrast to HAI could be explained by an improved xylan yield due to an alleviated hydroxidelevel. The low yield of EI can be assigned to continues peeling due to the prolongedimpregnation and loss of xylan when removing black liquor after impregnation. In terms ofproduction rate, yield, energy and chemical consumption the REF is the most efficientimpregnation condition for birch kraft cooking in this batchwise laboratory kraft cookingprocedure.</p>

w='OH- ],providing' val={'c': 'OH<sup>-</sup> ], providing', 's': 'diva2:1217827', 'n': 'correct in original'}
w='OH~60' val={'c': 'OH<sup>-</sup> (~60 min)', 's': 'diva2:1217827', 'n': 'correct in original - there was a newline after the superscript'}
w='(~60' val={'c': '~60', 's': 'diva2:1217827'}
w='min)' val={'c': 'minutes', 's': 'diva2:1217827'}

corrected abstract:
<p>In kraft pulping, one of the main issues is the extensive wood losses. With increasing prices of woody biomass an incentive towards minimizing the wood losses exists. Amongst the various process steps, the impregnation of wood chips has shown to enhance the cooking by providing a homogeneous distribution of chemicals inside the chips. It is proven that a more proficient impregnation phase can improve the overall yield in kraft pulping. However, there is a lack of scientific research comparing different impregnation techniques for hardwood. Hence, this thesis will attempt to clarify the impregnation of hardwood.</p><p>The impregnation efficiency was studied by comparing three different impregnation methods: High Alkali Impregnation (HAI), Extended Impregnation (EI) using a low alkali level and a Reference Impregnation (REF) to enable a comparison to the industrially established conditions. The cases were compared by analysing the yield, selectivity and homogeneity. The comparison was also made under cooking conditions with the objective to understand the impact of impregnation on the subsequent cooking phase. The cooking procedure was assessed by analysing the degree of delignification, yield and reject content.</p><p>In impregnation, most chemical consuming reactions occurred within the first 10-30 minutes, mainly contributed by deacetylation. HAI obtained the fastest homogeneous distribution of <sup>-</sup> (~60 min), but the fastest dissolution of wood. The effect was contributed by the high [OH<sup>-</sup>], providing fast diffusion of ions and rapid dissolution of xylan. In the contrary, EI attained the highest impregnation yield after a given impregnation time but required a prolonged duration to obtain a chemical equilibrium between the free and bound liquor (~120 min). REF showed a higher yield than HAI and similar chemical equilibrium as EI. The hydrosulphide sorption in impregnation was highest for EI due to the high initial sulphidity charge and similar for REF and HAI. For impregnations at 115°C, the HS<sup>-</sup> sorption was significantly increased for all cases, resulting from delignification.</p><p>In the subsequent cooking phase, it was prevalent that impregnation of chips under EI conditions were easier delignified, leading to a reduced cooking time to reach the defibration point. Birch was more prone to delignification than eucalyptus. In turn, eucalyptus also obtained a higher defibration point. Highest total cooking yield at similar kappa numbers was achieved with REF conditions, followed by HAI and lastly the EI conditions. The high yield of REF in contrast to HAI could be explained by an improved xylan yield due to an alleviated hydroxide level. The low yield of EI can be assigned to continues peeling due to the prolonged impregnation and loss of xylan when removing black liquor after impregnation. In terms of production rate, yield, energy and chemical consumption the REF is the most efficient impregnation condition for birch kraft cooking in this batchwise laboratory kraft cooking procedure.</p>
----------------------------------------------------------------------
In diva2:1745941 abstract is: <p>In Sweden almost three persons over the age of 65 years dies every daybecause of fall injuries. The overall societal costs of elderly fall accidentswere estimated to to be 14 billion SEK, and if no action is taken this cost isestimated to increase to 22 billion SEK until 2050. The individual decreasein life of quality due to pain, decrease of independence and, for those stillworking, a decrease in income is of course also well worth considering.It is well known that multitasking while walking will decrease attentionon the surroundings and gait behaviour which increases the risk of falling. Itis known that walking uses both sensory input and visual inputs to guide themotion. The visual input prepares the body to adjust itself before a step istaken to optimize the outcome.This study aimed to investigate the effect of multitasking on gaze strategiesand gait performance. Five healthy adults walked over a setup of ramps and astep while performing three different levels of cognitive loading: just walking,walking and performing mental arithmetic’s and walking and scrolling on amobile cell phone.The eye tracking device Pupil Core (Pupil Labs, Berlin, Germany) wasused to capture the gaze points of the participants and Vicon Nexus togetherwith force plates were used to capture data to compute the kinematics of theparticipants during the walking.The results revealed that four out of four participants had a lower ratio ofgaze fixations on objects of interest when scrolling on the phone comparedto just walking, and three out of four participants had a lower ratio of gazefixations on objects of interest when doing mental arithmetic’s compared tojust walking. Simultaneously the gait parameters and kinematics changed in away that might increase the risk of falling. Four out of four participants had adecrease in average stride length and average stride velocity when walkingwhile scrolling on a phone and a decrease in average stride velocity whenperforming mental arithmetic’s compared to just walking. Three out of fourparticipants had a decrease in average stride length when performing mentalarithmetic’s compared to just walking.Since the participant number was low more studies are needed to confirmthese results. The experimental design would benefit from adjustments to tryto separate the effect on gaze behaviour between altered cognitive loading andaltered gait pattern, but are a good base to use for further studies.</p>

corrected abstract:
<p>In Sweden almost three persons over the age of 65 years dies every day because of fall injuries. The overall societal costs of elderly fall accidents were estimated to to be 14 billion SEK, and if no action is taken this cost is estimated to increase to 22 billion SEK until 2050. The individual decrease in life of quality due to pain, decrease of independence and, for those still working, a decrease in income is of course also well worth considering.</p><p>It is well known that multitasking while walking will decrease attention on the surroundings and gait behaviour which increases the risk of falling. It is known that walking uses both sensory input and visual inputs to guide the motion. The visual input prepares the body to adjust itself before a step is taken to optimize the outcome.</p><p>This study aimed to investigate the effect of multitasking on gaze strategies and gait performance. Five healthy adults walked over a setup of ramps and a step while performing three different levels of cognitive loading: just walking, walking and performing mental arithmetic’s and walking and scrolling on a mobile cell phone.</p><p>The eye tracking device Pupil Core (Pupil Labs, Berlin, Germany) was used to capture the gaze points of the participants and Vicon Nexus together with force plates were used to capture data to compute the kinematics of the participants during the walking.</p><p>The results revealed that four out of four participants had a lower ratio of gaze fixations on objects of interest when scrolling on the phone compared to just walking, and three out of four participants had a lower ratio of gaze fixations on objects of interest when doing mental arithmetic’s compared to just walking. Simultaneously the gait parameters and kinematics changed in a way that might increase the risk of falling. Four out of four participants had a decrease in average stride length and average stride velocity when walking while scrolling on a phone and a decrease in average stride velocity when performing mental arithmetic’s compared to just walking. Three out of four participants had a decrease in average stride length when performing mental arithmetic’s compared to just walking.</p><p>Since the participant number was low more studies are needed to confirm these results. The experimental design would benefit from adjustments to try to separate the effect on gaze behaviour between altered cognitive loading and altered gait pattern, but are a good base to use for further studies.</p>
----------------------------------------------------------------------
In diva2:1463440 abstract is: <p>The automobile industry is one of the largest contributors to carbon emissions through vehicularemissions when in use. To make the transport sector more sustainable EU policies dictate toinclude a higher share of renewables in total energy consumption and one way to achieve that isto incorporate biofuels in the traditional fossil fuels. This would reduce carbon footprintgenerated by the automobile industry in an economical manner without making majortechnological modifications in existing engines. Scania AB has optimized their truck fuel systemto comply with the 10% blend of biodiesel in normal diesel fuel.Scania AB is a truck manufacturer in Sweden, with their research and development centresituated in Södertälje. They seek a solution to make their fuel filters in truck engines efficient tofilter out soft particles generated due to biofuel degradation. The aim of this project can bedivided in two major phases; the first phase is to develop a method to simulate fuel ageing anddegradation of biodiesel leading to formation of soft particles at laboratory conditions. The laterphase would be to use results from the first phase to generate a mock degraded fuel that would besubsequently used for testing in a full size filtration rig to assess the efficacy of Scania fuelfilters. These tests would give an insight about existing filters performance regarding degradedfuel and thus better filters could be designed to efficiently handle soft particles.In this thesis project, different iterations and methods of formation of soft particles arediscussed. An important assumption based on real deposit formations from trucks across theworld is that calcium is one of major causes of soft particles formation. Results and evidencefrom previous thesis projects have been used, modified and extended in this project. Newmethods have been developed based on empirical evidence from the experiments conductedduring this project which would be further improved as Scania progresses with this project. Amajor requirement for a mock degraded fuel was to make it stable in terms of suspension of softparticles in the biodiesel without a use of constant agitation in order to simulate the conditions ofthe fuel tank in a truck.The results presented in this thesis project are the optimized method to successfullyproduce soft particles and a working method to prepare test fuel concentrate. Analysis has beenperformed on test fuel concentrate in this project to check for viability of test fluid forconducting experiments on filtration rig. It has been concluded that 10 folds dilution test fuel isthe most promising test fuel sample that can be prepared with the given conditions and timerestrictions</p>

corrected abstract:
<p>The automobile industry is one of the largest contributors to carbon emissions through vehicular emissions when in use. To make the transport sector more sustainable EU policies dictate to include a higher share of renewables in total energy consumption and one way to achieve that is to incorporate biofuels in the traditional fossil fuels. This would reduce carbon footprint generated by the automobile industry in an economical manner without making major technological modifications in existing engines.</p><p>Scania AB has optimized their truck fuel system to comply with the 10% blend of biodiesel in normal diesel fuel. Scania AB is a truck manufacturer in Sweden, with their research and development centre situated in Södertälje. They seek a solution to make their fuel filters in truck engines efficient to filter out soft particles generated due to biofuel degradation. The aim of this project can be divided in two major phases; the first phase is to develop a method to simulate fuel ageing and degradation of biodiesel leading to formation of soft particles at laboratory conditions. The later phase would be to use results from the first phase to generate a mock degraded fuel that would be subsequently used for testing in a full size filtration rig to assess the efficacy of Scania fuel filters. These tests would give an insight about existing filters performance regarding degraded fuel and thus better filters could be designed to efficiently handle soft particles.</p><p>In this thesis project, different iterations and methods of formation of soft particles are discussed. An important assumption based on real deposit formations from trucks across the world is that calcium is one of major causes of soft particles formation. Results and evidence from previous thesis projects have been used, modified and extended in this project. New methods have been developed based on empirical evidence from the experiments conducted during this project which would be further improved as Scania progresses with this project. A major requirement for a mock degraded fuel was to make it stable in terms of suspension of soft particles in the biodiesel without a use of constant agitation in order to simulate the conditions of the fuel tank in a truck.</p><p>The results presented in this thesis project are the optimized method to successfully produce soft particles and a working method to prepare test fuel concentrate. Analysis has been performed on test fuel concentrate in this project to check for viability of test fluid for conducting experiments on filtration rig. It has been concluded that 10 folds dilution test fuel is the most promising test fuel sample that can be prepared with the given conditions and time restrictions.</p>

----------------------------------------------------------------------
In diva2:1228545 abstract is: <p>As the mission to the decrease global warming and phase out highly pollutingenvironmental practices globally, regulations including Euro 6 and policies generated by theUnited Nations Framework Convention on Climate Change (UNFCCC) are pushing companiesto be more innovative when it comes to their energy sources. These regulations involve manyfactors related to the cleanliness of the fuel and produced emissions, for example, propertiesof the fuels such as sulfur content, ash content, water content, and resulting emission valuesof Carbon dioxide (CO2) and Nitrogen Oxides (NOx). Furthermore, Sweden has set achallenging target of a fossil-fuel-independent vehicle fleet by 2030 and no net greenhousegasemissions by 2050.One way to cut down on the polluting properties in the fuel, as well as weakening thedependence on fossil fuel based fuel includes utilizing higher blending ratios of biofuels in thetransport sector. This transition to biofuels comes with many challenges to the transportindustry due to higher concentrations of these new fuels leads to clogging of the filters in theengine, as well as, internal diesel injector deposits (IDIDs) that produce injector fouling. Thisclogging of the filters leads to lower performance by the engines which leads to higher repairtimes (uptime) and less time on the road to transport goods. The formation of these softparticles at the root of the clogging issue is a pivotal issue because the precise mechanismsbehind their formation are highly unknown. Scania, a leader in the Swedish automotiveindustry, is very interested in figuring out what mechanisms are the most influential in theformation of these particles in the engine. Understanding the key mechanisms would allowScania to make appropriate adjustments to the fuel or the engines to ensure more time onthe road and less maintenance.There are many conditions known to be possible causes of the formation of softparticles in engines such as water content, ash content, and temperature. After generatingsoft particles using a modified accelerated method, particles were analyzed using infraredtechnology (RTX-FTIR) and a Scanning Electric Microscope (SEM-EDX). Many differentexperiments were performed to be able to make a conclusion as to which mechanisms weremost influential including temperature, time, water, air, and oil. The combination of agingbiofuels (B100, B10, HVO) with metals, and water produced the largest amount of particlesfollowed by aging the biofuels with aged oil, metals, and water. Aging the fuels with aged oilincreased particles, meanwhile the addition of water prevented particle production possiblydue to additives. B100 produced the highest amount of particles when aged with Copper, B10with Brass, and HVO with Iron.</p>

corrected abstract:
<p>As the mission to the decrease global warming and phase out highly polluting environmental practices globally, regulations including Euro 6 and policies generated by the United Nations Framework Convention on Climate Change (UNFCCC) are pushing companies to be more innovative when it comes to their energy sources. These regulations involve many factors related to the cleanliness of the fuel and produced emissions, for example, properties of the fuels such as sulfur content, ash content, water content, and resulting emission values of Carbon dioxide (CO<sub>2</sub>) and Nitrogen Oxides (NO<sub>x</sub>). Furthermore, Sweden has set a challenging target of a fossil-fuel-independent vehicle fleet by 2030 and no net greenhouse-gas emissions by 2050.</p><p>One way to cut down on the polluting properties in the fuel, as well as weakening the dependence on fossil fuel based fuel includes utilizing higher blending ratios of biofuels in the transport sector. This transition to biofuels comes with many challenges to the transport industry due to higher concentrations of these new fuels leads to clogging of the filters in the engine, as well as, internal diesel injector deposits (IDIDs) that produce injector fouling. This clogging of the filters leads to lower performance by the engines which leads to higher repair times (uptime) and less time on the road to transport goods. The formation of these soft particles at the root of the clogging issue is a pivotal issue because the precise mechanisms behind their formation are highly unknown. Scania, a leader in the Swedish automotive industry, is very interested in figuring out what mechanisms are the most influential in the formation of these particles in the engine. Understanding the key mechanisms would allow Scania to make appropriate adjustments to the fuel or the engines to ensure more time on the road and less maintenance.</p><p>There are many conditions known to be possible causes of the formation of soft particles in engines such as water content, ash content, and temperature. After generating soft particles using a modified accelerated method, particles were analyzed using infrared technology (RTX-FTIR) and a Scanning Electric Microscope (SEM-EDX). Many different experiments were performed to be able to make a conclusion as to which mechanisms were most influential including temperature, time, water, air, and oil. The combination of aging biofuels (B100, B10, HVO) with metals, and water produced the largest amount of particles followed by aging the biofuels with aged oil, metals, and water. Aging the fuels with aged oil increased particles, meanwhile the addition of water prevented particle production possibly due to additives. B100 produced the highest amount of particles when aged with Copper, B10 with Brass, and HVO with Iron.</p>
----------------------------------------------------------------------
In diva2:1676098 abstract is: <p>Introduction: The brain can change its structure and functionality as a result ofexternal factors. The working memory (WM) of the brain is where informationcan be held and manipulated during a short period of time, with the purpose ofachieving higher cognitive functions such as reasoning and learning. The WMimproves in capacity during the development from childhood into adulthood,and variation of improvement is possible as an effect of situational factors andstimuli.Goal: The main goal of this project was to examine the effects of a WMtraining program on power distribution, connectivity and synchronicity withinbrain networks, using an intra-individual analysis approach.Method: A series of magnetoencephalography (MEG) measurements wasacquired for four subjects while they were performing WM and control tasks,during a WM training program, along with an MRI image of the brain for eachof the participants. The data was preprocessed for noise and artifact removaland a source reconstruction was performed. Time-frequency representationsof the data were created and the frequencies were categories into alpha,beta and gamma bands. The power difference between the WM and controltask was calculated as a function of cognitive load of each frequency band,and its variation over load was calculated as a constructed metric called’area under power difference curve’ (AUPDC), and visualised using colourscale representation upon the brain MRI of each subject. Brain parcels thatsignificantly deviated from a random distribution of AUPDC values wereidentified using a Gaussian distribution fit.Results and discussion: All subjects showed a clear improvement inperformance accuracy of the tasks, but as the effect on the power distributionsvaried considerably for each subject and frequency band, other aspects besidepower need to be investigated in order to understand the mechanisms behindthe improvement. However, the overall results indicate that many significantAUPDC values seem to have decreased during the WM training, both forthe positive and negative significant AUPDC values, suggesting a strongerdecreasing trend in power difference over cognitive load and a weaker increasingtrend. This could suggest an improved brain activation efficiency as an effectof the WM training.</p>

corrected abstract:
<p><em>Introduction</em>: The brain can change its structure and functionality as a result of external factors. The working memory (WM) of the brain is where information can be held and manipulated during a short period of time, with the purpose of achieving higher cognitive functions such as reasoning and learning. The WM improves in capacity during the development from childhood into adulthood, and variation of improvement is possible as an effect of situational factors and stimuli.</p><p><em>Goal</em>: The main goal of this project was to examine the effects of a WM training program on power distribution, connectivity and synchronicity within brain networks, using an intra-individual analysis approach.</p><p><em>Method</em>: A series of magnetoencephalography (MEG) measurements was acquired for four subjects while they were performing WM and control tasks, during a WM training program, along with an MRI image of the brain for each of the participants. The data was preprocessed for noise and artifact removal and a source reconstruction was performed. Time-frequency representations of the data were created and the frequencies were categories into alpha, beta and gamma bands. The power difference between the WM and control task was calculated as a function of cognitive load of each frequency band, and its variation over load was calculated as a constructed metric called ’area under power difference curve’ (AUPDC), and visualised using colour scale representation upon the brain MRI of each subject. Brain parcels that significantly deviated from a random distribution of AUPDC values were identified using a Gaussian distribution fit.</p><p><em>Results and discussion</em>: All subjects showed a clear improvement in performance accuracy of the tasks, but as the effect on the power distributions varied considerably for each subject and frequency band, other aspects beside power need to be investigated in order to understand the mechanisms behind the improvement. However, the overall results indicate that many significant AUPDC values seem to have decreased during the WM training, both for the positive and negative significant AUPDC values, suggesting a stronger decreasing trend in power difference over cognitive load and a weaker increasing trend. This could suggest an improved brain activation efficiency as an effect of the WM training.</p>
----------------------------------------------------------------------
diva2:1455145 seems to be a duplicate of this


In diva2:1421531 abstract is: <p>Lignocellulosic biomass has potential to chip in the chemical and biofuels supplies in future societies,even though lignocellulose is a recalcitrant structure that has to be treated in several steps. After theirproper life cycle, wood-derived materials such as particleboards have few outcomes today apart fromenergy recovery for heat production. Then, they may be used as lignocellulosic biomass sources in theproduction of molecules of interest. Fermentation from wood-derived monosaccharides imposespreliminary sugar retrieval, for instance through pre-treatment and enzymatic hydrolysis. This studyfocuses on the potential of particleboards waste for chemical and biofuel production by comparingsaccharification through simulated steam explosion pre-treatment and enzymatic hydrolysis betweennative and particleboard-derived wood, with an insight in subsequent fermentation by Saccharomycescerevisiae. Urea-Formaldehyde bound particleboard was investigated, as well as some aspects ofMelamine-Urea-Formaldehyde bound particleboard.Pre-treatment resulted in apparition of lignocellulosic degraded compounds in a much larger extent innative wood than in particleboard, which seemed to be only superficially impacted. Formation ofdegraded compounds from sugars – furfural and 5-hydroxymethylfurfural – was enhanced when pretreatmentwas prolonged. Removal of a substantial fraction of the adhesive contained in theparticleboards was observed, leading to comparable concentrations in free urea, its degradedproducts, and formaldehyde between native wood and particleboards during enzymatic hydrolysis.Enzymatic hydrolysis with cellulases and hemicellulases highlighted a critical role of pre-treatment toenhance final yields, both in native wood and in Urea-Formaldehyde particleboard. Adding 20 minutessteam-explosion type pre-treatment at 160 °C resulted in glucose yields increase from 18.5 % to 32.8% for native wood and from 15.6 % to 37.4 % for particleboard. Prolonging pre-treatment residencetime to 35 minutes resulted in much better glucose extraction for native wood but only slight progressfor the particleboard, as glucose yields reached 64.5 % and 41.1 % respectively. Maximalconcentrations achieved were 277 and 184 mg/gbiomass respectively.Fermentation brought to light high inhibition from both native wood and particleboard sources ofmedia, which were attributed to components or degraded products of lignocellulose that were notanalysed in this project. Ethanol was formed during fermentation, with reduced productivity butincreased yields as compared with the control sample. Inhibition was so strong that no difference couldbe given between native and particleboard wood. In this situation, no inhibition potential of resin orits degradation products could be proved.</p>


corrected abstract:
<p>Lignocellulosic biomass has potential to chip in the chemical and biofuels supplies in future societies, even though lignocellulose is a recalcitrant structure that has to be treated in several steps. After their proper life cycle, wood-derived materials such as particleboards have few outcomes today apart from energy recovery for heat production. Then, they may be used as lignocellulosic biomass sources in the production of molecules of interest. Fermentation from wood-derived monosaccharides imposes preliminary sugar retrieval, for instance through pre-treatment and enzymatic hydrolysis. This study focuses on the potential of particleboards waste for chemical and biofuel production by comparing saccharification through simulated steam explosion pre-treatment and enzymatic hydrolysis between native and particleboard-derived wood, with an insight in subsequent fermentation by <em>Saccharomyces cerevisiae</em>. Urea-Formaldehyde bound particleboard was investigated, as well as some aspects of Melamine-Urea-Formaldehyde bound particleboard.</p><p>Pre-treatment resulted in apparition of lignocellulosic degraded compounds in a much larger extent in native wood than in particleboard, which seemed to be only superficially impacted. Formation of degraded compounds from sugars – furfural and 5-hydroxymethylfurfural – was enhanced when pretreatment was prolonged. Removal of a substantial fraction of the adhesive contained in the particleboards was observed, leading to comparable concentrations in free urea, its degraded products, and formaldehyde between native wood and particleboards during enzymatic hydrolysis. Enzymatic hydrolysis with cellulases and hemicellulases highlighted a critical role of pre-treatment to enhance final yields, both in native wood and in Urea-Formaldehyde particleboard. Adding 20 minutes steam-explosion type pre-treatment at 160 °C resulted in glucose yields increase from 18.5 % to 32.8 % for native wood and from 15.6 % to 37.4 % for particleboard. Prolonging pre-treatment residence time to 35 minutes resulted in much better glucose extraction for native wood but only slight progress for the particleboard, as glucose yields reached 64.5 % and 41.1 % respectively. Maximal concentrations achieved were 277 and 184 mg/g<sub>biomass</sub> respectively.</p><p>Fermentation brought to light high inhibition from both native wood and particleboard sources of media, which were attributed to components or degraded products of lignocellulose that were not analysed in this project. Ethanol was formed during fermentation, with reduced productivity but increased yields as compared with the control sample. Inhibition was so strong that no difference could be given between native and particleboard wood. In this situation, no inhibition potential of resin or its degradation products could be proved.</p>
----------------------------------------------------------------------
In diva2:1878490 abstract is: <p>Stroke is an enormous global burden, six and a half-million people die fromstroke annually [1]. Effectively monitoring blood hemodynamic parameters suchas blood velocity and volume flow permits to help and cure people. This projectaimed to calibrate a custom-made wearable system for measuring cerebral bloodflow (CBF) using a photoplethysmography (PPG) sensor. The measurementswere validated using Doppler ultrasound as a reference method. Five (N=5)subjects (age = 24±1.41 years) were selected for the project. The PPG and Dopplerultrasound probe were placed above the left and right common carotid arteries(CCA), respectively. Measurements were taken simultaneously for 12 secondseach, with six consecutive measurements per subject and 2 time-synchronizedECG recordings. Subsequently, using an extraction algorithm the velocityenvelope (TAMEAN) was extracted from the Doppler image to obtain the bloodvolume flow (ml/min). After synchronization, the PPG signal output expressedin volts was calibrated to the corresponding volume, and a calibration curve wascreated.The extraction algorithm achieved remarkable results, with almost perfectcorrelation with the Doppler image reference, rT AM EAN =0.951 and rvolume=0.975demonstrating its reliability. Challenges encountered during postprocessingand synchronization highlighted the need for careful refinement in the projectframework. Despite successful signal processing and alignment techniques,calibration results were suboptimal due to synchronization difficulties andmotion artifacts. Limitations included impractical measurement locations andsusceptibility to movement artifacts. The calibration process did not yield theexpected outcomes and the project aim was not achieved. All the linear regressionmodels for each subject failed to accurately predict the volume flow based on themeasured voltages. Future work could focus on refining calibration procedures,improving synchronization methods, and expanding studies to include largercohorts. Although the wearable device was tested, the project’s goal was onlypartially achieved, underscoring the complexity of accurately measuring cerebralblood flow using PPG sensors.</p>


corrected abstract:
<p>Stroke is an enormous global burden, six and a half-million people die from stroke annually [1]. Effectively monitoring blood hemodynamic parameters such as blood velocity and volume flow permits to help and cure people. This project aimed to calibrate a custom-made wearable system for measuring cerebral blood flow (CBF) using a photoplethysmography (PPG) sensor. The measurements were validated using Doppler ultrasound as a reference method. Five (N=5) subjects (age = 24±1.41 years) were selected for the project. The PPG and Doppler ultrasound probe were placed above the left and right common carotid arteries (CCA), respectively. Measurements were taken simultaneously for 12 seconds each, with six consecutive measurements per subject and 2 time-synchronized ECG recordings. Subsequently, using an extraction algorithm the velocity envelope (TAMEAN) was extracted from the Doppler image to obtain the blood volume flow (ml/min). After synchronization, the PPG signal output expressed in volts was calibrated to the corresponding volume, and a calibration curve was created. The extraction algorithm achieved remarkable results, with almost perfect correlation with the Doppler image reference, r<sub>TAMEAN</sub>=0.951 and r<sub>volume</sub>=0.975 demonstrating its reliability. Challenges encountered during postprocessing and synchronization highlighted the need for careful refinement in the project framework. Despite successful signal processing and alignment techniques, calibration results were suboptimal due to synchronization difficulties and motion artifacts. Limitations included impractical measurement locations and susceptibility to movement artifacts. The calibration process did not yield the expected outcomes and the project aim was not achieved. All the linear regression models for each subject failed to accurately predict the volume flow based on the measured voltages. Future work could focus on refining calibration procedures, improving synchronization methods, and expanding studies to include larger cohorts. Although the wearable device was tested, the project’s goal was only partially achieved, underscoring the complexity of accurately measuring cerebral blood flow using PPG sensors.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-23
======================================================================
In diva2:1688672 abstract is: <p>When the Covid-19 pandemic hit, many organizations had to adapt to new waysof working. For many, this meant that the work previously done in offices wasnow allowed to move home instead. This meant a major change, not least forauthorities that had not been able to carry out their work from home before. Theaim with the study was to investigate positive and negative effects in the workenvironment during a transition from office work to work from home.Authorities and other organizations may benefit from this new knowledge inanticipating risk factors and preventing them. A case study was conducted on aunit of an authority in a larger city in Sweden. A questionnaire was sent out tothe unit and two in-depth interviews were conducted. The results presentconnections that have emerged in the organizational and social workenvironment as well as the physical work environment. The results of the studyare interpreted as saying that most people were positive about working fromhome. All static relationships investigated regarding the organizational, socialand physical working environment showed significant correlations (correlationvalues ranged from 0.387-0.754). These results showed that a well-functioningdigital platform is a prerequisite for good communication, that undisturbed workaffects the experience of concentration, that productivity was affected by sittingundisturbed, that job satisfaction was affected by the support of management,that perception of their working position affected pain and discomfort in thebody and that what they thought about the physical work environment regardingtables and chair matters. Both the answers in the survey and in the interviewsturned out to be positive. All the relationships regarding the organizational andsocial work environment showed correlation that was significant. Also, all therelationships regarding the physical work environment showed correlation thatwas significant. Since the study showed such positive results, authorities shouldconsider about how to approach employees doing work from home even aftersociety has gained better control of the Covid-19 pandemic. This study hasgenerated new knowledge that can change any position regarding work fromhome for employees at authorities.</p>


corrected abstract:
<p>When the Covid-19 pandemic hit, many organizations had to adapt to new ways of working. For many, this meant that the work previously done in offices was now allowed to move home instead. This meant a major change, not least for authorities that had not been able to carry out their work from home before. The aim with the study was to investigate positive and negative effects in the work environment during a transition from office work to work from home. Authorities and other organizations may benefit from this new knowledge in anticipating risk factors and preventing them. A case study was conducted on a unit of an authority in a larger city in Sweden. A questionnaire was sent out to the unit and two in-depth interviews were conducted. The results present connections that have emerged in the organizational and social work environment as well as the physical work environment. The results of the study are interpreted as saying that most people were positive about working from home. All static relationships investigated regarding the organizational, social and physical working environment showed significant correlations (correlation values ranged from 0.387-0.754). These results showed that a well-functioning digital platform is a prerequisite for good communication, that undisturbed work affects the experience of concentration, that productivity was affected by sitting undisturbed, that job satisfaction was affected by the support of management, that perception of their working position affected pain and discomfort in the body and that what they thought about the physical work environment regarding tables and chair matters. Both the answers in the survey and in the interviews turned out to be positive. All the relationships regarding the organizational and social work environment showed correlation that was significant. Also, all the relationships regarding the physical work environment showed correlation that was significant. Since the study showed such positive results, authorities should consider about how to approach employees doing work from home even after society has gained better control of the Covid-19 pandemic. This study has generated new knowledge that can change any position regarding work from home for employees at authorities.</p>
----------------------------------------------------------------------
In diva2:1751618 abstract is: <p>The improvement of data acquisition and computer heavy methods in recentyears has paved the way for completely digital healthcare solutions. Digitaltherapeutics (DTx) are such solutions and are often provided as mobileapplications that must undergo clinical trials. A common method for suchapplications is to utilize cognitive behavioral-therapy (CBT), in order toprovide their patients with tools for self-improvement. The Swedish-basedcompany Alex Therapeutics is such a provider. They develop state-of-theartapplications that utilize CBT to help patients. Among their applications,they have one that aims to help users quit smoking. From this app, they havecollected user data with the goal of continuously improving their servicesthrough machine learning (ML). In their current application, they utilizemultiple ML methods to personalize the care, but have opened up possibilitiesfor the usage of reinforcement learning (RL). Often the wanted behavior isknown, such as to quitting smoking, but the optimal path, within the app, forhow to reach such a goal is not. By formalizing the problem as a Markovdecision process, where the transition probabilities have to be inferred fromuser data, such an optimal policy can be found. Standard methods of RL arereliant on direct access of an environment for sampling of data, whereas theuser data sampled from the application are to be treated as such. This thesisthus explores the possibilities of using RL on a static dataset in order to inferan optimal policy.</p><p>A double deep Q-network (DDQN) was chosen as the reinforcement learningagent. The agent was trained on two different datasets and showed goodconvergence for both, using a custom metric for the task. Using SHAPvaluesthe strategy of the agent is visualized and discussed, together with themethodological challenges. Lastly, future work for the proposed methods arediscussed.</p>

corrected abstract:
<p>The improvement of data acquisition and computer heavy methods in recent years has paved the way for completely digital healthcare solutions. Digital therapeutics (DTx) are such solutions and are often provided as mobile applications that must undergo clinical trials. A common method for such applications is to utilize cognitive behavioral-therapy (CBT), in order to provide their patients with tools for self-improvement. The Swedish-based company Alex Therapeutics is such a provider. They develop state-of-the-art applications that utilize CBT to help patients. Among their applications, they have one that aims to help users quit smoking. From this app, they have collected user data with the goal of continuously improving their services through machine learning (ML). In their current application, they utilize multiple ML methods to personalize the care, but have opened up possibilities for the usage of reinforcement learning (RL). Often the wanted behavior is known, such as to quitting smoking, but the optimal path, within the app, for how to reach such a goal is not. By formalizing the problem as a Markov decision process, where the transition probabilities have to be inferred from user data, such an optimal policy can be found. Standard methods of RL are reliant on direct access of an environment for sampling of data, whereas the user data sampled from the application are to be treated as such. This thesis thus explores the possibilities of using RL on a static dataset in order to infer an optimal policy.</p><p>A double deep Q-network (DDQN) was chosen as the reinforcement learning agent. The agent was trained on two different datasets and showed good convergence for both, using a custom metric for the task. Using SHAP-values the strategy of the agent is visualized and discussed, together with the methodological challenges. Lastly, future work for the proposed methods are discussed.</p>
----------------------------------------------------------------------
In diva2:1880445 abstract is: <p>This study aims to analyze factors and individual characteristics that affect the outcomes:work-life balance, work engagement and sense of coherence for white-collar workers whenworking remotely.</p><p>In today’s society where technology is constantly evolving, hybrid and remote work optionsare becoming more common. Research has shown that work-life balance, work engagement,and sense of coherence affect employee well-being and organizational performance. Theresearch on the extent to which remote work can contribute to sustainable work andemployee well-being is mixed, and most of the research on remote work has been conductedeither before or during the COVID-19 pandemic. Further post-pandemic research cancontribute to more sustainable work and society as a whole.</p><p>To analyze what factors affect the outcomes in a remote work setting, a mixed-methodapproach was used to gain a comprehensive understanding, including a quantitative and aqualitative study. The quantitative method included bivariate correlation tests, multiple linearregression and group difference testing on cross-sectional survey data collected from twolarge Swedish companies. The qualitative method included six interviews with experiencedwhite-collar workers, which helped us interpret the findings and gain a more in depthunderstanding of quantitative results. To analyze the study findings the theoretical model ofhuman, technology, and organization (HTO) was applied.</p><p>The quantitative study showed that social support from superiors, remote leadership quality,functionality of digital management systems and digital learning climate were identified asfactors that were associated with work-life balance, work engagement and sense ofcoherence. However, digital resources and social support from superiors were found to beinfluencing work engagement and sense of coherence to a greater extent. For work-lifebalance, social support from superiors and remote leadership quality was found to besignificant. The qualitative study showed that the interviewees' perception of work-lifebalance, work engagement, and sense of coherence was affected by flexible workingarrangements, functionality and use of digital tools, social interaction, collaboration,communication, inspirational relationships at the workplace, supportive colleagues, andsupportive and responsive superiors. Better understanding of what factors affect employees'personal and working life when working remotely can help organizations to promoteoccupational well-being and performance, which can contribute to more sustainable work.</p>

corrected abstract:
<p>This study aims to analyze factors and individual characteristics that affect the outcomes: work-life balance, work engagement and sense of coherence for white-collar workers when working remotely.</p><p>In today’s society where technology is constantly evolving, hybrid and remote work options are becoming more common. Research has shown that work-life balance, work engagement, and sense of coherence affect employee well-being and organizational performance. The research on the extent to which remote work can contribute to sustainable work and employee well-being is mixed, and most of the research on remote work has been conducted either before or during the COVID-19 pandemic. Further post-pandemic research can contribute to more sustainable work and society as a whole.</p><p>To analyze what factors affect the outcomes in a remote work setting, a mixed-method approach was used to gain a comprehensive understanding, including a quantitative and a qualitative study. The quantitative method included bivariate correlation tests, multiple linear regression and group difference testing on cross-sectional survey data collected from two large Swedish companies. The qualitative method included six interviews with experienced white-collar workers, which helped us interpret the findings and gain a more in depth understanding of quantitative results. To analyze the study findings the theoretical model of human, technology, and organization (HTO) was applied.</p><p>The quantitative study showed that social support from superiors, remote leadership quality, functionality of digital management systems and digital learning climate were identified as factors that were associated with work-life balance, work engagement and sense of coherence. However, digital resources and social support from superiors were found to be influencing work engagement and sense of coherence to a greater extent. For work-life balance, social support from superiors and remote leadership quality was found to be significant. The qualitative study showed that the interviewees' perception of work-life balance, work engagement, and sense of coherence was affected by flexible working arrangements, functionality and use of digital tools, social interaction, collaboration, communication, inspirational relationships at the workplace, supportive colleagues, and supportive and responsive superiors. Better understanding of what factors affect employees' personal and working life when working remotely can help organizations to promote occupational well-being and performance, which can contribute to more sustainable work.</p>
----------------------------------------------------------------------
In diva2:1451583 abstract is: <p>AbstractDevelopment of products and services has historically been preceded by a clearrequirements specification of the desired features, a fixed margin of expenditureand a launch time frame. Nowadays, agile methods have gradually becomestandard within software development and do not require an explicit requirementsspecification. However, the need for such a specification, or an alternative to it,may remain to assist in the management of overall planning and thereby functionas a bridge between old and new requirements management. A literature study wasconducted, reviewing three agile frameworks, including a mapping of whereproduct requirements exist in modern structures, which may correspond to atraditional requirements specification. The study demonstrated that overallrequirements are produced prior to the start of a project and later specified duringthe development process, to match individual user scenarios. By aggregating these,a requirements specification, corresponding to the traditional format, can beobtained.The Swedish Tax Agency is undergoing an organizational transformation to theagile framework named Scaled Agile Framework (SAFe), whereupon a need formanaging requirements and its documentation has arisen. The literature study,together with modeling of their current working methods in the software tools Jiraand Confluence, has been compared with "best practice". The comparison showedthat the agency follows the SAFe framework structure, but that discrepancy occurregarding the documentation structure used in the tools, whereupon amendmentsare presented.KeywordsAgile methods, software development, requirements specification, SAFe, agileframeworks.</p>


'whereupon' is set as one word in the original

corrected abstract:
<p>Abstract Development of products and services has historically been preceded by a clear requirements specification of the desired features, a fixed margin of expenditure and a launch time frame. Nowadays, agile methods have gradually become standard within software development and do not require an explicit requirements specification. However, the need for such a specification, or an alternative to it, may remain to assist in the management of overall planning and thereby function as a bridge between old and new requirements management. A literature study was conducted, reviewing three agile frameworks, including a mapping of where product requirements exist in modern structures, which may correspond to a traditional requirements specification. The study demonstrated that overall requirements are produced prior to the start of a project and later specified during the development process, to match individual user scenarios. By aggregating these, a requirements specification, corresponding to the traditional format, can be obtained.</p><p>The Swedish Tax Agency is undergoing an organizational transformation to the agile framework named Scaled Agile Framework (SAFe), whereupon a need for managing requirements and its documentation has arisen. The literature study, together with modeling of their current working methods in the software tools Jira and Confluence, has been compared with "best practice". The comparison showed that the agency follows the SAFe framework structure, but that discrepancy occur regarding the documentation structure used in the tools, where upon amendments are presented.</p>
----------------------------------------------------------------------
title: "Development of cell assay for cellbasedinteraction studies with Attana’s 3rd generation biosensor"
==>    "Development of cell assay for cellbased interaction studies with Attana’s 3rd generation biosensor"


In diva2:1454421 abstract is: <p>Optimization of experimental assay design is crucial in all areas of biomedical research.Assay development and experimental optimization were carried out in an effort toproduce a cell-based demonstration assay for Attana AB. The demonstration assaywould provide practical training for cell-based experiments in a comprehensive,practical, and effective manner when operating Attana CellTM 200 biosensor. The AttanaCellTM 200 system is a label-free, dual-channel, temperature-controlled biosensor basedon Quartz Crystal Microbalance (QCM) technology. Attana's QCM biosensors facilitatereal-time interactions and enable direct evaluation of quantitative and qualitativeparameters. The findings generated in this project were applied to help optimize aprotocol for the cell-based assay. These included determining optimal experimentalconditions and variables, including flow rate, running buffer, cell coverage, andconcentrations. Other desired aspects that were examined for optimization includedsustainability, stability, and reproducibility. Attana CellTM 200 system was used in thisstudy to determine specificity, kinetics, and affinity of cell-lectin interactions. Twometastatic colorectal cell lines, HT29 and SW480, representing different metastasispotential, were chosen to study lectin interactions with cell surface glycans. The cellswere immobilized on polystyrene-coated sensor chips, and interactions were studiedusing Helix promatia agglutinin (HPA) and Ricinus communis agglutinin (RCA) lectins.HPA and RCA have demonstrated to interact with cancer cells to the degree that isproportional to their metastatic capacity by displaying specificity for glycans withvarying degrees of N and O-linked glycosylation. The study demonstrated cleardifferences in the interaction profiles and kinetics between the two colorectal cancer cellswhen interacting with HPA and RCA, respectively. Furthermore, due to the two lectinsdifferent protein properties, variations in the interaction profiles between the lectinswere detected. The results indicated that the different interaction profiles could be usedas tools to demonstrate cell-based interactions when using the cell demonstration assay.</p>


w='AttanaCellTM' val={'c': 'Attana Cell™', 's': 'diva2:1454421'}

corrected abstract:
<p>Optimization of experimental assay design is crucial in all areas of biomedical research. Assay development and experimental optimization were carried out in an effort to produce a cell-based demonstration assay for Attana AB. The demonstration assay would provide practical training for cell-based experiments in a comprehensive, practical, and effective manner when operating Attana CellTM 200 biosensor. The Attana Cell™ 200 system is a label-free, dual-channel, temperature-controlled biosensor based on Quartz Crystal Microbalance (QCM) technology. Attana's QCM biosensors facilitate real-time interactions and enable direct evaluation of quantitative and qualitative parameters. The findings generated in this project were applied to help optimize a protocol for the cell-based assay. These included determining optimal experimental conditions and variables, including flow rate, running buffer, cell coverage, and concentrations. Other desired aspects that were examined for optimization included sustainability, stability, and reproducibility. Attana CellTM 200 system was used in this study to determine specificity, kinetics, and affinity of cell-lectin interactions. Two metastatic colorectal cell lines, HT29 and SW480, representing different metastasis potential, were chosen to study lectin interactions with cell surface glycans. The cells were immobilized on polystyrene-coated sensor chips, and interactions were studied using Helix promatia agglutinin (HPA) and Ricinus communis agglutinin (RCA) lectins. HPA and RCA have demonstrated to interact with cancer cells to the degree that is proportional to their metastatic capacity by displaying specificity for glycans with varying degrees of N and O-linked glycosylation. The study demonstrated clear differences in the interaction profiles and kinetics between the two colorectal cancer cells when interacting with HPA and RCA, respectively. Furthermore, due to the two lectins different protein properties, variations in the interaction profiles between the lectins were detected. The results indicated that the different interaction profiles could be used as tools to demonstrate cell-based interactions when using the cell demonstration assay.</p>
----------------------------------------------------------------------
In diva2:1438250 abstract is: <p>Abstract</p><p>With the current challenges for the healthcare such as increased demand for care, financial andresource constraints along with rapid changes and complexity there is high believe in digitalinnovation and digitalisation to efficacy resources and aid in delivering a safer, more accessibleand patient centred valuable care.</p><p>There is a digitalisation that is ongoing, being used and implemented over several differentareas of healthcare. Since healthcare can be seen as a complex adaptive system, there is a needto understand several agents. The aim is to gather more knowledge about perceptions withinthe physiotherapy staff and give recommendations and directions for improvements regardingdigital innovation.</p><p>Opinions about digital innovation have been gathered with open interviews and a semisystematicliterature review with focus on physiotherapy. Too find subjective data the mixedmethod Q methodology was applied.</p><p>The open interviews resulted in eight categories: digital innovation, digital innovation beingused, digital innovation not used, management, obstacles, education, wishful thinking,applications and systems and associated opinions. The semi-systematic literature reviewshowed on a rapid scientifically development, 25 articles was found and thematically analysed.140 cited viewpoints and facts was merged with the results from the open interviews. Tenphysiotherapists performed the q-sort consisting of 25 statements. Three factors were found.Interpreted as digital innovation optimism &amp; patient oriented, digital innovation scepticism &amp;management oriented and digital innovation sceptical optimism. Video-call technique isstrongly encouraged by factor one contrary to factor two. Integrity is the major conflictingviewpoint between the factors. The result shows that gender can affect if a physiotherapist iseither optimistic or sceptical to digital innovation. Using existing models such as UTAUT couldimprove acceptance about digital innovation. Education is perceived as important among allfactors. Nine participants responded on baseline questions showing low knowledge of the termmHealth and little communication with IT departments.</p><p>Keywords: Digital innovation, Digitalisation, eHealth, mHealth, Healthcare,Physiotherapists, Q methodology</p>


corrected abstract
<p>With the current challenges for the healthcare such as increased demand for care, financial and resource constraints along with rapid changes and complexity there is high believe in digital innovation and digitalisation to efficacy resources and aid in delivering a safer, more accessible and patient centred valuable care.</p><p>There is a digitalisation that is ongoing, being used and implemented over several different areas of healthcare. Since healthcare can be seen as a complex adaptive system, there is a need to understand several agents. The aim is to gather more knowledge about perceptions within the physiotherapy staff and give recommendations and directions for improvements regarding digital innovation.</p><p>Opinions about digital innovation have been gathered with open interviews and a semisystematic literature review with focus on physiotherapy. Too find subjective data the mixed method Q methodology was applied.</p><p>The open interviews resulted in eight categories: digital innovation, digital innovation being used, digital innovation not used, management, obstacles, education, wishful thinking, applications and systems and associated opinions. The semi-systematic literature review showed on a rapid scientifically development, 25 articles was found and thematically analysed. 140 cited viewpoints and facts was merged with the results from the open interviews. Ten physiotherapists performed the q-sort consisting of 25 statements. Three factors were found. Interpreted as digital innovation optimism &amp; patient oriented, digital innovation scepticism &amp; management oriented and digital innovation sceptical optimism. Video-call technique is strongly encouraged by factor one contrary to factor two. Integrity is the major conflicting viewpoint between the factors. The result shows that gender can affect if a physiotherapist is either optimistic or sceptical to digital innovation. Using existing models such as UTAUT could improve acceptance about digital innovation. Education is perceived as important among all factors. Nine participants responded on baseline questions showing low knowledge of the term mHealth and little communication with IT departments.</p>
----------------------------------------------------------------------
In diva2:1214017 abstract is: <p>Like in any modern civilization, roads in Iceland have an important role in thedaily lives of inhabitants. Consequently, road quality is of equal importance, butIcelandic roads have shown problems when surface dressing is used where itlooks decent after being paved during summer but then deforming pretty rapidlyafter being hit by elements of winter. Roads in Sweden however, do not seem tohave the same problem.The aim of this study is to minimize this road deformation by examining surfacedressing and aggregates. The Icelandic climate is also a factor to this problemsince the humidity is comparatively high, summers are cool, winters are mild andthe climate is overall challenging. Furthermore, winter thaws are distinctivecharacteristic of the Icelandic weather, which increases strain on the asphalt.An experiment was conducted where the adhesion of surface dressing that iscommon in Sweden was tested with two different aggregates by Vialit plateshock test method. First it was tested with Swedish granite and then withIcelandic basalt. The results from the aggregates were compared where theadhesion with the granite was stronger than with the basalt.Previous study have found that by choosing binder and aggregate that have highadhesivity at low temperature reduces the risk of surface dressing defects,especially when paving takes place in the early and late summer season. Whenchoosing aggregates for road construction the main criteria is cost, thereforeaggregates that are used usually reflect the local geology because transportingaggregates for significant distances is expensive. Concluding from theexperiment, it is not recommended to use the basalt with the Swedish surfacedressing in practice now due to the lesser adhesion compared to the granite.However further research on the asphalt mix with the Swedish surface dressingand the basalt should be conducted.</p>


corrected abstract:
<p>Like in any modern civilization, roads in Iceland have an important role in the daily lives of inhabitants. Consequently, road quality is of equal importance, but Icelandic roads have shown problems when surface dressing is used where it looks decent after being paved during summer but then deforming pretty rapidly after being hit by elements of winter. Roads in Sweden however, do not seem to have the same problem.</p><p>The aim of this study is to minimize this road deformation by examining surface dressing and aggregates. The Icelandic climate is also a factor to this problem since the humidity is comparatively high, summers are cool, winters are mild and the climate is overall challenging. Furthermore, winter thaws are distinctive characteristic of the Icelandic weather, which increases strain on the asphalt.</p><p>An experiment was conducted where the adhesion of surface dressing that is common in Sweden was tested with two different aggregates by Vialit plate shock test method. First it was tested with Swedish granite and then with Icelandic basalt. The results from the aggregates were compared where the adhesion with the granite was stronger than with the basalt.</p><p>Previous study have found that by choosing binder and aggregate that have high adhesivity at low temperature reduces the risk of surface dressing defects, especially when paving takes place in the early and late summer season. When choosing aggregates for road construction the main criteria is cost, therefore aggregates that are used usually reflect the local geology because transporting aggregates for significant distances is expensive. Concluding from the experiment, it is not recommended to use the basalt with the Swedish surface dressing in practice now due to the lesser adhesion compared to the granite. However further research on the asphalt mix with the Swedish surface dressing and the basalt should be conducted.</p>
----------------------------------------------------------------------
In diva2:1451748 abstract is: <p>AbstractThe degree project was carried out at the request of the Swedish Tax Agency's IntegrationCompetency Center (ICC), or Skatteverket’s ICC, which works with buildingsystem integrations and open APIs, as well as providing competence support in integrationdevelopment for other organizations within Skatteverket. Developers atSkatteverket’s ICC need local environments that correspond to the production environmentsexisting for services currently running. This is necessary to allow developersto investigate problems and further develop the system. The process for doingthis is currently time-consuming, manual and complex. The purpose of the degreeproject is to investigate whether it is possible to, through automation and standardization,reduce the number of steps required to set up the local development environment.To investigate this, a pre-study has been conducted, in which both previouswork in the subject area and the Swedish Tax Agency's system have been evaluated.Thereafter, a prototype has been developed that analyses the production environmentof the current production environment for necessary information, to then createa complete development environment using the previously retrieved information.In order to validate the results, tests have been conducted using the prototypeset against the manual approach.The result shows that it is possible to reduce the number of steps required to set upa local development environment corresponding to the current production environment.It also shows that most of the steps that are saved depend on the number ofdependencies that a service holds when its environment is replicated.KeywordsIntegration development, EAI, Skatteverket, Eclipse, automation, development environment,MuleSoft, Anypoint, dependencies.</p>


w='prototypeset' val={'c': 'prototype set', 's': 'diva2:1451748', 'n': 'correct in original'}

corrected abstract:
<p>The degree project was carried out at the request of the Swedish Tax Agency's Integration Competency Center (ICC), or Skatteverket’s ICC, which works with building system integrations and open APIs, as well as providing competence support in integration development for other organizations within Skatteverket. Developers at Skatteverket’s ICC need local environments that correspond to the production environments existing for services currently running. This is necessary to allow developers to investigate problems and further develop the system. The process for doing this is currently time-consuming, manual and complex. The purpose of the degree project is to investigate whether it is possible to, through automation and standardization, reduce the number of steps required to set up the local development environment. To investigate this, a pre-study has been conducted, in which both previous work in the subject area and the Swedish Tax Agency's system have been evaluated. Thereafter, a prototype has been developed that analyses the production environment of the current production environment for necessary information, to then create a complete development environment using the previously retrieved information. In order to validate the results, tests have been conducted using the prototype set against the manual approach.</p><p>The result shows that it is possible to reduce the number of steps required to set up a local development environment corresponding to the current production environment. It also shows that most of the steps that are saved depend on the number of dependencies that a service holds when its environment is replicated.</p>
----------------------------------------------------------------------
title: "Workplace Interventions forPromoting Physical Health: A Qualitative Study of Health-Promoting Initiatives in a GlobalPharmaceutical Company"
==> "Workplace Interventions for Promoting Physical Health: A Qualitative Study of Health-Promoting Initiatives in a Global Pharmaceutical Company"

In diva2:1887795 abstract is: <p>Workplaces provide promising prospects for enhanced public health through related efforts,intended to preserve, enhance and ensure safety and health. A Health and Wellness departmentat a pharmaceutical company conveyed curiosity regarding the reach of the health-promotinginitiatives they provided to their front-line workers. The aim of this study was thus to investigatehow the front-line workers perceive health-promoting initiatives and to identify promoting andhindering aspects for increased engagement. Applied methods included semi-structuredinterviews, document review, and a synthesis of collected empirical evidence to enable anoverview and formulation of proposals. The interviews contributed insight into how the healthpromotingefforts are perceived by the target group, which also highlighted nuanced aspects ofcommitment. The approach included 18 respondents, with nine front-line representatives, andthe rest with a reputable connection to health promotion efforts and the work of the target group.The document review included web-based material on health promotion initiatives, termedHealth-Toolbox, focusing on three of its domains which include physical health, competencedevelopment and Health Ambassadors. The synthesis was constructed via an applicableframework (HTO), systematic review and comparison of collected data. The results showedthat a variety of health-promoting efforts exist at the company, but that perceptions about itsavailability and applicability often vary from person to person. Promoting aspects for increasedengagement were found to include motivational work, support, communication andaccessibility as well as the Health Ambassador role. While hindering aspects included lack oftime and resources, format and availability. A recurring phenomenon in the results was to findaspects that sometimes highlighted two sides of the same coin, where, for example, accessibilitywas shown as a promoting aspect that required a separate interpretation from availability thathighlighted other hindering aspects. The comparison of the empirical evidence generatedrecommendations with a particular focus on making available information about healthpromotingefforts and benefits through focused channels adapted to the target group.Organisational climates with possibilities for motivational work are concluded as important forhealth-promoting activities.</p>


corrected abstract:
<p>Workplaces provide promising prospects for enhanced public health through related efforts, intended to preserve, enhance and ensure safety and health. A Health and Wellness department at a pharmaceutical company conveyed curiosity regarding the reach of the health-promoting initiatives they provided to their front-line workers. The aim of this study was thus to investigate how the front-line workers perceive health-promoting initiatives and to identify promoting and hindering aspects for increased engagement. Applied methods included semi-structured interviews, document review, and a synthesis of collected empirical evidence to enable an overview and formulation of proposals. The interviews contributed insight into how the health promoting efforts are perceived by the target group, which also highlighted nuanced aspects of commitment. The approach included 18 respondents, with nine front-line representatives, and the rest with a reputable connection to health promotion efforts and the work of the target group. The document review included web-based material on health promotion initiatives, termed Health-Toolbox, focusing on three of its domains which include physical health, competence development and Health Ambassadors. The synthesis was constructed via an applicable framework (HTO), systematic review and comparison of collected data. The results showed that a variety of health-promoting efforts exist at the company, but that perceptions about its availability and applicability often vary from person to person. Promoting aspects for increased engagement were found to include motivational work, support, communication and accessibility as well as the Health Ambassador role. While hindering aspects included lack of time and resources, format and availability. A recurring phenomenon in the results was to find aspects that sometimes highlighted two sides of the same coin, where, for example, accessibility was shown as a promoting aspect that required a separate interpretation from availability that highlighted other hindering aspects. The comparison of the empirical evidence generated recommendations with a particular focus on making available information about health promoting efforts and benefits through focused channels adapted to the target group. Organisational climates with possibilities for motivational work are concluded as important for health-promoting activities.</p>
----------------------------------------------------------------------
In diva2:1454856 abstract is: <p>Measurement of protein levels, one of the essential criteria to assess the health state ofpatients, is widely used in clinical settings nowadays. Various advanced proteomics assays forclinical use have been approved by the Food and Drug Administration (FDA) and cleared forlaboratory use in US, including immunoassays (IAs) and liquid chromatography tandem massspectrometry (LC-MS/MS)2. However, both of them have the limitation of sensitivity, specificity,and multiplexing ability despite their strengths3-11. In this project, a combination of these twotechnologies called immunocapture mass spectrometry (MS) will be, thus, developed to overcomethe concerns about the precise quantification of multiple targets in complex samples, multiplexingability, time, and cost for studies of large cohorts11. The final aim is to establish a refined protocolfor robust sample handling and to optimize the enrichment conditions for precise quantification oflow abundant proteins over time as well as for highly-parallel workflows required in clinicalcontext. This project used plasma, binders, and stable-isotope internal standards (SIS), availableresources provided by the research group and the Human Protein Atlas (HPA) project. Theexperiments demonstrated a great binding ability of agarose beads as solid phase support towardsbinders, monoclonal antibodies (mAbs), with 10- to 1000-fold increase in the amount of singlechainvariable fragment (scFv) found on beads without significant non-specific binding to theQuantification Tag (QTag) compared to other types of beads. The analysis also revealed that notall mAbs could capture their corresponding PrEST during validation: only seven out of 60 mAbsshowed the signals of their target but one showed off-target interaction. Besides, limit of detection(LOD), limit of quantification (LOQ), and the endogenous level of each peptide determined fromits equivalent protein in plasma were examined through constructing its standard curves bySelecting Reaction Monitoring (SRM) development. Nevertheless, those seven mAbs did notsuccessfully capture their target protein in diluted plasma to ensure successful enrichment pastLOD and LOQ. Those results suggest a further enhancement of the conditions for binder-targetincubation. Additionally, alternative for PrEST should be proposed to completely eliminateunwanted binding towards solid phase support and to increase the binding rate towards binders.Finally, mAbs should be thoroughly validated their right-target-capturing ability.</p>


w='cohorts11' val={'c': 'cohorts', 's': 'diva2:1454856', 'n': 'the 11 was a superscript citation'}
w='strengths3-11' val={'c': 'strengths', 's': 'diva2:1454856', 'n': 'the 3-11 was a superscript citation'}

corrected abstract:
<p>Measurement of protein levels, one of the essential criteria to assess the health state of patients, is widely used in clinical settings nowadays. Various advanced proteomics assays for clinical use have been approved by the Food and Drug Administration (FDA) and cleared for laboratory use in US, including immunoassays (IAs) and liquid chromatography tandem mass spectrometry (LC-MS/MS)<sup>2</sup>. However, both of them have the limitation of sensitivity, specificity, and multiplexing ability despite their strengths<sup>3-11</sup>. In this project, a combination of these two technologies called immunocapture mass spectrometry (MS) will be, thus, developed to overcome the concerns about the precise quantification of multiple targets in complex samples, multiplexing ability, time, and cost for studies of large cohorts<sup>11</sup>. The final aim is to establish a refined protocol for robust sample handling and to optimize the enrichment conditions for precise quantification of low abundant proteins over time as well as for highly-parallel workflows required in clinical context. This project used plasma, binders, and stable-isotope internal standards (SIS), available resources provided by the research group and the Human Protein Atlas (HPA) project. The experiments demonstrated a great binding ability of agarose beads as solid phase support towards binders, monoclonal antibodies (mAbs), with 10- to 1000-fold increase in the amount of single-chain variable fragment (scFv) found on beads without significant non-specific binding to the Quantification Tag (QTag) compared to other types of beads. The analysis also revealed that not all mAbs could capture their corresponding PrEST during validation: only seven out of 60 mAbs showed the signals of their target but one showed off-target interaction. Besides, limit of detection (LOD), limit of quantification (LOQ), and the endogenous level of each peptide determined from its equivalent protein in plasma were examined through constructing its standard curves by Selecting Reaction Monitoring (SRM) development. Nevertheless, those seven mAbs did not successfully capture their target protein in diluted plasma to ensure successful enrichment past LOD and LOQ. Those results suggest a further enhancement of the conditions for binder-target incubation. Additionally, alternative for PrEST should be proposed to completely eliminate unwanted binding towards solid phase support and to increase the binding rate towards binders. Finally, mAbs should be thoroughly validated their right-target-capturing ability.</p>
----------------------------------------------------------------------
In diva2:1843155 abstract is: <p>It is very expensive to develop ground-based infrastructure to supply the entire earth with internet.This is especially the case in loosely populated areas where the economic incentive is very low for acompany. Sweden which is a large country with a small population has many areas in this category.At the same time, Sweden is a technologically advanced nation where most people use the internetand mobile telephony daily.The Swedish government cooperates with teleoperators. These teleoperators buy licenses on auctionto be able to use radio spectrum within Sweden which is a limited resource. In exchange they need tofulfill certain quotas on reception and wireless service. The operators are in this way obligated by theSwedish government to supply the country with the potential for reception .However, this is not very economic in rural areas where few people live, and the usage of individualground stations is low. This leads to ground stations being very unprofitable and is also a waste ofresources such as land, materials and capital. Companies are willing to spend large sums of money toavoid obligation to do this.At the same time satellite internet is becoming increasingly sophisticated and more and more peopleworldwide are starting to adopt satellite internet.A lot of wireless technology uses specific different radio frequencies to transmit information. The newtype of satellite internet generally uses frequencies within the Ku - and Ka-bands which are both af-fected by water. This means that for example, rain or snow could affect the quality of the wirelessconnection.A question that should be discussed is how satellite internet compares to existing technologies like4G and 5G. If this is done, it will be easier for involved parties to estimate its usefulness.This bachelor’s thesis focuses on gathering information to help resolve the question of how usefulsatellite internet can be. To accomplish this, performance measurements have been performed on aservice that provides satellite internet. Starlink, which is the most deployed satellite internet service,has been the subject of these measurements. The result is measurements on the performance of Star-link and comparisons between 4G/5G and Starlinks that can hopefully help shed some light on Star-links usefulness in Scandinavia.The measurements indicate that Starlink delivers a reliable connection that can be compared to mo-bile service but that in Sweden there is an added delay of circa 20 milliseconds due to the groundstations being located in central Europe.</p>


corrected abstract:
<p>It is very expensive to develop ground-based infrastructure to supply the entire earth with internet. This is especially the case in loosely populated areas where the economic incentive is very low for a company. Sweden which is a large country with a small population has many areas in this category. At the same time, Sweden is a technologically advanced nation where most people use the internet and mobile telephony daily.</p><p>The Swedish government cooperates with teleoperators. These teleoperators buy licenses on auction to be able to use radio spectrum within Sweden which is a limited resource. In exchange they need to fulfill certain quotas on reception and wireless service. The operators are in this way obligated by the Swedish government to supply the country with the potential for reception.</p><p>However, this is not very economic in rural areas where few people live, and the usage of individual ground stations is low. This leads to ground stations being very unprofitable and is also a waste of resources such as land, materials and capital. Companies are willing to spend large sums of money to avoid obligation to do this.</p><p>At the same time satellite internet is becoming increasingly sophisticated and more and more people worldwide are starting to adopt satellite internet.</p><p>A lot of wireless technology uses specific different radio frequencies to transmit information. The new type of satellite internet generally uses frequencies within the Ku- and Ka-bands which are both affected by water. This means that for example, rain or snow could affect the quality of the wireless connection.</p><p>A question that should be discussed is how satellite internet compares to existing technologies like 4G and 5G. If this is done, it will be easier for involved parties to estimate its usefulness.</p><p>This bachelor’s thesis focuses on gathering information to help resolve the question of how useful satellite internet can be. To accomplish this, performance measurements have been performed on a service that provides satellite internet. Starlink, which is the most deployed satellite internet service, has been the subject of these measurements. The result is measurements on the performance of Starlink and comparisons between 4G/5G and Starlinks that can hopefully help shed some light on Starlinks usefulness in Scandinavia.</p><p>The measurements indicate that Starlink delivers a reliable connection that can be compared to mobile service but that in Sweden there is an added delay of circa 20 milliseconds due to the ground stations being located in central Europe.</p>
----------------------------------------------------------------------
In diva2:1272715 abstract is: <p>Henriksdals sewage treatment plant is undergoing a major reconstruction in order to handleincreased load as well as the stricter cleaning requirements expected in the future. Due to theplanned shutdown of Bromma treatment plant, additional wastewater will be led to Henriksdalssewage treatment plant. When a significant increase in population in the catchment area is alsoexpected, the treatment plant will need a doubling of its capacity compared to today. Today'snitrogen treatment requirements in the effluent wastewater are 10 mg/L and this is expected tobe 6 mg/L in future. To achieve these requirements, the biological treatment step of the currentactive sludge process will be combined with membrane filtration in a so-called membranebioreactor (MBR). In the biological treatment, the nitrogen removal is a two-step process usingbacteria. These steps are nitrification and denitrification.</p><p>In the pilot plant at Hammarby Sjöstadsverk, in 2018, it is of interest to study the presence ofnitrogen fractions in the process, since the first of seven MBR treatment lines will be started atHenriksdals WWTP early 2020. At the start of this line there are some limitations regardingdosage of chemicals for the removal of phosphorus and nitrogen. No carbon source orprecipitation chemicals will be possible to add, since the storage for these will not be constructedin time for the startup.</p><p>To examine how the nitrogen acts in the different zones, and if it is possible to achieve thedesired nitrogen removal under these conditions, 12 test points have been selected in thebiological process line. The total concentration of nitrogen, N-tot, and the concentration ofnitrogen in the form of NO<sub>2</sub>, NO<sub>3</sub>, NH<sub>4</sub>, as well as the COD have been measured in these pointson four separate occasions. The samples have been analyzed and mass balances over thedifferent zones in the process line have been set up. The results vary between the different tests,performed in the morning and after lunch, but some trends can be observed. The predenitrificationtakes place in two zones, of which the second zone, BR2, exhibits very low ornon-existent changes in the mass flow of nitrate nitrogen (NO<sub>3</sub>-N), indicating that one zone withpre-denitrification would be sufficient. The first zone of nitrification also shows poor results,which may indicate that the zone has a low oxygen content and needs better airflow. The zonewith after-denitrification still has large amounts of nitrate in the outflow, which means that twozones with post-denitrification could produce better results. At all samplings a total nitrogen (Ntot)content of less than 10 mg/L was measured in the purified permeate flow. The average ofthe four occasions was also below the future requirement of 6 mg/L.</p>


w='Ntot' val={'c': 'N-tot', 's': 'diva2:1272715', 'n': 'hyphen at end of line'}

corrected abstract:
<p>Henriksdals sewage treatment plant is undergoing a major reconstruction in order to handle increased load as well as the stricter cleaning requirements expected in the future. Due to the planned shutdown of Bromma treatment plant, additional wastewater will be led to Henriksdals sewage treatment plant. When a significant increase in population in the catchment area is also expected, the treatment plant will need a doubling of its capacity compared to today. Today's nitrogen treatment requirements in the effluent wastewater are 10 mg/L and this is expected to be 6 mg/L in future. To achieve these requirements, the biological treatment step of the current active sludge process will be combined with membrane filtration in a so-called membrane bioreactor (MBR). In the biological treatment, the nitrogen removal is a two-step process using bacteria. These steps are nitrification and denitrification.</p><p>In the pilot plant at Hammarby Sjöstadsverk, in 2018, it is of interest to study the presence of nitrogen fractions in the process, since the first of seven MBR treatment lines will be started at Henriksdals WWTP early 2020. At the start of this line there are some limitations regarding dosage of chemicals for the removal of phosphorus and nitrogen. No carbon source or precipitation chemicals will be possible to add, since the storage for these will not be constructed in time for the startup.</p><p>To examine how the nitrogen acts in the different zones, and if it is possible to achieve the desired nitrogen removal under these conditions, 12 test points have been selected in the biological process line. The total concentration of nitrogen, N-tot, and the concentration of nitrogen in the form of NO<sub>2</sub>, NO<sub>3</sub>, NH<sub>4</sub>, as well as the COD have been measured in these points on four separate occasions. The samples have been analyzed and mass balances over the different zones in the process line have been set up. The results vary between the different tests, performed in the morning and after lunch, but some trends can be observed. The predenitrification takes place in two zones, of which the second zone, BR2, exhibits very low or non-existent changes in the mass flow of nitrate nitrogen (NO<sub>3</sub>-N), indicating that one zone with pre-denitrification would be sufficient. The first zone of nitrification also shows poor results, which may indicate that the zone has a low oxygen content and needs better airflow. The zone with after-denitrification still has large amounts of nitrate in the outflow, which means that two zones with post-denitrification could produce better results. At all samplings a total nitrogen (N-tot)content of less than 10 mg/L was measured in the purified permeate flow. The average of the four occasions was also below the future requirement of 6 mg/L.</p>
----------------------------------------------------------------------
In diva2:853075 abstract is: <p>The purpose of this thesis was to examine the risk of hazardous air pollutantsthat employees at the harbor may be exposed to and assess whether anyexposure limits might be exceeded, and to review if the security measures taken,can be improved. The results of the study show that there is some risk ofexposure for those working in the harbor from both the cargo, diesel exhaustand residues from fumigants in the containers. The operations when the truckdriver is inside the forklift is considered relatively protected from both particlesand gases, but there are other tasks where the worker is not as protected.Measurements of nitrogen dioxide showed that exposure to diesel exhaust forthe employee who opened the containers during the days that the measurementswere performed did not exceed some critical values, but was slightly higherthan for the driver who was sitting in the forklift. Whether the exposure to dust,from the cargo, constitutes a risk for health effects is impossible to determinewithout a thorough risk assessment, where measurements of dust can give anindication of how risky the situation is for the exposed workers.The organization should implement safer practices and implementing measuresthat motivates employees to use existing protective equipment. This applies notonly for exposure to diesel exhaust and dust from loading and unloading ofgoods, but also at the opening the doors on the containers in which residues offumigants, in which some are suspected to be carcinogenic, can remain in thecontainer.Conclusions: Conduct a thorough risk assessment and identify potential health riskswith air pollution. Establish written work instructions where potential hazards exist. Install ventilation in the warehouses, which is controlled by the levels ofnitrogen dioxide, to ensure that high levels of diesel exhaust does notoccur. Need to increase motivation for safety and the use of protectiveequipment. Implement procedures to comply with the legislation for the managementof dust and gases that applies to carcinogens substances.</p>

corrected abstract:
<p>The purpose of this thesis was to examine the risk of hazardous air pollutants that employees at the harbor may be exposed to and assess whether any exposure limits might be exceeded, and to review if the security measures taken, can be improved. The results of the study show that there is some risk of exposure for those working in the harbor from both the cargo, diesel exhaust and residues from fumigants in the containers. The operations when the truck driver is inside the forklift is considered relatively protected from both particles and gases, but there are other tasks where the worker is not as protected. Measurements of nitrogen dioxide showed that exposure to diesel exhaust for the employee who opened the containers during the days that the measurements were performed did not exceed some critical values, but was slightly higher than for the driver who was sitting in the forklift. Whether the exposure to dust, from the cargo, constitutes a risk for health effects is impossible to determine without a thorough risk assessment, where measurements of dust can give an indication of how risky the situation is for the exposed workers.</p><p>The organization should implement safer practices and implementing measures that motivates employees to use existing protective equipment. This applies not only for exposure to diesel exhaust and dust from loading and unloading of goods, but also at the opening the doors on the containers in which residues of fumigants, in which some are suspected to be carcinogenic, can remain in the container.</p><p>Conclusions:<ul><li>Conduct a thorough risk assessment and identify potential health risks with air pollution.</li><li>Establish written work instructions where potential hazards exist.</li><li>Install ventilation in the warehouses, which is controlled by the levels of nitrogen dioxide, to ensure that high levels of diesel exhaust does not occur.</li><li>Need to increase motivation for safety and the use of protective equipment.</li><li>Implement procedures to comply with the legislation for the management of dust and gases that applies to carcinogens substances.</li> </ul></p>
----------------------------------------------------------------------
In diva2:1229284 abstract is: <p>The objective of this study is to investigate the eect of dierent fuels on two uidized bed boiler systemsat the energy company Soderenergi's site in Igelsta, called IKV and IGV P3. Today, recovered wastewood (RWW) is the major fuel share fed into the boilers. However, with an insecure fuel supply in thefuture, other fuel types must be considered. Based on knowledge from previous fuel usage in the boilers,an evaluation of how other potential fuel mixtures may eect the operation is conducted. The additionalfuels considered in the fuel blends are; stem wood chips, cutter shavings, solid recycled fuel (SRF) andrubber.With elemental analysis of the fuels and established key numbers, the previous fuel mixtures are evaluated.The indications by the guiding parameters are compared with experienced problems and the formercondition of the boilers, and the risk limits for the key numbers are adjusted to a suitable level. Thepotential mixtures are evaluated with the key numbers and the updated limits. In addition to the keynumbers, the heavy metal concentration, the heating value, the moisture content and the ash content ofthe fuel blends are included in the evaluation. The considered damages in the boilers caused by the fuelblends are corrosion, sintering and fouling.The damage level from the current fuel usage for IKV and IGV P3 is fairly low. The results from theanalyzed fuel mixtures show an increased damage risk in the boilers. Additionally, adjustments of theboiler systems are required by some of the analyzed fuel mixtures. In general, the corrosion risk andthe heavy metal content will increase in comparison with today's fuel. The fouling and slagging are aswell expected to increase for the assessed fuel mixtures. Moreover, the result illustrates an increased ashgeneration, which demands a reconstruction of the ash cooling system for IKV. Furthermore, the increaseof LHV in the assessed fuel mixtures to IGV P3, is likely to require an increased capacity of the ue gasrecirculation pump.In the analysis of the potential fuel mixtures it is found that the corrosion risk expressed by the keynumbers is reduced with a higher share of rubber. The heavy metal content is, however, increased,leading to e.g. an enhanced risk for formation of eutectic salts, which as well are corrosive. On thecontrary, the fuel mixtures with a high risk expressed by the key numbers, have the lowest concentrationsof heavy metals. Due to the results are con icting, a balance between the risk indicated by the keynumbers and the heavy metal concentration must be considered in the evaluation. The fuel mixturesconsidered causing least damage to IKV are a mixture of 42% RWW, 48% wood fuel and 15% SRF, and amixture of 70% wood fuel, 20% SRF and 10% rubber. The fuel mixtures considered causing least damageto IGV P3 are a mixture of 85% RWW and 15% rubber and a mixture of 70% RWW and 30% SRF.</p>

w='eect' val={'c': 'effect', 's': ['diva2:1344757', 'diva2:1229284'], 'n': 'missing ligature'}
w='uidized' val={'c': 'fluidized', 's': 'diva2:1229284', 'n': 'missing ligature'}
w='ue' val={'c': 'flue', 's': 'diva2:1229284', 'n': 'missing ligature'}

corrected abstract:
<p>The objective of this study is to investigate the effect of different fuels on two fluidized bed boiler systems at the energy company Söderenergi’s site in Igelsta, called IKV and IGV P3. Today, recovered waste wood (RWW) is the major fuel share fed into the boilers. However, with an insecure fuel supply in the future, other fuel types must be considered. Based on knowledge from previous fuel usage in the boilers, an evaluation of how other potential fuel mixtures may effect the operation is conducted. The additional fuels considered in the fuel blends are; stem wood chips, cutter shavings, solid recycled fuel (SRF) and rubber.</p><p>With elemental analysis of the fuels and established key numbers, the previous fuel mixtures are evaluated. The indications by the guiding parameters are compared with experienced problems and the former condition of the boilers, and the risk limits for the key numbers are adjusted to a suitable level. The potential mixtures are evaluated with the key numbers and the updated limits. In addition to the key numbers, the heavy metal concentration, the heating value, the moisture content and the ash content of the fuel blends are included in the evaluation. The considered damages in the boilers caused by the fuel blends are corrosion, sintering and fouling.</p><p>The damage level from the current fuel usage for IKV and IGV P3 is fairly low. The results from the analyzed fuel mixtures show an increased damage risk in the boilers. Additionally, adjustments of the boiler systems are required by some of the analyzed fuel mixtures. In general, the corrosion risk and the heavy metal content will increase in comparison with today’s fuel. The fouling and slagging are as well expected to increase for the assessed fuel mixtures. Moreover, the result illustrates an increased ash generation, which demands a reconstruction of the ash cooling system for IKV. Furthermore, the increase of LHV in the assessed fuel mixtures to IGV P3, is likely to require an increased capacity of the flue gas recirculation pump.</p><p>In the analysis of the potential fuel mixtures it is found that the corrosion risk expressed by the key numbers is reduced with a higher share of rubber. The heavy metal content is, however, increased, leading to e.g. an enhanced risk for formation of eutectic salts, which as well are corrosive. On the contrary, the fuel mixtures with a high risk expressed by the key numbers, have the lowest concentrations of heavy metals. Due to the results are conflicting, a balance between the risk indicated by the key numbers and the heavy metal concentration must be considered in the evaluation. The fuel mixtures considered causing least damage to IKV are a mixture of 42% RWW, 48% wood fuel and 15% SRF, and a mixture of 70% wood fuel, 20% SRF and 10% rubber. The fuel mixtures considered causing least damage to IGV P3 are a mixture of 85% RWW and 15% rubber and a mixture of 70% RWW and 30% SRF.</p>
----------------------------------------------------------------------
In diva2:1468986 abstract is: <p>Background:Approximately 15% of the world’s population are affected bysome kind of disability where over 150 conditions may affect the human gaitpattern. The ability to ambulate with ease is important for overall well-being.Various assistive devices have been developed to improve mobility of theirusers. A lot of research is currently focused on ankle exoskeletons, showingpromising results in providing important assistance during stance phase of gait.</p><p>Objective:To investigate how different combinations of active and passiveelements in an ankle exoskeleton affects the metabolic cost of walking.</p><p>Methods:Musculoskeletal simulations were carried out in OpenSim Moco.Different assistive configurations were tested over one gait cycle using a pas-sive element, an active element, and a parallel connection of the both. Parame-ter values were modified to find the most optimal setup for reducing metaboliccost.</p><p>Results:All assistive configurations were found successful in reducing bothwhole-body metabolic cost and the metabolic cost of the plantarflexors whencompared to the unassisted gait. Most whole-body metabolic cost reductionwas found when using a passive spring with resting length of 0.28 m and stiff-ness of 6 kN/m in parallel with an active motor capable of providing forceequal to 150% of body weight. The most reduction in metabolic cost of theplantarflexors was also found for a parallel connection of elements, but herewith a 100% body weight motor and spring with rest length of 0.19 m andstiffness of 10 kN/m. With higher assistance, more reduction in ankle mo-ment generated by the muscles was observed.</p><p>Conclusion:Powered ankle exoskeletons are promising in terms of minimiz-ing metabolic cost during walking due to assistance during late stance phaseof gait for ambulators requiring plantarflexor assistance.</p><p>Keywords:Simulation, exoskeleton, ankle, moco.</p>

w='mo-ment' val={'c': 'moment', 's': 'diva2:1468986', 'n': 'correct in original'}

corrected abstract:
<p><strong>Background</strong>: Approximately 15% of the world’s population are affected by some kind of disability where over 150 conditions may affect the human gait pattern. The ability to ambulate with ease is important for overall well-being. Various assistive devices have been developed to improve mobility of their users. A lot of research is currently focused on ankle exoskeletons, showing promising results in providing important assistance during stance phase of gait.</p><p><strong>Objective</strong>: To investigate how different combinations of active and passive elements in an ankle exoskeleton affects the metabolic cost of walking.</p><p><strong>Methods</strong>: Musculoskeletal simulations were carried out in OpenSim Moco. Different assistive configurations were tested over one gait cycle using a passive element, an active element, and a parallel connection of the both. Parameter values were modified to find the most optimal setup for reducing metabolic cost.</p><p><strong>Results</strong>: All assistive configurations were found successful in reducing both whole-body metabolic cost and the metabolic cost of the plantarflexors when compared to the unassisted gait. Most whole-body metabolic cost reduction was found when using a passive spring with resting length of 0.28 m and stiffness of 6 kN/m in parallel with an active motor capable of providing force equal to 150% of body weight. The most reduction in metabolic cost of the plantarflexors was also found for a parallel connection of elements, but here with a 100% body weight motor and spring with rest length of 0.19 m and stiffness of 10 kN/m. With higher assistance, more reduction in ankle moment generated by the muscles was observed.</p><p><strong>Conclusion</strong>: Powered ankle exoskeletons are promising in terms of minimizing metabolic cost during walking due to assistance during late stance phase of gait for ambulators requiring plantarflexor assistance.</p>
----------------------------------------------------------------------
In diva2:1739120 abstract is: <p>Automatically detecting events for people with diabetes mellitus using continuousglucose monitors is an important step in allowing insulin pumps to automaticallycorrect the blood glucose levels and for a more hands-off approach to thedisease. The automatic detection of events could also aid physicians whenassisting their patients when referring to their continuous glucose monitordata. A range of different deep learning algorithms has been applied forpredictions of different events for continuous glucose monitor data, such asthe onset for hyperglycemia, hypoglycemia or mealtime events. This thesisfocused on constructing sequences labelled from an unbalanced and assumedmisslabelled dataset to classify them as such using four different deep learningnetworks using convoluted neural networks and recurrent neural networks.Manual correction of the dataset allowed for only clear events starting witha high positive gradient to be labelled as positive. The classification wasperformed on exact timepoints and in time windows to allow the classificationto to be done around the beginning of an event instead of the exact timepoint.The results from using the unbalanced and assumed misslabelled datasetshowed the networks performing similarly, with high Recall and Precisionbelow 0.5, thus not found to be of use in a for automatic event detection.Further testing by using another dataset or further configurations is neededto clarify the capabilities of automatically detecting events. DDAnalytics willnot use any of the developed networks in any of their products.</p>

corrected abstract:
<p>Automatically detecting events for people with diabetes mellitus using continuous glucose monitors is an important step in allowing insulin pumps to automatically correct the blood glucose levels and for a more hands-off approach to the disease. The automatic detection of events could also aid physicians when assisting their patients when referring to their continuous glucose monitor data. A range of different deep learning algorithms has been applied for predictions of different events for continuous glucose monitor data, such as the onset for hyperglycemia, hypoglycemia or mealtime events. This thesis focused on constructing sequences labelled from an unbalanced and assumed misslabelled dataset to classify them as such using four different deep learning networks using convoluted neural networks and recurrent neural networks. Manual correction of the dataset allowed for only clear events starting with a high positive gradient to be labelled as positive. The classification was performed on exact timepoints and in time windows to allow the classification to to be done around the beginning of an event instead of the exact timepoint. The results from using the unbalanced and assumed misslabelled dataset showed the networks performing similarly, with high <em>Recall</em> and <em>Precision</em> below 0.5, thus not found to be of use in a for automatic event detection. Further testing by using another dataset or further configurations is needed to clarify the capabilities of automatically detecting events. DDAnalytics will not use any of the developed networks in any of their products.</p>
----------------------------------------------------------------------
In diva2:1864417 abstract is: <p>This thesis explores which transfer methods young adults prefer among Swish,Klarna, and Tink, as well as the technical implementation concerning code com-plexity for these transfer and payment methods. This is important to offer transferand payment methods that young adults feel comfortable with and to ensure thistarget group continues to move towards a cashless society. The study aims to fill theresearch gap regarding a direct comparison of these payment methods.Research shows that young adults prefer methods that are user-friendly and have astrong brand reputation. The research also highlights the need to specifically ad-dress young adults as they lack financial knowledge in the area.A prototype was developed to compare code complexity with the most popular pay-ment methods Swish, Klarna, and Tink. The code complexity for the implementa-tion of these methods was measured using Cyclomatic Complexity, and the resultsshowed that Swish, Klarna, and Tink all have similar implementations via API callsin their simplest form, which means they do not increase code complexity for devel-opers.Based on the comparison of code complexity, a complete prototype was chosen tobe developed for functional testing, implemented with Tink, using Quarkus for thebackend logic and SwiftUI for the frontend interface. SwiftUI was chosen to createa dynamic and user-friendly interface experience for Apple devices, while Quarkuswas chosen for its robustness and efficiency in server-side processing.The prototype was functionally tested and evaluated by the product owner Peak AMSecurities AB. The result was that the prototype's codebase can be used in futureproduction deployment.This study contributes to the understanding of young adults' preferences for digitalpayment methods and provides a technical foundation for further research in thearea. By comparing Swish, Klarna, and Tink, the work offers important insightsthat can help developers create better financial tools that meet users' needs and ex-pectations.</p>

w='devel-opers' val={'c': 'developers', 's': 'diva2:1864417'}
w='ad-dress' val={'c': 'address', 's': 'diva2:1864417'}
w='com-plex' val={'c': 'complex', 's': ['diva2:1864417', 'diva2:1802066']}

corrected abstract:
<p>This thesis explores which transfer methods young adults prefer among Swish, Klarna, and Tink, as well as the technical implementation concerning code complexity for these transfer and payment methods. This is important to offer transfer and payment methods that young adults feel comfortable with and to ensure this target group continues to move towards a cashless society. The study aims to fill the research gap regarding a direct comparison of these payment methods.</p><p>Research shows that young adults prefer methods that are user-friendly and have a strong brand reputation. The research also highlights the need to specifically address young adults as they lack financial knowledge in the area.</p><p>A prototype was developed to compare code complexity with the most popular payment methods Swish, Klarna, and Tink. The code complexity for the implementation of these methods was measured using Cyclomatic Complexity, and the results showed that Swish, Klarna, and Tink all have similar implementations via API calls in their simplest form, which means they do not increase code complexity for developers.</p><p>Based on the comparison of code complexity, a complete prototype was chosen to be developed for functional testing, implemented with Tink, using Quarkus for the backend logic and SwiftUI for the frontend interface. SwiftUI was chosen to create a dynamic and user-friendly interface experience for Apple devices, while Quarkus was chosen for its robustness and efficiency in server-side processing.</p><p>The prototype was functionally tested and evaluated by the product owner Peak AM Securities AB. The result was that the prototype's codebase can be used in future production deployment.</p><p>This study contributes to the understanding of young adults' preferences for digital payment methods and provides a technical foundation for further research in the area. By comparing Swish, Klarna, and Tink, the work offers important insights that can help developers create better financial tools that meet users' needs and expectations.</p>
----------------------------------------------------------------------
In diva2:1636873 abstract is: <p>Summary</p><p>Autonomous vehicles are a growing trend in society and are anticipated to change the wholetransportation system as we know it today. Many vehicle manufacturers are focusing ondevelopment of different kinds of self-driving vehicles. The variety of automation in vehiclescan be divided into six levels of automation, according to the Society for Automotive Engineers(SAE) International’s division. As the level of automation varies between vehicles, the level ofneeded driver action also varies. As the automation level gets higher the drivers role changesmore and more towards being a supervisory controller for the automation. This leads to adifferent need of information presented to the driver in an autonomous vehicle compared towhat is provided in a manual vehicle. The driver must be able to understand what the automationis doing and predict how it will behave in different situations. Hence, the design of the Human-Machine Interface (HMI) is utterly important for the safety of autonomous driving.</p><p>The main objective for the thesis work was to evaluate the driving compartment, from acognitive ergonomics point of view, for a special type of professional autonomous road vehicle.</p><p>A literature study focusing on evaluation and testing of HMIs in autonomous vehicles wereperformed and the evaluation was done through user tests in a simulator with 22 professionaldrivers. The tests were followed by questionnaires and interviews about the test participants’experience of the designed HMI.</p><p>In the literature study, only a few publications, were found addressing the validation of thewhole human machine interaction in autonomous vehicles. Most of the references had focus ontesting of limited subparts of the HMI, such as trust in automation, situational awareness andtransitions between manual and automatic driving mode.</p><p>The designed HMI worked as intended, since the test participants noticed and acted upon allHMI signals and were able to takeover and handover the control from/to the automation. Somerecommendations were, however, provided for the future development work of the evaluatedautonomous vehicle. For example, since there were some limitations in the simulator used andin the test design, a next iteration of the test was recommended for future development.</p><p>This evaluation was done as a first exploratory test and the aim of the thesis work has beenfulfilled. The design of the HMI, the manoeuvring devices for the autonomous vehicle and theconcept of two driver roles are, at this stage, considered adequate for continuing the project.</p><p>It was also concluded that the driving behaviour of the automation is a crucial aspect forfacilitation of user-trust in the autonomous vehicle and that clear procedures for communicationare needed to ensure traffic safety.</p><p>Further, it was concluded that standardization of frameworks and methods for evaluatingautonomous vehicles is beneficial for developers, authorities as well as end-users.</p>


corrected abstract:
<p>Autonomous vehicles are a growing trend in society and are anticipated to change the whole transportation system as we know it today. Many vehicle manufacturers are focusing on development of different kinds of self-driving vehicles. The variety of automation in vehicles can be divided into six levels of automation, according to the Society for Automotive Engineers (SAE) International’s division. As the level of automation varies between vehicles, the level of needed driver action also varies. As the automation level gets higher the drivers role changes more and more towards being a supervisory controller for the automation. This leads to a different need of information presented to the driver in an autonomous vehicle compared to what is provided in a manual vehicle. The driver must be able to understand what the automation is doing and predict how it will behave in different situations. Hence, the design of the Human-Machine Interface (HMI) is utterly important for the safety of autonomous driving.</p><p>The main objective for the thesis work was to evaluate the driving compartment, from a cognitive ergonomics point of view, for a special type of professional autonomous road vehicle.</p><p>A literature study focusing on evaluation and testing of HMIs in autonomous vehicles were performed and the evaluation was done through user tests in a simulator with 22 professional drivers. The tests were followed by questionnaires and interviews about the test participants’ experience of the designed HMI.</p><p>In the literature study, only a few publications, were found addressing the validation of the whole human machine interaction in autonomous vehicles. Most of the references had focus on testing of limited subparts of the HMI, such as trust in automation, situational awareness and transitions between manual and automatic driving mode.</p><p>The designed HMI worked as intended, since the test participants noticed and acted upon all HMI signals and were able to takeover and handover the control from/to the automation. Some recommendations were, however, provided for the future development work of the evaluated autonomous vehicle. For example, since there were some limitations in the simulator used and in the test design, a next iteration of the test was recommended for future development.</p><p>This evaluation was done as a first exploratory test and the aim of the thesis work has been fulfilled. The design of the HMI, the manoeuvring devices for the autonomous vehicle and the concept of two driver roles are, at this stage, considered adequate for continuing the project.</p><p>It was also concluded that the driving behaviour of the automation is a crucial aspect for facilitation of user-trust in the autonomous vehicle and that clear procedures for communication are needed to ensure traffic safety.</p><p>Further, it was concluded that standardization of frameworks and methods for evaluating autonomous vehicles is beneficial for developers, authorities as well as end-users.</p>
----------------------------------------------------------------------
In diva2:1568044 abstract is: <p>Finite Element (FE) head models are very convenient tools forthe study of Traumatic Brain Injuries (TBIs) but lack significantanatomical details for the investigation of morphology or age-dependantinjury mechanisms. In this context, the use of deformable registrationalgorithms for the generation of personalized head models is veryconsistent for the development of improved protection systems likehelmets. This thesis presents the performances of the registrationpipeline Demons combined to the Difformable Registration via AttributesMatching and Mutual-SaliencyWeighting (DRAMMS) for the generationof FE head models. Twelve subject-specific models are formed bymorphing the baseline mesh with the displacement fields resultingfrom the registration methods. The obtained models are assessedand compared through the evaluation of elements’ quality by analysisof the distortion index distribution. The Dice similarity coefficientis also calculated to estimate the personalization accuracy of theapplied pipeline. The Demons+DRAMMS registration pipeline showssatisfactory personalization accuracy for cranial mask and internalbrain structures. No significant degradation of mesh quality dueto the morphing process or specific subject morphology is observed.The present work corroborates previous study regarding the use ofDemons+DRAMMS registration pipeline for generating subject-specifichead models and validates the performances of the registration methodsand the repeatability of the morphing process for this purpose.</p>

w='age-dependant' val={'c': 'age-dependent', 's': 'diva2:1568044'}

corrected abstract:
<p>Finite Element (FE) head models are very convenient tools for the study of Traumatic Brain Injuries (TBIs) but lack significant anatomical details for the investigation of morphology or age-dependant injury mechanisms. In this context, the use of deformable registration algorithms for the generation of personalized head models is very consistent for the development of improved protection systems like helmets. This thesis presents the performances of the registration pipeline Demons combined to the Difformable Registration via Attributes Matching and Mutual-Saliency Weighting (DRAMMS) for the generation of FE head models. Twelve subject-specific models are formed by morphing the baseline mesh with the displacement fields resulting from the registration methods. The obtained models are assessed and compared through the evaluation of elements’ quality by analysis of the distortion index distribution. The Dice similarity coefficient is also calculated to estimate the personalization accuracy of the applied pipeline. The Demons+DRAMMS registration pipeline shows satisfactory personalization accuracy for cranial mask and internal brain structures. No significant degradation of mesh quality due to the morphing process or specific subject morphology is observed. The present work corroborates previous study regarding the use of Demons+DRAMMS registration pipeline for generating subject-specific head models and validates the performances of the registration methods and the repeatability of the morphing process for this purpose.</p>

----------------------------------------------------------------------
In diva2:730247 abstract is: <p>Thermalageing of the commercial selective catalytic reduction catalysts used inScania’s trucks was investigated using catalyst characterization techniques.Catalyst samples were oven-aged at 550 °C for up to 990 hours and investigatedwith nitrogen adsorption, oxygen chemisorption, X-ray fluorescence, X-raydiffraction, X-ray photoelectron spectroscopy and temperature-programmeddesorption of ammonia. The two latter methods are new to Scania and wereevaluated in depth. Furthermore, field-aged samples, which had had theircatalytic performance tested in another study, were investigated, in an attemptto find some link between characterization results and catalytic activity. Theinvestigation of oven-aged samples yielded information about the timescales ofcarrier sintering and sintering of catalytically active material, showing theformer to be much slower than the latter. It was also noted that the rate withwhich the catalyst’s ability to store ammonia decreases during thermal ageingwas similar to the rate of sintering of catalytically active material,suggesting that the loss of ammonia storage capability due to thermal ageing isrelated to sintering of the catalytically active material. X-ray photoelectronspectroscopy revealed that the fraction of vanadium in the outermost surfacelayer of the catalysts had increased during ageing. At the same time, thischaracterization technique appeared to have a low repeatability, possibly dueto the investigated catalyst having a high surface inhomogeneity. Whether ornot 500 ppm of NOx was present in the ageing atmosphere did notappear to affect the deactivation of the catalyst. Finally, no clear link couldbe found between characterization results and catalytic activity for field-agedsamples.</p>

w='Thermalageing' val={'c': 'Thermal ageing ', 's': 'diva2:730247', 'n': 'correct in original'}

corrected abstract:
<p>Thermal ageing of the commercial selective catalytic reduction catalysts used in Scania’s trucks was investigated using catalyst characterization techniques. Catalyst samples were oven-aged at 550 °C for up to 990 hours and investigated with nitrogen adsorption, oxygen chemisorption, X-ray fluorescence, X-ray diffraction, X-ray photoelectron spectroscopy and temperature-programmed desorption of ammonia. The two latter methods are new to Scania and were evaluated in depth. Furthermore, field-aged samples, which had had their catalytic performance tested in another study, were investigated, in an attempt to find some link between characterization results and catalytic activity. The investigation of oven-aged samples yielded information about the timescales of carrier sintering and sintering of catalytically active material, showing the former to be much slower than the latter. It was also noted that the rate with which the catalyst’s ability to store ammonia decreases during thermal ageing was similar to the rate of sintering of catalytically active material, suggesting that the loss of ammonia storage capability due to thermal ageing is related to sintering of the catalytically active material. X-ray photoelectron spectroscopy revealed that the fraction of vanadium in the outermost surface layer of the catalysts had increased during ageing. At the same time, this characterization technique appeared to have a low repeatability, possibly due to the investigated catalyst having a high surface inhomogeneity. Whether or not 500 ppm of NO<sub>x</sub> was present in the ageing atmosphere did not appear to affect the deactivation of the catalyst. Finally, no clear link could be found between characterization results and catalytic activity for field-aged samples.</p>
----------------------------------------------------------------------
In diva2:1352060 abstract is: <p><strong>Introduction:</strong> Clostridium thermocellum is considered a model organism forconsolidated bioprocessing, due to its ability to hydrolyze lignocellulosicbiomass more efficiently than many other organisms and to produce ethanol.In order to meet the industrial requirements of ethanol yield and titer, metabolicengineering efforts have been made resulting in a strain that successfullydisplays increased ethanol yield with reduced amount of some byproducts.However, the ethanol yield in this engineered strain still does not meet theindustrial requirements and significant amounts of amino acids are stillproduced. To attempt to decrease the level of amino acid excretion intended toimprove the ethanol yield in C. thermocellum, it is essential to understand itsmetabolism and how it is affected by different cultivation conditions and mediumcompositions. This study aimed to gain an insight in how carbon- and nitrogenlimitation affect amino acid excretion in C. thermocellum, with the hypothesisthat excess of carbon and nitrogen yields more amino acid excretion.</p><p><strong>Methods:</strong> Mass-balance based calculations of rates and yields were used toanalyze the metabolism of a wild-type of C. thermocellum (DSM 1313) grownanaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbonmedium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as thelimiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0and 400 RPM shaking at a dilution rate of 0.1 h-1.</p><p><strong>Conclusion:</strong>  Considering yields of total amino acids excreted in bothlimitations, it was hypothesized that C. thermocellum exploited the amino acidexcretion to maintain carbon balance around the pyruvate node caused byexcess of the carbon. Based on yield of valine excreted in particular, it washypothesized that amino acid excretion was used to maintain redox balance inthe metabolism of C. thermocellum, where malate shunt could play a major role.However, results of the Carbon-limitation did not allow any conclusion ofnitrogen excess having an effect on amino acid excretion in C. thermocellum.</p>


corrected abstract:
<p><strong>Introduction:</strong> <em>Clostridium thermocellum</em> is considered a model organism for consolidated bioprocessing, due to its ability to hydrolyze lignocellulosic biomass more efficiently than many other organisms and to produce ethanol. In order to meet the industrial requirements of ethanol yield and titer, metabolic engineering efforts have been made resulting in a strain that successfully displays increased ethanol yield with reduced amount of some byproducts. However, the ethanol yield in this engineered strain still does not meet the industrial requirements and significant amounts of amino acids are still produced. To attempt to decrease the level of amino acid excretion intended to improve the ethanol yield in <em>C. thermocellum</em>, it is essential to understand its metabolism and how it is affected by different cultivation conditions and medium compositions. This study aimed to gain an insight in how carbon- and nitrogen limitation affect amino acid excretion in <em>C. thermocellum</em>, with the hypothesis that excess of carbon and nitrogen yields more amino acid excretion.</p><p><strong>Methods:</strong> Mass-balance based calculations of rates and yields were used to analyze the metabolism of a wild-type of <em>C. thermocellum</em> (DSM 1313) grown anaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbon medium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as the limiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0 and 400 RPM shaking at a dilution rate of 0.1 h-1.</p><p><strong>Conclusion:</strong>  Considering yields of total amino acids excreted in both limitations, it was hypothesized that <em>C. thermocellum</em> exploited the amino acid excretion to maintain carbon balance around the pyruvate node caused by excess of the carbon. Based on yield of valine excreted in particular, it was hypothesized that amino acid excretion was used to maintain redox balance in the metabolism of <em>C. thermocellum</em>, where malate shunt could play a major role. However, results of the Carbon-limitation did not allow any conclusion of nitrogen excess having an effect on amino acid excretion in <em>C. thermocellum</em>.</p>
----------------------------------------------------------------------
In diva2:1038977 abstract is: <p>The cost of developing new pharmaceuticals has increased, while the number ofpharmaceuticals approved has declined. This highlights the importance for newpharmaceuticals to quickly become successful. The aim of this thesis is to explore thefactors of importance when launching new pharmaceuticals. Initially a literature reviewhas been conducted to explore general factors of importance for a pharmaceutical tobecome a commercial success. Furthermore, eleven in-depth interviews have beenperformed with stakeholders from the Swedish healthcare system to identify significantfactors on a national and regional level in Sweden. A thematic analysis was used tocategorize the data collected in the interviews.</p><p>The result of the literature review showed that the value creating process is of utmostimportance for a pharmaceutical to become a success. This is affected by a customeroriented focus, the design of the pharmaceutical study and the outcome from the healtheconomic analysis. Additionally, a number of challenges in the pharmaceutical supplychain were identified, which could cause a bottleneck during the launch of newpharmaceuticals.</p><p>In the empirical part a main theme ‘Trust’ with a total of seven subthemes was identified.The seven subthemes are factors that are essential to gain the trust and create the value,they are: ‘Guidelines and Regulations’, ‘Clinical Efficacy and Clinical Evidence’,‘Marketing’, ‘Information’, ‘Adherence and Compliance’, ‘Health Economics’ and‘Financial Aspect’. The subtheme ‘Guidelines and Regulations’ highlights the connectionbetween guidelines and utilization of pharmaceuticals. The ‘Clinical Efficacy and ClinicalEvidence’ was identified as the utmost important success factor. Furthermore the theme‘Value of Money’, including the subthemes ‘Health Economics’ and ‘Financial Aspect’,highlights the importance of the budget aspect and the increasing use of healtheconomics to evaluate health benefits and costs in healthcare. The subtheme ‘Adherenceand Compliance’ stresses the importance of information to the end user, whereas thesubtheme ‘Information’ highlights the information exchange among differentstakeholders. The subtheme ‘Marketing’ describes the effect of personal relationshipbetween industry and prescribers, and the effect on the pharmaceutical use it can have.</p>


corrected abstract:
<p>The cost of developing new pharmaceuticals has increased, while the number of pharmaceuticals approved has declined. This highlights the importance for new pharmaceuticals to quickly become successful. The aim of this thesis is to explore the factors of importance when launching new pharmaceuticals. Initially a literature review has been conducted to explore general factors of importance for a pharmaceutical to become a commercial success. Furthermore, eleven in-depth interviews have been performed with stakeholders from the Swedish healthcare system to identify significant factors on a national and regional level in Sweden. A thematic analysis was used to categorize the data collected in the interviews.</p><p>The result of the literature review showed that the value creating process is of utmost importance for a pharmaceutical to become a success. This is affected by a customer oriented focus, the design of the pharmaceutical study and the outcome from the health economic analysis. Additionally, a number of challenges in the pharmaceutical supply chain were identified, which could cause a bottleneck during the launch of new pharmaceuticals.</p><p>In the empirical part a main theme ‘Trust’ with a total of seven subthemes was identified. The seven subthemes are factors that are essential to gain the trust and create the value, they are: ‘Guidelines and Regulations’, ‘Clinical Efficacy and Clinical Evidence’, ‘Marketing’, ‘Information’, ‘Adherence and Compliance’, ‘Health Economics’ and ‘Financial Aspect’. The subtheme ‘Guidelines and Regulations’ highlights the connection between guidelines and utilization of pharmaceuticals. The ‘Clinical Efficacy and Clinical Evidence’ was identified as the utmost important success factor. Furthermore the theme ‘Value of Money’, including the subthemes ‘Health Economics’ and ‘Financial Aspect’, highlights the importance of the budget aspect and the increasing use of health economics to evaluate health benefits and costs in healthcare. The subtheme ‘Adherence and Compliance’ stresses the importance of information to the end user, whereas the subtheme ‘Information’ highlights the information exchange among different stakeholders. The subtheme ‘Marketing’ describes the effect of personal relationship between industry and prescribers, and the effect on the pharmaceutical use it can have.</p>
----------------------------------------------------------------------
In diva2:1228146 abstract is: <p>Using and finding applications from biomass is and will continue to be an important subject forresearch, and biomass from trees, has shown several outstanding aspects other than just for the pulpand paper applications. It is now, more than ever, time to find efficient uses for all the woodcomponents, in particular, the hemicelluloses. The hemicelluloses account for approximately onethirdof a dry composition of lignocellulosic wood biomasses. Of these hemicelluloses, xylan is themost abundant in many plants, particularly in hardwood. As for the Swedish forestry, xylan frombirch is considered as one of the most promising resources for the future.This thesis investigates the impact of acetylation of xylan on some properties such as solubility,thermal stability and film formation. Films were prepared using the non- and acetylated xylan withaddition of different plasticizers (glycerol, sorbitol and xylitol).Alkali-soluble birch xylan (ASX), obtained by ethanol/toluene extraction and sodium chloritedelignification of the wood sawdust followed by potassium hydroxide extraction of the obtainedholocellulose, and commercial xylan (CX) were acetylated to different degree of substitution withacetyl groups (DSAc), using acetic anhydride in dimethyl sulfoxide (DMSO) and 1-methylimidazole(NMI). Films were prepared by suspending non-acetylated xylan in water (H2O) and adding differentpercentages of plasticizers (20 and 40%) or by suspending acetylated xylan in chloroform (CHCl3).Characterizations of the non- and acetylated polymer (AcASX and AcCX) and films were conducted inorder to determine thermal and mechanical properties.CX and ASX presented different reactivity leading to different behaviour during acetylation and sodifferent DSAc. The thermal stability has been improved for both ASX and CX following the increase ofthe DSAc. Concerning film formation, ASX showed a great ability to form films through casting with orwithout plasticizers while it was impossible to obtain any films using only CX. For AcASX and AcCX thefilm formation using chloroform was depending on the DSAc and the dispersability in the solvent. Allthe films obtained have been mechanically and thermally tested. Best results for the mechanicaltests were obtained with 40% plasticizers with creation of a plastic behaviour and improvement ofthe flexibility. Thermally speaking, the thermal stability gained through acetylation of the samples islost by film casting, and use of plasticizers reduced the thermal stability as a new component wasadded to the composition.</p>

corrected abstract:
<p>Using and finding applications from biomass is and will continue to be an important subject for research, and biomass from trees, has shown several outstanding aspects other than just for the pulp and paper applications. It is now, more than ever, time to find efficient uses for all the wood components, in particular, the hemicelluloses. The hemicelluloses account for approximately one-third of a dry composition of lignocellulosic wood biomasses. Of these hemicelluloses, xylan is the most abundant in many plants, particularly in hardwood. As for the Swedish forestry, xylan from birch is considered as one of the most promising resources for the future.</p><p>This thesis investigates the impact of acetylation of xylan on some properties such as solubility, thermal stability and film formation. Films were prepared using the non- and acetylated xylan with addition of different plasticizers (glycerol, sorbitol and xylitol).<br>Alkali-soluble birch xylan (ASX), obtained by ethanol/toluene extraction and sodium chlorite delignification of the wood sawdust followed by potassium hydroxide extraction of the obtained holocellulose, and commercial xylan (CX) were acetylated to different degree of substitution with acetyl groups (DS<sub>Ac</sub>), using acetic anhydride in dimethyl sulfoxide (DMSO) and 1-methylimidazole (NMI). Films were prepared by suspending non-acetylated xylan in water (H<sub>2</sub>O) and adding different percentages of plasticizers (20 and 40%) or by suspending acetylated xylan in chloroform (CHCl<sub>3</sub>). Characterizations of the non- and acetylated polymer (AcASX and AcCX) and films were conducted in order to determine thermal and mechanical properties.</p><p>CX and ASX presented different reactivity leading to different behaviour during acetylation and so different DS<sub>Ac</sub>. The thermal stability has been improved for both ASX and CX following the increase of the DS<sub>Ac</sub>. Concerning film formation, ASX showed a great ability to form films through casting with or without plasticizers while it was impossible to obtain any films using only CX. For AcASX and AcCX the film formation using chloroform was depending on the DS<sub>Ac</sub> and the dispersability in the solvent. All the films obtained have been mechanically and thermally tested. Best results for the mechanical tests were obtained with 40% plasticizers with creation of a plastic behaviour and improvement of the flexibility. Thermally speaking, the thermal stability gained through acetylation of the samples is lost by film casting, and use of plasticizers reduced the thermal stability as a new component was added to the composition.</p>
----------------------------------------------------------------------
In diva2:1451603 abstract is: <p>AbstractThe organized crime and its need to launder money is growing all over theworld. A great responsibility to work with anti money laundering and counterterrorism financing lies on the financial institutions, and a big part of thatwork is transaction monitoring. The purpose of monitoring is to detect devianttransaction behavior that could indicate money laundering among customers.This thesis aimed to investigate what the Swedish law demand of a newlyfounded financial institution regarding the monitoring of transactions, andlater on develop an automated model to meet those demands. Sweden’s financialsupervisory authority demands a customer's transactions should bemonitored to find deviations from the expected behavior. Due to this demandthe model developed takes customers’ transactional history in account.The model makes use of a binary Bayesian network based on a number ofrules defined by well known money laundering transaction patterns.Through validation, against a manually evaluated transaction set, the developedmodel managed to find a number of potential cases of money laundering.On top of that around 90% of the planted known money launderingcases were found.KeywordsMoney laundering, terror financing, AML, CFT, transaction monitoring,Bayesian network</p>


corrected abstract:
<p>The organized crime and its need to launder money is growing all over the world. A great responsibility to work with anti money laundering and counter terrorism financing lies on the financial institutions, and a big part of that work is transaction monitoring. The purpose of monitoring is to detect deviant transaction behavior that could indicate money laundering among customers.</p><p>This thesis aimed to investigate what the Swedish law demand of a newly founded financial institution regarding the monitoring of transactions, and later on develop an automated model to meet those demands. Sweden’s financial supervisory authority demands a customer's transactions should be monitored to find deviations from the expected behavior. Due to this demand the model developed takes customers’ transactional history in account. The model makes use of a binary Bayesian network based on a number of rules defined by well known money laundering transaction patterns.</p><p>Through validation, against a manually evaluated transaction set, the developed model managed to find a number of potential cases of money laundering. On top of that around 90% of the planted known money laundering cases were found.</p>
----------------------------------------------------------------------
In diva2:1315697 abstract is: <p>Skin creams are one of Sky Resources key products, they are produced as an oil-in-water(O/W) emulsion. In order to form an emulsion the oil and water needs to be able to mixtogether, for that to happen the oil and water droplets have to be broken up into very smalldroplets (colloids).</p><p>There is a certain quality difference between the products from the research and developmentdepartment and the production department.</p><p>The skin creams have been made through a given recipe, which contains a number of chemicalformulas. Tests have been preformed and the results have been examined. The creams werefirst made in the research and development department’s laboratory and then that small scaleproduction was taken to big scale production in the production department. The results havebeen documented and the parameters that have been examined are speed, temperature and timeto see how they affect the viscosity of the creams.</p><p>A factorial experiment with three factors has been made. The factors are the time the skincream is homogenized, at what speed the cream is homogenized and at what temperature thephases are when the homogenizing is started. That gives a total of 8 creams from thelaboratory level.</p><p>The viscosity of the skin creams have been measured after 10 minutes, 24 hours, 48 hours and1 week to see how it is increasing with time and if it is increasing at all or perhaps decreasing.After 1 week the creams were also studied under a microscope to see how successful theemulsions were with different factors.</p><p>The fourth cream was the only cream from the laboratory that had a successful emulsion andstabilized viscosity. So the factors that are brought from laboratory to production is highhomogenization time, high homogenization speed and low temperature. Two differenthomogenizers were tried in production.</p><p>The factor that affects the viscosity the most seems to be the temperature. After these tests,there is still a difference between the products from the research and development departmentand the production department even with the lower temperatures on the oil and water phases.The second cream from production and cream 4 from the laboratory show the smallest qualitydifference. More tests need to be done in the production department with differenthomogenization speed and time with the lower temperature to establish the result.</p>

corrected abstract:
<p>Skin creams are one of Sky Resources key products, they are produced as an oil-in-water (O/W) emulsion. In order to form an emulsion the oil and water needs to be able to mix together, for that to happen the oil and water droplets have to be broken up into very small droplets (colloids).</p><p>There is a certain quality difference between the products from the research and development department and the production department.</p><p>The skin creams have been made through a given recipe, which contains a number of chemical formulas. Tests have been preformed and the results have been examined. The creams were first made in the research and development department’s laboratory and then that small scale production was taken to big scale production in the production department. The results have been documented and the parameters that have been examined are speed, temperature and time to see how they affect the viscosity of the creams.</p><p>A factorial experiment with three factors has been made. The factors are the time the skin cream is homogenized, at what speed the cream is homogenized and at what temperature the phases are when the homogenizing is started. That gives a total of 8 creams from the laboratory level.</p><p>The viscosity of the skin creams have been measured after 10 minutes, 24 hours, 48 hours and 1 week to see how it is increasing with time and if it is increasing at all or perhaps decreasing. After 1 week the creams were also studied under a microscope to see how successful the emulsions were with different factors.</p><p>The fourth cream was the only cream from the laboratory that had a successful emulsion and stabilized viscosity. So the factors that are brought from laboratory to production is high homogenization time, high homogenization speed and low temperature. Two different homogenizers were tried in production.</p><p>The factor that affects the viscosity the most seems to be the temperature. After these tests, there is still a difference between the products from the research and development department and the production department even with the lower temperatures on the oil and water phases. The second cream from production and cream 4 from the laboratory show the smallest quality difference. More tests need to be done in the production department with different homogenization speed and time with the lower temperature to establish the result.</p>
----------------------------------------------------------------------
In diva2:1229795 abstract is: <p>Water is an essential and indispensable component is the pulp- and paper production industry.The increase in energy costs, stricter environmental regulations and water resource shortageshave caused a reduction of the water footprint in the industry as well as an increase in waterrecycling and water circuit closure. Reducing water usage requires an understanding of wherecontaminants originate, as well as which streams are critical to the process and how they impactmill operation. The recirculation of water can cause contaminant accumulation; therefore millsemploy technologies for water treatment in the internal water cycles, the so-called ‘kidneys’.Application of membrane technology is one such option which can improve the recycled waterquality and reduce contaminant buildup.The present study was carried out on a lab-scale for the treatment of a tissue mill effluent usingmembrane separation. A combination of pretreatment methods and various membranes werecompared with regards to separation, flux and fouling. The AlfaLaval M20 device was to treatwastewater samples sent from the mill, where the permeate was recirculated to the feed tank.COD and TOC levels are compared with regards to determining the separation efficiency. Thepermeate flux was measured over the two-hour filtration period, as well as flux recovery todetermine fouling levels. Additionally, some economic aspects of the process are discussed.This study suggests the potential application of a combination of flocculation or centrifugationpretreatment, with reverse osmosis membranes for recycling water to replace freshwater intake.The results also indicate the possibility of using ultrafiltration as kidneys to decreasecontamination buildup for further water loop closure.</p>


corrected abstract:
<p>Water is an essential and indispensable component is the pulp- and paper production industry. The increase in energy costs, stricter environmental regulations and water resource shortages have caused a reduction of the water footprint in the industry as well as an increase in water recycling and water circuit closure. Reducing water usage requires an understanding of where contaminants originate, as well as which streams are critical to the process and how they impact mill operation. The recirculation of water can cause contaminant accumulation; therefore mills employ technologies for water treatment in the internal water cycles, the so-called ‘kidneys’. Application of membrane technology is one such option which can improve the recycled water quality and reduce contaminant buildup.</p><p>The present study was carried out on a lab-scale for the treatment of a tissue mill effluent using membrane separation. A combination of pretreatment methods and various membranes were compared with regards to separation, flux and fouling. The AlfaLaval M20 device was to treat wastewater samples sent from the mill, where the permeate was recirculated to the feed tank. COD and TOC levels are compared with regards to determining the separation efficiency. The permeate flux was measured over the two-hour filtration period, as well as flux recovery to determine fouling levels. Additionally, some economic aspects of the process are discussed.</p><p>This study suggests the potential application of a combination of flocculation or centrifugation pretreatment, with reverse osmosis membranes for recycling water to replace freshwater intake. The results also indicate the possibility of using ultrafiltration as kidneys to decrease contamination buildup for further water loop closure.</p>
----------------------------------------------------------------------
In diva2:745566 abstract is: <p>Tape samples of medium density polyethylene withthickness of 0.4 mm were supplied containing 0.1% and 0.3 % of four differentphenolic stabilizers. These samples exposed continuously and interrupted (6hour/day) to 10 ppm chlorine dioxide water solution with PH: 6.8 at 70˚C. The antioxidants life times were followed bydifferential scanning calorimetry through oxidation induction time. Sample with0.3% antioxidants exhibit an unstable and longer OIT results in the interruptedexposure system in comparison to o.3% samples in continues system. Solubilitydifficulties and precipitation of antioxidants with 0.3% concentration inpolyethylene matrix supposed to be reason for this non reliable data. Sampleswith 0.1% antioxidants approximately followed the same OIT profile as afunction of exposure time in both continues and interrupted system. Surface degradationsof tapes (with 0.1% stabilizer) were examined by IR spectroscopy and carbonylbond formation on sample after depletion time of stabilizers confirmed for allsamples. Carbonyl index rate corresponding to chemical degradation of samplesdepends on the antioxidant type even after depletion time (OIT: 0.0) ofantioxidants. Micrographs of exposed sample after necking were used tocalculation of chemical assisted crack growth as function of exposure time. Theonset for crack initiation times for matrix polymer and disintegration ofdegraded surface from fresh polymer occurs in longer time than depletion timeof antioxidants. It depends on antioxidants type in which the crack growth ratefor studied antioxidants including Irganox 1330 and Irganox 3114 were 5 µm and3.5 µm per 1000 min exposure time after crack initiation onset.      </p>


w='o.3' val={'c': '0.3', 's': 'diva2:745566', 'n': 'no full text'}

corrected abstract:
<p>Tape samples of medium density polyethylene with thickness of 0.4 mm were supplied containing 0.1% and 0.3 % of four different phenolic stabilizers. These samples exposed continuously and interrupted (6 hour/day) to 10 ppm chlorine dioxide water solution with PH: 6.8 at 70˚C. The antioxidants life times were followed by differential scanning calorimetry through oxidation induction time. Sample with 0.3% antioxidants exhibit an unstable and longer OIT results in the interrupted exposure system in comparison to 0.3% samples in continues system. Solubility difficulties and precipitation of antioxidants with 0.3% concentration in polyethylene matrix supposed to be reason for this non reliable data. Samples with 0.1% antioxidants approximately followed the same OIT profile as a function of exposure time in both continues and interrupted system. Surface degradations of tapes (with 0.1% stabilizer) were examined by IR spectroscopy and carbonyl bond formation on sample after depletion time of stabilizers confirmed for all samples. Carbonyl index rate corresponding to chemical degradation of samples depends on the antioxidant type even after depletion time (OIT: 0.0) of antioxidants. Micrographs of exposed sample after necking were used to calculation of chemical assisted crack growth as function of exposure time. The onset for crack initiation times for matrix polymer and disintegration of degraded surface from fresh polymer occurs in longer time than depletion time of antioxidants. It depends on antioxidants type in which the crack growth rate for studied antioxidants including Irganox 1330 and Irganox 3114 were 5 µm and 3.5 µm per 1000 min exposure time after crack initiation onset.</p>
----------------------------------------------------------------------
In diva2:1454832 abstract is: <p>A fundamental tool in anatomical pathology for disease diagnosis is preserving tissues in theform of formalin-fixed paraffin-embedded (FFPE) samples. A major advantage of this type ofsamples is its ability to maintain the morphology and structure of the cells, which is the basisof disease diagnosis and biomarker detection. This advantage has rendered FFPE specimensas the most popular approach for long-term preservation of tissues. However, since thecrosslinks introduced through the fixation of the tissues significantly affect the integrity of thenucleic acids within, their use is limited, especially in studies that involve gene expressionanalysis. Therefore, developing a workflow that enables determination of RNA quality in FFPEsamples will have a positive impact on both the research community and pathologydepartments. Results obtained from such quality control workflow can be used to guidedecision making regarding deeper levels of analysis.</p><p>Due to the high availability and usage of FFPE specimens, a number of studies has been doneto investigate and evaluate the integrity of their genomic content. Nevertheless, no studieshave been performed to provide estimations of FFPE nucleic acids integrity as a function oftheir spatial distribution. Being able to spatially determine RNA integrity in all sub-areas ofthe tissue is expected to facilitate the examination of FFPE specimens and ensure that allareas of the section have a good-enough quality to provide data from more expensive spatialtranscriptomics experiments. For this purpose, and building on the previous spatial RIN assaydeveloped by the spatial transcriptomics (ST) group at SciLifelab for fresh frozen tissues, wehave developed a quality control assay that is compatible with FFPE tissue specimens. Thedesigned assay combines being specialized in detection of short-length fragments;particularly in the size range of 50bp to 200bp, and providing the spatial localizations of thesefragments. This combination eventually enables an integrity estimation in the form of aspatial heat map.</p>

corrected abstract:
<p>A fundamental tool in anatomical pathology for disease diagnosis is preserving tissues in the form of formalin-fixed paraffin-embedded (FFPE) samples. A major advantage of this type of samples is its ability to maintain the morphology and structure of the cells, which is the basis of disease diagnosis and biomarker detection. This advantage has rendered FFPE specimens as the most popular approach for long-term preservation of tissues. However, since the crosslinks introduced through the fixation of the tissues significantly affect the integrity of the nucleic acids within, their use is limited, especially in studies that involve gene expression analysis. Therefore, developing a workflow that enables determination of RNA quality in FFPE samples will have a positive impact on both the research community and pathology departments. Results obtained from such quality control workflow can be used to guide decision making regarding deeper levels of analysis.</p><p>Due to the high availability and usage of FFPE specimens, a number of studies has been done to investigate and evaluate the integrity of their genomic content. Nevertheless, no studies have been performed to provide estimations of FFPE nucleic acids integrity as a function of their spatial distribution. Being able to spatially determine RNA integrity in all sub-areas of the tissue is expected to facilitate the examination of FFPE specimens and ensure that all areas of the section have a good-enough quality to provide data from more expensive spatial transcriptomics experiments. For this purpose, and building on the previous spatial RIN assay developed by the spatial transcriptomics (ST) group at SciLifelab for fresh frozen tissues, we have developed a quality control assay that is compatible with FFPE tissue specimens. The designed assay combines being specialized in detection of short-length fragments; particularly in the size range of 50bp to 200bp, and providing the spatial localizations of these fragments. This combination eventually enables an integrity estimation in the form of a spatial heat map.</p>
----------------------------------------------------------------------
In diva2:1796606 abstract is: <p>The analysis of human movement is important for diagnosis of as wellas planning and evaluating treatments of disorders or injuries affectingmovement. Optical motion capture combined with force plates provideaccurate measurements, but are confined to laboratory settings limiting theirpotential usefulness in clinical applications. Efforts are made to movemeasurements out of the laboratory making them more accessible, cheaperand easier to use for healthcare providers. This work aimed to assess thefeasibility of doing motion analysis with a wearable system consisting ofIMUs and pressure insole sensors, while also developing a methodology thatcould be used for subsequent validation. Six subjects performed walking, sideskipping, squats, chair stands and a balance exercise, while data was collectedsimultaneously from the wearable system and optical motion capture withforce plates. For demonstration, data from one example subject was analysedand included in this work. The wearable system showed promising results formeasuring ground reaction force. Center of pressure errors were relativelyhigh, likely influenced by the choice of method for coordinate transformationbetween the systems. Joint angle errors varied from low to very high fordifferent trials. Ankle dorsiflexion angle showed low errors and pelvis tiltangle high errors for all motion types. There is a need to investigate thecause for these high errors before more measurements are conducted. Themethodology presented in this work can, with a few recommended changes,be used for future validation of the wearable motion analysis system.</p>


corrected abstract:
<p>The analysis of human movement is important for diagnosis of as well as planning and evaluating treatments of disorders or injuries affecting movement. Optical motion capture combined with force plates provide accurate measurements, but are confined to laboratory settings limiting their potential usefulness in clinical applications. Efforts are made to move measurements out of the laboratory making them more accessible, cheaper and easier to use for healthcare providers. This work aimed to assess the feasibility of doing motion analysis with a wearable system consisting of IMUs and pressure insole sensors, while also developing a methodology that could be used for subsequent validation. Six subjects performed walking, side skipping, squats, chair stands and a balance exercise, while data was collected simultaneously from the wearable system and optical motion capture with force plates. For demonstration, data from one example subject was analysed and included in this work. The wearable system showed promising results for measuring ground reaction force. Center of pressure errors were relatively high, likely influenced by the choice of method for coordinate transformation between the systems. Joint angle errors varied from low to very high for different trials. Ankle dorsiflexion angle showed low errors and pelvis tilt angle high errors for all motion types. There is a need to investigate the cause for these high errors before more measurements are conducted. The methodology presented in this work can, with a few recommended changes, be used for future validation of the wearable motion analysis system.</p>
----------------------------------------------------------------------
In diva2:1237160 abstract is: <p>The induced rupturing of Poly Vinyl Alcohol (PVA) microbubbles with high mechanical index (MI)ultrasound beam is used in multiple medical application such as drug delivery, image contrastenhancement and perfusion imaging.In this work, Triggered imaging technique with subtraction algorithm is used to enhance themicrobubble’s (MB) contrast over tissue (CTR). The technique is performed by rupturing MBwith one destruction wave sequence followed by 100 B-mode imaging pulse sequences. Theimages obtained are then subtracted by a base image that is selected after the destruction pulse[1].The result of this technique depends mainly on the effectiveness of destruction pulse inrupturing highest number of MB. This has been tested through tissue mimicking phantomwithout replenishing the MB. The evaluation of the methods is done through the CTR and CNRcalculation for each of the 100 frames.The contrast enhancement technique used has also been tested with similar setup but withcontinuous replenishment of MB. The evaluation is done by comparing CNR and CTR results forthe 100 frames obtained by B-mode imaging with the ones resulted from the subtractionalgorithm.The contrast values obtained from both experiments are used in driving the characterization ofPVA response to high MI.The result for the destruction pulse effectiveness shows that the pulse indeed managed toreduce number of MB, but not to the lowest. This is because of leaked gas from cracked shell,the shell acoustic enhancement effect, and large bubbles which managed to survive.The Triggered imaging has shown large improvement in CTR value with use of the subtractionalgorithm when compared to B-mode results. In addition, it has provided an experimental wayfor perfusion imaging and quantification by monitoring CTR value after the destructive pulse[2]. This sets the bases for experimental research relevant to tissue perfusion at ultrasound labof KTH.</p>

corrected abstract:
<p>The induced rupturing of Poly Vinyl Alcohol (PVA) microbubbles with high mechanical index (MI) ultrasound beam is used in multiple medical application such as drug delivery, image contrast enhancement and perfusion imaging.</p><p>In this work, Triggered imaging technique with subtraction algorithm is used to enhance the microbubble’s (MB) contrast over tissue (CTR). The technique is performed by rupturing MB with one destruction wave sequence followed by 100 B-mode imaging pulse sequences. The images obtained are then subtracted by a base image that is selected after the destruction pulse [1].</p><p>The result of this technique depends mainly on the effectiveness of destruction pulse in rupturing highest number of MB. This has been tested through tissue mimicking phantom without replenishing the MB. The evaluation of the methods is done through the CTR and CNR calculation for each of the 100 frames.</p><p>The contrast enhancement technique used has also been tested with similar setup but with continuous replenishment of MB. The evaluation is done by comparing CNR and CTR results for the 100 frames obtained by B-mode imaging with the ones resulted from the subtraction algorithm.</p><p>The contrast values obtained from both experiments are used in driving the characterization of PVA response to high MI.</p><p>The result for the destruction pulse effectiveness shows that the pulse indeed managed to reduce number of MB, but not to the lowest. This is because of leaked gas from cracked shell, the shell acoustic enhancement effect, and large bubbles which managed to survive.</p><p>The Triggered imaging has shown large improvement in CTR value with use of the subtraction algorithm when compared to B-mode results. In addition, it has provided an experimental way for perfusion imaging and quantification by monitoring CTR value after the destructive pulse [2]. This sets the bases for experimental research relevant to tissue perfusion at ultrasound lab of KTH.</p>
----------------------------------------------------------------------
In diva2:1879784 abstract is: <p>With the growing demand for an increase in food production, whilst 30% of thecrop production is lost due to plant diseases, there is a need for plant diseasedetection that do not require laboratories and trained personnel. This can besolved by the usage of nanotechnology and the integration of microneedles.Microneedles can extract biofluids from the plants, and with the integrationwith a sensor, allow for on-site disease detection.The most recently developed type of microneedle is the porous microneedle,which extract biofluids by capillary action. Previous papers have proposeddifferent materials and processes for the fabrication of porous microneedlesbut state that further research has to be done. This Thesis aims to createprotocols for fabrication of porous microneedles from different cellulosicmaterials as well as evaluate their characteristics.Before the protocols were established, two master molds were created forthe casting of the porous microneedles. One by engraving a PMMA sheetusing a CO2 laser, and the other by 3D printing a resin mold. The twomaterials that were used were cellulose acetate and chromatography paper.The materials were used both separately and combined in the fabricationprocess. To characterize the porous microneedles, three properties wereevaluated, porosity, mechanical stability and extraction capability.The results from the fabrication of porous microneedles and the propertytests has provided information for further development of the project as wellas future work regarding porous microneedles.</p>

corrected abstract:
<p>With the growing demand for an increase in food production, whilst 30% of the crop production is lost due to plant diseases, there is a need for plant disease detection that do not require laboratories and trained personnel. This can be solved by the usage of nanotechnology and the integration of microneedles. Microneedles can extract biofluids from the plants, and with the integration with a sensor, allow for on-site disease detection.</p><p>The most recently developed type of microneedle is the porous microneedle, which extract biofluids by capillary action. Previous papers have proposed different materials and processes for the fabrication of porous microneedles but state that further research has to be done. This Thesis aims to create protocols for fabrication of porous microneedles from different cellulosic materials as well as evaluate their characteristics.</p><p>Before the protocols were established, two master molds were created for the casting of the porous microneedles. One by engraving a PMMA sheet using a CO<sub>2</sub> laser, and the other by 3D printing a resin mold. The two materials that were used were cellulose acetate and chromatography paper. The materials were used both separately and combined in the fabrication process. To characterize the porous microneedles, three properties were evaluated, porosity, mechanical stability and extraction capability.</p><p>The results from the fabrication of porous microneedles and the property tests has provided information for further development of the project as well as future work regarding porous microneedles.</p>
----------------------------------------------------------------------
In diva2:1269990 abstract is: <p>The pelvic floor muscle is a series of muscle plates in the bottom of the abdominal cavity that supportthe internal organs. These organs are slightly different in men and women. A group of muscles formthe pelvic floor muscles and can be seen as a hammock. The pelvic floor muscles have many importantfunctions, such as control of the intestines and urinary bladder, stabilization and support of the spineby keeping the internal organs in place, but also, support during pregnancy.If the pelvic floor tissues are weakened and exposed to stress, one or more organs may descend intothe pelvic floor. This condition is called Prolapse and three types are presented: Anal prolapse, Rectalprolapse, and uterine prolapse. These conditions are examined with a chair, that called strain chair(kryststol). A strain chair consists basically of a hygiene chair with a mirror mounted under the seat.Today a strain chair is used at the surgery clinic at Södersjukhuset. This chair has someflaws andneeds some improvement. The chair is old and the ergonomics are not the best, which causesunpleasant and uncomfortable conditions for both healthcare professionals and patients.This report describes the development of a new strain chair (kryststol) for use in the surgery clinic atSödersjukhuset.The result of this work has shown that a new strain chair (kryststol) should be height adjustable.This solution result in a reduction of the poor working positions for the healthcare professionals thatare forced to cope with at present. An adjustable height function also means that patients can easilyget on and off the chair. Mounting a lamp on/under the seat is a solution to facilitate examination anddiagnosis. Even, mounting a mirror under the seat is also essential for examination. Regarding theabove information, some solutions have been developed. This report investigated the solutionscarefully. To develop these</p>


corrected abstract:
<p>The pelvic floor muscle is a series of muscle plates in the bottom of the abdominal cavity that support the internal organs. These organs are slightly different in men and women. A group of muscles form the pelvic floor muscles and can be seen as a hammock. The pelvic floor muscles have many important functions, such as control of the intestines and urinary bladder, stabilization and support of the spine by keeping the internal organs in place, but also, support during pregnancy. If the pelvic floor tissues are weakened and exposed to stress, one or more organs may descend into the pelvic floor. This condition is called Prolapse and three types are presented: Anal prolapse, Rectal prolapse, and uterine prolapse. These conditions are examined with a chair, that called strain chair (kryststol). A strain chair consists basically of a hygiene chair with a mirror mounted under the seat.</p><p>Today a strain chair is used at the surgery clinic at Södersjukhuset. This chair has some flaws and needs some improvement. The chair is old and the ergonomics are not the best, which causes unpleasant and uncomfortable conditions for both healthcare professionals and patients. This report describes the development of a new strain chair (kryststol) for use in the surgery clinic at Södersjukhuset.</p><p>The result of this work has shown that a new strain chair (kryststol) should be height adjustable. This solution result in a reduction of the poor working positions for the healthcare professionals that are forced to cope with at present. An adjustable height function also means that patients can easily get on and off the chair. Mounting a lamp on/under the seat is a solution to facilitate examination and diagnosis. Even, mounting a mirror under the seat is also essential for examination. Regarding the above information, some solutions have been developed. This report investigated the solutions carefully. To develop these solutions, literature surveys and interviews with healthcare professionals have been very helpful.</p>
----------------------------------------------------------------------
title: "Configuration and device identification on networkgateways"
==>    "Configuration and device identification on network gateways"

In diva2:661292 abstract is: <p>To set up port forwarding rules on network gateways, certain technical skills are requiredfrom end-users. These assumptions in the gateway software stack, can lead to an increasein support calls to network operators and resellers of customer premises equipment. Theuser interface itself is also an important part of the product and a complicated interfacewill contribute to a lessened user experience. Other issues with an overwhelming userinterface include the risk of faulty configuration by the user, potentially leaving the networkvulnerable to attacks.We present an enhancement of the current port forwarding configuration in the gatewaysoftware, with an extensible library of presets along with usability improvements. To helpusers with detecting available services, a wrapper for a network scanner is implemented, fordetecting devices and services on the local network. These parts combined relieves end-usersof looking up forwarding rules for ports and protocols to configure their gateway, basingtheir decisions on data collected by the network scanner or by using an applications nameinstead of looking up its ports. Another usability improvement is an internal DNS service,which enables access to the gateway interface through a human-memorable domain name,instead of using the LAN IP address.Using the Nmap utility for identifying services on the network, could be consideredharmful activity by network admins and intrusion detection systems. The preset libraryis extensible and generic enough to be included in the default software suite shipping withthe network equipment. Working within the unified configuration system of OpenWrt, thepreset design will add value and allow resellers to easily customize it to their services. Thisproposal could reduce support costs for the service operators and improve user experiencein configuring network gateways.</p>

corrected abstract:
<p>To set up port forwarding rules on network gateways, certain technical skills are required from end-users. These assumptions in the gateway software stack, can lead to an increase in support calls to network operators and resellers of customer premises equipment. The user interface itself is also an important part of the product and a complicated interface will contribute to a lessened user experience. Other issues with an overwhelming user interface include the risk of faulty configuration by the user, potentially leaving the network vulnerable to attacks.</p><p>We present an enhancement of the current port forwarding configuration in the gateway software, with an extensible library of presets along with usability improvements. To help users with detecting available services, a wrapper for a network scanner is implemented, for detecting devices and services on the local network. These parts combined relieves end-users of looking up forwarding rules for ports and protocols to configure their gateway, basing their decisions on data collected by the network scanner or by using an applications name instead of looking up its ports. Another usability improvement is an internal DNS service, which enables access to the gateway interface through a human-memorable domain name, instead of using the LAN IP address.</p><p>Using the Nmap utility for identifying services on the network, could be considered harmful activity by network admins and intrusion detection systems. The preset library is extensible and generic enough to be included in the default software suite shipping with the network equipment. Working within the unified configuration system of OpenWrt, the preset design will add value and allow resellers to easily customize it to their services. This proposal could reduce support costs for the service operators and improve user experience in configuring network gateways.</p>
----------------------------------------------------------------------
In diva2:1272696 abstract is: <p>The solar cell industry is one of the fastest growing industries in the world. This is due to thedeclining prices of solar cells and that many countries now try to reduce their greenhouse gasemissions. The growing industry leads to an increased range of variants and suppliers of solarcells on the market. The environmental problems of solar cells occur during the production ofthe various components, as well as in the recovery of the used solar cells.</p><p>The studied solar cells are mono- and multi-crystalline silicon cells, cadmium telluride (CdTe)and CIGS/CIS. The mono- and multi-crystalline solar cells are produced from purified siliconwhich achieves a purity of 6N (SG-Si), where silicon is doped with phosphorus to produce nsemiconductorand the p-semiconductor is doped with boron.</p><p>For thin film solar cell CdTe, CdS is used as the n-semiconductor and the p-semiconductorconsists of cadmium and tellurium. For thin film solar cell CIGS/CIS, copper, indium, galliumand selenium are used as p-semiconductors and CdS as n-semiconductors.</p><p>For the monocrystalline solar cells, a recovery rate of 96% can be achieved, which is doneeconomically and environmentally. For CdTe, a 95% recycled material is obtained for glass,90% for CdTe and 90% for CdS. While for CIGS/CIS, glass, EVA, selenium, aluminium,indium and gallium materials can be recycled.</p><p>The chemicals used during the processes have been classified within a risk categorization,where the majority of the chemicals used are classified as high- and very high risk. The greaterimpact on the environment at the production location is due to the energy supply used sinceonly transport accounts for 1,6 to 2,8 % of carbondioxide emissions from solar cells.</p><p>The parameters that were considered to have a major impact on the environment are the loadfrom critical material extraction, the power supply used during production, the hazardouschemicals used during production and recycling, and the air and waterborne emissions thatarise during production and recycling.</p><p>For all solar cells, non-virgin aluminium should be used as construction material for the frame,or it should be without frames. Manufacturers of solar cells should clean the waterconsumption that occurs and recycle water to the their utmost ability. The factories should alsouse a recycling center for their trash and residues, or recycle at the factory. A clear follow-upand residual product plan should exist for the produced solar cells, which can be done throughPV CYCLE. Companies should work actively in matters relating to health, safety, humanrights, labour law and comply with the rules prevailing in the current country.</p>

w='nsemiconductor' val={'c': 'n-semiconductor', 's': 'diva2:1272696', 'n': 'correct in original'}

corrected abstract:
<p>The solar cell industry is one of the fastest growing industries in the world. This is due to the declining prices of solar cells and that many countries now try to reduce their greenhouse gas emissions. The growing industry leads to an increased range of variants and suppliers of solar cells on the market. The environmental problems of solar cells occur during the production of the various components, as well as in the recovery of the used solar cells.</p><p>The studied solar cells are mono- and multi-crystalline silicon cells, cadmium telluride (CdTe) and CIGS/CIS. The mono- and multi-crystalline solar cells are produced from purified silicon which achieves a purity of 6N (SG-Si), where silicon is doped with phosphorus to produce n-semiconductor and the p-semiconductor is doped with boron.</p><p>For thin film solar cell CdTe, CdS is used as the n-semiconductor and the p-semiconductor consists of cadmium and tellurium. For thin film solar cell CIGS/CIS, copper, indium, gallium and selenium are used as p-semiconductors and CdS as n-semiconductors.</p><p>For the monocrystalline solar cells, a recovery rate of 96% can be achieved, which is done economically and environmentally. For CdTe, a 95% recycled material is obtained for glass, 90% for CdTe and 90% for CdS. While for CIGS/CIS, glass, EVA, selenium, aluminium, indium and gallium materials can be recycled.</p><p>The chemicals used during the processes have been classified within a risk categorization, where the majority of the chemicals used are classified as high- and very high risk. The greater impact on the environment at the production location is due to the energy supply used since only transport accounts for 1,6 to 2,8 % of carbondioxide emissions from solar cells.</p><p>The parameters that were considered to have a major impact on the environment are the load from critical material extraction, the power supply used during production, the hazardous chemicals used during production and recycling, and the air and waterborne emissions that arise during production and recycling.</p><p>For all solar cells, non-virgin aluminium should be used as construction material for the frame, or it should be without frames. Manufacturers of solar cells should clean the water consumption that occurs and recycle water to the their utmost ability. The factories should also use a recycling center for their trash and residues, or recycle at the factory. A clear follow-up and residual product plan should exist for the produced solar cells, which can be done through PV CYCLE. Companies should work actively in matters relating to health, safety, human rights, labour law and comply with the rules prevailing in the current country.</p>
----------------------------------------------------------------------
In diva2:1762551 abstract is: <p>Matched frequency responses are a fundamental starting point for a variety ofimplementations for microphone arrays. In this report, two methods for frequencyresponse-calibration of a pre-assembled microphone array are presented andevaluated. This is done by extracting the deviation in frequency responses of themicrophones in relation to a selected reference microphone, using a swept sine asa stimulus signal and an inverse filter. The swept sine includes all frequencieswithin the bandwidth of human speech. This allows for a full frequency responsemeasurements from all microphones using a single recording.Using the swept sine, the deviation in frequency response between the microphonescan be obtained. This deviation represents the scaling factor that all microphonesmust be calibrated with to match the reference microphone. Applying the scalingfactors on the recorded stimulus signal shows an improvement for both implementedmethods, and where one method matches the frequency response of the microphoneswith high accuracy.Once the scaling factors of the various microphones is obtained, it can be usedto calibrate other recorded signals. This leads to an minor improvement formatching the frequency responses, as it has been shown that the differencesin frequency response between the microphones is signal-dependent and variesbetween recordings. The response differences between the microphones dependson the design of the array, speaker, room and the acoustic frequency dispersionthat occurs with sound waves. This makes it difficult to calibrate the frequencyresponses of the microphones without appropriate equipment because the responseof the microphones is noticeably affected by these other factors. Proposals to addressthese problems are discussed in the report as future work.</p>

corrected abstract:
<p>Matched frequency responses are a fundamental starting point for a variety of implementations for microphone arrays. In this report, two methods for frequency response-calibration of a pre-assembled microphone array are presented and evaluated. This is done by extracting the deviation in frequency responses of the microphones in relation to a selected reference microphone, using a swept sine as a stimulus signal and an inverse filter. The swept sine includes all frequencies within the bandwidth of human speech. This allows for a full frequency response measurements from all microphones using a single recording.</p><p>Using the swept sine, the deviation in frequency response between the microphones can be obtained. This deviation represents the scaling factor that all microphones must be calibrated with to match the reference microphone. Applying the scaling factors on the recorded stimulus signal shows an improvement for both implemented methods, and where one method matches the frequency response of the microphones with high accuracy.</p><p>Once the scaling factors of the various microphones is obtained, it can be used to calibrate other recorded signals. This leads to an minor improvement for matching the frequency responses, as it has been shown that the differences in frequency response between the microphones is signal-dependent and varies between recordings. The response differences between the microphones depends on the design of the array, speaker, room and the acoustic frequency dispersion that occurs with sound waves. This makes it difficult to calibrate the frequency responses of the microphones without appropriate equipment because the response of the microphones is noticeably affected by these other factors. Proposals to address these problems are discussed in the report as future work.</p>
----------------------------------------------------------------------
In diva2:1446988 abstract is: <p>Lignin is one of the most common biopolymers in the world. Together with cellulose andhemicellulose it constitutes the fibers in the wood. It has a high molecular weight due to its complexstructure consisting of crossed-linked phenolic monomers and is concatenated with different types ofcarbon and ether bonds.In pulping processes, lignin is extracted in large quantities and used on site to produce energy for milloperations but is also removed as a waste product. This enables a product with high resources andaccessibility due to lignin's diverse properties. Therefore, lignin has the potential to be utilized inhigher value applications such as polymer materials, as well as a source of platform chemicals. Atpresent, the value applications of lignin are promising as additives for different kinds of productssuch as emulsifiers and especially as biofuel due to lignin's high carbon content.New technologies for development for utilization lignin are emerging for different kinds ofapplications due to lignin’s biocompatibility. The possibilities of lignin combined with existingresearch of nanotechnology gives opportunities to improve biomedical applications. By designinglignin derived nanoparticles with incorporated magnetic materials, the NPs obtainsuperparamagnetic properties which can be utilized for target drug delivery. This could be promisingagainst intractable cancer such as pancreatic cancer.This report presents a protocol for developing magnetic lignin nanoparticles from the lowestmolecular weight kraft lignin fractions of eucalyptus (hardwood) and spruce (softwood). By a methodof self-assembly, particles with a doughnut and core-shell morphology, as indicated by SEM and TEM,were yielded with a 10-50μL content of water-stabilized magnetite. The particle size distribution andzeta potential were determined by DLS and the possibility of the particles being suitable forbiomedical applications was discussed.</p>


Note that the corrected abstract is based on the actual abstract in the thesis and not the abstract above.
corrected abstract:
<p>Lignin is one of the most common biopolymers in the world, together with cellulose and hemicellulose it constitutes the fibers in the wood. It has a high molecular weight due to its complex structure consisting of crossed-linked phenolic monomers and is concatenated with different types of carbon and ether bonds.</p><p>In pulping processes, lignin is extracted in large quantities and used on site to produce energy for mill operations, and is considered a waste product. Lignin has the potential to be utilized in higher value applications such as polymer materials, as well as a source of platform chemicals. At present, the value applications of lignin are promising as additives for different kinds of products such as emulsifiers and especially as biofuel due to lignin's high carbon content.</p><p>New technologies utilizing lignin are emerging in different kinds of applications, mainly due to lignin’s biocompatibility. The possibilities of lignin combined with existing research of nanotechnology gives opportunities to improve biomedical applications. By designing lignin derived nanoparticles with incorporated magnetic materials, the NPs obtain superparamagnetic properties which can be utilized for target drug delivery. This could be promising against intractable cancer such as pancreatic cancer.</p><p>The purpose of this project was to develop a method for creating magnetic lignin nanoparticles. This was achieved using the lowest molecular weight kraft lignin, after four-step solvent fractionation of both eucalyptus (hardwood) and spruce (softwood). By a method of self-assembly, particles with a doughnut and core-shell morphology, as indicated by SEM and TEM, were yielded with a 10-50µL content of water-stabilized magnetite. The particle size distribution and zeta potential were determined by DLS and the possibility of the particles being suitable for biomedical applications was discussed.</p>
----------------------------------------------------------------------
In diva2:1250709 abstract is: <p>Workers within production and assembly lines are often exposed to ergonomically unfavorable tasksand conditions. Reaction forces and reaction torques generated by industrial power tools may causenot only discomfort but also health issues and injury. The forceful tasks in combination with highlyrepetitive hand-arm motions and prolonged tool use paves the way for loss in workforce capacitywhich in turn can lead to great losses in productivity and product quality. An umbrella term for themany injuries and diseases that may arise from the use of such tools is Cumulative Trauma Disorders(CTD).This study aimed to investigate the ergonomic effect of power tool use for various tool and taskrelated conditions. The study required the setup of a test rig with a simulated handle of the tool. Theergonomic impact was assessed by measuring the torques associated with different tighteningstrategies, as well as measuring the angular displacement of the tool handle. By varying the jointstiffness and workplace orientation, the complexity of the task was varied and thus quantified.Measurements of muscle activity during each tightening procedure provided a quantification of thephysiological impact on the operator. By combining the measurements on the operator withsubjective assessment of perceived exertion and discomfort, a more holistic perspective on thetightening procedure was obtained.The results obtained from the study stressed the negative impact on the operator which the QuickStep tightening strategy on medium hard joints implies, regardless of workspace orientation. TheTurbo Tight and Tensor Pulse tightening strategies turned out to generate the lowest reactiontorques and handle deflections, regardless of joint stiffness and workspace orientation. The findingsfrom the muscle activity measurements in combination with the subjective evaluation methodsfurther confirmed the mildness of the Turbo Tight and Tensor Pulse strategies. Moreover, horizontalworkspace resulted in lower tool handle deflection compared to vertical workspace for all tighteningstrategies and joint stiffnesses.</p>


corrected abstract:
<p>Workers within production and assembly lines are often exposed to ergonomically unfavorable tasks and conditions. Reaction forces and reaction torques generated by industrial power tools may cause not only discomfort but also health issues and injury. The forceful tasks in combination with highly repetitive hand-arm motions and prolonged tool use paves the way for loss in workforce capacity which in turn can lead to great losses in productivity and product quality. An umbrella term for the many injuries and diseases that may arise from the use of such tools is Cumulative Trauma Disorders (CTD).</p><p>This study aimed to investigate the ergonomic effect of power tool use for various tool and task related conditions. The study required the setup of a test rig with a simulated handle of the tool. The ergonomic impact was assessed by measuring the torques associated with different tightening strategies, as well as measuring the angular displacement of the tool handle. By varying the joint stiffness and workplace orientation, the complexity of the task was varied and thus quantified. Measurements of muscle activity during each tightening procedure provided a quantification of the physiological impact on the operator. By combining the measurements on the operator with subjective assessment of perceived exertion and discomfort, a more holistic perspective on the tightening procedure was obtained.</p><p>The results obtained from the study stressed the negative impact on the operator which the Quick Step tightening strategy on medium hard joints implies, regardless of workspace orientation. The Turbo Tight and Tensor Pulse tightening strategies turned out to generate the lowest reaction torques and handle deflections, regardless of joint stiffness and workspace orientation. The findings from the muscle activity measurements in combination with the subjective evaluation methods further confirmed the mildness of the Turbo Tight and Tensor Pulse strategies. Moreover, horizontal workspace resulted in lower tool handle deflection compared to vertical workspace for all tightening strategies and joint stiffnesses.</p>
----------------------------------------------------------------------
 diva2:1865304 abstract is: <p>This work examines the temperature dynamics and final temperature of conductorsin electrical installation cables, for various cases of load current. A laboratory modeland a time-stepping numerical model have been made and compared. Results arecompared with the ampacity given in installation standards, and also with theoverload levels that could be sustained by fuses and circuit breakers, followingEuropean standards.Examples of use-cases for the study’s results are cable ratings with intermittentloads, and potential overload due to heavy load combined with extra infeeds intoa circuit (e.g. plug-in solar power). The studied cables were filled and non filled 3-conductor (of which 2 loaded) PEX insulated installation cables, all with 1.5 mm2copper conductors (EQLQ 3G 1.5mm2). They were tested in open air and in a sectionof insulated cavity wall. Currents were applied from a controlled dc current source.The conductor temperature was measured by logging the voltage drop across a 10 cmlength of conductor, bearing in mind the temperature coefficient of resistance. Thismethod was verified by tests in a heating chamber.Results showed that conductors reached a maximum of 92 % of the permissibleoperating temperature (90 ◦C) at 150 % load of typical fuse-size. Combined withextra infeeds the maximum temperature reached 118 % of operating temperatureat 170 % load of typical fuse-size. The results also showed a temperature differencebetween horizontal and vertical orientations, varying from 3-10 ◦C higher in thevertical scenario, depending on load and cable</p>


I have used the unicode degree-C ℃ symbol in the corrected abstract - rather than the white bullet and letter "C".
corrected abstract:
<p>This work examines the temperature dynamics and final temperature of conductors in electrical installation cables, for various cases of load current. A laboratory model and a time-stepping numerical model have been made and compared. Results are compared with the ampacity given in installation standards, and also with the overload levels that could be sustained by fuses and circuit breakers, following European standards.</p><p>Examples of use-cases for the study’s results are cable ratings with intermittent loads, and potential overload due to heavy load combined with extra infeeds into a circuit (e.g. plug-in solar power). The studied cables were filled and non filled 3-conductor (of which 2 loaded) PEX insulated installation cables, all with 1.5 mm<sup>2</sup> copper conductors (EQLQ 3G 1.5mm<sup>2</sup>). They were tested in open air and in a section of insulated cavity wall. Currents were applied from a controlled dc current source. The conductor temperature was measured by logging the voltage drop across a 10 cm length of conductor, bearing in mind the temperature coefficient of resistance. This method was verified by tests in a heating chamber.</p><p>Results showed that conductors reached a maximum of 92 % of the permissible operating temperature (90 ℃) at 150 % load of typical fuse-size. Combined with extra infeeds the maximum temperature reached 118 % of operating temperature at 170 % load of typical fuse-size. The results also showed a temperature difference between horizontal and vertical orientations, varying from 3-10 ℃ higher in the vertical scenario, depending on load and cable</p>
----------------------------------------------------------------------
In diva2:1642787 abstract is: <p><strong>Background</strong>: Duchenne muscular dystrophy is an X-linked chromosomal inheritedrecessive severe muscular dystrophy disorder, caused by a mutation in thedystrophin gene. Affected individuals undergo a progressive disease, where theyexperience a loss of muscle mass and consequently the loss of muscle function, overthe years, sorrowfully, with fatal ending.</p><p>The screening of newborns using dried blood spots (DBS) could aid early diagnoseand initiation of treatment. DBS are a minimal invasive method of collecting bloodby disposing a small volume of blood, a droplet, on specially prepared filter paperfor subsequent analysis and storage<sup>[1]</sup>.</p><p><strong>Objectives</strong>: The aim of this study is to optimize elution of proteins from DSS andexplore detection of biomarkers for DMD</p><p><strong>Methods</strong>: A total of 15 serum samples, collected at clinical sites were analyzedthrough DSS elution by BCA, SDS-PAGE electrophoresis and suspension bead arrayplatform to assess CA3 as a biomarker targeted for DMD detection.Results: The presented results show that there is a relation between theconcentration of Tween 20 detergent and the protein extract, as the higher theconcentration the higher the protein extracted was. As well as reported an evidentrift effect between 1 and 5% Tween 20. Although elution of proteins is overall highcertain proteins are prone to elute at high detergent concentration whereas othersnot. Results also showed that the bibliographic time of elution could be optimizedand reduced up to 4 hours of elution, in two 2-hour cycles.</p><p><strong>Conclusions</strong>: The developed elution method in combination with the bead-basedimmunoassay constitute a feasible method for rapid assessment of biomarkersabundance such as CA3. The optimization of the elution joint method is feasible andcustomizable for the selected biomarkers. </p>


Added <strong> for the Result section for consistency.
corrected abstract:
<p><strong>Background</strong>: Duchenne muscular dystrophy is an X-linked chromosomal inherited recessive severe muscular dystrophy disorder, caused by a mutation in the dystrophin gene. Affected individuals undergo a progressive disease, where they experience a loss of muscle mass and consequently the loss of muscle function, over the years, sorrowfully, with fatal ending.</p><p>The screening of newborns using dried blood spots (DBS) could aid early diagnose and initiation of treatment. DBS are a minimal invasive method of collecting blood by disposing a small volume of blood, a droplet, on specially prepared filter paper for subsequent analysis and storage<sup>[1]</sup>.</p><p><strong>Objectives</strong>: The aim of this study is to optimize elution of proteins from DSS and explore detection of biomarkers for DMD</p><p><strong>Methods</strong>: A total of 15 serum samples, collected at clinical sites were analyzed through DSS elution by BCA, SDS-PAGE electrophoresis and suspension be ad array platform to assess CA3 as a biomarker targeted for DMD detection.</p><p><strong>Results</strong>: The presented results show that there is a relation between the concentration of Tween 20 detergent and the protein extract, as the higher the concentration the higher the protein extracted was. As well as reported an evident rift effect between 1 and 5% Tween 20. Although elution of proteins is overall high certain proteins are prone to elute at high detergent concentration whereas others not. Results also showed that the bibliographic time of elution could be optimized and reduced up to 4 hours of elution, in two 2-hour cycles.</p><p><strong>Conclusions</strong>: The developed elution method in combination with the be ad-based immunoassay constitute a feasible method for rapid assessment of biomarkers abundance such as CA3. The optimization of the elution joint method is feasible and customizable for the selected biomarkers. </p>
----------------------------------------------------------------------
In diva2:1454456 abstract is: <p>In 2014 SÖRAB constructed a continuous biological treatment system (KBR) to handle leachate waterfrom the landfill at the facility in Löt, north of Stockholm. The KBR is mainly focused on removal ofammonium nitrogen which would otherwise be released in to the recipient and contribute toeutrophication and damage to the environment. This project has focused on replacing the currentcarbon source in the process Brenntaplus VP1 and evaluating the efficiency of denitrification andeconomy of transitioning to a new carbon source. The carbon sources glycerol and ethanol wereevaluated and compared to Brenntaplus VP1 for the denitrification efficiency and microbial profile.The experiments were performed in laboratory conditions and in pilot scale using leachate water fromLöt. The reduction of ammonia was evaluated by chemical precipitation, addition of carbon sources bymeasuring ammonia-N and nitrate-N under aerobic (nitrification) and anaerobic (denitrification)conditions. The combination of ethanol and glycerol showed an enhanced denitrification and increasedmicrobial community both in lab and pilot scale studies with reduced hydraulic retention time. Therate of nitrate reduction was 0.23 mgNO3-N 1 -1 h -1 for ethanol/glycerol compared to 0.12-0.17mgNO 3- -N 1 -1 h -1 for Brenntaplus VP1 in pilot scale. The results indicate that using ethanol, glycerolor a mix of the two as a substitute for Brenntaplus VP1 is viable. This has been based on laboratoryand pilot scale studies. Each of the carbon sources examined during this project have showed a uniqueimpact on the process and its parameters such as: denitrification rate, microbial density and microbialcomposition. The carbon sources had an impact with temperature fluctuation and faster denitrificationcompared to the conventional KBR system. This implies that the carbon sources tested in this projectcan be advantageous and beneficial for Sörab depending on the carbon source availability and theseasonal variations.</p>

Note the first mgNO3 is as set in the thesis - wihtout the 3 being a subscript.
corrected abstract:
<p>In 2014 SÖRAB constructed a continuous biological treatment system (KBR) to handle leachate water from the landfill at the facility in Löt, north of Stockholm. The KBR is mainly focused on removal of ammonium nitrogen which would otherwise be released in to the recipient and contribute to eutrophication and damage to the environment. This project has focused on replacing the current carbon source in the process Brenntaplus VP1 and evaluating the efficiency of denitrification and economy of transitioning to a new carbon source. The carbon sources glycerol and ethanol were evaluated and compared to Brenntaplus VP1 for the denitrification efficiency and microbial profile. The experiments were performed in laboratory conditions and in pilot scale using leachate water from Löt. The reduction of ammonia was evaluated by chemical precipitation, addition of carbon sources by measuring ammonia-N and nitrate-N under aerobic (nitrification) and anaerobic (denitrification) conditions. The combination of ethanol and glycerol showed an enhanced denitrification and increased microbial community both in lab and pilot scale studies with reduced hydraulic retention time. The rate of nitrate reduction was 0.23 mgNO3-N 1<sup>-1</sup> h<sup>-1</sup> for ethanol/glycerol compared to 0.12-0.17 mgNO<sub>3</sub><sup>-</sup>-N 1<sup>-1</sup> h<sup>-1</sup> for Brenntaplus VP1 in pilot scale. The results indicate that using ethanol, glycerol or a mix of the two as a substitute for Brenntaplus VP1 is viable. This has been based on laboratory and pilot scale studies. Each of the carbon sources examined during this project have showed a unique impact on the process and its parameters such as: denitrification rate, microbial density and microbial composition. The carbon sources had an impact with temperature fluctuation and faster denitrification compared to the conventional KBR system. This implies that the carbon sources tested in this project can be advantageous and beneficial for Sörab depending on the carbon source availability and the seasonal variations.</p>
----------------------------------------------------------------------
In diva2:1455012 abstract is: <p>This project has taken place over a 20 week period with the aim to find asuitable replacement for the current desalting process of large product volumes ofoligonucleotides at the company Scandinavian Gene Synthesis. In the beginning of theproject, a literature and market research was conducted to evaluate suitable options ofdesalting processes, followed by a cost analysis to choose the most cost effective andsustainable option. After selecting the most optimal solution, it was tested with variouskinds of oligonucleotides in order to validate the method. When this method validationwas proven successful, an attempt to scale up the new desalting process were executed.Key aspects that were being evaluated were product recovery, desalting performance,process time etc. Furthermore, a contamination analysis with MS was performed toinvestigate potential reuse of the new desalting process. The expected outcome ofthis project is an implementation a new desalting process that provides competitivefinancial and time advantage. In addition, it should create better sustainability andenvironmental profiles while continuing to ensure high safety and quality standards.This will enhance an increase of production and ensure that more customer demandswill be met, resulting in an expansion of their customer base and growth as a companyin general.</p>


w='masspectrometry' val={'c': 'mass spectrometry', 's': 'diva2:1455012', 'n': 'no full text'}

corrected abstract:
<p>This project has taken place over a 20 week period with the aim to find a suitable replacement for the current desalting process of large product volumes of oligonucleotides at the company Scandinavian Gene Synthesis. In the beginning of the project, a literature and market research was conducted to evaluate suitable options of desalting processes, followed by a cost analysis to choose the most cost effective and sustainable option. After selecting the most optimal solution, it was tested with various kinds of oligonucleotides in order to validate the method. When this method validation was proven successful, an attempt to scale up the new desalting process were executed. Key aspects that were being evaluated were product recovery, desalting performance, process time etc. Furthermore, a contamination analysis with MS was performed to investigate potential reuse of the new desalting process. The expected outcome of this project is an implementation a new desalting process that provides competitive financial and time advantage. In addition, it should create better sustainability and environmental profiles while continuing to ensure high safety and quality standards. This will enhance an increase of production and ensure that more customer demands will be met, resulting in an expansion of their customer base and growth as a company in general.</p>
----------------------------------------------------------------------
In diva2:1452644 abstract is: <p>Pharmacokinetic (PK)- and pharmacodynamic (PD) modeling are useful tools whenassessing treatment effect. A patient’s adherence can potentially be rate-limiting, since it isthe first process in a chain of processes that determines treatment effect. Therefore agreater system taking into consideration PKPD as well as adherence models couldpotentially unlock a greater system understanding. This study focuses on investigating thefeasibility of combining models concerning adherence, PK and PD.</p><p>An extensive mapping of previously made work on the topics of PKPD model developmentand adherence models concerning type 2 diabetes was conducted. Results concluded thatthere are gaps in research regarding adequate adherence-scoring methods that easily can belinked to dosing regimens. Furthermore, there is lacking research regarding feedback fromexposure-response to adherence. A simple model was implemented to provide a proposedlinkage inhowthe connection could be made between adherence and a PKPD-model.Sensitivity analysis showed that the adherence scoring used (Summary of DiabetesSelf-Care Activities measure, SDSCA) had a moderate correlation to the final response onfasting plasma glucose (Spearman ρ=−0.478∗∗∗). This result suggests that adherenceshould be considered as a relatively important factor to weave in to systems models ofpharmacology and future research should be made on further developing modelsimplementing both social factors, such as adherence, as well as pharmacologic response. Apossible way could be linking dose regimen to adherence scoring.</p>


corrected abstract:
<p>Pharmacokinetic (PK)- and pharmacodynamic (PD) modeling are useful tools when assessing treatment effect. A patient’s adherence can potentially be rate-limiting, since it is the first process in a chain of processes that determines treatment effect. Therefore a greater system taking into consideration PKPD as well as adherence models could potentially unlock a greater system understanding. This study focuses on investigating the feasibility of combining models concerning adherence, PK and PD.</p><p>An extensive mapping of previously made work on the topics of PKPD model development and adherence models concerning type 2 diabetes was conducted. Results concluded that there are gaps in research regarding adequate adherence-scoring methods that easily can be linked to dosing regimens. Furthermore, there is lacking research regarding feedback from exposure-response to adherence. A simple model was implemented to provide a proposed linkage in how the connection could be made between adherence and a PKPD-model. Sensitivity analysis showed that the adherence scoring used (Summary of Diabetes Self-Care Activities measure, SDSCA) had a moderate correlation to the final response on fasting plasma glucose (Spearman <em>ρ</em> = −0.478<sup>∗∗∗</sup>). This result suggests that adherence should be considered as a relatively important factor to weave in to systems models of pharmacology and future research should be made on further developing models implementing both social factors, such as adherence, as well as pharmacologic response. A possible way could be linking dose regimen to adherence scoring.</p>
----------------------------------------------------------------------
Although there is not full text, it is likely that the title "GAP43 AS A POTENTIALBIOMARKER FORNEURODEGENERATION: Division of Affinity Proteomics, SciLifeLabDepartment of Protein Science
"
==> "GAP43 AS A POTENTIAL BIOMARKER FOR NEURODEGENERATION: Division of Affinity Proteomics, SciLifeLab Department of Protein Science
"

In diva2:1228129 abstract is: <p>Alzheimer’s disease (AD) is a common neurodegenerative disorder characterized by progressivedamage and loss of neurons. Despite the efforts, the scientific community still lacks current insights intothe onset and progression of this disease. Previous multiplex analysis of cerebrospinal fluid (CSF)protein profiles on the suspension bead array (SBA) platform, applied in the context of AD, hasunderlined growth-associated protein’s 43 (GAP43 or neuromodulin) association to the disease. GAP43is a brain enriched cytoplasmic protein involved in neuronal growth and axonal regeneration. Becauseof its post translational modifications, mainly phosphorylation, GAP43 plays an important role in memoryand learning. This project aims to further explore the relation of GAP43 to AD through the identificationof antibodies that can capture this protein and their application on Western blot (WB) and brain tissuemicroarray (TMA) protocols. Our antibody selection includes eight antibodies, two of which target thephosphorylated GAP43 (ph-GAP43). All antibodies were tested on Western blots of CSF pools of ADcases and controls with elevated and decreased GAP43 levels respectively. Additionally, four of theseantibodies were tested on TMAs consisting of cortical tissue cores from ten AD patients, ten patientswith Lewy Body Dementia (DLB) and nine controls. Our results indicate that although all antibodiessuccessfully detected GAP43 both on WB and TMAs, ph-GAP43 was only captured on TMAs.Immunohistochemical analysis of temporal and frontal cortex from AD patients revealed an almostcomplementary pattern of GAP43 and ph-GAP43 higher expression in cortical tissue cores obtainedfrom AD patients compared to controls. Lastly, more intense phosphorylation was observed in temporalthan in frontal cortex.</p>


corrected abstract:
<p>Alzheimer’s disease (AD) is a common neurodegenerative disorder characterized by progressive damage and loss of neurons. Despite the efforts, the scientific community still lacks current insights into the onset and progression of this disease. Previous multiplex analysis of cerebrospin al fluid (CSF) protein profiles on the suspension bead array (SBA) platform, applied in the context of AD, has underlined growth-associated protein’s 43 (GAP43 or neuromodulin) association to the disease. GAP43 is a brain enriched cytoplasmic protein involved in neuronal growth and axonal regeneration. Because of its post translational modifications, mainly phosphorylation, GAP43 plays an important role in memory and learning. This project aims to further explore the relation of GAP43 to AD through the identification of antibodies that can capture this protein and their application on Western blot (WB) and brain tissue microarray (TMA) protocols. Our antibody selection includes eight antibodies, two of which target the phosphorylated GAP43 (ph-GAP43). All antibodies were tested on Western blots of CSF pools of AD cases and controls with elevated and decreased GAP43 levels respectively. Additionally, four of these antibodies were tested on TMAs consisting of cortical tissue cores from ten AD patients, ten patients with Lewy Body Dementia (DLB) and nine controls. Our results indicate that although all antibodies successfully detected GAP43 both on WB and TMAs, ph-GAP43 was only captured on TMAs. Immunohistochemical analysis of temporal and frontal cortex from AD patients revealed an almost complementary pattern of GAP43 and ph-GAP43 higher expression in cortical tissue cores obtained from AD patients compared to controls. Lastly, more intense phosphorylation was observed in temporal than in frontal cortex.</p>
----------------------------------------------------------------------
In diva2:1455019 abstract is: <p>Microorganisms play crucial roles in aquatic environments in determining ecosystemstability and driving the turnover of elements essential to life. Understanding thedistribution and evolution of aquatic microorganisms will help us predict how aquaticecosystems will respond to Global Change, and such understanding can be gained bystudying these processes of the past. In this project, we investigate the evolutionaryrelationship between brackish water bacteria from the Baltic Sea and Caspian Seawith freshwater and marine bacteria, with the goal of understanding how brackishwater bacteria have evolved. 11,276 bacterial metagenome-assembled genomes(MAGs) from seven metagenomic datasets were used to conduct a comparativeanalysis of freshwater, brackish and marine bacteria. When clustering the genomes bypairwise average nucleotide identity (ANI) at the approximate species level (96.5%ANI), the Baltic Sea genomes were more likely to form clusters with the Caspian Seagenomes than with Swedish lakes genomes, even though geographic distancesbetween Swedish lakes and the Baltic Sea are much smaller. Phylogenomic analysisand ancestral state reconstruction showed that approximately half of the brackishMAGs had freshwater ancestors and half had marine ancestors. Phylogeneticdistances were on average shorter to freshwater ancestors, but when subsampling thetree to the same number of freshwater and marine MAG clusters, the distances werenot significantly different. Brackish genomes belonging to Acidimicrobiia,Actinobacteria and Cyanobacteriia tended to originate from freshwater bacteria, whilethose of Alphaproteobacteria and Bacteroidia mainly had evolved from marinebacteria.</p>

corrected abstract:
<p>Microorganisms play crucial roles in aquatic environments in determining ecosystem stability and driving the turnover of elements essential to life. Understanding the distribution and evolution of aquatic microorganisms will help us predict how aquatic ecosystems will respond to Global Change, and such understanding can be gained by studying these processes of the past. In this project, we investigate the evolutionary relationship between brackish water bacteria from the Baltic Sea and Caspian Sea with freshwater and marine bacteria, with the goal of understanding how brackish water bacteria have evolved. 11,276 bacterial metagenome-assembled genomes (MAGs) from seven metagenomic datasets were used to conduct a comparative analysis of freshwater, brackish and marine bacteria. When clustering the genomes by pairwise average nucleotide identity (ANI) at the approximate species level (96.5% ANI), the Baltic Sea genomes were more likely to form clusters with the Caspian Sea genomes than with Swedish lakes genomes, even though geographic distances between Swedish lakes and the Baltic Sea are much smaller. Phylogenomic analysis and ancestral state reconstruction showed that approximately half of the brackish MAGs had freshwater ancestors and half had marine ancestors. Phylogenetic distances were on average shorter to freshwater ancestors, but when subsampling the tree to the same number of freshwater and marine MAG clusters, the distances were not significantly different. Brackish genomes belonging to Acidimicrobiia, Actinobacteria and Cyanobacteriia tended to originate from freshwater bacteria, while those of Alphaproteobacteria and Bacteroidia mainly had evolved from marine bacteria.</p>
----------------------------------------------------------------------
In diva2:1044163 abstract is: <p>The impact of carbon-dioxide emission on the environment is one of our generation’sbiggest environmental challenges, and the use of more sustainable energy sources isneeded. Bioethanol can act as a more sustainable resource instead of petroleum basedfuels. The biggest obstacle during production of second-generation bioethanol is thepre-treatment step, where the lignin is removed from the cellulolytic material.Laccases are enzymes that have the ability to oxidise lignin and thus remove ligninfrom the material. Recent articles reporting on enzymes belonging to the proteinfamily DUF152 have shown that they possess a laccase-like activity. Therefor,members of the DUF152 family are interesting targets for possible future applicationsthat involve pre-treatment of cellulolytic materials. In this report structural andpreliminary biochemical studies are presented for several novel DUF152 membersreferred to as B05, B10 and Dv152.</p><p>The proteins were produced in Escherichia coli cells and purified by immobilisedmetal ion affinity chromatography and size exclusion chromatography. Crystallisationexperiments were successful and yielded high-resolution diffraction data for all threeenzymes. The crystal structures were determined at 2.10, 2.30 and 1.60 Å resolutionfor B05 was 2.1 Å, B10 2.3 Å and for Dv152 1.6 Å respectively. The amino acids inthe active site were shown to be conserved and included two catalytic 2 histidineresidues and one cysteine residue. To further evauate similarities and differences ofstructural features, homology models were also generated for DUF152 members thathave been biochemically characterised but where no crystal structure is available.</p>

w='evauate' val={'c': 'evaluate', 's': 'diva2:1044163', 'n': 'no full text'}
w='ligninfrom' val={'c': 'lignin from', 's': 'diva2:1044163', 'n': 'no full text'}

corrected abstract:
<p>The impact of carbon-dioxide emission on the environment is one of our generation’s biggest environmental challenges, and the use of more sustainable energy sources is needed. Bioethanol can act as a more sustainable resource instead of petroleum based fuels. The biggest obstacle during production of second-generation bioethanol is the pre-treatment step, where the lignin is removed from the cellulolytic material. Laccases are enzymes that have the ability to oxidise lignin and thus remove lignin from the material. Recent articles reporting on enzymes belonging to the protein family DUF152 have shown that they possess a laccase-like activity. Therefor, members of the DUF152 family are interesting targets for possible future applications that involve pre-treatment of cellulolytic materials. In this report structural and preliminary biochemical studies are presented for several novel DUF152 members referred to as B05, B10 and Dv152.</p><p>The proteins were produced in Escherichia coli cells and purified by immobilised metal ion affinity chromatography and size exclusion chromatography. Crystallisation experiments were successful and yielded high-resolution diffraction data for all three enzymes. The crystal structures were determined at 2.10, 2.30 and 1.60 Å resolution for B05 was 2.1 Å, B10 2.3 Å and for Dv152 1.6 Å respectively. The amino acids in the active site were shown to be conserved and included two catalytic 2 histidine residues and one cysteine residue. To further evaluate similarities and differences of structural features, homology models were also generated for DUF152 members that have been biochemically characterised but where no crystal structure is available.</p>
----------------------------------------------------------------------
In diva2:812041 abstract is: <p>This diploma work has been carried out on behalf of Terco. TercosPST 2220 Transmission Line and Distribution Module works as a physicalmodel of a real transmission and distribution grid where five different typesof networks based on length, voltage and apparent effect are available. Thereis today a need of a model where the user self can adjust these parameters sothat the model more precisely can reflect the characteristics that the specificgrid has. Here it’s investigated how the length and its impact on a line canbe varied in a model.A presentation of how the transmission and distribution grid works andare described theoretically provides the basics for the different models thatcan describe a whole network and its properties.Since the R, L and C components needs to be able to be varied to be ableto physically realize this theoretical model, the different methods that thiscan be realized through are investigated. Two approaches are investigated,the cascaded pi-model and variable active-passive reactance (VAPAR).A number of aspects like space, cost and variability makes the variableactive-passive reactance the most suited solution. Its function as a variablevoltage source, made out of an four switches, operated with control techno-logy and pulse width modulation, makes it possible to imitate R, L and Csproperties and effect on a transmission line. The result is that the necessaryR,L,C components are made adjustable in order to be incorporated in aadjustable transmission lin model.The result and the goal are verified with simulations where variableactive-passive reactance is proved able for further development and practicaltests to model transmission and distribution lines with different length.</p><p><strong>Keywords. </strong>Variability, Inverter, Impedance, DC-AC, Pulse width modula-tion, Harmonics, H-bridge, Transmission line, Voltage drop, Reactive effect.</p>

w='Csproperties' val={'c': 'C properties', 's': 'diva2:812041', 'n': 'error in original'}

corrected abstract:
<p>This diploma work has been carried out on behalf of Terco. Tercos PST 2220 Transmission Line and Distribution Module works as a physical model of a real transmission and distribution grid where five different types of networks based on length, voltage and apparent effect are available. There is today a need of a model where the user self can adjust these parameters so that the model more precisely can reflect the characteristics that the specific grid has. Here it’s investigated how the length and its impact on a line can be varied in a model.</p><p>A presentation of how the transmission and distribution grid works and are described theoretically provides the basics for the different models that can describe a whole network and its properties.</p><p>Since the R, L and C components needs to be able to be varied to be able to physically realize this theoretical model, the different methods that this can be realized through are investigated. Two approaches are investigated, the cascaded pi-model and variable active-passive reactance (VAPAR).</p><p>A number of aspects like space, cost and variability makes the variable active-passive reactance the most suited solution. Its function as a variable voltage source, made out of an four switches, operated with control technology and pulse width modulation, makes it possible to imitate R, L and Cs properties and effect on a transmission line. The result is that the necessary R,L,C components are made adjustable in order to be incorporated in a adjustable transmission lin model.</p><p>The result and the goal are verified with simulations where variable active-passive reactance is proved able for further development and practical tests to model transmission and distribution lines with different length.</p>
----------------------------------------------------------------------
In diva2:1217588 abstract is: <p>In vivo, esterases together with lipases catalyse the cleavage of esters into acids and alcohols through hydrolysis.However in anhydrous environments, their activity is reversed to instead catalyze acyl transfer reactions. This reactiontype is responsible for many vital physiological functions, and have the potential to be used for a wide variety ofapplications. MsAcT from M. smegmatis is one of few esterases with the ability to perform acyl transfer reactionsin aqueous solutions. In vitro, MsAcT forms octamers resulting in the formation of hydrophobic channels within thecomplexes. The active site is buried within such a channel which consitute the putative explanation for the uniquecatalytic properties of the enzyme.Two variants of the enzyme; L12A and T93A/F154A were investigated in more detail using acyl donors of differentlengths (dimethyl succinate, -adipate, -suberate, and -sebacate), and varying types of acyl acceptors (1-octanol,butanediol vinyl ether, trimethylolpropane oxetane, and 1,4-butanediol). Both variants showed an extended substratescope compared to the wilde type (wt), with the T93A/F154A accepting acyl donors up to 10 carbons in length(sebacate) and had the highest overall activity for all alcohols tested. However, the L12A variant yields mixedvinyl adipate esters with higher purity than the wt and T93A/F154A variants. With 1,4-butanediol, the wt andT93A/F154A are able to form oligomers, which has never been demonstrated before. A better understanding of theseresults were acquired through molecular dynamics simulation of the enzyme-substrate interactions.The properties of these MsAcT variants could be very interesting from an industrial point of view. Being ableto customize the specificity of the MsAcT would be valuable for the biosynthesis of various substances and materialssuch as hybrid pharmaceuticals and different polymers.Key words: esterase, acyl transfer, MsAcT, mixed dicarboxylic esters, acyl donor, acyl acceptor, specificity,molecular dynamics simulation, immobilization.</p>

w='consitute' val={'c': 'constitute', 's': 'diva2:1217588', 'n': 'no full text'}
w='wilde' val={'c': 'wild', 's': 'diva2:1217588', 'n': 'no full text'}

corrected abstract:
<p>In vivo, esterases together with lipases catalyse the cleavage of esters into acids and alcohols through hydrolysis. However in anhydrous environments, their activity is reversed to instead catalyze acyl transfer reactions. This reaction type is responsible for many vital physiological functions, and have the potential to be used for a wide variety of applications. MsAcT from <em>M. smegmatis</em> is one of few esterases with the ability to perform acyl transfer reactions in aqueous solutions. In vitro, MsAcT forms octamers resulting in the formation of hydrophobic channels within the complexes. The active site is buried within such a channel which constitute the putative explanation for the unique catalytic properties of the enzyme. Two variants of the enzyme; L12A and T93A/F154A were investigated in more detail using acyl donors of different lengths (dimethyl succinate, -adipate, -suberate, and -sebacate), and varying types of acyl acceptors (1-octanol,butanediol vinyl ether, trimethylolpropane oxetane, and 1,4-butanediol). Both variants showed an extended substrate scope compared to the wild type (wt), with the T93A/F154A accepting acyl donors up to 10 carbons in length (sebacate) and had the highest overall activity for all alcohols tested. However, the L12A variant yields mixed vinyl adipate esters with higher purity than the wt and T93A/F154A variants. With 1,4-butanediol, the wt and T93A/F154A are able to form oligomers, which has never been demonstrated before. A better understanding of these results were acquired through molecular dynamics simulation of the enzyme-substrate interactions. The properties of these MsAcT variants could be very interesting from an industrial point of view. Being able to customize the specificity of the MsAcT would be valuable for the biosynthesis of various substances and materials such as hybrid pharmaceuticals and different polymers.</p>
----------------------------------------------------------------------
In diva2:1115384 abstract is: <p>MRI is one of the biggest and most growing imaging techniques. Even though itis one of the most harmless technologies a big portion of the patients experienceanxiety during the exam. By improving the patient experience unnecessary psychologicalstress for the patient can be prevented, the patient movement wouldthen decrease and therefore the imaging can be improved without changing thetechnique. Participant observations at four dierent MRI departments werecompleted with six interviews with radiographers and technical MRI personnelin order to get insight in the work around an MRI exam and the problemsthat patients experience. The data collection resulted in three improvementareas: the atmosphere of the waiting room, the atmosphere of the MRI roomand the headset used by the patient during the MRI exam. These improvementareas were paired up with solution suggestions which were then controlled andcommented by one MRI specialist, one MRI developer and one radiographer tovalidate the suggestions. The conclusion was that there is already much doneto improve the environment in the MRI room, even though more can be done.The waiting room, on the other hand ,has not been an object for studies orfor improvements before. Therefore more calculation about how big of a protit could be, to improving the atmosphere in the waiting room, should be doneso one knows how much resources one can be put into that improvement area.Lastly there are potential solutions for how to create a much better headset butbecause the generated solutions in this area are so technically challenging moreresearch has to be done before it can be realised.</p>

w='protit' val={'c': 'profit', 's': 'diva2:1115384'}

Note that the "hand ,has" is an error in the original.
corrected abstract:
<p>MRI is one of the biggest and most growing imaging techniques. Even though it is one of the most harmless technologies a big portion of the patients experience anxiety during the exam. By improving the patient experience unnecessary psychological stress for the patient can be prevented, the patient movement would then decrease and therefore the imaging can be improved without changing the technique. Participant observations at four different MRI departments were completed with six interviews with radiographers and technical MRI personnel in order to get insight in the work around an MRI exam and the problems that patients experience. The data collection resulted in three improvement areas: the atmosphere of the waiting room, the atmosphere of the MRI room and the headset used by the patient during the MRI exam. These improvement areas were paired up with solution suggestions which were then controlled and commented by one MRI specialist, one MRI developer and one radiographer to validate the suggestions. The conclusion was that there is already much done to improve the environment in the MRI room, even though more can be done. The waiting room, on the other hand ,has not been an object for studies or for improvements before. Therefore more calculation about how big of a profit it could be, to improving the atmosphere in the waiting room, should be done so one knows how much resources one can be put into that improvement area. Lastly there are potential solutions for how to create a much better headset but because the generated solutions in this area are so technically challenging more research has to be done before it can be realised.</p>
----------------------------------------------------------------------
In diva2:1773505 abstract is: <p>Home-based exercise is a popular physical activity of maintaining fitness, health andwellness in general. However, without proper supervision and basic knowledge of theexercises in the workout plan, there is an increased risk of injury. Considering that noteveryone is willing to attend crowded gyms or schedule professional personal trainingsessions, in this study, a novel feedback system is proposed, in the form of a mobileapplication. Accelerometer and gyroscope data were collected from 10 volunteersperforming 3 exercises, squats, lunges and bridges, with inertial sensors attachedto their back lumbar region, on both shanks and on both thighs. Each participantperformed 5 repetitions of the correct technique and 5 repetitions of 4 mistakes foreach exercise. The accuracies of 3 classifiers, a SVM, a RF and DT were comparedwith the SVM performing the best across all 3 exercises. The best location and numberof sensors was determined by examining the accuracy of a SVM model for 15 uniquemulti-sensor configurations. The best performing setup, being the configuration with 2sensors, one at the lumbar area and one at the shank, was used in exploring the efficacyof different data processing techniques. Time-domain statistical features, sensor angletimeseries and the filtered signal timeseries were evaluated as input to a NN. The timedomainfeatures performed the best achieving the highest accuracy in all 3 exercises,with an accuracy of 67% for the squats, 87% for the lunges and 75% for the hip bridges.Overall, the final model demonstrated promising capabilities of classifying exercisetechnique of basic lower-body exercises, with a real-time feedback implementationbeing a feasible solution for self-efficient fitness.</p>

corrected abstract:
<p>Home-based exercise is a popular physical activity of maintaining fitness, health and wellness in general. However, without proper supervision and basic knowledge of the exercises in the workout plan, there is an increased risk of injury. Considering that not everyone is willing to attend crowded gyms or schedule professional personal training sessions, in this study, a novel feedback system is proposed, in the form of a mobile application. Accelerometer and gyroscope data were collected from 10 volunteers performing 3 exercises, squats, lunges and bridges, with inertial sensors attached to their back lumbar region, on both shanks and on both thighs. Each participant performed 5 repetitions of the correct technique and 5 repetitions of 4 mistakes for each exercise. The accuracies of 3 classifiers, a SVM, a RF and DT were compared with the SVM performing the best across all 3 exercises. The best location and number of sensors was determined by examining the accuracy of a SVM model for 15 unique multi-sensor configurations. The best performing setup, being the configuration with 2 sensors, one at the lumbar area and one at the shank, was used in exploring the efficacy of different data processing techniques. Time-domain statistical features, sensor angle timeseries and the filtered signal timeseries were evaluated as input to a NN. The time-domain features performed the best achieving the highest accuracy in all 3 exercises, with an accuracy of 67% for the squats, 87% for the lunges and 75% for the hip bridges. Overall, the final model demonstrated promising capabilities of classifying exercise technique of basic lower-body exercises, with a real-time feedback implementation being a feasible solution for self-efficient fitness.</p>
----------------------------------------------------------------------
In diva2:935006 abstract is: <p>Wireless devices search for access points when they want to connect to a network. A devicechooses an access point based on the received signal strength between the device and theaccess point. That method is good for staying connected in a local area network but it doesnot always offer the best performance, which can result in a slower connection. This is thestandard method of connection for wireless clients, which will be referred to as the standardprotocol. Larger networks commonly have a lot of access points in an area, which increasesthe coverage area and makes loss of signal a rare occurrence. Overlapping coverage zonesare also common, offering multiple choices for a client. The company Inteno wanted an alternativeconnection method for their gateways. The new method that was developed wouldforce the client to connect to an access point depending on the bitrate to the master, as wellas the received signal strength. These factors are affected by many different parameters.These parameters were noise, signal strength, link-rate, bandwidth usage and connectiontype. A new metric had to be introduced to make the decision process easier by unifying theavailable parameters. The new metric that was introduced is called score. A score system wascreated based on these metrics. The best suited access point would be the one with the highestscore. The developed protocol chose the gateway with the highest bitrate available, while thestandard protocol would invariably pick the closest gateway regardless. The developed protocolcould have been integrated to the standard protocol to gain the benefits of both. Thiscould not be accomplished since the information was not easily accessible on Inteno’s gatewaysand had to be neglected in this thesis.</p>

partal corrected: diva2:935006: <p>Wireless devices search for access points when they want to connect to a network. A device chooses an access point based on the received signal strength between the device and the access point. That method is good for staying connected in a local area network but it does not always offer the best performance, which can result in a slower connection. This is the standard method of connection for wireless clients, which will be referred to as the standard protocol. Larger networks commonly have a lot of access points in an area, which increases the coverage area and makes loss of signal a rare occurrence. Overlapping coverage zones are also common, offering multiple choices for a client. The company Inteno wanted an alternative connection method for their gateways. The new method that was developed would force the client to connect to an access point depending on the bitrate to the master, as well as the received signal strength. These factors are affected by many different parameters. These parameters were noise, signal strength, link-rate, bandwidth usage and connection type. A new metric had to be introduced to make the decision process easier by unifying the available parameters. The new metric that was introduced is called score. A score system was created based on these metrics. The best suited access point would be the one with the highest score. The developed protocol chose the gateway with the highest bitrate available, while the standard protocol would invariably pick the closest gateway regardless. The developed protocol could have been integrated to the standard protocol to gain the benefits of both. This could not be accomplished since the information was not easily accessible on Inteno’s gateways and had to be neglected in this thesis .</p>

corrected abstract:
<p>Wireless devices search for access points when they want to connect to a network. A device chooses an access point based on the received signal strength between the device and the access point. That method is good for staying connected in a local area network but it does not always offer the best performance, which can result in a slower connection. This is the standard method of connection for wireless clients, which will be referred to as the standard protocol. Larger networks commonly have a lot of access points in an area, which increases the coverage area and makes loss of signal a rare occurrence. Overlapping coverage zones are also common, offering multiple choices for a client. The company Inteno wanted an alternative connection method for their gateways. The new method that was developed would force the client to connect to an access point depending on the bitrate to the master, as well as the received signal strength. These factors are affected by many different parameters. These parameters were noise, signal strength, link-rate, bandwidth usage and connection type. A new metric had to be introduced to make the decision process easier by unifying the available parameters. The new metric that was introduced is called score. A score system was created based on these metrics. The best suited access point would be the one with the highest score. The developed protocol chose the gateway with the highest bitrate available, while the standard protocol would invariably pick the closest gateway regardless. The developed protocol could have been integrated to the standard protocol to gain the benefits of both. This could not be accomplished since the information was not easily accessible on Inteno’s gateways and had to be neglected in this thesis.</p>
----------------------------------------------------------------------
In diva2:1219371 abstract is: <p>The transplantations of biological or non-biological materials are the heart of many medical innovations and lifesaving therapies. Many studies have revealed that a balanced immune modulation after transplantation iscrucial for the materials ability to integrate with the host tissue. Mucins, agroup of heavily glycosylated proteins present in the mucosa, have shown to have an immune modulating capacity in different species, and could therefore be used to modulate the immune response to implanted devices. To explore this possibility, we aim to develop biomaterials, rich in mucins,which have the capacity to coat the transplants to act as an interfacebetween the body and the transplanted material. Monolayer mucincoating can be obtained through the simple adsorption of mucins onvariety of surfaces. However, mucin monolayers can have limitations suchas imperfect coverage and instability over time. We thus aim to developmucin-rich thin films using the layer-by-layer assembly technique.To achieve this goal, we synthesized ClickMucins by functionalizingbovine submaxillary mucins (BSM) with tetrazine and norbornene via twografting strategies. We either targeted the carboxyl groups of the mucinsor the cis-glycol groups of the mucin-associated glycans. The click reaction between tetrazine and norbornene allows the functionalized BSM to formpure mucin hydrogels via covalent bonding. The layer-by-layer assembly ofalternating layers of tetrazine and norbornene-functionalized mucin haddifferent characteristics depending on the functionalization method used.We found significant differences in the initial adsorption of the mucins tothe surface and in the buildup of the second layer. Functionalization bytargeting the carboxyl group gave better adsorption to the substratesurface, while functionalization by targeting the cis-glycol gave a more significant second-layer buildup. The buildup of the multilayer film waslimited to two layers, which we hypothesized could be due to disbalanced grafting density of the two functionalities on the mucins.</p>

w='SMITH' val={'c': 'SMITh', 's': 'diva2:1219371', 'n': 'Université Claude Bernard Lyon 1 - see https://www.icbms.fr/en/team/10-smith-html'}

corrected abstract:
<p>The transplantations of biological or non-biological materials are the heart of many medical innovations and lifesaving therapies. Many studies have revealed that a balanced immune modulation after transplantation is crucial for the materials ability to integrate with the host tissue. Mucins, a group of heavily glycosylated proteins present in the mucosa, have shown to have an immune modulating capacity in different species, and could therefore be used to modulate the immune response to implanted devices. To explore this possibility, we aim to develop biomaterials, rich in mucins, which have the capacity to coat the transplants to act as an interface between the body and the transplanted material. Monolayer mucin coating can be obtained through the simple adsorption of mucins on variety of surfaces. However, mucin monolayers can have limitations such as imperfect coverage and instability over time. We thus aim to develop mucin-rich thin films using the layer-by-layer assembly technique. To achieve this goal, we synthesized ClickMucins by functionalizing bovine submaxillary mucins (BSM) with tetrazine and norbornene via two grafting strategies. We either targeted the carboxyl groups of the mucins or the cis-glycol groups of the mucin-associated glycans. The click reaction between tetrazine and norbornene allows the functionalized BSM to form pure mucin hydrogels via covalent bonding. The layer-by-layer assembly of alternating layers of tetrazine and norbornene-functionalized mucin had different characteristics depending on the functionalization method used. We found significant differences in the initial adsorption of the mucins to the surface and in the buildup of the second layer. Functionalization by targeting the carboxyl group gave better adsorption to the substrate surface, while functionalization by targeting the cis-glycol gave a more significant second-layer buildup. The buildup of the multilayer film was limited to two layers, which we hypothesized could be due to disbalanced grafting density of the two functionalities on the mucins.</p>
----------------------------------------------------------------------
In diva2:1604124 abstract is: <p>Radiotherapy has become an ever more successful treatment option for cancer.Advances in imaging protocols combined with precise therapy devices suchas linear accelerators contribute towards millimeter precision of treatmentdelivery with far fewer side effects. The ultimate goal of radiotherapy is tomaximize tumor control while minimizing adverse effects to healthy tissues,more importantly organs at risk surrounding the tumor. External beamradiotherapy is currently on the brink of breaking a new frontier: MagneticResonance Imaging (MRI) guided tumor tracking. Here, a combined linearaccelerator and MRI system can be used to treat and follow the tumor duringirradiation, called Real-time Adaptive Radiotherapy (ART). Tailoring of thebeam shape, by means of the Multi-leaf Collimator (MLC) on the fly has thepotential to complete a fully automated radiotherapy process.</p><p>Recent advances in Reinforcement Learning (RL), a sub field of artificialintelligence has pushed the frontiers further in sequential decision making processesfurther in various fields. In a MLC tracking scenario, we hypothesizethat an RL agent trained on real-time tumor delineations and dose informationcould fulfill a specified dosimetric criteria on the fly over the moving target.To investigate the feasibility of RL for MLC tracking further: we designeda simulator, devised an appropriate RL framework and interfaced them to aDeep Q-Network (DQN) algorithm.</p><p>Our results demonstrate the feasibility of employing RL for MLC trackingalong with numerous design choices that need to be considered while developingsuch a system. We believe to have taken the first step to bridge MLCtracking and RL by proposing a closed loop solution using dose information.</p>


corrected abstract:
<p>Radiotherapy has become an ever more successful treatment option for cancer. Advances in imaging protocols combined with precise therapy devices such as linear accelerators contribute towards millimeter precision of treatment delivery with far fewer side effects. The ultimate goal of radiotherapy is to maximize tumor control while minimizing adverse effects to healthy tissues, more importantly organs at risk surrounding the tumor. External beam radiotherapy is currently on the brink of breaking a new frontier: Magnetic Resonance Imaging (MRI) guided tumor tracking. Here, a combined linear accelerator and MRI system can be used to treat and follow the tumor during irradiation, called Real-time Adaptive Radiotherapy (ART). Tailoring of the beam shape, by means of the Multi-leaf Collimator (MLC) on the fly has the potential to complete a fully automated radiotherapy process.</p><p>Recent advances in Reinforcement Learning (RL), a sub field of artificial intelligence has pushed the frontiers further in sequential decision making processes further in various fields. In a MLC tracking scenario, we hypothesize that an RL agent trained on real-time tumor delineations and dose information could fulfill a specified dosimetric criteria on the fly over the moving target. To investigate the feasibility of RL for MLC tracking further: we designed a simulator, devised an appropriate RL framework and interfaced them to a Deep Q-Network (DQN) algorithm.</p><p>Our results demonstrate the feasibility of employing RL for MLC tracking along with numerous design choices that need to be considered while developing such a system. We believe to have taken the first step to bridge MLC tracking and RL by proposing a closed loop solution using dose information.</p>
----------------------------------------------------------------------
In diva2:1774643 abstract is: <p>The work of this master thesis, at the Royal Institute of Technology, is primarily a study of rootcauses of fatal injuries at work, related to building and construction industries includinginstallation. As a term, root cause has not only obvious synonyms, since the meaning can vary fromword to word. The author of the thesis is discussing how a root cause should be defined with somealternatives. Here, a common nominator can be that the level of impact caused should not be builtinto the term. Moreover, the definition must not hinder the practical use of root causes through unintentional misunderstanding.</p><p>Fundamentally, this study is to determine whether a root cause is appropriate for use related toaccident prevention at work. Consequently, the author found 19 of them organised under theseprincipal headings: responsibility, risk analysis, communication, and protective measure.</p><p>This study is accomplished through a qualitative research methodology only, based on aliterature study. A substantial part of it was to analyse court judgments related to fatal, and workrelatedaccidents, and if possible, find root causes. There were two interviews by focus groups withone employer organisation, Byggföretagen, and one trade union, Byggnads, represented.</p><p>Since one of the purposes was to make a suitability assessment of the root causes and seriouslydiscuss these in the light of the introduced trails, a major part of this thesis is about the underlyingreasons for the fatal accidents of the industries. Hence, the result section of the report is worthwhileto read since it is based on wide and deep knowledge by the interviewed representatives in workenvironment related issues of the building and construction industries.</p><p>Another part of the thesis is highlighting how useful statistics of accidents, from the SwedishWork Environment Authority (SWEA) and related sources, are. From the authors side, there arepoints of view about how precise the data of the registers are, since there is a growing labour importconsisting of posted workers and a workforce within the black-grey market.</p><p>Some conclusions that are drawn: Societal actors would benefit from more suitable data androot causes, possible to use as tools, in both proactive and reactive purposes.</p>

corrected abstract:
<p>The work of this master thesis, at the Royal Institute of Technology, is primarily a study of root causes of fatal injuries at work, related to building and construction industries including installation. As a term, root cause has not only obvious synonyms, since the meaning can vary from word to word. The author of the thesis is discussing how a root cause should be defined with some alternatives. Here, a common nominator can be that the level of impact caused should not be built into the term. Moreover, the definition must not hinder the practical use of root causes through unintentional misunderstanding.</p><p>Fundamentally, this study is to determine whether a root cause is appropriate for use related to accident prevention at work. Consequently, the author found 19 of them organised under these principal headings: responsibility, risk analysis, communication, and protective measure.</p><p>This study is accomplished through a qualitative research methodology only, based on a literature study. A substantial part of it was to analyse court judgments related to fatal, and work-related accidents, and if possible, find root causes. There were two interviews by focus groups with one employer organisation, Byggföretagen, and one trade union, Byggnads, represented.</p><p>Since one of the purposes was to make a suitability assessment of the root causes and seriously discuss these in the light of the introduced trails, a major part of this thesis is about the underlying reasons for the fatal accidents of the industries. Hence, the result section of the report is worthwhile to read since it is based on wide and deep knowledge by the interviewed representatives in work environment related issues of the building and construction industries.</p><p>Another part of the thesis is highlighting how useful statistics of accidents, from the Swedish Work Environment Authority (SWEA) and related sources, are. From the authors side, there are points of view about how precise the data of the registers are, since there is a growing labour import consisting of posted workers and a workforce within the black-grey market.</p><p>Some conclusions that are drawn: Societal actors would benefit from more suitable data and root causes, possible to use as tools, in both proactive and reactive purposes.</p>
----------------------------------------------------------------------
In diva2:1455037 abstract is: <p>State-of-the-art fluorescent imaging research is strictly limited to eight fluorophore labels duringthe study of intercellular interactions among organelles. The number of excited fluorophore colorsis restricted due to overlap in the narrow spectra of visual wavelength. However, this requires aconsiderable effort of analysis to be able to tell the overlapping signals apart. Significant overlapalready occurs with the use of more than four fluorophores and is leaving researchers limited to asmall number of labels and the hard decision to prioritize between cellular labels to use.</p><p>Except for the physical limitations of fluorescent labeling, the labeling itself causes behavioralabnormalities due to sample perturbation. In addition to this, the labeling dye or dye-adjacentantibodies are potentially causing phototoxicity and photobleaching thus limiting the timescale oflive cell imaging. Nontoxic imaging modalities such as transmitted-light microscopes, such asbright-field and phase contrast methods, are available but not nearly achieving images of thespecificity as when using fluorophore labeling.</p><p>An approach that could increase the number of organelles simultaneously studied withfluorophore labels, while being cost-effective and nontoxic as transmitted-light microscopes wouldbe an invaluable tool in the quest to enhance knowledge of cellular studies of organelles. Here wepresent a deep learning solution, using convolutional neural networks built to predict thefluorophore labeling effect on the nucleus, from a transmitted-light input. This solution renders afluorescent channel available for another marker and would eliminate the process of labeling thenucleus with dye or dye-conjugated antibodies by instead using deep convolutional neuralnetworks.</p>

corrected abstract:
<p>State-of-the-art fluorescent imaging research is strictly limited to eight fluorophore labels during the study of intercellular interactions among organelles. The number of excited fluorophore colors is restricted due to overlap in the narrow spectra of visual wavelength. However, this requires a considerable effort of analysis to be able to tell the overlapping signals apart. Significant overlap already occurs with the use of more than four fluorophores and is leaving researchers limited to a small number of labels and the hard decision to prioritize between cellular labels to use.</p><p>Except for the physical limitations of fluorescent labeling, the labeling itself causes behavioral abnormalities due to sample perturbation. In addition to this, the labeling dye or dye-adjacent antibodies are potentially causing phototoxicity and photobleaching thus limiting the timescale of live cell imaging. Nontoxic imaging modalities such as transmitted-light microscopes, such as bright-field and phase contrast methods, are available but not nearly achieving images of the specificity as when using fluorophore labeling.</p><p>An approach that could increase the number of organelles simultaneously studied with fluorophore labels, while being cost-effective and nontoxic as transmitted-light microscopes would be an invaluable tool in the quest to enhance knowledge of cellular studies of organelles. Here we present a deep learning solution, using convolutional neural networks built to predict the fluorophore labeling effect on the nucleus, from a transmitted-light input. This solution renders a fluorescent channel available for another marker and would eliminate the process of labeling the nucleus with dye or dye-conjugated antibodies by instead using deep convolutional neural networks.</p>
----------------------------------------------------------------------
In diva2:1673165 abstract is: <p>Non-invasive methods to evaluate skeletal muscle oxidative capacity have beenemerging as a viable substitute for invasive methods in recent years. One ofthose methods utilises near-infrared spectroscopy (NIRS) to calculate V O2mrecovery off-kinetics following an exercise. The data analysis of the measuredsignals from the NIRS is still done manually in a time-consuming and dauntingprocess. The present thesis aimed to develop software, associated with theNIRS method, capable of analysing the recovery from a repeated arterialocclusion protocol following an exercise to assess muscle oxidative capacity.Additionally, to analyse the recovery from ischemic preconditioning as a singletest to assess muscle oxidative capacity. A method that has never been utilisedbefore.11 active, healthy subjects were analysed to calculate their recovery rate.Subjects underwent ischemic preconditioning before exercising for 6 minutesat 80% of gas exchange threshold. A repeated arterial occlusion protocol wascarried out after the exercise. A software was developed in R that utilised linearregression as well as exponential fitting to calculate the recovery rate of eachsubject during both the ischemic preconditioning and the occlusion protocol.The calculated results were compared to predetermined recovery rate results ofeach subject. The calculated results of the repeated arterial occlusion protocolgave similar results to the predetermined ones and even more data on eachsubject’s recovery from an exercise. The calculated results of the ischemicpreconditioning were promising and implied that ischemic preconditioning asa single test can be utilised as a method to assess muscle oxidative capacity.However, further research is required to confirm it. </p>

corrected abstract:
<p>Non-invasive methods to evaluate skeletal muscle oxidative capacity have been emerging as a viable substitute for invasive methods in recent years. One of those methods utilises near-infrared spectroscopy (NIRS) to calculate <em>V O<sub>2</sub>m</em> recovery off-kinetics following an exercise. The data analysis of the measured signals from the NIRS is still done manually in a time-consuming and daunting process. The present thesis aimed to develop software, associated with the NIRS method, capable of analysing the recovery from a repeated arterial occlusion protocol following an exercise to assess muscle oxidative capacity. Additionally, to analyse the recovery from ischemic preconditioning as a single test to assess muscle oxidative capacity. A method that has never been utilised before.</p><p>11 active, healthy subjects were analysed to calculate their recovery rate. Subjects underwent ischemic preconditioning before exercising for 6 minutes at 80% of gas exchange threshold. A repeated arterial occlusion protocol was carried out after the exercise. A software was developed in R that utilised linear regression as well as exponential fitting to calculate the recovery rate of each subject during both the ischemic preconditioning and the occlusion protocol. The calculated results were compared to predetermined recovery rate results of each subject. The calculated results of the repeated arterial occlusion protocol gave similar results to the predetermined ones and even more data on each subject’s recovery from an exercise. The calculated results of the ischemic preconditioning were promising and implied that ischemic preconditioning as a single test can be utilised as a method to assess muscle oxidative capacity. However, further research is required to confirm it.</p>
----------------------------------------------------------------------
In diva2:1454447 abstract is: <p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish andcultivated fish, causing severe losses in the fish industry every year. Aquacultured fishesexists worldwide and constitutes an important food source for humans ever since overfishingstarted. Previously used methods for controlling pathogenic infections e.g. malachite green,has been proven non safe. It is today crucial to find an environmentally friendly method forcontrolling pathogenic infection such as saprolegniosis. One of six identified cellulosesynthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls andtherefore suggested as a potential target as drug control for saprolegniosis. To enablecharacterization of the structure and function of SpCesA3.1, it needs to be purified in itscatalytically active full-lengths form. The aim of this project was to test a method forheterologous expression, solubilization and purification of SpCesA3.1. The expected projectoutcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in itscatalytically active form. Solubilization and purification of SpCesA3.1 was assessedexperimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agentand IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experimentsresulted in successful heterologous expressions in yeast strain LoGSA and FGY217,transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negativepurification of SpCesA3.1. Results were compared to previous research and a protocol onhow to proceed with the optimization of solubilization and purification of SpCesA3.1 ispresented here. It is proposed that the presented protocol is applied in further research on thissubject.</p>

partal corrected: diva2:1454447: <p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish and cultivated fish, causing severe losses in the fish industry every year. Aquacultured fishes exists worldwide and constitutes an important food source for humans ever since overfishing started. Previously used methods for controlling pathogenic infections e.g. malachite green, has been proven non safe. It is today crucial to find an environmentally friendly method for controlling pathogenic infection such as saprolegniosis. One of six identified cellulose synthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls and therefore suggested as a potential target as drug control for saprolegniosis. To enable characterization of the structure and function of SpCesA3.1, it needs to be purified in its catalytically active full-lengths form. The aim of this project was to test a method for heterologous expression, solubilization and purification of SpCesA3.1. The expected project outcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in its catalytically active form. Solubilization and purification of SpCesA3.1 was assessed experimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agent and IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experiments resulted in successful heterologous expressions in yeast strain LoGSA and FGY217, transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negative purification of SpCesA3.1. Results were compared to previous research and a protocol on how to proceed with the optimization of solubilization and purification of SpCesA3.1 is presented here. It is proposed that the presented protocol is applied in further research on this subject.</p>

corrected abstract:
<p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish and cultivated fish, causing severe losses in the fish industry every year. Aquacultured fishes exists worldwide and constitutes an important food source for humans ever since overfishing started. Previously used methods for controlling pathogenic infections e.g. malachite green, has been proven non safe. It is today crucial to find an environmentally friendly method for controlling pathogenic infection such as saprolegniosis. One of six identified cellulose synthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls and therefore suggested as a potential target as drug control for saprolegniosis. To enable characterization of the structure and function of SpCesA3.1, it needs to be purified in its catalytically active full-lengths form. The aim of this project was to test a method for heterologous expression, solubilization and purification of SpCesA3.1. The expected project outcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in its catalytically active form. Solubilization and purification of SpCesA3.1 was assessed experimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agent and IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experiments resulted in successful heterologous expressions in yeast strain LoGSA and FGY217, transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negative purification of SpCesA3.1. Results were compared to previous research and a protocol on how to proceed with the optimization of solubilization and purification of SpCesA3.1 is presented here. It is proposed that the presented protocol is applied in further research on this subject.</p>
----------------------------------------------------------------------
In diva2:1454447 abstract is: <p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish andcultivated fish, causing severe losses in the fish industry every year. Aquacultured fishesexists worldwide and constitutes an important food source for humans ever since overfishingstarted. Previously used methods for controlling pathogenic infections e.g. malachite green,has been proven non safe. It is today crucial to find an environmentally friendly method forcontrolling pathogenic infection such as saprolegniosis. One of six identified cellulosesynthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls andtherefore suggested as a potential target as drug control for saprolegniosis. To enablecharacterization of the structure and function of SpCesA3.1, it needs to be purified in itscatalytically active full-lengths form. The aim of this project was to test a method forheterologous expression, solubilization and purification of SpCesA3.1. The expected projectoutcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in itscatalytically active form. Solubilization and purification of SpCesA3.1 was assessedexperimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agentand IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experimentsresulted in successful heterologous expressions in yeast strain LoGSA and FGY217,transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negativepurification of SpCesA3.1. Results were compared to previous research and a protocol onhow to proceed with the optimization of solubilization and purification of SpCesA3.1 ispresented here. It is proposed that the presented protocol is applied in further research on thissubject.</p>

Note: I am unsure why the "Sp" in italizied in just to places: "The aim of this project was to test a method for
heterologous expression, solubilization and purification of SpCesA3.1. The expected project
outcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in its
catalytically active form." However, this is the way it is in https://kth.diva-portal.org/smash/get/diva2:1454447/SUMMARY01.pdf
corrected abstract:
<p>The oomycete <em>Saprolegnia parasitica</em> is a pathogen that infects mainly wild fish and cultivated fish, causing severe losses in the fish industry every year. Aquacultured fishes exists worldwide and constitutes an important food source for humans ever since overfishing started. Previously used methods for controlling pathogenic infections e.g. malachite green, has been proven non safe. It is today crucial to find an environmentally friendly method for controlling pathogenic infection such as saprolegniosis. One of six identified cellulose synthases in <em>S. parasitica</em> (SpCesA3.1) is highly expressed in the hyphal cell walls and therefore suggested as a potential target as drug control for saprolegniosis. To enable characterization of the structure and function of SpCesA3.1, it needs to be purified in its catalytically active full-lengths form. The aim of this project was to test a method for heterologous expression, solubilization and purification of <em>Sp</em>CesA3.1. The expected project outcome was an optimized protocol for the purification of membrane protein <em>Sp</em>CesA3.1 in its catalytically active form. Solubilization and purification of SpCesA3.1 was assessed experimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agent and IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experiments resulted in successful heterologous expressions in yeast strain LoGSA and FGY217, transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negative purification of SpCesA3.1. Results were compared to previous research and a protocol on how to proceed with the optimization of solubilization and purification of SpCesA3.1 is presented here. It is proposed that the presented protocol is applied in further research on this subject.</p>
----------------------------------------------------------------------
In diva2:1454857 abstract is: <p>Breast cancer is the most common cancer and the largest cause of cancer-related deathsworldwide for women. Risk prediction of breast cancer allows individualised andpreventative treatment. Many risk factors are known, such as age and breast density, buttoday there are no validated blood biomarkers for the risk prediction of breast cancer.Using blood samples for the prediction of breast cancer risk would increase thepossibilities for a more individualised, preventive treatment before diseasemanifestation.</p><p>To work towards such a goal, protein profiles had been generated with antibodies fromthe Human Protein Atlas project in human plasma samples. A total of 711 proteins werethen analysed for their potential relationship with breast density and breast cancer. Theproteomics data was generated using multiplexed immunoassays developed atKTH/Science for Life Laboratory (SciLifeLab), Stockholm, Sweden, on plasma samplesfrom 585 participants of the KARMA project at Karolinska Institutet in Stockholm,Sweden.</p><p>In this thesis project, the main focus was to apply advanced data analysis tools, beginningwith filtering and normalisation of the data. The participants were then stratified into fiveclusters based on differences in their plasma proteomics profiles using archetypalanalysis. Differences between the archetype clusters were found for clinical parameters,such as case control status, tumour characteristics and therapy.</p><p>Using the data from hundreds of plasma proteomes, substantially different molecularprofiles were found between the clusters obtained from archetypal analysis. A hypothesiswas that these differences might be likely driven by clinical traits that remain detectablein the circulating proteome over many years. In the future, more samples and clinical aswell as other omics data will be investigated to verify the observation.</p>

corrected abstract:
<p>Breast cancer is the most common cancer and the largest cause of cancer-related deaths worldwide for women. Risk prediction of breast cancer allows individualised and preventative treatment. Many risk factors are known, such as age and breast density, but today there are no validated blood biomarkers for the risk prediction of breast cancer. Using blood samples for the prediction of breast cancer risk would increase the possibilities for a more individualised, preventive treatment before disease manifestation.</p><p>To work towards such a goal, protein profiles had been generated with antibodies from the Human Protein Atlas project in human plasma samples. A total of 711 proteins were then analysed for their potential relationship with breast density and breast cancer. The proteomics data was generated using multiplexed immunoassays developed at KTH/Science for Life Laboratory (SciLifeLab), Stockholm, Sweden, on plasma samples from 585 participants of the KARMA project at Karolinska Institutet in Stockholm, Sweden.</p><p>In this thesis project, the main focus was to apply advanced data analysis tools, beginning with filtering and normalisation of the data. The participants were then stratified into five clusters based on differences in their plasma proteomics profiles using archetypal analysis. Differences between the archetype clusters were found for clinical parameters, such as case control status, tumour characteristics and therapy.</p><p>Using the data from hundreds of plasma proteomes, substantially different molecular profiles were found between the clusters obtained from archetypal analysis. A hypothesis was that these differences might be likely driven by clinical traits that remain detectable in the circulating proteome over many years. In the future, more samples and clinical as well as other omics data will be investigated to verify the observation.</p>
----------------------------------------------------------------------
In diva2:1077206 abstract is: <p>Patients treated at intensive care units (ICUs) are failing in one or several organs and requireappropriate monitoring and treatment in order to maintain a meaningful life. Today clinicians inintensive care units (ICUs) manage a large amount of data generated from monitoring devices.The monitoring parameters can either be noted down manually on a monitoring sheet or, for some parameters, transferred automatically to storage. In both cases the information is stored withthe aim to support clinicians throughout the intensive care and be easily accessible. Patient datamanagement systems (PDMSs) facilitate ICUs to retrieve and integrate data. Before managinga new configuration of patient data system, it is required that the ICU makes careful analysis ofwhat data desired to be registered. This pilot study provides knowledge of how the monitoringis performed in an Intensive Care Unit in an emergency hospital in Stockholm.The aim of this thesis project was to collect data about what the clinicians require and whatequipment they use today for monitoring. Requirement elicitation is a technique to collectrequirements. Methods used to collect data were active observations and qualitative interviews.Patterns have been found about what the assistant nurses, nurses and physicians’ require of systems supporting the clinician’s with monitoring parameters. Assistant nurses would like tobe released from tasks of taking notes manually. They also question the need for atomized datacollection since they are present observing the patient bed-side. Nurses describe a demanding burden of care and no more activities increasing that burden of care is required. Physicians require support in order to see how an intervention leads to a certain result for individual patients.The results also show that there is information about decision support but no easy way to applythem, better than the ones used today. Clinicians state that there is a need to be able to evaluatethe clinical work with the help of monitoring parameters. The results provide knowledge about which areas the clinicians needs are not supported enough by the exciting tools.To conclude results show that depending on what profession and experience the clinicians have the demands on monitoring support di↵ers. Monitoring at the ICU is performed while observing individual patients, parameters from medical devices, results from medical tests and physical examinations. Information from all these sources is considered by the clinicians and is desired to be supported accordingly before clinicians commit to action resulting in certain treatment,diagnosis and/or care.</p>

corrected abstract:
<p>Patients treated at intensive care units (ICUs) are failing in one or several organs and require appropriate monitoring and treatment in order to maintain a meaningful life. Today clinicians in intensive care units (ICUs) manage a large amount of data generated from monitoring devices. The monitoring parameters can either be noted down manually on a monitoring sheet or, for some parameters, transferred automatically to storage. In both cases the information is stored with the aim to support clinicians throughout the intensive care and be easily accessible. Patient data management systems (PDMSs) facilitate ICUs to retrieve and integrate data. Before managing a new configuration of patient data system, it is required that the ICU makes careful analysis of what data desired to be registered. This pilot study provides knowledge of how the monitoring is performed in an Intensive Care Unit in an emergency hospital in Stockholm.</p><p>The aim of this thesis project was to collect data about what the clinicians require and what equipment they use today for monitoring. Requirement elicitation is a technique to collect requirements. Methods used to collect data were active observations and qualitative interviews.</p><p>Patterns have been found about what the assistant nurses, nurses and physicians’ require of systems supporting the clinician’s with monitoring parameters. Assistant nurses would like to be released from tasks of taking notes manually. They also question the need for atomized data collection since they are present observing the patient bed-side. Nurses describe a demanding burden of care and no more activities increasing that burden of care is required. Physicians require support in order to see how an intervention leads to a certain result for individual patients. The results also show that there is information about decision support but no easy way to apply them, better than the ones used today. Clinicians state that there is a need to be able to evaluate the clinical work with the help of monitoring parameters. The results provide knowledge about which areas the clinicians needs are not supported enough by the exciting tools.</p><p>To conclude results show that depending on what profession and experience the clinicians have the demands on monitoring support differs. Monitoring at the ICU is performed while observing individual patients, parameters from medical devices, results from medical tests and physical examinations. Information from all these sources is considered by the clinicians and is desired to be supported accordingly before clinicians commit to action resulting in certain treatment, diagnosis and/or care.</p>
----------------------------------------------------------------------
In diva2:1454837 abstract is: <p>Recombinant silk proteins show promising use in applications such as tissue engineering andas materials for medical purposes. One of these proteins, 4RepCT, has earlier beenfunctionalized with various peptide domains and proteins to introduce desired functions.Genetic fusion has mainly been used, but also Sortase A coupling, allowing production indifferent expression hosts of the proteins to be coupled. Previous studies using the enzymeSortase A suggest that five glycines at the N-terminal of the silk protein improve theefficiency of the coupling. The aim of the project was to produce such a silk protein and toevaluate the Sortase A coupling with the IgG-binding domain Z and two single-chain variablefragments, in solution and to silk coatings. First, production of a silk protein with a solubilityand purification tag at the C-terminal was investigated but provided insufficient amounts dueto difficulties in protein expression. Instead a new construct, similar to a standard silk proteinwith the tag on the N-terminal side of the protein, was produced to obtain G5-FN-4RepCT,which was able to self-assemble into silk fibers and nanowires. The new silk protein wasfunctionalized by Sortase A coupling in solution and the efficiency was analyzed using SDSPAGE.G5-FN-4RepCT was proven to achieve an increased product amount and formationrate, and lower Sortase A concentrations could also be used compared to the standard silk, GFN-4RepCT. The sortase coupling to silk coatings, followed by functional binding analysis,were evaluated using a biosensor. The results indicated coupling to the coatings, however,also unspecific binding of Sortase A and further analysis is required. In conclusion, thesortase coupling in solution was improved using the new G5-silk protein, andfunctionalization to other silk materials, e.g. nanowires, show potential for various medicalapplications, such as cancer immunotherapy.</p>

w='SDSPAGE' val={'c': 'SDS-PAGE', 's': 'diva2:1454837', 'n': 'correct in original'}

corrected abstract:
<p>Recombinant silk proteins show promising use in applications such as tissue engineering and as materials for medical purposes. One of these proteins, 4RepCT, has earlier been functionalized with various peptide domains and proteins to introduce desired functions. Genetic fusion has mainly been used, but also Sortase A coupling, allowing production in different expression hosts of the proteins to be coupled. Previous studies using the enzyme Sortase A suggest that five glycines at the N-terminal of the silk protein improve the efficiency of the coupling. The aim of the project was to produce such a silk protein and to evaluate the Sortase A coupling with the IgG-binding domain Z and two single-chain variable fragments, in solution and to silk coatings. First, production of a silk protein with a solubility and purification tag at the C-terminal was investigated but provided insufficient amounts due to difficulties in protein expression. Instead a new construct, similar to a standard silk protein with the tag on the N-terminal side of the protein, was produced to obtain G<sub>5</sub>-FN-4RepCT, which was able to self-assemble into silk fibers and nanowires. The new silk protein was functionalized by Sortase A coupling in solution and the efficiency was analyzed using SDS-PAGE. G<sub>5</sub>-FN-4RepCT was proven to achieve an increased product amount and formation rate, and lower Sortase A concentrations could also be used compared to the standard silk, GFN-4RepCT. The sortase coupling to silk coatings, followed by functional binding analysis, were evaluated using a biosensor. The results indicated coupling to the coatings, however, also unspecific binding of Sortase A and further analysis is required. In conclusion, the sortase coupling in solution was improved using the new G<sub>5</sub>-silk protein, and functionalization to other silk materials, e.g. nanowires, show potential for various medical applications, such as cancer immunotherapy.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-24
======================================================================
title: "Production and BiochemicalCharacterisation of Glycoside Hydrolases from Chitinophaga pinensis"
==> "Production and Biochemical Characterisation of Glycoside Hydrolases from <em>Chitinophaga pinensis</em>"

In diva2:1454416 abstract is: <p>The increased use of chemical pesticides worldwide poses a problem where they can leak out of thesoil and cause damage to plants, animals and even humans. Because of this, interest in biologicalpesticides as an alternative to synthetic chemicals is increasing. One potential candidate for bacterialbiocontrol of fungal phytopathogens is the soil bacterium Chitinophaga pinensis which has beenshown to grow on fungal fruiting bodies and produce anti-fungal peptides that disrupt themembrane of the fungi. It is also predicted to produce a large number of enzymes that can attack thefungal cell wall. From genome sequencing, two proteins of interest have been found: a GH16glycoside hydrolase and one consisting of a GH64 glycoside hydrolase domain and a CBM6carbohydrate binding domain. This study shows that all proteins and protein domains can beeffectively and reliably expressed and purified from E. coli. The GH16 protein shows potential activityon β-1,3-galactan, as predicted. The two-domain protein shows activity on both β-1,3-glucan and β-1,6-glucan, compared to the single GH64 domain alone, which only shows the expected activity on β-1,3-glucan. This study has shown that the CBM􀏲 binds to β-1,6-glucan, something not observedpreviously and essential for GH64 to have any activity β-1,6-glucan. I have also shown that CBM6 isincapable of binding to β-1,6-glucan by itself and only binds when part of the full length protein.These data give us a better understanding of how C. pinensis is able to degrade polysaccharides fromnatural biomass, gives insight into the biology of this soil bacterium, and helps with the developmentof a biological pesticide.</p>


corrected abstract:
<p>The increased use of chemical pesticides worldwide poses a problem where they can leak out of the soil and cause damage to plants, animals and even humans. Because of this, interest in biological pesticides as an alternative to synthetic chemicals is increasing. One potential candidate for bacterial biocontrol of fungal phytopathogens is the soil bacterium <em>Chitinophaga pinensis</em> which has been shown to grow on fungal fruiting bodies and produce anti-fungal peptides that disrupt the membrane of the fungi. It is also predicted to produce a large number of enzymes that can attack the fungal cell wall. From genome sequencing, two proteins of interest have been found: a GH16 glycoside hydrolase and one consisting of a GH64 glycoside hydrolase domain and a CBM6 carbohydrate binding domain. This study shows that all proteins and protein domains can be effectively and reliably expressed and purified from <em>E. coli</em>. The GH16 protein shows potential activity on β-1,3-galactan, as predicted. The two-domain protein shows activity on both β-1,3-glucan and β-1,6-glucan, compared to the single GH64 domain alone, which only shows the expected activity on β-1,3-glucan. This study has shown that the CBM6 binds to β-1,6-glucan, something not observed previously and essential for GH64 to have any activity β-1,6-glucan. I have also shown that CBM6 is incapable of binding to β-1,6-glucan by itself and only binds when part of the full length protein. These data give us a better understanding of how <em>C. pinensis</em> is able to degrade polysaccharides from natural biomass, gives insight into the biology of this soil bacterium, and helps with the development of a biological pesticide.</p>
----------------------------------------------------------------------
In diva2:1680873 abstract is: <p>Perioperative hypotension (PH), commonly a side effect of anesthesia,is one of the main mortality causes during the 30 posterior days of asurgical procedure. Novel research lines propose combining machinelearning algorithms with the Arterial Blood Pressure (ABP) waveform tonotify healthcare professionals about the onset of a hypotensive event withtime advance and prevent its occurrence. Nevertheless, ABP waveformsare heterogeneous among patients, consequently, a general model maypresent different predictive capabilities per individual. This project aimsat improving the performance of an artificial neural network (ANN) topredict hypotension events with time advance by applying personalizedmachine learning techniques, like data grouping and domain adaptation. Wehypothesize its implementation will allow us to cluster patients with similardemographic and ABP discriminative characteristics and tailor the modelto each specific group, resulting in a worst overall but better individualperformance. Results present a slight but not clinical significant improvementwhen comparing AUROC values between the group-specific and the generalmodel. This suggests even though personalization could be a good approach todealing with patient heterogeneity, the clustering algorithm presented in thisthesis is not sufficient to make the ANN clinically feasible.</p>


corrected abstract:
<p>Perioperative hypotension (PH), commonly a side effect of anesthesia, is one of the main mortality causes during the 30 posterior days of a surgical procedure. Novel research lines propose combining machine learning algorithms with the Arterial Blood Pressure (ABP) waveform to notify healthcare professionals about the onset of a hypotensive event with time advance and prevent its occurrence. Nevertheless, ABP waveforms are heterogeneous among patients, consequently, a general model may present different predictive capabilities per individual. This project aims at improving the performance of an artificial neural network (ANN) to predict hypotension events with time advance by applying personalized machine learning techniques, like data grouping and domain adaptation. We hypothesize its implementation will allow us to cluster patients with similar demographic and ABP discriminative characteristics and tailor the model to each specific group, resulting in a worst overall but better individual performance. Results present a slight but not clinical significant improvement when comparing AUROC values between the group-specific and the general model. This suggests even though personalization could be a good approach to dealing with patient heterogeneity, the clustering algorithm presented in this thesis is not sufficient to make the ANN clinically feasible.</p>
----------------------------------------------------------------------
In diva2:1038973 abstract is: <p>Genetically encoded, site-specific incorporation of unnatural amino acids (UAA)into proteins through selective recoding of an amber stop codon provides apowerful route for expressing synthetic proteins in living cells. Recoding of theamber stop codon is achieved by introducing an amber suppressortRNA/synthetase pair orthogonal to the endogenous tRNA complement intocells. Methanosarcina is a methane producing archaea with the unusualcapability of suppressing the stop codon (specifically the amber codon). Bysuppressing the amber codon Methanosarcina facilitate the incorporation of thenon-canonical amino acid pyrrolysine (pyl). The suppressing mechanismoriginates from a evolutionary unique Pyrrolysyl-tRNA synthetase (PylRS) and itsmatching tRNApyl. The PylRS has been further evolved and modified to allowincorporation of a wide range of UAAs. Amber suppression is today used tocontrol and study protein function in living cells. By making a series of wellcontrolledexperiments with HEK293T cells we aimed to develop this techniqueinto a robust and general tool for mammalian cell biology. Specifically we weretesting the incorporation of the unnatural amino acid bicyclononyne (BCN) by aset of known PylRS mutants. Our results suggest the mutant aaRS PylRS “AF” isthe most robust and efficient synthetase for BCN. We have improved ambersuppression by determining which factors leads to a more efficient method andsimultaneously decreasing the cost of the method.</p>

w='tRNApyl' val={'c': 'tRNA<sup>pyl</sub>', 's': 'diva2:1038973'}

corrected abstract:
<p>Genetically encoded, site-specific incorporation of unnatural amino acids (UAA) into proteins through selective recoding of an amber stop codon provides a powerful route for expressing synthetic proteins in living cells. Recoding of the amber stop codon is achieved by introducing an amber suppressor tRNA/synthetase pair orthogonal to the endogenous tRNA complement into cells. <em>Methanosarcina</em> is a methane producing archaea with the unusual capability of suppressing the stop codon (specifically the amber codon). By suppressing the amber codon <em>Methanosarcina</em> facilitate the incorporation of the non-canonical amino acid pyrrolysine (pyl). The suppressing mechanism originates from a evolutionary unique Pyrrolysyl-tRNA synthetase (PylRS) and its matching tRNA<sup>pyl</sub>. The PylRS has been further evolved and modified to allow incorporation of a wide range of UAAs. Amber suppression is today used to control and study protein function in living cells. By making a series of well-controlled experiments with HEK293T cells we aimed to develop this technique into a robust and general tool for mammalian cell biology. Specifically we were testing the incorporation of the unnatural amino acid bicyclononyne (BCN) by a set of known PylRS mutants. Our results suggest the mutant aaRS PylRS “AF” is the most robust and efficient synthetase for BCN. We have improved amber suppression by determining which factors leads to a more efficient method and simultaneously decreasing the cost of the method.</p>
----------------------------------------------------------------------
In diva2:1665663 abstract is: <p>Convolutional neural networks (CNN) have come a long way and can be trained toclassify many of the objects around us. Despite this, researchers do not fullyunderstand how CNN models learn features (edges, shapes, contours, etc.) fromdata. For this reason, it is reasonable to investigate if a CNN model can learn toclassify objects under extreme conditions. An example of such an extreme conditioncould be a car that drives towards the camera at night, and therefore does not haveany distinct features because the light from the headlights covers large parts of thecar.The aim of this thesis is to investigate how the performance of a CNN model isaffected, when trained on objects under extreme conditions. A YOLOv4 model willbe trained on three different extreme cases: light polluted vehicles, nighttimeobjects and snow-covered vehicles. A validation will then be conducted on a testdataset to see if the performance decreases or improves, compared to when themodel trained is on normal conditions. Generally, the training was stable for allextreme cases and the results show an improved or similar performance incomparison to the normal cases. This indicates that models can be trained with allextreme cases. Snow-covered vehicles with mosaic data augmentation and the IOUthreshold 0,25 had the best overall performance compared to the normal cases, witha difference of +14,95% in AP for cars, −0,73% in AP for persons, +8,08% in AP fortrucks, 0 in precision and +9% in recall. </p>


corrected abstract:
<p>Convolutional neural networks (CNN) have come a long way and can be trained to classify many of the objects around us. Despite this, researchers do not fully understand how CNN models learn features (edges, shapes, contours, etc.) from data. For this reason, it is reasonable to investigate if a CNN model can learn to classify objects under extreme conditions. An example of such an extreme condition could be a car that drives towards the camera at night, and therefore does not have any distinct features because the light from the headlights covers large parts of the car.</p><p>The aim of this thesis is to investigate how the performance of a CNN model is affected, when trained on objects under extreme conditions. A YOLOv4 model will be trained on three different extreme cases: light polluted vehicles, nighttime objects and snow-covered vehicles. A validation will then be conducted on a test dataset to see if the performance decreases or improves, compared to when the model trained is on normal conditions. Generally, the training was stable for all extreme cases and the results show an improved or similar performance in comparison to the normal cases. This indicates that models can be trained with all extreme cases. Snow-covered vehicles with mosaic data augmentation and the IOU threshold 0,25 had the best overall performance compared to the normal cases, with a difference of +14,95% in AP for cars, −0,73% in AP for persons, +8,08% in AP for trucks, 0 in precision and +9% in recall.</p>
----------------------------------------------------------------------
title: "Improving Type 1 DiabetesPatients’ Quality of LifeThrough Data Collection"
==> "Improving Type 1 Diabetes Patients’ Quality of Life Through Data Collection"


In diva2:1529839 abstract is: <p>Type 1 diabetes (T1D) is a complex chronic disease without treatment. When anindividual is diagnosed with T1D they are taught how to monitor blood glucoselevel as well as external insulin administration. While this management strategyhelps prolong the individual’s life, there are other lifestyle factors not consideredthat negatively impact the patients’ life.</p><p>This thesis aims to investigate the types of data that can be gathered to benefit T1D patients and healthcare specialists by improving life quality.</p><p>To do so, the work employs a literature review and its qualitative analysis, aninterviewing process and its qualitative analysis as well as overall findings analysiswhere data is interpreted in order to identify areas of interest, common topics andtrends. 43 literature publications, 3 healthcare professionals and 3 T1D patientsparticipated in this study.</p><p>Results show initial education is limited leaving patients to initiate their ownresearch which could be a cause for stress. Technological integration does not seemchallenging provided the right training of more complex solutions. Education asa means to reduce stress seems effective both for patients but also for their socialnetworks. Finally, there are currently useful data markers not being used that couldprovide a wider range of information to healthcare specialists aiding in better patientcare and improved T1D patients’ Quality of Life (QOL).</p><p>To conclude, T1D is a complex chronic disease that requires both clinical andnon-clinical interventions. It is not sufficient to only address its clinical implicationsbut is important to investigate factors that impact the lifestyle and quality of life. Byextracting proper data markers, collecting and analyzing them, it is believed thattechnology can assist healthcare and ultimately improve T1D patient’s quality oflife.</p>

corrected abstract:
<p>Type 1 diabetes (T1D) is a complex chronic disease without treatment. When an individual is diagnosed with T1D they are taught how to monitor blood glucose level as well as external insulin administration. While this management strategy helps prolong the individual’s life, there are other lifestyle factors not considered that negatively impact the patients’ life.</p><p>This thesis aims to investigate the types of data that can be gathered to benefit T1D patients and healthcare specialists by improving life quality.</p><p>To do so, the work employs a literature review and its qualitative analysis, an interviewing process and its qualitative analysis as well as overall findings analysis where data is interpreted in order to identify areas of interest, common topics and trends. 43 literature publications, 3 healthcare professionals and 3 T1D patients participated in this study.</p><p>Results show initial education is limited leaving patients to initiate their own research which could be a cause for stress. Technological integration does not seem challenging provided the right training of more complex solutions. Education as a means to reduce stress seems effective both for patients but also for their social networks. Finally, there are currently useful data markers not being used that could provide a wider range of information to healthcare specialists aiding in better patient care and improved T1D patients’ Quality of Life (QOL).</p><p>To conclude, T1D is a complex chronic disease that requires both clinical and non-clinical interventions. It is not sufficient to only address its clinical implications but is important to investigate factors that impact the lifestyle and quality of life. By extracting proper data markers, collecting and analyzing them, it is believed that technology can assist healthcare and ultimately improve T1D patient’s quality of life.</p>
----------------------------------------------------------------------
title: "Sweat Lactate Sensor Integrated with Microfluidicand Iontophoresis System for Analysing Sweat without Physical Activity"
==> "Sweat Lactate Sensor Integrated with Microfluidic and Iontophoresis System for Analysing Sweat without Physical Activity"


In diva2:1692807 abstract is: <p>Background: Understanding lactate levels can provide important information aboutour body’s condition. For athletes, this can improve their training and prevent earlyfatigue. In healthcare, monitoring lactate can provide valuable information and potentially prevent life-threatening episodes. Lactate can be measured non-invasively byanalyzing sweat. This is advantageous over the typical blood sampling since it is saferand pain-free. Sweat can be stimulated by using a method called iontophoresis. Itapplies a small current between two electrodes placed on the skin’s surface, deliveringsubstances to the inner layer of the skin.</p><p>Objectives: The aim of this study was to design a device that implements iontophoresis to activate sweat production and uses a microfluidic system to collect thesweat and deliver it to a lactate sensor and provide a signal.</p><p>Methodology: A device was designed in AutoCAD and 3D printed. It was improvedby trial and error. A sweat collecting test was performed to validate the iontophoresissystem. The efficiency of the microfluidic system was tested by recording the time ittakes to collect enough sweat to get a lactate signal. Finally, calibration tests wereperformed to validate the lactate signal in the form of batch-mode and flow-mode.</p><p>Results: The sweat collection test produced 27 µL of sweat in 15 minutes and 49 µLin 30 minutes. The microfluidic system delivered sweat to the sensor and activated itin less than 3 minutes. The linearity of the batch-mode calibration, R2-value, was0.9994, and for the flow-mode it was 0.8908.Conclusions: The iontophoresis system stimulated sweat production, which themicrofluidic system delivered to the lactate sensor successfully. The lactate sensorwas implemented into the device, and a signal was detected. However, it could not becalibrated efficiently enough to display the electric signal as a lactate concentration.</p>

corrected abstract:
<p><strong>Background</strong>: Understanding lactate levels can provide important information about our body’s condition. For athletes, this can improve their training and prevent early fatigue. In healthcare, monitoring lactate can provide valuable information and potentially prevent life-threatening episodes. Lactate can be measured non-invasively by analyzing sweat. This is advantageous over the typical blood sampling since it is safer and pain-free. Sweat can be stimulated by using a method called iontophoresis. It applies a small current between two electrodes placed on the skin’s surface, delivering substances to the inner layer of the skin.</p><p><strong>Objectives</strong>: The aim of this study was to design a device that implements iontophoresis to activate sweat production and uses a microfluidic system to collect the sweat and deliver it to a lactate sensor and provide a signal.</p><p><strong>Methodology</strong>: A device was designed in AutoCAD and 3D printed. It was improved by trial and error. A sweat collecting test was performed to validate the iontophoresis system. The efficiency of the microfluidic system was tested by recording the time it takes to collect enough sweat to get a lactate signal. Finally, calibration tests were performed to validate the lactate signal in the form of batch-mode and flow-mode.</p><p><strong>Results</strong>: The sweat collection test produced 27 µL of sweat in 15 minutes and 49 µL in 30 minutes. The microfluidic system delivered sweat to the sensor and activated it in less than 3 minutes. The linearity of the batch-mode calibration, R<sup>2</sup>-value, was 0.9994, and for the flow-mode it was 0.8908.</p><p><strong>Conclusions</strong>: The iontophoresis system stimulated sweat production, which the microfluidic system delivered to the lactate sensor successfully. The lactate sensor was implemented into the device, and a signal was detected. However, it could not be calibrated efficiently enough to display the electric signal as a lactate concentration.</p>
----------------------------------------------------------------------
In diva2:734297 abstract is: <p>The Arterial Spin Labelling (ASL) method is a Magnetic Resonance technique used toquantify the cerebral perfusion. It has the big advantage to be non-invasive so doesn’tneed the injection of any contrast agent. But due to a relatively low Signal-to-NoiseRatio (SNR) of the signal acquired (only approximately 1% of the image intensity), ithas been hampered to be widely used in a clinical setting so far.The primary objective of this project is to make the method more robust by improvingthe quality of the images, the SNR, and by reducing the acquisition time. DifferentASL protocols with different sets of parameters have been investigated. The modificationsperformed on the protocol have been investigated by analyzing images acquired onhealthy volunteers. An optimized protocol leading to a good trade-off between the differentaspects of the method, has been suggested. It is characterized by a 3:43:44:0mm3with a two-segment acquisition.A more advanced ASL method implies the acquisition of images at different inversiontimes (TI), which is called the mutli-TI method. The influence of the range of TI used inthe method has been explored. An optimized TI range (from 410ms to 3860ms, sampledevery 150ms) has been suggested to make the ASL method as performant as possible.A numerical model and a fitting algorithm have been used to extract the informationon the perfusion from the images acquired. Different models have been investigated aswell as their influence on the reliability of the results.Finally, a criterion has been implemented to evaluate the reliability of the results sothat the clinician or the user of the method can figure out how much he can count onthe results provided by the method.</p>


w='mutli-TI' val={'c': 'multi-TI', 's': 'diva2:734297', 'n': 'error in original'}

corrected abstract:
<p>The Arterial Spin Labelling (ASL) method is a Magnetic Resonance technique used to quantify the cerebral perfusion. It has the big advantage to be non-invasive so doesn’t need the injection of any contrast agent. But due to a relatively low Signal-to-Noise Ratio (SNR) of the signal acquired (only approximately 1% of the image intensity), it has been hampered to be widely used in a clinical setting so far.</p><p>The primary objective of this project is to make the method more robust by improving the quality of the images, the SNR, and by reducing the acquisition time. Different ASL protocols with different sets of parameters have been investigated. The modifications performed on the protocol have been investigated by analyzing images acquired on healthy volunteers. An optimized protocol leading to a good trade-off between the different aspects of the method, has been suggested. It is characterized by a 3.4×3.4×4.0mm<sup>3</sup> with a two-segment acquisition.</p><p>A more advanced ASL method implies the acquisition of images at different inversion times (TI), which is called the mutli-TI method. The influence of the range of TI used in the method has been explored. An optimized TI range (from 410ms to 3860ms, sampled every 150ms) has been suggested to make the ASL method as performant as possible.</p><p>A numerical model and a fitting algorithm have been used to extract the information on the perfusion from the images acquired. Different models have been investigated as well as their influence on the reliability of the results.</p><p>Finally, a criterion has been implemented to evaluate the reliability of the results so that the clinician or the user of the method can figure out how much he can count on the results provided by the method.</p>
----------------------------------------------------------------------
In diva2:1884448 abstract is: <p>This thesis addresses the pressing issue of frailty management in an aging population,aiming to develop a simulation model to evaluate the effectiveness of preventioninterventions among elderly individuals. Through a systematic approach integratingliterature reviews, interviews, and computational modeling, five separate simulationmodels are created to analyze intervention effects on frailty components. Theresults highlight significant differences in intervention impacts, with nutritionalinterventions notably delaying frailty onset in slowness and activity components byup to 4.5 and 26 years, respectively. Physical interventions also play a crucial rolein delaying frailty, particularly in slowness and activity. However, combinationinterventions do not simply sum benefits, suggesting potential interactions amonginterventions. Additionally, cognitive interventions exhibit notable impacts onexhaustion and weakness components, emphasizing their multifaceted nature inpromoting overall well-being in older adults. The simulation models underscorethe importance of considering intervention compatibility and synergistic effects.This research contributes evidence-based insights into effective frailty preventionstrategies, bridging theoretical understanding with practical application for healthcareprofessionals, policymakers, and stakeholders involved in promoting healthyaging.</p>


corrected abstract:
<p>This thesis addresses the pressing issue of frailty management in an aging population, aiming to develop a simulation model to evaluate the effectiveness of prevention interventions among elderly individuals. Through a systematic approach integrating literature reviews, interviews, and computational modeling, five separate simulation models are created to analyze intervention effects on frailty components. The results highlight significant differences in intervention impacts, with nutritional interventions notably delaying frailty onset in slowness and activity components by up to 4.5 and 26 years, respectively. Physical interventions also play a crucial role in delaying frailty, particularly in slowness and activity. However, combination interventions do not simply sum benefits, suggesting potential interactions among interventions. Additionally, cognitive interventions exhibit notable impacts on exhaustion and weakness components, emphasizing their multifaceted nature in promoting overall well-being in older adults. The simulation models underscore the importance of considering intervention compatibility and synergistic effects. This research contributes evidence-based insights into effective frailty prevention strategies, bridging theoretical understanding with practical application for healthcare professionals, policymakers, and stakeholders involved in promoting healthy aging.</p>
----------------------------------------------------------------------
In diva2:1763199 abstract is: <p>A cost-effective small robot car that is remote-controlled via Wi-Fi hasbeen designed and tested for inspecting and identifying potentialhazards at accident sites. The robot car is intended for reconnaissanceand inspection purposes and can contribute to formulating anadequate action plan. Wheel suspension and wheels were constructedusing a 3D printer. The finished robot car is equipped with theRaspberry Pi microcontroller, which has severalfeatures that make ituseful in various scenarios. There is a camera that allows remoteinspection of the car's surroundings. In addition to the camera, thereare three sensors connected to the Raspberry Pi unit: a gas sensor todetect dangerous gases, an ultrasonic sensor to measure the distanceto the nearest object, and a temperature sensor to measure theambient temperature. The robot car uses a motor control module tocontrol its movement and two servo motors to enable the rotation ofthe camera in the vertical and horizontal directions. The robot car ispowered by a battery, and two voltage converters are used to regulatethe voltage to the motor control module and the Raspberry Pi unit. Byintegrating these components into a single unit and programming theRaspberry Pi unit to control them, the robot car can effectively assistin investigating and managing potential hazards.</p>


Note: "severalfeatures" is set as one word in the thesis.
corrected abstract:
<p>A cost-effective small robot car that is remote-controlled via Wi-Fi has been designed and tested for inspecting and identifying potential hazards at accident sites. The robot car is intended for reconnaissance and inspection purposes and can contribute to formulating an adequate action plan. Wheel suspension and wheels were constructed using a 3D printer. The finished robot car is equipped with the Raspberry Pi microcontroller, which has severalfeatures that make it useful in various scenarios. There is a camera that allows remote inspection of the car's surroundings. In addition to the camera, there are three sensors connected to the Raspberry Pi unit: a gas sensor to detect dangerous gases, an ultrasonic sensor to measure the distance to the nearest object, and a temperature sensor to measure the ambient temperature. The robot car uses a motor control module to control its movement and two servo motors to enable the rotation of the camera in the vertical and horizontal directions. The robot car is powered by a battery, and two voltage converters are used to regulate the voltage to the motor control module and the Raspberry Pi unit. By integrating these components into a single unit and programming the Raspberry Pi unit to control them, the robot car can effectively assist in investigating and managing potential hazards.</p>
----------------------------------------------------------------------
In diva2:1441893 abstract is: <p>Three-dimensional (3D) printing has an important role for fabrication of degradable scaffoldsfor soft tissue regeneration. Among the 3D printing techniques, photopolymerization-based 3Dprinting is one of fastest growing, offering environmental benefits and high precision of 3Dobjects. In this approach, photocurable macromonomers/monomers are cross-linked layer bylayer in the presence of photoinitiators under visible or UV light to fabricate 3D designedobjects. However, a limited biomedical material selection has prevented it from spreading overclinical application. Furthermore, poly(ε-caprolactone), a common degradable polymer usedfor 3D printing, shows not satisfactory physical properties for soft tissue regeneration. Thedearth of materials with proper properties raises the need for novel degradable materials,which should be not only compatible for photopolymerization-based 3D printing but alsosuitable for soft and gel-like scaffold fabrication.</p><p>Here, the aim was to design photocurable macromonomers consisting of oligo(ε-caprolactoneran-p-dioxanone), oCLDX, with acrylate chain-end groups. A metal-free synthetic strategy wasdeveloped for the bulk ring-opening of ε-caprolactone (CL) and p-dioxanone (DX) at roomtemperature using diphenyl phosphate (DPP) as organocatalyst and multifunctional initiators.</p><p>The oligomers had low dispersity (&lt;1.2) and targeted molecular weight around 2000 g mol-1.The random sequence and the control over chain growth of oCLDXs were confirmed byreactivity ratios using 1D and 2D NMR analysis. Kinetics study of co-oligomerizationdemonstrated that within DPP-catalysed reaction, DX possessed higher reactivity than CL andthe ring-opening co-oligomerization followed an activated monomer mechanism (AMM). Thetopology of the co-oligomers could also be varied by using different alcohol initiators.</p><p>The co-oligomers possessed lower degree of crystallinity than homopolymers of DX or CL and,depending on the composition, they were liquid at room temperature. The lower melting pointand gel-like appearance make them good candidates for photopolymerization-based 3Dprinting. The suitability toward photopolymerization was proven for the ethylene glycol-initiatedco-oligomer containing 30 mol% of DX. The cross-linked gels were soft but brittle and showedgood water uptake capacity.</p>

w='glycol-initiatedco-oligomer' val={'c': 'glycol-initiated co-oligomer', 's': 'diva2:1441893', 'n': 'correct in original'}

corrected abstract:
<p>Three-dimensional (3D) printing has an important role for fabrication of degradable scaffolds for soft tissue regeneration. Among the 3D printing techniques, photopolymerization-based 3D printing is one of fastest growing, offering environmental benefits and high precision of 3D objects. In this approach, photocurable macromonomers/monomers are cross-linked layer by layer in the presence of photoinitiators under visible or UV light to fabricate 3D designed objects. However, a limited biomedical material selection has prevented it from spreading over clinical application. Furthermore, poly(ε-caprolactone), a common degradable polymer used for 3D printing, shows not satisfactory physical properties for soft tissue regeneration. The dearth of materials with proper properties raises the need for novel degradable materials, which should be not only compatible for photopolymerization-based 3D printing but also suitable for soft and gel-like scaffold fabrication.</p><p>Here, the aim was to design photocurable macromonomers consisting of oligo(ε-caprolactone-ran-p-dioxanone), oCLDX, with acrylate chain-end groups. A metal-free synthetic strategy was developed for the bulk ring-opening of ε-caprolactone (CL) and p-dioxanone (DX) at room temperature using diphenyl phosphate (DPP) as organocatalyst and multifunctional initiators.</p><p>The oligomers had low dispersity (&lt;1.2) and targeted molecular weight around 2000 g mol<sup>-1</sup>. The random sequence and the control over chain growth of oCLDXs were confirmed by reactivity ratios using 1D and 2D NMR analysis. Kinetics study of co-oligomerization demonstrated that within DPP-catalysed reaction, DX possessed higher reactivity than CL and the ring-opening co-oligomerization followed an activated monomer mechanism (AMM). The topology of the co-oligomers could also be varied by using different alcohol initiators.</p><p>The co-oligomers possessed lower degree of crystallinity than homopolymers of DX or CL and, depending on the composition, they were liquid at room temperature. The lower melting point and gel-like appearance make them good candidates for photopolymerization-based 3D printing. The suitability toward photopolymerization was proven for the ethylene glycol-initiated co-oligomer containing 30 mol% of DX. The cross-linked gels were soft but brittle and showed good water uptake capacity.</p>
----------------------------------------------------------------------
In diva2:1038989 abstract is: <p>Interstitial Fluid is a complex sample, highly abundant in the human body that can give information regardingtissue secretion, intracellular signaling and tissue health status. The composition of the interstitial fluid can giveinformation regarding the processes occurring in muscles and alterations due to pathological changes occurringduring disease progression. Currently this sample has not yet been characterized within rare diseases like musculardystrophies. Facioscapulohumeral Muscular Dytrophy is an inherited progressive myopathy, characterized by thedegeneration and progressive muscular fiber necrosis of muscles from the face, upper arms and lower limbs. It canbe diagnosed; but in an advanced stage where weakness in the muscles have already occur. Meanwhile there is nocurrent understanding of the mechanisms happening in the muscle. In this project an immunoassay protocol wasdeveloped using suspension bead array technology to create an optimal method to analyze the protein content ofthese samples. The technological platform allows antibody-based capturing and detection of protein targets frombiotinylated biological samples. By modifying an existing protocol for analysis of serum and plasma samplesabundance of 63 protein targets was measured in muscle interstitial fluid from healthy individuals and patientsaffected by facioscapulohumeral dystrophy (FSHD), The optimized steps were the sample pre-treatment, the assaybuffer dilution ratio and the incubation time for capturing the protein targets. The findings of this project indicatethat using 1 μl of muscle interstitial fluid sample with minimized dilution factor and 60-fold molar excess biotinrelative to sample protein concentration enables detection of Interstitial fluid protein components. The proteinsdetected are ret finger protein-like 4B (RFPL4B) and albumin in from affected muscle and histone cluster(HIST1H3A) and albumin in non affected muscle.</p>

w='Dytrophy' val={'c': 'Dystrophy', 's': 'diva2:1038989', 'n': 'error in original'}

corrected abstract:
<p>Interstitial Fluid is a complex sample, highly abundant in the human body that can give information regarding tissue secretion, intracellular signaling and tissue health status. The composition of the interstitial fluid can give information regarding the processes occurring in muscles and alterations due to pathological changes occurring during disease progression. Currently this sample has not yet been characterized within rare diseases like muscular dystrophies. Facioscapulohumeral Muscular Dytrophy is an inherited progressive myopathy, characterized by the degeneration and progressive muscular fiber necrosis of muscles from the face, upper arms and lower limbs. It can be diagnosed; but in an advanced stage where weakness in the muscles have already occur. Meanwhile there is no current understanding of the mechanisms happening in the muscle. In this project an immunoassay protocol was developed using suspension bead array technology to create an optimal method to analyze the protein content of these samples. The technological platform allows antibody-based capturing and detection of protein targets from biotinylated biological samples. By modifying an existing protocol for analysis of serum and plasma samples abundance of 63 protein targets was measured in muscle interstitial fluid from healthy individuals and patients affected by facioscapulohumeral dystrophy (FSHD), The optimized steps were the sample pre-treatment, the assay buffer dilution ratio and the incubation time for capturing the protein targets. The findings of this project indicate that using 1 μl of muscle interstitial fluid sample with minimized dilution factor and 60-fold molar excess biotin relative to sample protein concentration enables detection of Interstitial fluid protein components. The proteins detected are ret finger protein-like 4B (RFPL4B) and albumin in from affected muscle and histone cluster (HIST1H3A) and albumin in non affected muscle.</p>
----------------------------------------------------------------------
In diva2:1217907 abstract is: <p>Modern medication allows patients to be treated more efficiently if taken properly.When patients themselves are responsible for their medication, it can be wronglydosed or forgotten, which changes the conditions for its effects. Forgetfulness combinedwith impaired senses make it difficult for the patient to take care of his medication.The solution to this problem today is increased home care, where healthcare professionalshelp patients to take the right medication at the right time. A technical solutioncan be an embedded system that reminds the patient, saves missed dosages, and canprovide more information to doctors who can make more informed decisions for furthercare.The results show that a pill dispenser can be used to remind patients to take their medicationand send missed dosages with near field communication. As a microcontroller,a STM32 Nucleo was used, with a state machine as software solution. In further studies,a real-time operating system can be used. The system was programmed with Arduino'sdevelopment environment, but an alternative is IAR. The prototype of the projectcould achieve the specifications battery life with most modules, except to thesound module used. To achieve this result, two theoretical extremes were created inwhich the prototype was tested.The pill dispenser can lead to reduced waste of pills and a lower cost, as fewer homevisits are required. In addition, this results in increased independence for the patient.These factors cause the pill dispenser to contribute to increased sustainable development.</p>

corrected abstract:
<p>Modern medication allows patients to be treated more efficiently if taken properly. When patients themselves are responsible for their medication, it can be wrongly dosed or forgotten, which changes the conditions for its effects. Forgetfulness combined with impaired senses make it difficult for the patient to take care of his medication.</p><p>The solution to this problem today is increased home care, where healthcare professionals help patients to take the right medication at the right time. A technical solution can be an embedded system that reminds the patient, saves missed dosages, and can provide more information to doctors who can make more informed decisions for further care.</p><p>The results show that a pill dispenser can be used to remind patients to take their medication and send missed dosages with near field communication. As a microcontroller, a STM32 Nucleo was used, with a state machine as software solution. In further studies, a real-time operating system can be used. The system was programmed with Arduino's development environment, but an alternative is IAR. The prototype of the project could achieve the specifications battery life with most modules, except to the sound module used. To achieve this result, two theoretical extremes were created in which the prototype was tested.</p><p>The pill dispenser can lead to reduced waste of pills and a lower cost, as fewer home visits are required. In addition, this results in increased independence for the patient. These factors cause the pill dispenser to contribute to increased sustainable development.</p>
----------------------------------------------------------------------
In diva2:1451771 abstract is: <p>AbstractThere are a lot of old software in the world that has not been supported or kept up todate and would need to be updated to seal security vulnerabilities, as well as to updatefunctions in the program. In those cases where the source code has been lost ordeliberately deleted, would it be possible to use reverse engineering to retrieve thesource code?This study aims to show what java bytecode is and how it is used, as well as how oneis able to go from java bytecode back to source code in a process called Reverse Engineering.Furthermore, the study will show previous work in reverse engineering,in obfuscation and to explain further details about what Java Virtual machine,bytecode and obfuscation is and how they work. Three programs of various complexityare made into bytecode and then obfuscated. The difference between the originalcode and the obfuscated code are then analyzed.The results show that it is possible to reverse engineer obfuscated code but someparts. Obfuscation does protect the code, as all the variable names are changed andevery unused method are removed, as well as some methods changed to non-conventionalways to program.KeywordsReverse engineering, Java, JVM, bytecode, obfuscation, safety.</p>


corrected abstract:
<p>There are a lot of old software in the world that has not been supported or kept up to date and would need to be updated to seal security vulnerabilities, as well as to update functions in the program. In those cases where the source code has been lost or deliberately deleted, would it be possible to use reverse engineering to retrieve the source code?</p><p>This study aims to show what java bytecode is and how it is used, as well as how one is able to go from java bytecode back to source code in a process called Reverse Engineering. Furthermore, the study will show previous work in reverse engineering, in obfuscation and to explain further details about what Java Virtual machine, bytecode and obfuscation is and how they work. Three programs of various complexity are made into bytecode and then obfuscated. The difference between the original code and the obfuscated code are then analyzed.</p><p>The results show that it is possible to reverse engineer obfuscated code but some parts. Obfuscation does protect the code, as all the variable names are changed and every unused method are removed, as well as some methods changed to non-conventional ways to program.</p>
----------------------------------------------------------------------
In diva2:1454824 abstract is: <p>Transcription factors (TFs) are key regulatory proteins that regulate transcriptionthrough precise, but highly variable binding events to cis-regulatory elements.The complexity of their regulatory patterns makes it difficult to determinethe roles of different TFs, a task which the field is still struggling with.Experimental procedures for this purpose, such as knock out experiments, arehowever costly and time consuming, and with the ever-increasing availabilityof sequencing data, computational methods for inferring the activity of TFsfrom such data have become of great interest. Current methods are howeverlacking in several regards, which necessitates further exploration of alternatives.</p><p>A novel tool for estimating the activity of individual TFs over time fromlongitudinal mRNA expression data was in this project therefore put togetherand tested on data from Mus musculus liver and brain. The tool is based onprincipal component analysis, which is applied to data subsets containing theexpression data of genes likely regulated by a specific TF to acquire an estimationof its activity. Though initial tests on 17 selected TFs showed issues withunspecific trends in the estimations, further testing is required for a statementon the potential of the estimator.</p>

corrected abstract:
<p><em>Transcription factors</em> (TFs) are key regulatory proteins that regulate transcription through precise, but highly variable binding events to cis-regulatory elements. The complexity of their regulatory patterns makes it difficult to determine the roles of different TFs, a task which the field is still struggling with. Experimental procedures for this purpose, such as knock out experiments, are however costly and time consuming, and with the ever-increasing availability of sequencing data, computational methods for inferring the activity of TFs from such data have become of great interest. Current methods are however lacking in several regards, which necessitates further exploration of alternatives.</p><p>A novel tool for estimating the activity of individual TFs over time from longitudinal mRNA expression data was in this project therefore put together and tested on data from <em>Mus musculus</em> liver and brain. The tool is based on principal component analysis, which is applied to data subsets containing the expression data of genes likely regulated by a specific TF to acquire an estimation of its activity. Though initial tests on 17 selected TFs showed issues with unspecific trends in the estimations, further testing is required for a statement on the potential of the estimator.</p>
----------------------------------------------------------------------
In diva2:1454438 abstract is: <p>Hemicelluloses are synthesized by proteins encoded by genes from the cellulose synthasegene superfamily. One subgroup of this gene family is the cellulose synthase-like B, which islargely uncharacterized and unexplored. The common model organism Nicotianabenthamiana has one such gene in its genome, NbCslB, encoding a membrane protein. Theexpression of this gene has previously been studied in vivo, but in order to study the protein invitro a viable solubilization and purification protocol is required. This study evaluated the useof the detergent n-Dodecyl β-D-maltoside (DDM) for solubilization, followed by purificationusing immobilized metal ion affinity chromatography (IMAC), and thereafter reconstitutionof the protein into proteoliposomes. SDS-PAGE as well as Western blot analyses showed thatthe purification was successful and provided a pure sample of protein. Throughout theanalyses performed, an anti-FLAG antibody was discovered to bind well to the protein, andthereby be especially useful for analysis. An activity assay was performed on the purifiedprotein, to characterize its function and evaluate whether the protein had maintained itsactivity and conformation after the steps of purification and reconstitution. No activity couldbe detected in the enzymatic assay, which indicated that the purification protocol may havebeen too rough on the protein, that the reconstitution was not successful, or that the assayconditions were not optimal. These results can be used as a base for future research, where theprotocols for solubilization, purification, and reconstitution should be further refined in orderto obtain an end result where the purified protein is active. When an active and pure proteinsample is achieved, it will be possible to perform further attempts at characterizing thefunction of the protein using enzymatic activity assays. Additionally, the results showed thatthe choice of antibody can be crucial for proper analysis of this protein.</p>


corrected abstract:
<p>Hemicelluloses are synthesized by proteins encoded by genes from the cellulose synthase gene superfamily. One subgroup of this gene family is the cellulose synthase-like B, which is largely uncharacterized and unexplored. The common model organism <em>Nicotiana benthamiana</em> has one such gene in its genome, NbCslB, encoding a membrane protein. The expression of this gene has previously been studied <em>in vivo</em>, but in order to study the protein <em>in vitro</em> a viable solubilization and purification protocol is required. This study evaluated the use of the detergent n-Dodecyl β-D-maltoside (DDM) for solubilization, followed by purification using immobilized metal ion affinity chromatography (IMAC), and thereafter reconstitution of the protein into proteoliposomes. SDS-PAGE as well as Western blot analyses showed that the purification was successful and provided a pure sample of protein. Throughout the analyses performed, an anti-FLAG antibody was discovered to bind well to the protein, and thereby be especially useful for analysis. An activity assay was performed on the purified protein, to characterize its function and evaluate whether the protein had maintained its activity and conformation after the steps of purification and reconstitution. No activity could be detected in the enzymatic assay, which indicated that the purification protocol may have been too rough on the protein, that the reconstitution was not successful, or that the assay conditions were not optimal. These results can be used as a base for future research, where the protocols for solubilization, purification, and reconstitution should be further refined in order to obtain an end result where the purified protein is active. When an active and pure protein sample is achieved, it will be possible to perform further attempts at characterizing the function of the protein using enzymatic activity assays. Additionally, the results showed that the choice of antibody can be crucial for proper analysis of this protein.</p>
----------------------------------------------------------------------
In diva2:1732100 abstract is: <p>It is well-known that many people like to listen to music when they are running,something that is not always available for people with hearing aids [1].The experience of music is not often taken into account when people have lostparts of their hearing, but music is in many ways important for the well-being[2]. In this project has an application been developed, to make certain audioparameters accessible for adjustments according to the users individual hearing.The parameters chosen were latency, compression, balance and loudness. Theseparameters have been chosen with the argument that they have big impact onthe experience of the music. As an example there might be latency betweenthe hearing aids between the left and the right hearing. Very often people withhearing aids have different hearing at the left and the right ear, therefore thebalance was chosen as a parameter. Compression were chosen with respect tothe fact that hearing loss often leads to less capability to hear frequencies indifferent frequencies areas. Loudness was chosen due to that it is a subjectiveparameter, that will have different impact on different individuals. The parametershave been chosen with a hypothesis that they may affect the hearingexperience for different individuals.</p><p></p><p>The result show that the participants changed audio settings, each of theaudio parameters to different values and appreciated the function to changethese parameters. The participants said that the application developed in thisproject, improved their music experience and they said that they would appreciateto use an application like this, to get more value of listening to music. Theapplication was well designed and easy to use during a sport session.</p>


corrected abstract:
<p>It is well-known that many people like to listen to music when they are running, something that is not always available for people with hearing aids [1]. The experience of music is not often taken into account when people have lost parts of their hearing, but music is in many ways important for the well-being [2]. In this project has an application been developed, to make certain audio parameters accessible for adjustments according to the users individual hearing. The parameters chosen were latency, compression, balance and loudness. These parameters have been chosen with the argument that they have big impact on the experience of the music. As an example there might be latency between the hearing aids between the left and the right hearing. Very often people with hearing aids have different hearing at the left and the right ear, therefore the balance was chosen as a parameter. Compression were chosen with respect to the fact that hearing loss often leads to less capability to hear frequencies in different frequencies areas. Loudness was chosen due to that it is a subjective parameter, that will have different impact on different individuals. The parameters have been chosen with a hypothesis that they may affect the hearing experience for different individuals.</p><p>The result show that the participants changed audio settings, each of the audio parameters to different values and appreciated the function to change these parameters. The participants said that the application developed in this project, improved their music experience and they said that they would appreciate to use an application like this, to get more value of listening to music. The application was well designed and easy to use during a sport session.</p>
----------------------------------------------------------------------
In diva2:1308051 abstract is: <p>Genome organization is increasingly believed to influence and participate in gene regulation. Thestudy of the interactions that constitute this organization is increasingly performed using chromosomeconformation capture (3C) technologies. While good for the study of pairwise interactions betweenelements, higher order structures are inaccessible due to the use of proximal ligation to link elements.This thesis thus endeavored to develop a method to study these higher order structures in an effort tomove away from proximal ligation. Instead of pairwise linking interacting fragments through ligation,strategies for barcoding of chromatin complexes were tested. These complexes were extracted fromcrosslinked cells and captured using chromatin immunoprecipitation (ChIP) procedure. The fragmentswithin the complex were then fitted with adaptors and barcodes according to two approaches. The first(called Approach A) being adaptor ligation with subsequent ligation onto barcoded beads, the second(called Approach B) tagmentation with subsequent barcoding through emulsion PCR (emPCR).Several steps of optimization were performed and a total six libraries created and sequenced, twousing approach A and four using approach B. Analysis of these libraries demonstrated progress inseveral key areas such as barcode clusters containing multiple fragments and phasing. In the B datasetswas also found an enrichment form short range interactions, in accordance with 3C observations. TheB datasets outperformed A in most regards and was thus deemed the preferred path for future studies.The greatest challenge yet to overcome is to lower duplication rates which currently are at a minimumof 79%. To decrease rates several parameters for optimization have been identified for futuredevelopment.</p>


corrected abstract:
<p>Genome organization is increasingly believed to influence and participate in gene regulation. The study of the interactions that constitute this organization is increasingly performed using chromosome conformation capture (3C) technologies. While good for the study of pairwise interactions between elements, higher order structures are inaccessible due to the use of proximal ligation to link elements. This thesis thus endeavored to develop a method to study these higher order structures in an effort to move away from proximal ligation. Instead of pairwise linking interacting fragments through ligation, strategies for barcoding of chromatin complexes were tested. These complexes were extracted from crosslinked cells and captured using chromatin immunoprecipitation (ChIP) procedure. The fragments within the complex were then fitted with adaptors and barcodes according to two approaches. The first (called Approach A) being adaptor ligation with subsequent ligation onto barcoded beads, the second (called Approach B) tagmentation with subsequent barcoding through emulsion PCR (emPCR). Several steps of optimization were performed and a total six libraries created and sequenced, two using approach A and four using approach B. Analysis of these libraries demonstrated progress in several key areas such as barcode clusters containing multiple fragments and phasing. In the B datasets was also found an enrichment form short range interactions, in accordance with 3C observations. The B datasets outperformed A in most regards and was thus deemed the preferred path for future studies. The greatest challenge yet to overcome is to lower duplication rates which currently are at a minimum of 79%. To decrease rates several parameters for optimization have been identified for future development.</p>
----------------------------------------------------------------------
title: "How Covid-19 has affected thepsychosocial work environmentfor gig-workers in the fooddelivery sector: A qualitative interview study"
==> "How Covid-19 has affected the psychosocial work environmentfor gig-workers in the food delivery sector: A qualitative interview study"

In diva2:1593641 abstract is: <p>The topic of gig-work has been widely debated in the western world for its laborrights and work environment issues. Whereas companies operating within thegig-economy boast the freedom and flexibility of gig-work as perks for workers,unions highlight the precarious nature and poor working conditions of gig-workas exploitative of workers and push for unionizing gig-workers and getting themcovered by collective bargaining agreements. However, due to the novelty of thephenomenon, not much scholarly work has been done on the subject and its longterm implications for the labor markets of developed capitalist countries.In Sweden, conditions caused by the Covid-19 pandemic led to an influx ofworkers in the food-delivery sector due to increased demands for services andhigh unemployment as gig-work was regarded as a means to sustain one’slivelihood during a time of economic uncertainty. As these food-delivery couriershad to work outside the home, where the chance of contracting the virus was high,while also having jobs where they were not regarded as employees, the aim ofthis thesis project was to investigate how the Covid-19 pandemic has affectedfood-delivery couriers’ perceived exposure to psychosocial stressors and risksthrough a qualitative interview study. The findings were generated throughinductive reasoning from analyzing conducted six interviews and discussedthrough the lens of the Human, Technology and Organization (HTO)-model andthe Job-Demands-Control-Support (JDCS)-model.</p>

corrected abstract:
<p>The topic of gig-work has been widely debated in the western world for its labor rights and work environment issues. Whereas companies operating within the gig-economy boast the freedom and flexibility of gig-work as perks for workers, unions highlight the precarious nature and poor working conditions of gig-work as exploitative of workers and push for unionizing gig-workers and getting them covered by collective bargaining agreements. However, due to the novelty of the phenomenon, not much scholarly work has been done on the subject and its longterm implications for the labor markets of developed capitalist countries.</p><p>In Sweden, conditions caused by the Covid-19 pandemic led to an influx of workers in the food-delivery sector due to increased demands for services and high unemployment as gig-work was regarded as a means to sustain one’s livelihood during a time of economic uncertainty. As these food-delivery couriers had to work outside the home, where the chance of contracting the virus was high, while also having jobs where they were not regarded as employees, the aim of this thesis project was to investigate how the Covid-19 pandemic has affected food-delivery couriers’ perceived exposure to psychosocial stressors and risks through a qualitative interview study. The findings were generated through inductive reasoning from analyzing conducted six interviews and discussed through the lens of the Human, Technology and Organization (HTO)-model and the Job-Demands-Control-Support (JCDS)-model.</p>
----------------------------------------------------------------------
In diva2:1454829 abstract is: <p>Myelodysplastic syndrome (MDS) constitutes a group of hematological disorders that impairshematopoiesis and frequently predisposes to acute myeloid leukemia (AML). Mutations intwo homologous and evolutionary conserved tumor suppressors called SAMD9 and SAMD9Lhave recently been associated with increased susceptibility to MDS/AML, but also with awider spectrum of diseases, including life-threatening autoinflammatory syndromes as well asataxia. Representing a novel class of tumor suppressors, they have been implicated in thecontrol of viral infection and cell cycle regulation. However, the structure-functionrelationships and cellular activity of these proteins remain undefined. In order to elucidate themolecular mechanisms by which SAMD9 and SAMD9L restrict cell proliferation and howdisease-causing mutations can potentiate their antiproliferative activity, stable cell lines withinducible expression of SAMD9L wild-type, patient-derived or predicted damaging variantswere established and evaluated by Western blot as well as a quantitative cell cycle assay.From generated results, a genotype-phenotype relationship was established for two noveldisease-associated variants. Furthermore, a region important for controlling theantiproliferative activity of SAMD9L was defined. These results can help to explain clinicalcases with novel variants and provide deepened understanding of the cellular mechanismswhereby SAMD9L regulates cell cycle. With poor prognosis and limited treatments available,such molecular insight is of outmost importance since it can aid diagnostics and may lead tonovel therapies that can benefit patients.</p>


corrected abstract:
<p>Myelodysplastic syndrome (MDS) constitutes a group of hematological disorders that impairs hematopoiesis and frequently predisposes to acute myeloid leukemia (AML). Mutations in two homologous and evolutionary conserved tumor suppressors called SAMD9 and SAMD9L have recently been associated with increased susceptibility to MDS/AML, but also with a wider spectrum of diseases, including life-threatening autoinflammatory syndromes as well as ataxia. Representing a novel class of tumor suppressors, they have been implicated in the control of viral infection and cell cycle regulation. However, the structure-function relationships and cellular activity of these proteins remain undefined. In order to elucidate the molecular mechanisms by which SAMD9 and SAMD9L restrict cell proliferation and how disease-causing mutations can potentiate their antiproliferative activity, stable cell lines with inducible expression of SAMD9L wild-type, patient-derived or predicted damaging variants were established and evaluated by Western blot as well as a quantitative cell cycle assay. From generated results, a genotype-phenotype relationship was established for two novel disease-associated variants. Furthermore, a region important for controlling the antiproliferative activity of SAMD9L was defined. These results can help to explain clinical cases with novel variants and provide deepened understanding of the cellular mechanisms whereby SAMD9L regulates cell cycle. With poor prognosis and limited treatments available, such molecular insight is of outmost importance since it can aid diagnostics and may lead to novel therapies that can benefit patients.</p>
----------------------------------------------------------------------
In diva2:1697647 abstract is: <p>Interactive projected augmented reality is a subfield within projected augmented reality, where the interactivity is about projecting virtual data onto an entity that canpotentially be in movement. This projection can be accomplished by using a calibrated projector-depth camera system that detects entities using computer vision.This interactive system has a varying number of application areas; however, a criticalproblem emerges, which is the accuracy of these systems. The accuracy in this caseis how correctly the projection takes place on the specific entity, the accuracy istherefore an important aspect to validate if certain applications are possible to implement correctly. The solution for this problem is by implementing interactive projectedaugmented reality and perform prototype tests with the implemented system andthen further analyzing the accuracy with test data. The prototype is calibrated withexisting tools from previous studies, detects using pose detection, which can thenproject points on specific body parts that also follows the person during movement.The result that got developed was a prototype that was tested for accuracy. The testsare done using computer vision to extract measurement data such as the projectedpoint and the expected point.</p><p>The result from the analysis of the test data showedthat the accuracy of the projection is suitable for applications that do not requireexceptional accuracy, such as entertainment, art, games and so on. Further research is required for applications that require exceptional accuracy such ashealthcare and surgical applications.</p>

corrected abstract:
<p>Interactive projected augmented reality is a subfield within projected augmented reality, where the interactivity is about projecting virtual data onto an entity that can potentially be in movement. This projection can be accomplished by using a calibrated projector-depth camera system that detects entities using computer vision. This interactive system has a varying number of application areas; however, a critical problem emerges, which is the accuracy of these systems. The accuracy in this case is how correctly the projection takes place on the specific entity, the accuracy is therefore an important aspect to validate if certain applications are possible to implement correctly. The solution for this problem is by implementing interactive projected augmented reality and perform prototype tests with the implemented system and then further analyzing the accuracy with test data. The prototype is calibrated with existing tools from previous studies, detects using pose detection, which can then project points on specific body parts that also follows the person during movement.</p><p>The result that got developed was a prototype that was tested for accuracy. The tests are done using computer vision to extract measurement data such as the projected point and the expected point. The result from the analysis of the test data showed that the accuracy of the projection is suitable for applications that do not require exceptional accuracy, such as entertainment, art, games and so on. Further research is required for applications that require exceptional accuracy such as healthcare and surgical applications.</p>
----------------------------------------------------------------------
In diva2:1180456 abstract is: <p>To ensure that production is equivalent to consumption, hydro power units are used becauseof their high reliability as a regulation power source.Regulation is used due to the unbalance in the power grid between the production and con-sumption, for which reserved hydro power units for Primary and Secondary regulation areused. Primary regulation is done to balance the frequency in the grid by automatically ad-justing the water flow that runs through such units.Secondary regulation units are used for compensation of lost power production or for takingaway the surplus of power production and has become more intermittent today, which re-sults in more starts and stops of units. The increased intermittency of the units causes strainson the various components especially the generators. Therefore it is of great interest to knowthe total start-stop cost of the secondary regulated units. The theoretically calculated costsmay not coincide with the actual costs, as there are many parameters and complex calcula-tions that are included in the start-stop costs.A revised version of a calculation model was done for the secondary regulated units, basedon an earlier work, and showed higher start-stop costs compared to the earlier study, mostlybecause of these parameters:• Start and stop frequency for each unit has increased because of a higher flexibility in theelectrical market prices.• Age of the generators and turbines. Many of these components are near their time forrehabilitation which results in a higher maintenance cost.• The average failure frequency for each hydropower unit was thoroughly investigated andshowed to be higher than earlier estimated. A more individually calculated failure fre-quency for each unit showed to be higher than the earlier used standard average values.One important result is that the units with the lowest start and stop costs are preferred to beused, with the exception that the units with high start and stop cost, because of considerableleakage cost, should be used as much as possible.</p>


corrected abstract:
<p>To ensure that production is equivalent to consumption, hydro power units are used because of their high reliability as a regulation power source.</p><p>Regulation is used due to the unbalance in the power grid between the production and consumption, for which reserved hydro power units for Primary and Secondary regulation are used. Primary regulation is done to balance the frequency in the grid by automatically adjusting the water flow that runs through such units.</p><p>Secondary regulation units are used for compensation of lost power production or for taking away the surplus of power production and has become more intermittent today, which results in more starts and stops of units. The increased intermittency of the units causes strains on the various components especially the generators. Therefore it is of great interest to know the total start-stop cost of the secondary regulated units. The theoretically calculated costs may not coincide with the actual costs, as there are many parameters and complex calculations that are included in the start-stop costs.</p><p>A revised version of a calculation model was done for the secondary regulated units, based on an earlier work, and showed higher start-stop costs compared to the earlier study, mostly because of these parameters:<ul><li>Start and stop frequency for each unit has increased because of a higher flexibility in the electrical market prices.</li><li>Age of the generators and turbines. Many of these components are near their time for rehabilitation which results in a higher maintenance cost.</li><li>The average failure frequency for each hydropower unit was thoroughly investigated and showed to be higher than earlier estimated. A more individually calculated failure frequency for each unit showed to be higher than the earlier used standard average values.</li></ul></p><p>One important result is that the units with the lowest start and stop costs are preferred to be used, with the exception that the units with high start and stop cost, because of considerable leakage cost, should be used as much as possible.</p>
----------------------------------------------------------------------
In diva2:1454816 abstract is: <p>The grey wolf was the first animal to be domesticated, and its descendant, the dog, is unique in its broad phenotypicdiversity, short time span of evolution and extraordinary relationship with humans. By studying differences in regulatorypatterns, such as regulatory DNA, it is possible to unveil the genetic changes that generate phenotypical differencesbetween the wolf and the dog. This could provide novel information about the unique evolution and domestication of thedog. The main objective of this thesis was to provide an epigenetic map for two types of regulatory DNA-regions: enhancersand promoters, in wolf liver tissue. The regions were identified by locating the histone markers H3K4me3, H3K4me1,H3K27Ac and the transcription factor CTCF, using the recently developed method CUT&amp;Run. CUT&amp;Run uses antibodiesto target the histone markers of interest, and protein A/G-MNase to cut and release the DNA interacting with the histonemarker. The extracted DNA was sequenced and heatmaps were generated based on each antibody’s enrichment arounddog regulatory regions. Following this, a gene ontology list enrichment analysis on wolf was made, and it was comparedto previous studies on dog. The method was successful for all samples, although at varying degrees. The list enrichmentanalysis results firstly show that many regulatory regions in wolf liver tissue are associated with metabolic processes, andsecondly suggest that wolves and dogs share the recently evolved regulatory regions associated with viral process, proteintargeting and mRNA catabolic process in their liver tissue. Furthermore, the results indicate that there are several recentlyevolved dog enhancers that were evolved after the lineage separation between the wolf and the dog. However, theconclusions that can be made solely on this thesis are limited, primarily due to sample restrictions, and further studies arerequired.</p>

partal corrected: diva2:1454816: <p>The grey wolf was the first animal to be domesticated, and its descendant, the dog, is unique in its broad phenotypic diversity, short time span of evolution and extraordin ary relationship with humans. By studying differences in regulatory patterns, such as regulatory DNA, it is possible to unveil the genetic changes that generate phenotypical differences between the wolf and the dog. This could provide novel information about the unique evolution and domestication of the dog. The main objective of this thesis was to provide an epigenetic map for two types of regulatory DNA-regions: enhancers and promoters, in wolf liver tissue. The regions were identified by locating the his tone markers H3K4me3, H3K4me1, H3K27Ac and the transcription factor CTCF, using the recently developed method CUT&amp;Run. CUT&amp;Run uses antibodies to target the his tone markers of interest, and protein A/G-MNase to cut and release the DNA interacting with the his tonemarker. The extracted DNA was sequenced and heatmaps were generated based on each antibody’s enrichment around dog regulatory regions. Following this, a gene ontology list enrichment analysis on wolf was made, and it was compared to previous studies on dog. The method was successful for all samples, although at varying degrees. The list enrichment analysis results firstly show that many regulatory regions in wolf liver tissue are associated with metabolic processes, and secondly suggest that wolves and dogs share the recently evolved regulatory regions associated with viral process, protein targeting and mRNA catabolic process in their liver tissue. Furthermore, the results indicate that there are several recently evolved dog enhancers that were evolved after the lineage separation between the wolf and the dog. However, the conclusions that can be made solely on this thesis are limited, primarily due to sample restrictions, and further studies are required.</p>

corrected abstract:
<p>The grey wolf was the first animal to be domesticated, and its descendant, the dog, is unique in its broad phenotypic diversity, short time span of evolution and extraordinary relationship with humans. By studying differences in regulatory patterns, such as regulatory DNA, it is possible to unveil the genetic changes that generate phenotypical differences between the wolf and the dog. This could provide novel information about the unique evolution and domestication of the dog. The main objective of this thesis was to provide an epigenetic map for two types of regulatory DNA-regions: enhancers and promoters, in wolf liver tissue. The regions were identified by locating the histone markers H3K4me3, H3K4me1, H3K27Ac and the transcription factor CTCF, using the recently developed method CUT&amp;Run. CUT&amp;Run uses antibodies to target the histone markers of interest, and protein A/G-MNase to cut and release the DNA interacting with the histone marker. The extracted DNA was sequenced and heatmaps were generated based on each antibody’s enrichment around dog regulatory regions. Following this, a gene ontology list enrichment analysis on wolf was made, and it was compared to previous studies on dog. The method was successful for all samples, although at varying degrees. The list enrichment analysis results firstly show that many regulatory regions in wolf liver tissue are associated with metabolic processes, and secondly suggest that wolves and dogs share the recently evolved regulatory regions associated with viral process, protein targeting and mRNA catabolic process in their liver tissue. Furthermore, the results indicate that there are several recently evolved dog enhancers that were evolved after the lineage separation between the wolf and the dog. However, the conclusions that can be made solely on this thesis are limited, primarily due to sample restrictions, and further studies are required.</p>
----------------------------------------------------------------------
In diva2:751697 abstract is: <p>A common problem in the workplace is sharing digital documents with coworkers. Forsome companies the problem extends to wanting the documentskept internally backedup and controlling which people in the company has rights to read and revise certaindocuments.This paper shows different systems and models for access control, version control,and distribution of the documents that can be used to create asystem that solves theseproblems.One requirement for this system was a user interface where users can upload, down-load and manage access to their documents. Another requirement was a service thathandles version control for the documents, and a way to quickly connect and distributethe documents. The system also needed to be able to handle access control of the ver-sioned documents on document level, referred to as "fine grained access control" in thispaper.These models and systems were evaluated based on aspects of the access control mod-els, version control systems, and distribution systems andprotocols. After evaluating,appropriate selections were made to create a prototype to test the system as a whole.The prototype ended up meeting the goals that Nordicstationset for the project butonly with basic functionality. Functionality for retrieving any version from a docu-ments history, controlling access for the documents at document level, and a simpleweb based user interface for managing the documents.</p>

w='ver-sioned' val={'c': 'versioned', 's': 'diva2:751697'}

corrected abstract:
<p>A common problem in the workplace is sharing digital documents with coworkers. For some companies the problem extends to wanting the documents kept internally backed up and controlling which people in the company has rights to read and revise certain documents.</p><p>This paper shows different systems and models for access control, version control, and distribution of the documents that can be used to create a system that solves these problems.</p><p>One requirement for this system was a user interface where users can upload, download and manage access to their documents. Another requirement was a service that handles version control for the documents, and a way to quickly connect and distribute the documents. The system also needed to be able to handle access control of the versioned documents on document level, referred to as "fine grained access control" in this paper.</p><p>These models and systems were evaluated based on aspects of the access control models, version control systems, and distribution systems and protocols. After evaluating, appropriate selections were made to create a prototype to test the system as a whole.</p><p>The prototype ended up meeting the goals that Nordicstation set for the project but only with basic functionality. Functionality for retrieving any version from a documents history, controlling access for the documents at document level, and a simple web based user interface for managing the documents.</p>
----------------------------------------------------------------------
In diva2:1454429 abstract is: <p>Inflammatory Bowel Disease (IBD) is a chronic disorder that affects millions of peopleworldwide. Although the etiology behind the disease is yet unknown, current theoriespropose a complex interplay between genetic susceptibility, exposure to environmentalfactors and exacerbated immune responses. While important efforts have been made to linkgenetics and environmental factors to IBD pathogenesis, a major challenge remains to assignthem a causative role. Particularly since most of the IBD-risk genetic polymorphisms arefound in non-coding regions (NCRs) with unknown regulatory activity, and for the lack ofknowledge about how environmental factors can modulate the function of these elements invivo . A main problem to address this challenge in IBD research is the lack of an appropriatemodel system in vivo that allows for high-throughput experiments with combinations ofdifferent IBD-risk factors, while keeping the in vivo context. In this work, we sought toovercome this issue by using a zebrafish reporter for a specific human IBD-risk NCR, inorder to investigate the modulation of this element by two groups of common environmentalfactors: pollutants, such as PolyFluoroAlkyl Substances (PFASs); and diet, by activation ofdietary sensors.</p><p>We found that the activity of the WT-NCR in zebrafish larvae was increased in the presenceof PFAS, while the activation of the dietary sensor PPAR δ decreased the activity. These datalead us to suggest that the function of PFAS can be counteracted by PPARδ activation.Therefore, we propose zebrafish as a suitable in vivo model in which we can screen forpotentially harmful or beneficial effects of environmental factors in the activity of humannon-coding regions.</p>

corrected abstract:
<p>Inflammatory Bowel Disease (IBD) is a chronic disorder that affects millions of people worldwide. Although the etiology behind the disease is yet unknown, current theories propose a complex interplay between genetic susceptibility, exposure to environmental factors and exacerbated immune responses. While important efforts have been made to link genetics and environmental factors to IBD pathogenesis, a major challenge remains to assign them a causative role. Particularly since most of the IBD-risk genetic polymorphisms are found in non-coding regions (NCRs) with unknown regulatory activity, and for the lack of knowledge about how environmental factors can modulate the function of these elements <em>in vivo</em>. A main problem to address this challenge in IBD research is the lack of an appropriate model system <em>in vivo</em> that allows for high-throughput experiments with combinations of different IBD-risk factors, while keeping the <em>in vivo</em> context. In this work, we sought to overcome this issue by using a zebrafish reporter for a specific human IBD-risk NCR, in order to investigate the modulation of this element by two groups of common environmental factors: pollutants, such as PolyFluoroAlkyl Substances (PFASs); and diet, by activation of dietary sensors.</p><p>We found that the activity of the WT-NCR in zebrafish larvae was increased in the presence of PFAS, while the activation of the dietary sensor PPARδ decreased the activity. These data lead us to suggest that the function of PFAS can be counteracted by PPARδ activation. Therefore, we propose zebrafish as a suitable <em>in vivo</em> model in which we can screen for potentially harmful or beneficial effects of environmental factors in the activity of human non-coding regions.</p>
----------------------------------------------------------------------
In diva2:1147592 abstract is: <p>Virulent strains of S. pneumoniae are known to evade the alternative pathway ofcomplement immunity by means of a surface bound protein called PspC that recruits thehuman complement regulator Factor H. Factor H is a self surface marker that inhibits theactivity of the alternative pathway of complement immunity and is comprised of 20Complement Control Protein (CCP) domains in a “bead on a string” fashion. It has beenconcluded that PspC can use two different mechanisms of binding Factor H. The PspCallele of the TIGR4 strain of S. pneumoniae has been shown to have affinity for the 9thCCP of Factor H through a “lock and key” mechanism mediated by a critical Tyr90residue. However, PspC from the D39 strain of S. pneumoniae does not posess Tyr90 andit hasn’t been conclusively shown which CCP it binds to. In an effort to elucidate thismechanism, individual CCP domains were expressed as fusion proteins with MaltoseBinding Protein in an E. coli based expression system. The fusion proteins were used inexperiments with recombinant PspC cloned from the BHN_418 strain of S. pneumoniaewhich is homologous to that of D39 PspC. Affinity interactions were investigated with apulldown assay, copurification, microscale thermophoresis and ligand tracer assays. Theresults are inconclusive. The recombinant PspC is shown to bind full length Factor H butnot any of the individual CCP-MBP fusion proteins, most likely due to the CCPs failingto achieve proper tertiary structure during expression.</p>

w='linterplanetary' val={'c': 'interplanetary', 's': 'diva2:1147592', 'n': 'The abstract does not match the thesis - it is completely different'}
w='potentia' val={'c': 'potential', 's': 'diva2:1147592', 'n': 'The abstract does not match the thesis - it is completely different'}
w='posess' val={'c': 'possess', 's': 'diva2:1147592', 'n': 'error in original'}
w='Tyr90' val={'c': 'Tyr<sup>90</sup>', 's': 'diva2:1147592', 'n': 'appears without superscript in abstract'}

Note also that E.coli should have been set in italics, but is not in the actual abstract.


corrected abstract:
<p>Virulent strains of <em>S. pneumoniae</em> are known to evade the alternative pathway of complement immunity by means of a surface bound protein called PspC that recruits the human complement regulator Factor H. Factor H is a self surface marker that inhibits the activity of the alternative pathway of complement immunity and is comprised of 20 Complement Control Protein (CCP) domains in a “bead on a string” fashion. It has been concluded that PspC can use two different mechanisms of binding Factor H. The PspC allele of the TIGR4 strain of <em>S. pneumoniae</em> has been shown to have affinity for the 9<sup>th</sup> CCP of Factor H through a “lock and key” mechanism mediated by a critical Tyr90 residue. However, PspC from the D39 strain of <em>S. pneumoniae</em> does not posess Tyr90 and it hasn’t been conclusively shown which CCP it binds to. In an effort to elucidate this mechanism, individual CCP domains were expressed as fusion proteins with Maltose Binding Protein in an E. coli based expression system. The fusion proteins were used in experiments with recombinant PspC cloned from the BHN_418 strain of <em>S. pneumoniae</em> which is homologous to that of D39 PspC. Affinity interactions were investigated with a pulldown assay, copurification, microscale thermophoresis and ligand tracer assays. The results are inconclusive. The recombinant PspC is shown to bind full length Factor H but not any of the individual CCP-MBP fusion proteins, most likely due to the CCPs failing to achieve proper tertiary structure during expression.</p>
----------------------------------------------------------------------
In diva2:1332164 abstract is: <p>Prostheses available today are either passive or active. Active prostheses canapply external energy to the joints, while passive cannot. A well functionalprosthesis that is adapted to the wearer can improve the locomotion and alsodecrease the metabolic cost. Existing active prostheses are mainly controlledby information from the prosthesis itself in order to detect the phases in thegait. Due to lack of information from the contralateral leg, asymmetry in thegait can be present.Therefor, this study aims to investigate if it is feasible to use kinematic informationfrom a individual’s sound leg in order to prescribe the motion ofan aid. Three healthy test subjects were used to collect kinematic and EMGdata in a motion capture lab. Two different movements were performed: "Startand stop" and "Start, stop and squat". The data obtained was processed and togetherwith some created criteria a script could calculate knee and ankle anglesfor both reference and mirrored leg. The results conclude that it is feasible tomirror position information from one leg to the other. Further research is recommendedto gain more knowledge about from different trials together withreal aids.</p>

corrected abstract:
<p>Prostheses available today are either passive or active. Active prostheses can apply external energy to the joints, while passive cannot. A well functional prosthesis that is adapted to the wearer can improve the locomotion and also decrease the metabolic cost. Existing active prostheses are mainly controlled by information from the prosthesis itself in order to detect the phases in the gait. Due to lack of information from the contralateral leg, asymmetry in the gait can be present.</p><p>Therefor, this study aims to investigate if it is feasible to use kinematic information from a individual’s sound leg in order to prescribe the motion of an aid. Three healthy test subjects were used to collect kinematic and EMG data in a motion capture lab. Two different movements were performed: "Start and stop" and "Start, stop and squat". The data obtained was processed and together with some created criteria a script could calculate knee and ankle angles for both reference and mirrored leg. The results conclude that it is feasible to mirror position information from one leg to the other. Further research is recommended to gain more knowledge about from different trials together with real aids.</p>
----------------------------------------------------------------------
In diva2:860831 abstract is: <p>Henriksdal wastewater treatment plant is one of the largest plants in Europe located underthe ground. The wastewater is treated mechanically, chemically and biologically.</p><p>The basic concept for the biological treatment to work properly is that the amount ofmicroorganisms is kept at a high level. This is done by recirculating some of the sludge backto the biological pools. Periodically, when the flow into the plant is very high, the capacity ofthe biological step is not sufficient. At these times, there are currently two possible options:- Pretreated water is directed past the biological treatment to the sand filters.- Pretreated water is directed past the biological treatment and the sand filtersstraight out to the recipient.</p><p>To solve these problems, Henriksdal is planning a reconstruction of the biological treatment.During the reconstruction, the capacity of the biological treatment is going to be even lessthan it is today.</p><p>Hence there is an interest, at high flows, to increase the purification in the pre-precipitationstep with new types of precipitation chemicals.</p><p>The aim of this project is to examine two different precipitation chemicals in lab- and in largescale. This was done by flocculation tests in the lab with a flocculator to determine thedosage for each chemical to be tested in large scale. The two chemicals that were testedwere iron chloride (PIX-111) and polyaluminiumchloride (PAX-21).</p><p>The results from the labtests showed that an optimal dose for PIX was 0,12 ml PIX/l water.While the optimal dose for PAX was 0,10 ml PAX/l water.</p><p>The results from the large scale test with PAX showed a high reduction in both turbidity andsuspended particles. The reduction of both was between 50-75 %.</p><p>The results from the large scale test with PIX did not show as high reduction as that of PAX.The reduction of turbidity was around 20 %, with a maximum value of 36,6 %. The reductionof phosphate was between 70-80 %.</p><p>During the project there were only a few days of high water flows and only PAX could betested during these conditions. Hence, the two chemicals cannot be compared accuratelyuntil also PIX is tested during high water flows</p>

w='PIX' val={'c': 'PIX-111', 's': 'diva2:860831', 'n': 'PIX is used in the abstract aas shorthand for PIX-111'}

corrected abstract:
<p>Henriksdal wastewater treatment plant is one of the largest plants in Europe located under the ground. The wastewater is treated mechanically, chemically and biologically.</p><p>The basic concept for the biological treatment to work properly is that the amount of microorganisms is kept at a high level. This is done by recirculating some of the sludge back to the biological pools. Periodically, when the flow into the plant is very high, the capacity of the biological step is not sufficient. At these times, there are currently two possible options:<ul><li>Pretreated water is directed past the biological treatment to the sand filters.</li><li>Pretreated water is directed past the biological treatment and the sand filters straight out to the recipient.</li></ul></p><p>To solve these problems, Henriksdal is planning a reconstruction of the biological treatment. During the reconstruction, the capacity of the biological treatment is going to be even less than it is today.</p><p>Hence there is an interest, at high flows, to increase the purification in the pre-precipitation step with new types of precipitation chemicals.</p><p>The aim of this project is to examine two different precipitation chemicals in lab- and in large scale. This was done by flocculation tests in the lab with a flocculator to determine the dosage for each chemical to be tested in large scale. The two chemicals that were tested were iron chloride (PIX-111) and polyaluminiumchloride (PAX-21).</p><p>The results from the labtests showed that an optimal dose for PIX was 0,12 ml PIX/l water. While the optimal dose for PAX was 0,10 ml PAX/l water.</p><p>The results from the large scale test with PAX showed a high reduction in both turbidity and suspended particles. The reduction of both was between 50-75 %.</p><p>The results from the large scale test with PIX did not show as high reduction as that of PAX. The reduction of turbidity was around 20 %, with a maximum value of 36,6 %. The reduction of phosphate was between 70-80 %.</p><p>During the project there were only a few days of high water flows and only PAX could be tested during these conditions. Hence, the two chemicals cannot be compared accurately until also PIX is tested during high water flows</p>
----------------------------------------------------------------------
In diva2:1261107 abstract is: <p>The wet-end of the centre ply of the paperboard machine n˚ 2 (“Kartongmaskin 2” or KM2) inIggesund was simulated to obtain both qualitative and quantitative results on its start-up dynamics andits variation damping capacity. The accuracy of the model was controlled by comparing the simulationresults with data from the real production process. Furthermore, alternative strategies with theobjective of reducing the time needed for start-ups and grade changes were evaluated.The modeling was done using the Paperfront simulation software. Variables such as layout for themixing system, retention level and amplitude and period of inlet concentration were used asparameters. As alternative operation strategies for start-up and grade change, increased flowrate in theearly phase of a start-up and the temporary overdosing during the beginning of a stock change wereevaluated.The results indicate a strong control of the mixing system over the short circulation both in dynamicsat start-up and in general operation with a time constant for the machine close to 35 minutes. Inaddition, variations in consistency are more easily controlled by a double-chest mixing system and, ingeneral, variations are more easily damped the higher the active volume in the mixing system. Finally,times needed for start-up and composition changes could be reduced by as much as 50% by using amore proactive approach.</p>

corrected abstract:
<p>The wet-end of the centre ply of the paperboard machine n˚ 2 (“Kartongmaskin 2” or KM2) in Iggesund was simulated to obtain both qualitative and quantitative results on its start-up dynamics and its variation damping capacity. The accuracy of the model was controlled by comparing the simulation results with data from the real production process. Furthermore, alternative strategies with the objective of reducing the time needed for start-ups and grade changes were evaluated.</p><p>The modeling was done using the Paperfront simulation software. Variables such as layout for the mixing system, retention level and amplitude and period of inlet concentration were used as parameters. As alternative operation strategies for start-up and grade change, increased flowrate in the early phase of a start-up and the temporary overdosing during the beginning of a stock change were evaluated.</p><p>The results indicate a strong control of the mixing system over the short circulation both in dynamics at start-up and in general operation with a time constant for the machine close to 35 minutes. In addition, variations in consistency are more easily controlled by a double-chest mixing system and, in general, variations are more easily damped the higher the active volume in the mixing system. Finally, times needed for start-up and composition changes could be reduced by as much as 50% by using a more proactive approach.</p>
----------------------------------------------------------------------
In diva2:826739 abstract is: <p>Many European growers of organic fruit have large losses in yield due to reduced quality oftheir products because of pest insect damage. These pest insect damages have made it difficultfor farmers all over Europe to grow organic fruit without risking significant economic losses.In organic soft fruit production there are no effective control measures for many of these pestinsects. Strawberry production is one part of the ecological fruit-production that is severleyaffected by these pest insects. This has become a big problem as there are no effective controlmeasures for pest insects. For herbivore insects species host plant volatiles are of majorimportance in location of host plants for feeding and oviposition. Therefore, there is potentialfor using these insect-host plant interactions to develop new strategies and effective controlmeasures for pest insects. For doing that it is needed to know which volatile organiccompounds the strawberry plants are releasing and which of these compounds that are used byinsects to detect the plants.</p><p>In this project, volatiles from buds, leaves and flowers from plants of wild strawberry speciesand cultivated strawberry species has been collected. Dynamic and static methods forsampling of volatiles have been used. Quantitative and qualitative analyses of volatiles havebeen carried out by gas chromatography and mass spectrometry techniques. The study showsthat the wild species in total releases six chemical compounds; Benzaldehyde , Limonene ,Benzylalcohol, Methyl salicylate, α - Muurolene and E,E-α - Farnesene . The study alsoshows that the cultivated species in total releases six chemical compounds; Limonene,Benzylalcohol , Methyl salicylate , p-Anisaldehyde, and (Z) -3- hexene- 1 -ol acetate. Withquantification it was found that p- Anisaldehyde is the chemical compound with the largestquantity emitted from the cultivated flowering plants .</p>

w='severley' val={'c': 'severely', 's': 'diva2:826739', 'n': 'no full text'}

corrected abstract:
<p>Many European growers of organic fruit have large losses in yield due to reduced quality of their products because of pest insect damage. These pest insect damages have made it difficult for farmers all over Europe to grow organic fruit without risking significant economic losses. In organic soft fruit production there are no effective control measures for many of these pestinsects. Strawberry production is one part of the ecological fruit-production that is severely affected by these pest insects. This has become a big problem as there are no effective controlmeasures for pest insects. For herbivore insects species host plant volatiles are of major importance in location of host plants for feeding and oviposition. Therefore, there is potential for using these insect-host plant interactions to develop new strategies and effective controlmeasures for pest insects. For doing that it is needed to know which volatile organic compounds the strawberry plants are releasing and which of these compounds that are used by insects to detect the plants.</p><p>In this project, volatiles from buds, leaves and flowers from plants of wild strawberry species and cultivated strawberry species has been collected. Dynamic and static methods for sampling of volatiles have been used. Quantitative and qualitative analyses of volatiles have been carried out by gas chromatography and mass spectrometry techniques. The study shows that the wild species in total releases six chemical compounds; Benzaldehyde, Limonene, Benzylalcohol, Methyl salicylate, α -Muurolene and E,E-α-Farnesene. The study also shows that the cultivated species in total releases six chemical compounds; Limonene, Benzylalcohol, Methyl salicylate, p-Anis aldehyde, and (Z)-3- hexene-1-ol acetate. With quantification it was found that p- Anis aldehyde is the chemical compound with the largest quantity emitted from the cultivated flowering plants.</p>
----------------------------------------------------------------------
In diva2:1421348 abstract is: <p>Within the type 1 diabetic research several applications have been evaluated to improve lifequality among type 1 diabetic patients. The aim of this project was to validate in vitro thefeasibility of using mucin hydrogel for islet transplantation . The project started with macroencapsulation of Min6-M9 cancer cell line extracted from 13 weeks old mice. The projectcontinued with encapsulation of Min6-M9 cells in microscopic hydrogel using a microfluidicdevice to evaluate the possibility to automate the encapsulation process, but also to facilitatethe future transplantation process upon surgery due to practical reasons since microscopicmucin hydrogel has smaller diameter than macro-encapsulated mucin hydrogel. Finally,primary islets isolated from 4 months old C57B1/6J laboratory mice was obtained and macroencapsulated in mucin hydrogel. In the experiments the samples were cultivated long termand continuously tested through quantitative data assay e.g. insulin secretion assay andAlamar Blue assay followed with qualitative data analysis through e.g. bright field imagesand measurements of the spherical diameters from single cells to clusters. This was followedwith LIVE/DEAD staining. From the obtained data from both Min6-M9 inmacro/microscopic hydrogel and primary islets it was possibility to show the long term isletssurvival of encapsulated Min6-M9 cells and primary islets. In addition, all samples secretedinsulin wherein primary islets had a clear insulin response upon glucose stimulation. Fromthe experiments in vitro the conclusion can be made that mucin hydrogel do supportencapsulated islets long term function and survival. From this, the feasibility usingencapsulated islets in mucin hydrogel needs to be investigated in vivo.</p>

partal corrected: diva2:1421348: <p>Within the type 1 diabetic research several applications have been evaluated to improve life quality among type 1 diabetic patients. The aim of this project was to validate in vitro the feasibility of using mucin hydrogel for islet transplantation . The project started with macroencapsulation of Min6-M9 cancer cell line extracted from 13 weeks old mice. The project continued with encapsulation of Min6-M9 cells in microscopic hydrogel using a microfluidic device to evaluate the possibility to automate the encapsulation process, but also to facilitate the future transplantation process upon surgery due to practical reasons since microscopic mucin hydrogel has smaller diameter than macro-encapsulated mucin hydrogel. Fin ally, primary islets isolated from 4 months old C57B1/6J laboratory mice was obtained and macroencapsulated in mucin hydrogel. In the experiments the samples were cultivated long term and continuously tested through quantitative data assay e.g. insulin secretion assay and Alamar Blue assay followed with qualitative data analysis through e.g. bright field images and measurements of the spherical diameters from single cells to clusters. This was followed with LIVE/DEAD staining. From the obtained data from both Min6-M9 in macro/microscopic hydrogel and primary islets it was possibility to show the long term islets survival of encapsulated Min6-M9 cells and primary islets. In addition, all samples secreted insulin wherein primary islets had a clear insulin response upon glucose stimulation. Fromthe experiments in vitro the conclusion can be made that mucin hydrogel do support encapsulated islets long term function and survival. From this, the feasibility using encapsulated islets in mucin hydrogel needs to be investigated in vivo.</p>

Markup is assumed to be italics for "in vitro" and "in vivo"

corrected abstract:
<p>Within the type 1 diabetic research several applications have been evaluated to improve life quality among type 1 diabetic patients. The aim of this project was to validate <em>in vitro</em> the feasibility of using mucin hydrogel for islet transplantation. The project started with macroencapsulation of Min6-M9 cancer cell line extracted from 13 weeks old mice. The project continued with encapsulation of Min6-M9 cells in microscopic hydrogel using a microfluidic device to evaluate the possibility to automate the encapsulation process, but also to facilitate the future transplantation process upon surgery due to practical reasons since microscopic mucin hydrogel has smaller diameter than macro-encapsulated mucin hydrogel. Finally, primary islets isolated from 4 months old C57B1/6J laboratory mice was obtained and macroencapsulated in mucin hydrogel. In the experiments the samples were cultivated long term and continuously tested through quantitative data assay e.g. insulin secretion assay and Alamar Blue assay followed with qualitative data analysis through e.g. bright field images and measurements of the spherical diameters from single cells to clusters. This was followed with LIVE/DEAD staining. From the obtained data from both Min6-M9 in macro/microscopic hydrogel and primary islets it was possibility to show the long term islets survival of encapsulated Min6-M9 cells and primary islets. In addition, all samples secreted insulin wherein primary islets had a clear insulin response upon glucose stimulation. From the experiments <em>in vitro</em> the conclusion can be made that mucin hydrogel do support encapsulated islets long term function and survival. From this, the feasibility using encapsulated islets in mucin hydrogel needs to be investigated <em>in vivo</em>.</p>
----------------------------------------------------------------------
In diva2:1366646 abstract is: <p>Remote control and management functions are widely utilized in multiple industries.The remote control and management functions has allowed for peopleto connect and interact to solve technical problems more efficiently. However,the healthcare organizations have not utilized the remote controlling and managementfunctions to a degree similar to other industries. Telephoning ande-mailing are still two mainstream ways of work when it comes to solvingtechnical support issues in-house. In order to understand what the technicalpersonnel and the clinical users at a hospital desires in new solutions, thismaster thesis project aimed at finding the existing needs in terms of remotecontrolling and management functions. To find these needs, Q-methodologywas applied for collection of subjective data from healthcare personnel abouta software device that aims at providing remote controlling and managementfunctions. In addition to finding and defining the needs, this thesis also aimedat examining how well such systems can address these needs.</p><p>Performing this methodology three factors where found representing three differentattitudes regarding the needs for remote functions. The three factorsare "Technical Communication is Significant", "Functionality Appreciativeand Experienced" and "Do if fast!". These factors and their interpretationhelps to be aware of and to evaluate remote support solutions in a systematicway.</p>

partal corrected: diva2:1366646: <p>Remote control and management functions are widely utilized in multiple industries. The remote control and management functions has allowed for people to connect and interact to solve technical problems more efficiently. However, the healthcare organizations have not utilized the remote controlling and management functions to a degree similar to other industries. Telephoning and e-mailing are still two mainstream ways of work when it comes to solving technical support issues in-house. In order to understand what the technical personnel and the clinical users at a hospital desires in new solutions, this master thesis project aimed at finding the existing needs in terms of remote controlling and management functions. To find these needs, Q-methodologywas applied for collection of subjective data from healthcare personnel about a software device that aims at providing remote controlling and management functions. In addition to finding and defining the needs, this thesis also aimed at examining how well such systems can address these needs.</p><p>Performing this methodology three factors where found representing three different attitudes regarding the needs for remote functions. The three factors are "Technical Communication is Significant", "Functionality Appreciative and Experienced" and "Do if fast!". These factors and their interpretation helps to be aware of and to evaluate remote support solutions in a systematic way.</p>

The error in lack of left double quote is in the actual abstract (as is the use of the right double quote).

corrected abstract:
<p>Remote control and management functions are widely utilized in multiple industries. The remote control and management functions has allowed for people to connect and interact to solve technical problems more efficiently. However, the healthcare organizations have not utilized the remote controlling and management functions to a degree similar to other industries. Telephoning and e-mailing are still two mainstream ways of work when it comes to solving technical support issues in-house. In order to understand what the technical personnel and the clinical users at a hospital desires in new solutions, this master thesis project aimed at finding the existing needs in terms of remote controlling and management functions. To find these needs, Q-methodology was applied for collection of subjective data from healthcare personnel about a software device that aims at providing remote controlling and management functions. In addition to finding and defining the needs, this thesis also aimed at examining how well such systems can address these needs.</p><p>Performing this methodology three factors where found representing three different attitudes regarding the needs for remote functions. The three factors are ”Technical Communication is Significant”, ”Functionality Appreciative and Experienced” and ”Do if fast!”. These factors and their interpretation helps to be aware of and to evaluate remote support solutions in a systematic way.</p>
----------------------------------------------------------------------
title: "Digitalization of Healthcare for Heart Failure Patients: An Analysis ofthe Opportunities and Restrictions in the Implementation of Self-Monitoring Systems for Heart Failure Patients in Sweden"
==> "Digitalization of Healthcare for Heart Failure Patients: An Analysis of the Opportunities and Restrictions in the Implementation of Self-Monitoring Systems for Heart Failure Patients in Sweden"

In diva2:1679918 abstract is: <p>The Swedish government and the Swedish Association of Local Authorities and Regions, cametogether on a vision that Sweden will be the most advanced country in offering opportunities fordigitalization and e-health by the year of 2025. Self-monitoring is one aspect that can digitizeworkflow in healthcare.</p><p>This master thesis sought to investigate the opportunities and restrictions when implementingself-monitoring for heart failure patients in Swedish healthcare. Starting from the holisticperspective as an analysis of the clinician’s interest and the need for self-monitoring systemsby conducting a mixed method approach. Surveys were sent out to different county councilsin Sweden working with heart failure patients, and interviews were conducted withrepresentatives from primary care, specialist care, and advanced home health care. Secondly,interviews were conducted with stakeholders from the field of implementation in order togather qualitative results on the organizational, legal, and technical influencing factors,challenges, and requirements in Swedish healthcare. The qualitative data were processedthrough thematic analysis and the quantitative data was processed through descriptive analysis.</p><p>The results gathered from clinicians consisted of three areas regarding functionality demandsin self-monitoring systems. The results gathered in the aspect of implementation consisted ofthree main organizational influencing aspects, three solution models from a legal perspective,as well as three aspects related in the field of technology and IT.</p>

corrected abstract:
<p>The Swedish government and the Swedish Association of Local Authorities and Regions, came together on a vision that Sweden will be the most advanced country in offering opportunities for digitalization and e-health by the year of 2025. Self-monitoring is one aspect that can digitize workflow in healthcare.</p><p>This master thesis sought to investigate the opportunities and restrictions when implementing self-monitoring for heart failure patients in Swedish healthcare. Starting from the holistic perspective as an analysis of the clinician’s interest and the need for self-monitoring systems by conducting a mixed method approach. Surveys were sent out to different county councils in Sweden working with heart failure patients, and interviews were conducted with representatives from primary care, specialist care, and advanced home health care. Secondly, interviews were conducted with stakeholders from the field of implementation in order to gather qualitative results on the organizational, legal, and technical influencing factors, challenges, and requirements in Swedish healthcare. The qualitative data were processed through thematic analysis and the quantitative data was processed through descriptive analysis.</p><p>The results gathered from clinicians consisted of three areas regarding functionality demands in self-monitoring systems. The results gathered in the aspect of implementation consisted of three main organizational influencing aspects, three solution models from a legal perspective, as well as three aspects related in the field of technology and IT.</p>
----------------------------------------------------------------------
In diva2:1678223 abstract is: <p>The ability of an individual to withstand elevated head-to-toe gravitoinertial (+Gz) forces is determined by the capacity of their body to maintain sufficient head-level arterial pressure. Recent studies have shown a relationship between resting blood-vessel stiffness and an individual’s +Gz-tolerance, although the mechanisms behind this relationship are unclear. The aim of this project is to determine whether or not +Gz-tolerance is affected by a change inresting vasomotor tone. To evaluate this relationship, seven participants were asked to complete a +Gz-tolerance protocol using a human-use centrifugeon two different occasions. On both visits, gradual onset rate (0.1 G.s−1)and rapid onset rate (3.5 G.s−1) tests were done to evaluate the participants+Gz-tolerance. On one of the two visits, prior to the +Gz-tolerance testing,participants performed a 20-min cycle intervention to induce postexercisehypotension, with the aim of temporarily reducing participants’ resting bloodpressure and vasomotor tone. The cycling intervention was successful atinducing postexercise hypotension, as mean arterial pressure was significantlylower on the cycling visit (P&lt;0.05). +Gz-tolerance was significantly lower(P&lt;0.05) on the cycling visit compared with the non-cycling visit for both theGOR and ROR tests (absolute difference of 0.5 G and 0.25 G, respectively).The effect of the type of test on +Gz-tolerance was not influenced by the effectof the cycling intervention (P&gt;0.05). Being the most documented mechanismlinked to postexercise hypotension, sustained vasodilation was assumed tohave occurred. This would have increased distensibility of the affected vessels,explaining the decrease in +Gz-tolerance. The decrease in +Gz-tolerance wassimilar for both tests, indicating that the baroreflex was not affected by thecycling intervention. Assuming that vasodilation occurred, this study showedthat a decrease in resting vasomotor tone decreased +Gz-tolerance, indicatingthe importance of this variable in the relationship between resting blood-vesselstiffness and an individual’s +Gz-tolerance.</p>


corrected abstract:
<p>The ability of an individual to withstand elevated head-to-toe gravitoinertial (+Gz) forces is determined by the capacity of their body to maintain sufficient head-level arterial pressure. Recent studies have shown a relationship between resting blood-vessel stiffness and an individual’s +Gz-tolerance, although the mechanisms behind this relationship are unclear. The aim of this project is to determine whether or not +Gz-tolerance is affected by a change in resting vasomotor tone. To evaluate this relationship, seven participants were asked to complete a +Gz-tolerance protocol using a human-use centrifuge on two different occasions. On both visits, gradual onset rate (0.1 G.s<sup>−1</sup>) and rapid onset rate (3.5 G.s<sup>−1</sup>) tests were done to evaluate the participants +Gz-tolerance. On one of the two visits, prior to the +Gz-tolerance testing, participants performed a 20-min cycle intervention to induce postexercise hypotension, with the aim of temporarily reducing participants’ resting blood pressure and vasomotor tone. The cycling intervention was successful at inducing postexercise hypotension, as mean arterial pressure was significantly lower on the cycling visit (<em>P</em>&lt;0.05). +Gz-tolerance was significantly lower (<em>P</em>&lt;0.05) on the cycling visit compared with the non-cycling visit for both the GOR and ROR tests (absolute difference of 0.5 G and 0.25 G, respectively). The effect of the type of test on +Gz-tolerance was not influenced by the effect of the cycling intervention (P&gt;0.05). Being the most documented mechanism linked to postexercise hypotension, sustained vasodilation was assumed to have occurred. This would have increased distensibility of the affected vessels, explaining the decrease in +Gz-tolerance. The decrease in +Gz-tolerance was similar for both tests, indicating that the baroreflex was not affected by the cycling intervention. Assuming that vasodilation occurred, this study showed that a decrease in resting vasomotor tone decreased +Gz-tolerance, indicating the importance of this variable in the relationship between resting blood-vessel stiffness and an individual’s +Gz-tolerance.</p>
----------------------------------------------------------------------
In diva2:773317 abstract is: <p>The Mandometer® Method helps people with eating disorders by teaching them how to eat ina ”correct” way by training with a product called Mandometer®. The users learn the correcteating behavior by receiving feedback from the display on the Mandometer® handheldcomputer on whether they eat too fast or too slow during the meal. Mando Group AB, thecompany which has developed the Mandometer® Method, has also created a computerprogram that works as a simulation of a meal with the Mandometer®. The purpose of thisstudy was to examine if training with the simulation could give the same effect as trainingwith Mandometer®. By letting a group of healthy testpersons practice with the simulationduring three weeks a result was given which clearly showed that the eating behavior of thetest persons had changed in the desired direction. The result indicates that there is a possibilityto add the simulation to the Mandometer® Method in the future. However, more testing anddeveloping of the program is first required.To improve Mandometer® further, a new version of the product has been developed wherethe handheld computer has been replaced by a cellphone and its functions by anapplication. The functionality and usability of the application was tested with the methodcognitive walkthrough, which showed that most of the pages of the application maintaineda high standard. With some adjustment of the pages where the usability was low, theapplication should be able to replace the old version of Mandometer® in the future.</p>

The actual abstract uses the double right quotation mark for both start and end of a quote.
The first "testpersons" does not have a space, while the second use does.

corrected abstract:
<p>The Mandometer® Method helps people with eating disorders by teaching them how to eat in a ”correct” way by training with a product called Mandometer®. The users learn the correct eating behavior by receiving feedback from the display on the Mandometer® handheld computer on whether they eat too fast or too slow during the meal. Mando Group AB, the company which has developed the Mandometer® Method, has also created a computer program that works as a simulation of a meal with the Mandometer®. The purpose of this study was to examine if training with the simulation could give the same effect as training with Mandometer®. By letting a group of healthy testpersons practice with the simulation during three weeks a result was given which clearly showed that the eating behavior of the test persons had changed in the desired direction. The result indicates that there is a possibility to add the simulation to the Mandometer® Method in the future. However, more testing and developing of the program is first required.</p><p>To improve Mandometer® further, a new version of the product has been developed where the handheld computer has been replaced by a cellphone and its functions by an application. The functionality and usability of the application was tested with the method cognitive walkthrough, which showed that most of the pages of the application maintained a high standard. With some adjustment of the pages where the usability was low, the application should be able to replace the old version of Mandometer® in the future.</p>
----------------------------------------------------------------------
In diva2:731857 abstract is: <p>This report presents the design and manufacturing process of a bionic signal messagebroker (BSMB), intended to allow communication between implanted electrodes andprosthetic legs designed by Ossur. The BSMB processes and analyses the data intorelevant information to control the bionic device. The intention is to carry out eventdetection in the BSMB, where events in the muscle signal are matched to the events ofthe gait cycle (toe-o, stance, swing).The whole system is designed to detect muscle contraction via sensors implantedin residual muscles and transmit the signals wireless to a control unit that activatesassociated functions of a prosthetic leg. Two users, one transtibial and one transfemoral,underwent surgery in order to get electrodes implantable into their residual leg muscles.They are among the rst users in the world to get this kind of implanted sensors.A prototype of the BSMB was manufactured. The process took more time thanexpected, mainly due to the fact that it was decided to use a ball grid array (BGA)microprocessor in order to save space. That meant more complicated routing and higherstandards for the manufacturing of the board. The results of the event detection indicatethat the data from the implanted electrodes can be used in order to get sucient controlover prosthetic legs. These are positive ndings for users of prosthetic legs and shouldincrease their security and quality of life.It is important to keep in mind when the results of this report are evaluated that allthe testing carried out were only done on one user each.</p>

w='ndings' val={'c': 'findings', 's': 'diva2:731857', 'n': 'missing ligature'}
w='sucient' val={'c': 'sufficient', 's': 'diva2:731857', 'n': 'missing ligature'}

corrected abstract:
<p>This report presents the design and manufacturing process of a bionic signal message broker (BSMB), intended to allow communication between implanted electrodes and prosthetic legs designed by Össur. The BSMB processes and analyses the data into relevant information to control the bionic device. The intention is to carry out event detection in the BSMB, where events in the muscle signal are matched to the events of the gait cycle (toe-off, stance, swing).</p><p>The whole system is designed to detect muscle contraction via sensors implanted in residual muscles and transmit the signals wireless to a control unit that activates associated functions of a prosthetic leg. Two users, one transtibial and one transfemoral, underwent surgery in order to get electrodes implantable into their residual leg muscles. They are among the first users in the world to get this kind of implanted sensors.</p><p>A prototype of the BSMB was manufactured. The process took more time than expected, mainly due to the fact that it was decided to use a ball grid array (BGA) microprocessor in order to save space. That meant more complicated routing and higher standards for the manufacturing of the board. The results of the event detection indicate that the data from the implanted electrodes can be used in order to get sufficient control over prosthetic legs. These are positive findings for users of prosthetic legs and should increase their security and quality of life.</p><p>It is important to keep in mind when the results of this report are evaluated that all the testing carried out were only done on one user each.</p>
----------------------------------------------------------------------
In diva2:1315699 abstract is: <p>This report investigates the feasibility of several types of biomass to be used as feedstock forproduction of biochar by slow pyrolysis. A literature review and case studies for all investigatedfeedstocks resulted in two models: one for the characterization of physical and chemical propertiesof biochar at different high treatment temperatures, and the other for determining to what degreethe system will be thermally self-sustaining, if at all. This by determining the energy required by thereactor in comparison to the energy available in the pyrolysis gas. The primary investigatedfeedstocks were: fibre sludge, lignin pellets, olive wastes, sunflower seeds and exhausted coffeeresidue. Additionally, cashew nut shells, coconut shells, rice husks and almond shells were alsoinvestigated to determine their suitability for future use by Stockholm Exergi. The literature reviewshowed that there are various process parameters or parameters within the composition of thefeedstock that effects both the quality of the produced biochar, product distributions, and benefitswithin the system. To quantify the effect of all the parameters proved difficult due to the lack ofdata. However, enough data regarding the effects of the treatment temperature was collected andcould be used for modelling. Model 1 showed that biochar produced from nutshells generallyproduced biochar of higher quality than biochar made from kernels, different types of straw andfeedstocks with high content of water and ash. Most nutshells would, according to the conductedcase study, be more suited for processes where the primary objective is production of bio-oil. Model2 showed that almond shells and olive kernels should generate a thermally self-sustaining process attemperatures above 400 °C.</p>

corrected abstract:
<p>This report investigates the feasibility of several types of biomass to be used as feedstock for production of biochar by slow pyrolysis. A literature review and case studies for all investigated feedstocks resulted in two models: one for the characterization of physical and chemical properties of biochar at different high treatment temperatures, and the other for determining to what degree the system will be thermally self-sustaining, if at all. This by determining the energy required by the reactor in comparison to the energy available in the pyrolysis gas. The primary investigated feedstocks were: fibre sludge, lignin pellets, olive wastes, sunflower seeds and exhausted coffee residue. Additionally, cashew nut shells, coconut shells, rice husks and almond shells were also investigated to determine their suitability for future use by Stockholm Exergi. The literature review showed that there are various process parameters or parameters within the composition of the feedstock that effects both the quality of the produced biochar, product distributions, and benefits within the system. To quantify the effect of all the parameters proved difficult due to the lack of data. However, enough data regarding the effects of the treatment temperature was collected and could be used for modelling. Model 1 showed that biochar produced from nutshells generally produced biochar of higher quality than biochar made from kernels, different types of straw and feedstocks with high content of water and ash. Most nutshells would, according to the conducted case study, be more suited for processes where the primary objective is production of bio-oil. Model 2 showed that almond shells and olive kernels should generate a thermally self-sustaining process at temperatures above 400 °C.</p>
----------------------------------------------------------------------
title: "Production and evaluation of spider silk proteins forsortase A-mediated functionalization"
==>    "Production and evaluation of spider silk proteins for sortase A-mediated functionalization"


In diva2:1455140 abstract is: <p>The recombinant spider silk protein 4RepCT has been functionalized in many ways throughthe years. Sortase A coupling is an important method used when proteins or binders cannot beexpressed as fusion proteins with 4RepCT, because of size, complexity or if the protein is inneed of post-translational modifications. Previous studies have shown that an increasingnumber of glycine residues increase the yield and rate of the enzymatic coupling. The projectaim was to investigate two new silk proteins, 4RepCT-Srt and G5-4RepCT, to be able toevaluate if the sortase A coupling could be improved. The IgG binding domain Z(containingthree N-terminal glycine residues) was coupled both in solution and on coatings of 4RepCTSrt,both on coatings of 4RepCT-Srt and in solution using SDS-PAGE. IgG-fluorophore wasused to be able to detect and get an estimation of coupled product (4RepCT-Z), and SDSPAGEwas used to estimate the amount and rate of product formation, which was comparedto the wild type silk, G-4RepCT. The G5-4RepCT had to be re-cloned, due to difficulties inexpression and purification of the previous construct. The results show that three or moreglycines improve the rate of coupling, and 4RepCT-Srt works at least as efficient as the4RepCT. Improved sortase coupling could be beneficial for future cancer immunotherapyusing 4RepCT silk, where sortase can be used to covalently attach antibody fragments to thesilk.</p>

w='inexpression' val={'c': 'in expression', 's': 'diva2:1455140', 'n': 'correct in original'}

corrected abstract:
<p>The recombinant spider silk protein 4RepCT has been functionalized in many ways through the years. Sortase A coupling is an important method used when proteins or binders cannot be expressed as fusion proteins with 4RepCT, because of size, complexity or if the protein is in need of post-translational modifications. Previous studies have shown that an increasing number of glycine residues increase the yield and rate of the enzymatic coupling. The project aim was to investigate two new silk proteins, 4RepCT-Srt and G<sub>5</sub>-4RepCT, to be able to evaluate if the sortase A coupling could be improved. The IgG binding domain Z(containing three N-terminal glycine residues) was coupled both in solution and on coatings of 4RepCTSrt, both on coatings of 4RepCT-Srt and in solution using SDS-PAGE. IgG-fluorophore was used to be able to detect and get an estimation of coupled product (4RepCT-Z), and SDS-PAGE was used to estimate the amount and rate of product formation, which was compared to the wild type silk, G-4RepCT. The G<sub>5</sub>-4RepCT had to be re-cloned, due to difficulties in expression and purification of the previous construct. The results show that three or more glycines improve the rate of coupling, and 4RepCT-Srt works at least as efficient as the 4RepCT. Improved sortase coupling could be beneficial for future cancer immunotherapy using 4RepCT silk, where sortase can be used to covalently attach antibody fragments to the silk.</p>
----------------------------------------------------------------------
title: "Production and evaluation ofspider silk protein for sortase Amediatedfunctionalization"
==>    "Production and evaluation of spider silk protein for sortase A mediated functionalization"

This version without full text seems to be the same thesis as diva2:1455140
----------------------------------------------------------------------
In diva2:706730 abstract is: <p>The aim of this thesis was to analyze and evaluate possible matches betweeninvoices and assets from two different systems. This degree project was done incooperation with DGC, a network operator with an network stretching beyondthe borders of Sweden. The network solutions are provided through rentingthe copper and fiber connections (access services) from network owners andusing these to connect customers to their own network backbone. As a firststep towards a clearer overview of which customer pays for what, there was aneed to connect incomes generated from a paying customer with the outcomegenerated for renting an asset.  The approach taken to solve the problem involved analyzing the theory offuzzy logic and developing system support for matching using fuzzy logic ina .NET Windows Presentation Foundation application using the Model ViewViewModel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly createpermanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map theinvoices to the assets tremendously. This will make future steps in automatedcost analysis possible, giving the company a better overview of their financesand helping them to more easily find strange occurrences where an asset is beingrented without a customer paying for it, or vice versa.</p>

partal corrected: diva2:706730: <p>The aim of this thesis was to analyze and evaluate possible matches between invoices and assets from two different systems. This degree project was done in cooperation with DGC, a network operator with an network stretching beyond the borders of Sweden. The network solutions are provided through renting the copper and fiber connections (access services) from network owners and using these to connect customers to their own network backbone. As a first step towards a clearer overview of which customer pays for what, there was a need to connect incomes generated from a paying customer with the outcome generated for renting an asset.  The approach taken to solve the problem involved analyzing the theory of fuzzy logic and developing system support for matching using fuzzy logic in a .NET Windows Presentation Foundation application using the Model ViewViewModel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly create permanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map the invoices to the assets tremendously. This will make future steps in automated cost analysis possible, giving the company a better overview of their fin ancesand helping them to more easily find strange occurrences where an asset is be ingrented without a customer paying for it, or vice versa.</p>
w='ViewViewModel' val={'c': 'Model–view–viewmodel', 's': 'diva2:706730', 'n': 'no full text'}

corrected abstract:
<p>The aim of this thesis was to analyze and evaluate possible matches between invoices and assets from two different systems. This degree project was done in cooperation with DGC, a network operator with an network stretching beyond the borders of Sweden. The network solutions are provided through renting the copper and fiber connections (access services) from network owners and using these to connect customers to their own network backbone. As a first step towards a clearer overview of which customer pays for what, there was a need to connect incomes generated from a paying customer with the outcome generated for renting an asset.  The approach taken to solve the problem involved analyzing the theory of fuzzy logic and developing system support for matching using fuzzy logic in a .NET Windows Presentation Foundation application using the Model Model–view–viewmodel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly create permanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map the invoices to the assets tremendously. This will make future steps in automated cost analysis possible, giving the company a better overview of their fin ancesand helping them to more easily find strange occurrences where an asset is be ingrented without a customer paying for it, or vice versa.</p>
In diva2:706730 abstract is: <p>The aim of this thesis was to analyze and evaluate possible matches betweeninvoices and assets from two different systems. This degree project was done incooperation with DGC, a network operator with an network stretching beyondthe borders of Sweden. The network solutions are provided through rentingthe copper and fiber connections (access services) from network owners andusing these to connect customers to their own network backbone. As a firststep towards a clearer overview of which customer pays for what, there was aneed to connect incomes generated from a paying customer with the outcomegenerated for renting an asset.  The approach taken to solve the problem involved analyzing the theory offuzzy logic and developing system support for matching using fuzzy logic ina .NET Windows Presentation Foundation application using the Model ViewViewModel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly createpermanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map theinvoices to the assets tremendously. This will make future steps in automatedcost analysis possible, giving the company a better overview of their financesand helping them to more easily find strange occurrences where an asset is beingrented without a customer paying for it, or vice versa.</p>

w='ViewViewModel' val={'c': 'Model–view–viewmodel', 's': 'diva2:706730', 'n': 'no full text'}

corrected abstract:
<p>The aim of this thesis was to analyze and evaluate possible matches between invoices and assets from two different systems. This degree project was done in cooperation with DGC, a network operator with an network stretching beyond the borders of Sweden. The network solutions are provided through renting the copper and fiber connections (access services) from network owners and using these to connect customers to their own network backbone. As a first step towards a clearer overview of which customer pays for what, there was a need to connect incomes generated from a paying customer with the outcome generated for renting an asset.  The approach taken to solve the problem involved analyzing the theory of fuzzy logic and developing system support for matching using fuzzy logic in a .NET Windows Presentation Foundation application using the Model Model–view–viewmodel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly create permanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map the invoices to the assets tremendously. This will make future steps in automated cost analysis possible, giving the company a better overview of their finances and helping them to more easily find strange occurrences where an asset is being rented without a customer paying for it, or vice versa.</p>
----------------------------------------------------------------------
In diva2:1142793 abstract is: <p>Gas sensing in medical applications requiresmall, precise and sensitive sensors. This projecthas developed a laboratory setup for characterisationof a waveguide-based gas sensor for carbon dioxide andmethane working in the mid-IR range of 2 - 10 μm. Thissetup utilizes an IR-camera to image the waveguideswhen a mid-IR laser is coupled into them. Along thelaboratory work, a program for optimisation of waveguidelength has been made and a study of on-marketmedical carbon dioxide sensors has been done. Thelaboratory setup shows potential for good measurementof waveguide losses, but several problems was identifiedwith the measurement methods currently used. Fromthe sensor study, the standard performance for currentsensors is presented as well as areas where gas sensorscould be improved. Size, speed and accuracy were someof the characteristics a waveguide-based sensor couldimprove on and open up for new sensor application in,for example, hand-held medical devices.</p>


The PDF abstract uses Unicode Character “µ” (U+00B5) - while the version below uses Unicode Character “μ” (U+03BC).

corrected abstract:
<p>Gas sensing in medical applications require small, precise and sensitive sensors. This project has developed a laboratory setup for characterisation of a waveguide-based gas sensor for carbon dioxide and methane working in the mid-IR range of 2 - 10 μm. This setup utilizes an IR-camera to image the waveguides when a mid-IR laser is coupled into them. Along the laboratory work, a program for optimisation of waveguide length has been made and a study of on-market medical carbon dioxide sensors has been done. The laboratory setup shows potential for good measurement of waveguide losses, but several problems was identified with the measurement methods currently used. From the sensor study, the standard performance for current sensors is presented as well as areas where gas sensors could be improved. Size, speed and accuracy were some of the characteristics a waveguide-based sensor could improve on and open up for new sensor application in, for example, hand-held medical devices.</p>
----------------------------------------------------------------------
In diva2:1454472 abstract is: <p>Growing concerns of the negative effects on the environment and dependency of fossil fuelsare major driving forces for finding novel sustainable production pathways for plastic.Metabolic engineering has emerged as a powerful tool to enable microorganisms to producenon-native metabolites. The aim of this project was recombinant production of 4-hydroxybutyrate (4-HB) by expressing two enzymes in the model organism Escherichia coli.α-ketoglutarate decarboxylase (SucA) from Mycobacterium smegmatis followed by 4-hydroxybutyrate dehydrogenase (4-HBd) from Clostridium kluyveri was expressed inEscherichia coli. Results showed that the genes were successfully transformed and expressedin E. coli and after protein purification a concentration of 0.9 g/L SucA and 9.8 g/L 4-HBdwas achieved. Furthermore, some protein activity was detected by a coupled reaction withSucA and 4-HBd. When the enzymes got coupled together a change in NADH concentrationcould be detected spectrophotometrically. The enzymes were also tested for substratespecificity by using substrates with various carbon chain lengths and a decrease in NADHconcentration was seen. However, a decrease in the negative control for the experiments wasalso seen indicating a breakdown of NADH over time rather than consumption. Therefore, noconclusion could be drawn about the promiscuity of the enzymes. Lastly a single plasmidssystem was tested where both the genes were ligated on the same plasmid (pCDF duet) andexpressed successfully in E. coli Bl21DE3.</p>

corrected abstract:
<p>Growing concerns of the negative effects on the environment and dependency of fossil fuels are major driving forces for finding novel sustainable production pathways for plastic. Metabolic engineering has emerged as a powerful tool to enable microorganisms to produce non-native metabolites. The aim of this project was recombinant production of 4-hydroxybutyrate (4-HB) by expressing two enzymes in the model organism <em>Escherichia coli</em>. α-ketoglutarate decarboxylase (SucA) from <em>Mycobacterium smegmatis</em> followed by 4-hydroxybutyrate dehydrogenase (4-HBd) from Clostridium kluyveri was expressed in <em>Escherichia coli</em>. Results showed that the genes were successfully transformed and expressed in <em>E. coli</em> and after protein purification a concentration of 0.9 g/L SucA and 9.8 g/L 4-HBd was achieved. Furthermore, some protein activity was detected by a coupled reaction with SucA and 4-HBd. When the enzymes got coupled together a change in NADH concentration could be detected spectrophotometrically. The enzymes were also tested for substrate specificity by using substrates with various carbon chain lengths and a decrease in NADH concentration was seen. However, a decrease in the negative control for the experiments was also seen indicating a breakdown of NADH over time rather than consumption. Therefore, no conclusion could be drawn about the promiscuity of the enzymes. Lastly a single plasmids system was tested where both the genes were ligated on the same plasmid (pCDF duet) and expressed successfully in <em>E. coli</em> Bl21DE3.</p>
----------------------------------------------------------------------
In diva2:1553336 abstract is: <p>Muscle morphological parameters such as fascicle length (FL), pennationangle (PA) and physiologic cross-sectional area (PCSA) can provide an insightinto the reasons of the deteriorated muscle functions caused by pathologies.This study investigates the 3D structure of the lower leg muscles using 3Dfreehand ultrasound (3DfUS). This imaging modality uses a motion capturesystem to track the position of the US probe during acquisition and thusreconstruct the structure of the tissues in 3D. In this study, two subjects werescanned on the medial gastrocnemius (MG) and tibialis anterior (TA) musclesin the lower leg using 3DfUS system. The FL and PA of the muscles werecalculated and compared with the values previously measured using diffusiontensor imaging (DTI). The results using 3DfUS were averagely 19.2% largerin FL and 2.9%larger in PA. In conclusion, 3DfUS can successfully determinemuscle morphological parameters within a physiologically acceptable range.But the differences in FL observed between the two imaging modalities werequite big, which probably was due to the differences in sample size and area.The values can also differ greatly within the 3DfUS measurements as a resultof different manipulations during data processing, and the 3DfUS protocolneeds to be further improved in future studies.</p>


w='pennationangle' val={'c': 'pennation angle', 's': 'diva2:1553336', 'n': 'there was a new line between the words'}

corrected abstract:
<p>Muscle morphological parameters such as fascicle length (FL), pennation angle (PA) and physiologic cross-sectional area (PCSA) can provide an insight into the reasons of the deteriorated muscle functions caused by pathologies. This study investigates the 3D structure of the lower leg muscles using 3D freehand ultrasound (3DfUS). This imaging modality uses a motion capture system to track the position of the US probe during acquisition and thus reconstruct the structure of the tissues in 3D. In this study, two subjects were scanned on the medial gastrocnemius (MG) and tibialis anterior (TA) muscles in the lower leg using 3DfUS system. The FL and PA of the muscles were calculated and compared with the values previously measured using diffusion tensor imaging (DTI). The results using 3DfUS were averagely 19.2% larger in FL and 2.9% larger in PA. In conclusion, 3DfUS can successfully determine muscle morphological parameters within a physiologically acceptable range. But the differences in FL observed between the two imaging modalities were quite big, which probably was due to the differences in sample size and area. The values can also differ greatly within the 3DfUS measurements as a result of different manipulations during data processing, and the 3DfUS protocol needs to be further improved in future studies.</p>
----------------------------------------------------------------------
In diva2:1451620 abstract is: <p>Abstract</p><p>In recent years, interest has increased for so-called chatbot agents which uses patternrecognition to provide solutions in the customer service area that offer businessbenefits in the form of reduced staff costs combined with high customer accessibility.In order to optimize customer benefit, frequent maintenance is required. The accuracyof the system is increased by adapting chatbot response based on previous conversations.In order to investigate further improvement measures in pattern recognitionfor chatbot systems, a comparative study is conducted between two differenttechniques. The aim of the study is to determine which of the two techniques thatinterpret the intent of a question being asked most accurately.Several prototypes have been developed during the study, to evaluate the techniqueslemmatization and stemming for the Swedish and English language.The result show that lemmatization and stemming techniques for the Swedish languagecan be used to increase the accuracy of a pattern recognition system in textclassification and that lemmatization do so to a larger extent than stemming. Prototypestested with the English language show evidence which suggests that lemmatizationincrease the level of accuracy in comparison with the use of stemming, whichitself did not increase the accuracy.</p><p>Keywords</p><p>Conversational AI, NLP, NLU, Rasa, Chatbot-optimization, Chatbot, Text classification,lemmatization, stemming</p>



corrected abstract:
<p>In recent years, interest has increased for so-called chatbot agents which uses pattern recognition to provide solutions in the customer service area that offer business benefits in the form of reduced staff costs combined with high customer accessibility.</p><p>In order to optimize customer benefit, frequent maintenance is required. The accuracy of the system is increased by adapting chatbot response based on previous conversations. In order to investigate further improvement measures in pattern recognition for chatbot systems, a comparative study is conducted between two different techniques. The aim of the study is to determine which of the two techniques that interpret the intent of a question being asked most accurately.</p><p>Several prototypes have been developed during the study, to evaluate the techniques lemmatization and stemming for the Swedish and English language.</p><p>The result show that lemmatization and stemming techniques for the Swedish language can be used to increase the accuracy of a pattern recognition system in text classification and that lemmatization do so to a larger extent than stemming. Prototypes tested with the English language show evidence which suggests that lemmatization increase the level of accuracy in comparison with the use of stemming, which itself did not increase the accuracy.</p>
----------------------------------------------------------------------
In diva2:1688735 abstract is: <p>The spread of the Covid-19 has led to a worldwide pandemic which altered many aspects ofhuman life including higher education. As a result of a worldwide pandemic the educationsystem shifted from university campuses to virtual setups. This shift had a major impact onthe faculty members and professors teaching across the globe. While there the phenomenonwas being studied from students‟ perspective, this study highlights the impact of digitalteaching on the university professors at two universities in Sweden and Pakistan.The aim of this thesis project was to conduct a comparative study which explores howuniversity professors adapted their work environment in light of COVID-19 and e-learning.The universities primarily being studied are KTH Royal Institute of Technology situated inStockholm, Sweden and National University of Science and Technology situated inIslamabad, Pakistan. The ambition was also to discern measures to cater positive health andwork environment, and a diverse knowledge pool of best practices through a qualitativeinterview based study. These findings were generated through inductive reasoning byanalyzing eleven interviews conducted in both countries. The discussion was steered by theHuman, Technology, and Organization (HTO) - Model and concepts of ResilienceEngineering.</p>

corrected abstract:
<p>The spread of the Covid-19 has led to a worldwide pandemic which altered many aspects of human life including higher education. As a result of a worldwide pandemic the education system shifted from university campuses to virtual setups. This shift had a major impact on the faculty members and professors teaching across the globe. While there the phenomenon was being studied from students‟ perspective, this study highlights the impact of digital teaching on the university professors at two universities in Sweden and Pakistan. The aim of this thesis project was to conduct a comparative study which explores how university professors adapted their work environment in light of COVID-19 and e-learning. The universities primarily being studied are KTH Royal Institute of Technology situated in Stockholm, Sweden and National University of Science and Technology situated in Islamabad, Pakistan. The ambition was also to discern measures to cater positive health and work environment, and a diverse knowledge pool of best practices through a qualitative interview based study. These findings were generated through inductive reasoning by analyzing eleven interviews conducted in both countries. The discussion was steered by the Human, Technology, and Organization (HTO) - Model and concepts of Resilience Engineering.</p>
----------------------------------------------------------------------
In diva2:949728 abstract is: <p>Patients with cerebral palsy account for great upper extremities deviationswhile walking. However, the number of studies assessing their upper bodygait kinematics are rare and no studies have been conducted interested inthe whole body kinematics during walking. In this study, we created awhole body index, the Body Profile Score made of modified existing kinematicindexes assessing the gait pattern of children with cerebral palsy. TheBody Profile Score (BPS) is an average of combination of the Gait ProfileScore (GPS), a modified Trunk Profile Score (TPS), a modified Arm PostureScore (APS) and a also new index called Head Profile Score (HPS), basedon a similar calculation. Dierent versions of the BPS were tested on threegroups: a control group, a CP group before botulinum toxin A treatmentand a CP group after botulinum toxin A treatment. The results showed apoor level of linear correlations between the dierent BPS versions and theGait Profile Score, indicating that lower body indexes such as the GPS orGait Deviation Index (GDI) and full body index such as the BPS do not renderthe same information. The BPS is the first index proposing a full bodykinematic analysis and aims at showing that such an analysis is needed ingait assessment of spastic children in order to have a realistic overview ofthe pathological walking condition.</p>


w='Dierent' val={'c': 'Different', 's': ['diva2:1344757', 'diva2:949728'], 'n': 'missing ligature'}

corrected abstract:
<p>Patients with cerebral palsy account for great upper extremities deviations while walking. However, the number of studies assessing their upper body gait kinematics are rare and no studies have been conducted interested in the whole body kinematics during walking. In this study, we created a whole body index, the Body Profile Score made of modified existing kinematic indexes assessing the gait pattern of children with cerebral palsy. The Body Profile Score (BPS) is an average of combination of the Gait Profile Score (GPS), a modified Trunk Profile Score (TPS), a modified Arm Posture Score (APS) and a also new index called Head Profile Score (HPS), based on a similar calculation. Different versions of the BPS were tested on three groups: a control group, a CP group before botulinum toxin A treatment and a CP group after botulinum toxin A treatment. The results showed a poor level of linear correlations between the different BPS versions and the Gait Profile Score, indicating that lower body indexes such as the GPS or Gait Deviation Index (GDI) and full body index such as the BPS do not render the same information. The BPS is the first index proposing a full body kinematic analysis and aims at showing that such an analysis is needed in gait assessment of spastic children in order to have a realistic overview of the pathological walking condition.</p>
----------------------------------------------------------------------
In diva2:1451610 abstract is: <p>Abstract</p><p>Fingerprinting a website is the process of identifying what technologies a websiteuses, such as their used web applications and JavaScript frameworks. Currentfingerprinting methods use manually created fingerprints for each technology itlooks for. These fingerprints consist of multiple text strings that are matchedagainst an HTTP response from a website. Creating these fingerprints for eachtechnology can be time-consuming, which limits what technologies fingerprints canbe built for. This thesis presents a potential solution by utilizing unsupervisedmachine learning techniques to cluster websites by their used web application andJavaScript frameworks, without requiring manually created fingerprints. Oursolution uses multiple bag-of-words models combined with the dimensionalityreduction technique t-SNE and clustering algorithm OPTICS. Results show thatsome technologies, for example, Drupal, achieve a precision of 0.731 and recall of0.485 without any training data. These results lead to the conclusion that theproposed solution could plausibly be used to cluster websites by their webapplication and JavaScript frameworks in use. However, further work is needed toincrease the precision and recall of the results.</p><p>Keywords</p><p>Clustering, fingerprinting, OPTICS, t-SNE, headless browser, bag-of-words,unsupervised machine learning</p>


corrected abstract:
<p>Fingerprinting a website is the process of identifying what technologies a website uses, such as their used web applications and JavaScript frameworks. Current fingerprinting methods use manually created fingerprints for each technology it looks for. These fingerprints consist of multiple text strings that are matched against an HTTP response from a website. Creating these fingerprints for each technology can be time-consuming, which limits what technologies fingerprints can be built for. This thesis presents a potential solution by utilizing unsupervised machine learning techniques to cluster websites by their used web application and JavaScript frameworks, without requiring manually created fingerprints. Our solution uses multiple bag-of-words models combined with the dimensionality reduction technique t-SNE and clustering algorithm OPTICS. Results show that some technologies, for example, Drupal, achieve a precision of 0.731 and recall of 0.485 without any training data. These results lead to the conclusion that the proposed solution could plausibly be used to cluster websites by their web application and JavaScript frameworks in use. However, further work is needed to increase the precision and recall of the results.</p>
----------------------------------------------------------------------
In diva2:1887799 abstract is: <p>Exoskeletons have gone through an unprecedented evolution resulting in vast evaluation approaches inresearch. Within several work sectors and industries the exoskeletons show a potential in having an impact onwork-related musculoskeletal disorders by the ability to reduce musculoskeletal load.</p><p>In the context of the construction sector, field assessments of exoskeletons are lacking. This study will try toexplore the current research gap by assessing a passive upper limb supporting exoskeleton in a constructionwork setting from conducting a surface electromyographic measurement and administering a usability questionnaire.</p><p>Preliminary results suggest that the exoskeleton evaluated tends to reduce muscle activation but may notprovide sufficient rest time for the users to not be in the risk zone of developing MSDs. The results also showthat preliminary usability ratings are ‘Excellent’ but decreases to ‘Good’ or ‘OK’ when more time is spentwearing and using the exoskeleton. This gives an indication of a higher acceptability of the system in shortertime of usage. When considering individual differences, the exoskeleton shows a variety in the impact onmuscle activation as well as perceived usability, which aligns with previous literature. The results also hints thatfactors connected to usability such as, comfort or adaptability which might facilitate the use of exoskeletoncould be important to consider when implementing an exoskeleton, but also in early design or developmentstages.</p><p>To the knowledge of the author, there is no literature in the same spirit as this one. The fact that this is a pilotstudy provides a unique exploratory research perspective into a field assessment conducted in a large-scaleconstruction project.</p>


corrected abstract:
<p>Exoskeletons have gone through an unprecedented evolution resulting in vast evaluation approaches in research. Within several work sectors and industries the exoskeletons show a potential in having an impact on work-related musculoskeletal disorders by the ability to reduce musculoskeletal load.</p><p>In the context of the construction sector, field assessments of exoskeletons are lacking. This study will try to explore the current research gap by assessing a passive upper limb supporting exoskeleton in a construction work setting from conducting a surface electromyographic measurement and administering a usability questionnaire.</p><p>Preliminary results suggest that the exoskeleton evaluated tends to reduce muscle activation but may not provide sufficient rest time for the users to not be in the risk zone of developing MSDs. The results also show that preliminary usability ratings are ‘Excellent’ but decreases to ‘Good’ or ‘OK’ when more time is spent wearing and using the exoskeleton. This gives an indication of a higher acceptability of the system in shorter time of usage. When considering individual differences, the exoskeleton shows a variety in the impact on muscle activation as well as perceived usability, which aligns with previous literature. The results also hints that factors connected to usability such as, comfort or adaptability which might facilitate the use of exoskeleton could be important to consider when implementing an exoskeleton, but also in early design or development stages.</p><p>To the knowledge of the author, there is no literature in the same spirit as this one. The fact that this is a pilot study provides a unique exploratory research perspective into a field assessment conducted in a large-scale construction project.</p>
----------------------------------------------------------------------
In diva2:1298660 abstract is: <p>While the rodent model has long been used in brain research, there exists no standardisedprocessing routine that can be employed for analysis and investigation of disease models. Thepresent thesis attempts to investigate a diseased brain model by implementing a collection ofscripts, combined with algorithms from existing neuroimaging software, and adapting themto the rodent brain, in an attempt to examine when and how monaural canal atresia affectsthe functional connectivity of the brain. We show that it is possible to use software tailoredto the human brain to pre-process the rodent model. Following conventional pipelines andresting state functional MRI (rs-fMRI)-specific strategies, the developed processing routineimplements the most basic steps suggested in literature. On the single-subject level, skullstripping was done using Mialite software, motion correction and distortion correction werebased on FMRIB software library (FSL) algorithms and motion artefacts were removed usingICA-based Automatic Removal Of Motion Artifacts (ICA-AROMA). Following denoising,normalisation to standard space, smoothing and temporal filtering, group level analysis wasperformed. A univariate, hypothesis-driven method and a multivariate, data-driven methodwere used for group comparison and statistical inference. While seed-based correlationanalysis (SCA) did not return any significant results, independent component analysis (ICA) identified two components that show activation in areas of interest.</p>


corrected abstract:
<p>While the rodent model has long been used in brain research, there exists no standardised processing routine that can be employed for analysis and investigation of disease models. The present thesis attempts to investigate a diseased brain model by implementing a collection of scripts, combined with algorithms from existing neuroimaging software, and adapting them to the rodent brain, in an attempt to examine when and how monaural canal atresia affects the functional connectivity of the brain. We show that it is possible to use software tailored to the human brain to pre-process the rodent model. Following conventional pipelines and resting state functional MRI (rs-fMRI)-specific strategies, the developed processing routine implements the most basic steps suggested in literature. On the single-subject level, skull stripping was done using Mialite software, motion correction and distortion correction were based on FMRIB software library (FSL) algorithms and motion artefacts were removed using ICA-based Automatic Removal Of Motion Artifacts (ICA-AROMA). Following denoising, normalisation to standard space, smoothing and temporal filtering, group level analysis was performed. A univariate, hypothesis-driven method and a multivariate, data-driven method were used for group comparison and statistical inference. While seed-based correlation analysis (SCA) did not return any significant results, independent component analysis (ICA) identified two components that show activation in areas of interest.</p>
----------------------------------------------------------------------
title: "Client controlled, secure endpointto-endpoint storage in the cloud"
==>    "Client controlled, secure endpoint-to-endpoint storage in the cloud"

In diva2:1451799 abstract is: <p>Abstract</p><p>Softronic's customers do not want them to store sensitive data in a cloud environment as theydistrust the cloud providers with keeping sensitive data secret and are afraid of violating GDPR.Softronic wants to prove that data can be kept protected using encryption, even though it is storedin a cloud, and the goal of this thesis is to find a cryptographic solution with good security and performance.</p><p>The chosen solution was to implement object-level encryption with both encryption and decryptiondone on-site at Softronic with the cloud provider kept outside of the encryption process. Encrypteddata can then safely be stored in the cloud and decrypted on demand on-site again.</p><p>The cryptography used in the solution was determined after multiple evaluations comparingencryption algorithms and the effects of key lengths, block sizes, and modes of operation. Theevaluations showed big performance differences between encryption algorithms as well as fordifferent encryption modes, where the biggest difference was between those with and withoutintegrity checks built-in. The key length used did not affect object-level encryption performance andthe biggest key size can, therefore, be used for maximum security. The different block sizes did notaffect performance either, but a 128-bit one, as opposed to a 64-bit one, requires less maintenance,as key rotations are not required as frequently.</p><p>The secure transport protocol, TLS, performed in-transit encryption of the object-level encrypteddata as it was sent to the cloud for storage which adversely affects performance. TLS encryptionsuites were, therefore, evaluated to find the one with the smallest performance impact. Theevaluations found that the key size affected performance when doing in-transit encryption, asopposed to object-level encryption, and that the encryption suite, TLS_AES_128_GCM_SHA256,with the smallest key performed the best.</p><p>Keywords</p><p>Encryption, data protection, cloud databases, symmetric encryption, TLS, GDPR, AEAD, Crypto</p>


corrected abstract:
<p>Softronic's customers do not want them to store sensitive data in a cloud environment as they distrust the cloud providers with keeping sensitive data secret and are afraid of violating GDPR. Softronic wants to prove that data can be kept protected using encryption, even though it is stored in a cloud, and the goal of this thesis is to find a cryptographic solution with good security and performance.</p><p>The chosen solution was to implement object-level encryption with both encryption and decryption done on-site at Softronic with the cloud provider kept outside of the encryption process. Encrypted data can then safely be stored in the cloud and decrypted on demand on-site again.</p><p>The cryptography used in the solution was determined after multiple evaluations comparing encryption algorithms and the effects of key lengths, block sizes, and modes of operation. The evaluations showed big performance differences between encryption algorithms as well as for different encryption modes, where the biggest difference was between those with and without integrity checks built-in. The key length used did not affect object-level encryption performance and the biggest key size can, therefore, be used for maximum security. The different block sizes did not affect performance either, but a 128-bit one, as opposed to a 64-bit one, requires less maintenance, as key rotations are not required as frequently.</p><p>The secure transport protocol, TLS, performed in-transit encryption of the object-level encrypted data as it was sent to the cloud for storage which adversely affects performance. TLS encryption suites were, therefore, evaluated to find the one with the smallest performance impact. The evaluations found that the key size affected performance when doing in-transit encryption, as opposed to object-level encryption, and that the encryption suite, TLS_AES_128_GCM_SHA256, with the smallest key performed the best.</p>
----------------------------------------------------------------------
In diva2:1218172 abstract is: <p>In case of build out and rebuilding electricity grids, the electrical characteristicschange. This means that protection features may need to be adjusted to suit theelectrical grids’ new characteristics. Adjustments may lead to loss of selectivity betweenprotective relays. Selectivity means that only the protection device closest tothe fault break the current, so only the smallest possible part of the electrical gridgets disconnected. To assure selectivity, selective tripping schedule is created andprotective relays are adjusted if needed. Rotebro substation in Sollentuna is ownedand operated by Sollentuna Energi &amp; Miljö (SEOM). The substation is facing rebuildingand its selective tripping schedule needs to be reviewed.Since the problem is that selectivity between the protective relays needs to bechecked the goal is to create a selective tripping schedule for Rotebro substation.This is based on the substation protective settings and the connected electricalgrids characteristics. The thesis resulted in a model of Rotebro substation createdin the computer program PSS Sincal. From this a selective tripping schedule weregenerated that shows that small adjustments of the protective relays can be made.A general analysis was also conducted which shows that a selective tripping scheduleis of great importance to the society.</p>


corrected abstract:
<p>In case of build out and rebuilding electricity grids, the electrical characteristics change. This means that protection features may need to be adjusted to suit the electrical grids’ new characteristics. Adjustments may lead to loss of selectivity between protective relays. Selectivity means that only the protection device closest to the fault break the current, so only the smallest possible part of the electrical grid gets disconnected. To assure selectivity, selective tripping schedule is created and protective relays are adjusted if needed. Rotebro substation in Sollentuna is owned and operated by Sollentuna Energi &amp; Miljö (SEOM). The substation is facing rebuilding and its selective tripping schedule needs to be reviewed.</p><p>Since the problem is that selectivity between the protective relays needs to be checked the goal is to create a selective tripping schedule for Rotebro substation. This is based on the substation protective settings and the connected electrical grids characteristics. The thesis resulted in a model of Rotebro substation created in the computer program PSS Sincal. From this a selective tripping schedule were generated that shows that small adjustments of the protective relays can be made. A general analysis was also conducted which shows that a selective tripping schedule is of great importance to the society.</p>
----------------------------------------------------------------------
title: "Implementation of a MobileHealthcare Solution at an InpatientWard"
==>    "Implementation of a Mobile Healthcare Solution at an Inpatient Ward"

In diva2:1438504 abstract is: <p>Abstract</p><p>Healthcare is a complex system under great pressure for meeting the patients' needs.Implementing technology at inpatient wards might possibly support healthcare professionalsand improve quality of care. However, these technologies might come withissues and the system might not be used as intended.</p><p>This master thesis project investigates how healthcare professionals communicateat an inpatient ward and how this might be affected by implementing a MobileHealthcare Solution (MHS). Further, it sought to question why healthcare professionsmight, or might not, use the MHS as a support of their daily work and whatsome reasons for this might be. Research methods were of qualitative approach.Field studies were performed at an inpatient ward and further, two healthcare professionalswere interviewed. Grounded Theory (GT) was chosen as a method toprocess the data and obtain understanding for communication at the inpatient ward.</p><p>The results showed that healthcare professionals communicate verbally, written andby reading, using different tools. The most prominent ways of communication wereverbally, where it was common to report or discuss about a patient. The means forcommunication did not get drastically affected by implementing the MHS and reasonsfor this were of social, technical and organizational types. Some reasons for notusing the MHS were habits and due to healthcare professionals perceiving the MHSas more time consuming than manual handling. However, a specic investigation of whether this might affect the usage of the MHS is yet needed.</p><p>Keywords: Mobile Healthcare Solutions, Electronic Health Records, HealthcareInnovation System, User and Implementation, Ethnographyii</p>

w='specic' val={'c': 'specific', 's': ['diva2:1438504', 'diva2:949949'], 'n': 'missing ligature'}
w='Ethnographyii' val={'c': 'Ethnographyy', 's': 'diva2:1438504', 'n': 'correct in original - note this is a keyword and not in the abstract'}

corrected abstract:
<p>Healthcare is a complex system under great pressure for meeting the patients’ needs. Implementing technology at inpatient wards might possibly support healthcare professionals and improve quality of care. However, these technologies might come with issues and the system might not be used as intended.</p><p>This master thesis project investigates how healthcare professionals communicate at an inpatient ward and how this might be affected by implementing a Mobile Healthcare Solution (MHS). Further, it sought to question why healthcare professions might, or might not, use the MHS as a support of their daily work and what some reasons for this might be. Research methods were of qualitative approach. Field studies were performed at an inpatient ward and further, two healthcare professionals were interviewed. Grounded Theory (GT) was chosen as a method to process the data and obtain understanding for communication at the inpatient ward.</p><p>The results showed that healthcare professionals communicate verbally, written and by reading, using different tools. The most prominent ways of communication were verbally, where it was common to report or discuss about a patient. The means for communication did not get drastically affected by implementing the MHS and reasons for this were of social, technical and organizational types. Some reasons for not using the MHS were habits and due to healthcare professionals perceiving the MHS as more time consuming than manual handling. However, a specific investigation of whether this might affect the usage of the MHS is yet needed.</p>
----------------------------------------------------------------------
In diva2:1454864 abstract is: <p>Poor prognosis for high-risk neuroblastoma patients makes it necessary to find novel treatmentstrategies. This work aims to understand the cell cycle behavior of various high-riskneuroblastoma cell lines following chemotherapy treatment. Here, we mapped the expressionof cell cycle dependent proteins, p21 and p27, in seven high-risk neuroblastoma celllines. All cell lines showed an overall impaired growth following doxorubicin treatment.However, regrowth was observed in all cell lines between day 6 to 15 by forming colonies.The expression of p21 and p27 was measured in all cell lines showing an upregulationof p21 in 3 out of 5 p53 mutated cell lines while it was downregulated in the 2 cell lineswith a p53 wild type. Furthermore, inhibition assays using inhibitors of CHK1/2, p21 ,andSKP2 were performed. The results were promising as the CHK1/2 inhibitor reduced cellviability in all tested cell lines, while the p21 inhibitor had an effect in 3 out of 6 testedcell lines and the SKP2 inhibitor in 4 out of 6 tested cell lines. Confluency measurementover 15 days showed impaired growth following treatment with the CHK1/2 inhibitor for3 out of 6 tested cell lines and p21 inhibitor in 1 out of 6 tested cell lines. The obtainedresults were encouraging and might aid in finding a novel treatment strategy preventingresistance and relapse in neuroblastoma. However, further studies are needed in order tovalidate the efficacy and safety of these promising drugs in neuroblastoma patients.</p>


Note the "p21 ,and" is in the actual abstract in the thesis.

corrected abstract:
<p>Poor prognosis for high-risk neuroblastoma patients makes it necessary to find novel treatment strategies. This work aims to understand the cell cycle behavior of various high-risk neuroblastoma cell lines following chemotherapy treatment. Here, we mapped the expression of cell cycle dependent proteins, p21 and p27, in seven high-risk neuroblastoma cell lines. All cell lines showed an overall impaired growth following doxorubicin treatment. However, regrowth was observed in all cell lines between day 6 to 15 by forming colonies. The expression of p21 and p27 was measured in all cell lines showing an upregulation of p21 in 3 out of 5 p53 mutated cell lines while it was downregulated in the 2 cell lines with a p53 wild type. Furthermore, inhibition assays using inhibitors of CHK1/2, p21 ,and SKP2 were performed. The results were promising as the CHK1/2 inhibitor reduced cell viability in all tested cell lines, while the p21 inhibitor had an effect in 3 out of 6 tested cell lines and the SKP2 inhibitor in 4 out of 6 tested cell lines. Confluency measurement over 15 days showed impaired growth following treatment with the CHK1/2 inhibitor for 3 out of 6 tested cell lines and p21 inhibitor in 1 out of 6 tested cell lines. The obtained results were encouraging and might aid in finding a novel treatment strategy preventing resistance and relapse in neuroblastoma. However, further studies are needed in order to validate the efficacy and safety of these promising drugs in neuroblastoma patients.</p>
----------------------------------------------------------------------
In diva2:1472559 abstract is: <p>AbstractDevelopment and deployment of software has transitioned from being aprocess that could take years to being as short as a few hours. The reason forthis is the DevOps methodology gaining momentum in the industry and theuse of automation tools such as Jenkins. DevOps is a collaborative effortbetween development and operation of software where automation has animportant role. Security has not had a high priority due to short developmentcycles and if security testing is done within organizations, it’s usually donemanually a few months apart. Could security tests be automated and give ahigh enough bar for application security within DevOps? To answer thisquestion different types of application security testing has been done toward abenchmarking tool to test for efficiency. A software prototype has beendeveloped to measure other factors such as automation and developmenttools. The conclusion is that there are good possibilities for automated securitytests but the answer to what is best depends on the type of application to testand what organization is in question.KeywordsApplication Security Testing, SAST, DAST, IAST, SCA, DevSecOps, CI/CD, OWASP</p>

corrected abstract:
<p>Development and deployment of software has transitioned from being a process that could take years to being as short as a few hours. The reason for this is the DevOps methodology gaining momentum in the industry and the use of automation tools such as Jenkins. DevOps is a collaborative effort between development and operation of software where automation has an important role. Security has not had a high priority due to short development cycles and if security testing is done within organizations, it’s usually done manually a few months apart. Could security tests be automated and give a high enough bar for application security within DevOps? To answer this question different types of application security testing has been done toward a benchmarking tool to test for efficiency. A software prototype has been developed to measure other factors such as automation and development tools. The conclusion is that there are good possibilities for automated security tests but the answer to what is best depends on the type of application to test and what organization is in question.</p>
----------------------------------------------------------------------
title: "Design, production and evaluation of rigid helix fusion between affibody molecules and framework proteins aimed for Affibody:antigen structure determination using single-particlecryo-EM"
==>    "Design, production and evaluation of rigid helix fusion between affibody molecules and framework proteins aimed for Affibody:antigen structure determination using single-particle cryo-EM"

In diva2:1454440 abstract is: <p>Single particle cryo-electron microscopy (cryo-EM) is an emerging and growing technique forstructural analysis of proteins. However, limitations within the technique leaves the majority ofproteins too small for structural analysis by cryo-EM with high resolution. A method tocircumvent this limitation has been to rigidly link the protein of interest to a larger symmetricprotein scaffold followed by analysis of the larger complex. Preferably, the linkage should beperformed genetically via a shared rigid alpha helix junction, improving the resolution due torestricted flexibility. However, creating rigid linkages between proteins with shared fusionhelices is not trivial and requires that both proteins have a terminal helix available. To workaround this, alpha-helix containing affinity proteins such as affibodies or DARPins have beenshown amenable to fusion via shared rigid helices to different scaffolds, providing bio-affinitybasedplatforms for non-covalent capture of target proteins for structural studies by cryo-EM.In this project new designs of rigidly alpha-helix-linked affibody-scaffold fusion proteins forcryo-EM applications were designed and initially evaluated with the long-term goal to create ageneral affibody-based cryo-EM scaffold platform. Analyses of in silico designed andrecombinantly produced fusion proteins using size exclusion chromatography, massspectrometry and surface plasmon resonance show at least one promising candidate for futurestructural analysis using cryo-EM.</p>

corrected abstract:
<p>Single particle cryo-electron microscopy (cryo-EM) is an emerging and growing technique for structural analysis of proteins. However, limitations within the technique leaves the majority of proteins too small for structural analysis by cryo-EM with high resolution. A method to circumvent this limitation has been to rigidly link the protein of interest to a larger symmetric protein scaffold followed by analysis of the larger complex. Preferably, the linkage should be performed genetically via a shared rigid alpha helix junction, improving the resolution due to restricted flexibility. However, creating rigid linkages between proteins with shared fusion helices is not trivial and requires that both proteins have a terminal helix available. To work around this, alpha-helix containing affinity proteins such as affibodies or DARPins have been shown amenable to fusion via shared rigid helices to different scaffolds, providing bio-affinity-based platforms for non-covalent capture of target proteins for structural studies by cryo-EM. In this project new designs of rigidly alpha-helix-linked affibody-scaffold fusion proteins for cryo-EM applications were designed and initially evaluated with the long-term goal to create a general affibody-based cryo-EM scaffold platform. Analyses of in silico designed and recombinantly produced fusion proteins using size exclusion chromatography, mass spectrometry and surface plasmon resonance show at least one promising candidate for future structural analysis using cryo-EM.</p>
----------------------------------------------------------------------
In diva2:1275060 abstract is: <p>This report examines preoperative body wash performed by the patients themselves at home and itspotential to prevent surgical site infections. This was accomplished by studying current recommendedprotocols and active substances in Sweden in relation to a potential replacement product. Products identified and included in the report are Descutan, manufactured by Fresenius Kabi AB and Prontoderm by B. Braun Medical AB, with the active substances of chlorhexidine and polyhexanide.After comparisons, an experiment and evaluations, a new proposed protocol has been developed.The proposed protocol involves decolonization of bacteria, including multidrug resistant organisms.In that way, an alternative to handle the increased antibiotic resistance in the world is described. Inaddition, costs of the products have been compared and by avoiding surgical site infections, potentialsavings have been calculated.From the review, ambiguous results were obtained regarding the effect of preoperative body wash onsurgical site infections.In summary, there are many benefits and a lot of money to save, with the new active substance andby avoiding surgical site infections. The conclusion is that Prontoderm is more expensive, but thatthe total cost of the product kit could be reduced by adjusting the size of the products after protocoland dosage. Further, it was decided that three washes are sufficient for a full preoperative body wash.Keywords: surgical site infection, chlorhexidine, polyhexanide, decolonization, preoperative bodywash, Prontoderm, Descutan, patient protocol, multidrug resistant organisms.</p>

corrected abstract:
<p>This report examines preoperative body wash performed by the patients themselves at home and its potential to prevent surgical site infections. This was accomplished by studying current recommended protocols and active substances in Sweden in relation to a potential replacement product. Products identified and included in the report are Descutan, manufactured by Fresenius Kabi AB and Prontoderm by B. Braun Medical AB, with the active substances of chlorhexidine and polyhexanide.</p><p>After comparisons, an experiment and evaluations, a new proposed protocol has been developed. The proposed protocol involves decolonization of bacteria, including multidrug resistant organisms. In that way, an alternative to handle the increased antibiotic resistance in the world is described. In addition, costs of the products have been compared and by avoiding surgical site infections, potential savings have been calculated.</p><p>From the review, ambiguous results were obtained regarding the effect of preoperative body wash on surgical site infections.</p><p>In summary, there are many benefits and a lot of money to save, with the new active substance and by avoiding surgical site infections. The conclusion is that Prontoderm is more expensive, but that the total cost of the product kit could be reduced by adjusting the size of the products after protocol and dosage. Further, it was decided that three washes are sufficient for a full preoperative body wash.</p>
----------------------------------------------------------------------
In diva2:1764375 abstract is: <p>Limited processor and memory capacity is a major challenge for logging sensorsignals in engine control units. In order to be able to store larger amounts of data,compression can be used. To successfully implement compression algorithms inmotor control units, it is essential that the algorithms can effectively handle thelimitations associated with processor capacity while achieving an acceptable level ofcompression.This thesis compares compression algorithms on sensor data from motor controlunits in order to investigate which algorithm(s) are best suited to implement forthis application. The work aims to improve the possibilities of logging sensor dataand thus make the troubleshooting of the engine control units more efficient. Thiswas done by developing a system that performs compression on sampled sensorsignals and calculates the compression time and ratio.The results indicated that delta-of-delta compression performed better than xorcompression for the tested data sets. Delta-of-delta had a significantly bettercompression ratio while the differences between the algorithms regardingcompression time were minor. Delta-of-delta compression was judged to have goodpotential for implementation in engine control unit logging systems. The algorithmis deemed to be well suited for logging smaller time series during important events.For continuous logging of larger time series, further research is suggested in orderto investigate the possibility of improving the compression ratio further. </p>

corrected abstract:
<p>Limited processor and memory capacity is a major challenge for logging sensor signals in engine control units. In order to be able to store larger amounts of data, compression can be used. To successfully implement compression algorithms in motor control units, it is essential that the algorithms can effectively handle the limitations associated with processor capacity while achieving an acceptable level of compression.</p><p>This thesis compares compression algorithms on sensor data from motor control units in order to investigate which algorithm(s) are best suited to implement for this application. The work aims to improve the possibilities of logging sensor data and thus make the troubleshooting of the engine control units more efficient. This was done by developing a system that performs compression on sampled sensor signals and calculates the compression time and ratio.</p><p>The results indicated that delta-of-delta compression performed better than xor compression for the tested data sets. Delta-of-delta had a significantly better compression ratio while the differences between the algorithms regarding compression time were minor. Delta-of-delta compression was judged to have good potential for implementation in engine control unit logging systems. The algorithm is deemed to be well suited for logging smaller time series during important events. For continuous logging of larger time series, further research is suggested in order to investigate the possibility of improving the compression ratio further.</p>
----------------------------------------------------------------------
title: "MULTI-CENTER QUANTITATIVE MEASUREMENT OF T1 ANDT2 RELAXATION TIMES IN THE RAT BRAIN"
==>    "MULTI-CENTER QUANTITATIVE MEASUREMENT OF T1 AND T2 RELAXATION TIMES IN THE RAT BRAIN"

In diva2:1231794 abstract is: <p>This project revolves around the measurement of T1 and T2 relaxation times in the ratbrain, in a multi-center way. That is to say, elaborate an efficient protocol to calculate highresolution 3D map of the brain. This protocol should be applied in different centers andreturn similar results. Finally, procedures should be defined to ease the collaborationbetween the different centers. The first step consisted in in vitro experiments, in whichdifferent sequences were tested. It resulted that the MDEFT sequence with inversionpreparation (MPRAGE) gives the best results in the shortest time for T1. For T2, theMSME sequence was chosen. The next step moved on in vivo experiments on three rats inorder to get used to manipulating living animals and make new adjustments. As thephysiology is not the same on in vitro and in vivo experiments, some parameters had to beslightly adapted. Once the final 2h-protocol was established, it was tested on a populationof ten rats. Experiments were made at the GIN and CRMBM. Different fitting pipelineswere tried (GIN, CRMBM, MIRCEN). The brain was segmented into different regions. Itresulted that the GIN and CRMBM pipelines return the same T1 values using the differentdatasets. The MIRCEN pipeline under-estimates by 200 ms. The three pipelines return similar T2 values. The GIN and CRMBM datasets provide comparable T1 values, but theGIN center presents slightly higher T2 values. Regarding the multi-center collaboration,the different pipelines were ported to the VIP platform so that the scientific community caneasily reuse them.</p>


corrected abstract:
<p>This project revolves around the measurement of T1 and T2 relaxation times in the rat brain, in a multi-center way. That is to say, elaborate an efficient protocol to calculate high resolution 3D map of the brain. This protocol should be applied in different centers and return similar results. Finally, procedures should be defined to ease the collaboration between the different centers. The first step consisted in <em>in vitro</em> experiments, in which different sequences were tested. It resulted that the MDEFT sequence with inversion preparation (MPRAGE) gives the best results in the shortest time for T1. For T2, the MSME sequence was chosen. The next step moved on <em>in vivo</em> experiments on three rats in order to get used to manipulating living animals and make new adjustments. As the physiology is not the same on <em>in vitro</em> and <em>in vivo</em> experiments, some parameters had to be slightly adapted. Once the final 2h-protocol was established, it was tested on a population of ten rats. Experiments were made at the GIN and CRMBM. Different fitting pipelines were tried (GIN, CRMBM, MIRCEN). The brain was segmented into different regions. It resulted that the GIN and CRMBM pipelines return the same T1 values using the different datasets. The MIRCEN pipeline under-estimates by 200 ms. The three pipelines return similar T2 values. The GIN and CRMBM datasets provide comparable T1 values, but the GIN center presents slightly higher T2 values. Regarding the multi-center collaboration, the different pipelines were ported to the VIP platform so that the scientific community can easily reuse them.</p>
----------------------------------------------------------------------
title: "Reduction of selected pharmaceutical substances byfungi isolated from wastewater"
==>    "Reduction of selected pharmaceutical substances by fungi isolated from wastewater"

In diva2:1218610 abstract is: <p>In recent years, adverse effects have been shown in marine living organisms due to pharmaceuticalsubstances released into nature. The emission of pharmaceutical substances into the environmentis mainly due to excretion by humans, which conventional wastewater treatment technologies arenot always capable of removing. Previous studies have observed fungi as promising treatmentagents for removal of pharmaceutical substances. However, the optimum removal efficiency forthe fungi has been observed at pH ranging from 3.5 to 5.5, while pH of the wastewater rangesbetween 6.5 – 10.This study has investigated the ability of removing pharmaceutical substances by using fungiisolated from a sewage wastewater sample. Initially ten fungal isolates were cultivated in presenceof carbamazepine, diclofenac, ibuprofen and sulfamethoxazol and all ten isolates tolerated a totalpharmaceutical concentration of 5 mg/l. Further, the ability of the isolated fungi to reducecarbamazepine and diclofenac was investigated and Trametes versicolor was used as control fungus.Isolate 6, identified to Aspergillus luchuensis, showed a complete reduction of diclofenac after 10 days,cultivated in a medium with an initial pH of 6.3. Furthermore, A. luchuensis was also observed toreduce diclofenac faster compared to, the previous studied fungi, T. versicolor. From what is known,these results have not been observed in any previous studies. In conclusion, this study has foundA. luchuensis as a promising treatment agent for removal of pharmaceuticals, but also that there areother fungi present in wastewater that can be further studied for this purpose.</p>

w='sulfamethoxazol' val={'c': 'sulfamethoxazole', 's': 'diva2:1218610', 'n': 'no full text'}

Assuming that the orginal had properly italicized species.
corrected abstract:
<p>In recent years, adverse effects have been shown in marine living organisms due to pharmaceutical substances released into nature. The emission of pharmaceutical substances into the environment is mainly due to excretion by humans, which conventional wastewater treatment technologies are not always capable of removing. Previous studies have observed fungi as promising treatment agents for removal of pharmaceutical substances. However, the optimum removal efficiency for the fungi has been observed at pH ranging from 3.5 to 5.5, while pH of the wastewater ranges between 6.5 – 10. This study has investigated the ability of removing pharmaceutical substances by using fungi isolated from a sewage wastewater sample. Initially ten fungal isolates were cultivated in presence of carbamazepine, diclofenac, ibuprofen and sulfamethoxazole and all ten isolates tolerated a total pharmaceutical concentration of 5 mg/l. Further, the ability of the isolated fungi to reduce carbamazepine and diclofenac was investigated and <em>Trametes versicolor</em> was used as control fungus. Isolate 6, identified to <em>Aspergillus luchuensis</em>, showed a complete reduction of diclofenac after 10 days, cultivated in a medium with an initial pH of 6.3. Furthermore, <em>A. luchuensis</em> was also observed to reduce diclofenac faster compared to, the previous studied fungi, <em>T. versicolor</em>. From what is known, these results have not been observed in any previous studies. In conclusion, this study has found <em>A. luchuensis</em> as a promising treatment agent for removal of pharmaceuticals, but also that there are other fungi present in wastewater that can be further studied for this purpose.</p>
----------------------------------------------------------------------
In diva2:1225421 abstract is: <p>The primary motivation of this bachelor thesis was to address the acute lack of affordablemedical technologies in low-resource settings in order to reduce child mortality in theneonatal period. It is essential to develop affordable medical devices for empowering healthworkers and village doctors and train them to use smart devices and appropriate ICT tools,and connect them to the medical experts to manage the serious health problems. The workincludes: a) developing programs for safe use of devices and manage consultation with themedical experts and b) developing appropriate e-Learning content on health education fordisease prevention.</p><p>A handheld, safe and user friendly heart rate monitor has been developed using smart andlow cost sensors. The device consists of two major parts. First, low cost sensors (piezo andoptical) interfaced with analog front-end circuits including capacitors, resistors andamplifiers. Second, the filtered and amplified signal is digitally processed and converted toprovide statistically significant information about the patient’s heart rate and presentirregularity report in the heart activity if any. The piezoelectric sensor head is placed onthe point where the radial artery crosses the bones of the wrist, whereas the photoelectricsensor head is placed on a fingertip in order to detect the arterial pulse rate.</p><p>At present, the interpretation of sensor readings is focused towards determining the heartrate and dynamics of heart functional characteristics. This low cost handheld device isbeing further enhanced with more sensors to provide additional relevant vital parametersfor quality treatment guidelines.</p>

corrected abstract:
<p>The primary motivation of this bachelor thesis was to address the acute lack of affordable medical technologies in low-resource settings in order to reduce child mortality in the neonatal period. It is essential to develop affordable medical devices for empowering health workers and village doctors and train them to use smart devices and appropriate ICT tools, and connect them to the medical experts to manage the serious health problems. The work includes: a) developing programs for safe use of devices and manage consultation with the medical experts and b) developing appropriate e-Learning content on health education for disease prevention.</p><p>A handheld, safe and user friendly heart rate monitor has been developed using smart and low cost sensors. The device consists of two major parts. First, low cost sensors (piezo and optical) interfaced with analog front-end circuits including capacitors, resistors and amplifiers. Second, the filtered and amplified signal is digitally processed and converted to provide statistically significant information about the patient’s heart rate and present irregularity report in the heart activity if any. The piezoelectric sensor head is placed on the point where the radial artery crosses the bones of the wrist, whereas the photoelectric sensor head is placed on a fingertip in order to detect the arterial pulse rate.</p><p>At present, the interpretation of sensor readings is focused towards determining the heart rate and dynamics of heart functional characteristics. This low cost handheld device is being further enhanced with more sensors to provide additional relevant vital parameters for quality treatment guidelines.</p>
----------------------------------------------------------------------
In diva2:1188957 abstract is: <p>The establishment of the European Union Water Framework Directive (2000/60/EG) has created a common platform for the EU Member States regarding action in the field of waterpolicy. In 2004 the directive was implemented in the Swedish legislation and it constitutes the foundation of the water management in Sweden. The responsibility of the water management is assigned to a water authority that consists of five County Administrative Boards. The objects of their work are to attain good quality of water environment and prevent further deterioration. In order to determine the quality status of a so-called surface water body, concentration of various chemicals substances in the water are measured. Some of these substances are toxic, persistent and bioaccumulative organic pollutants. Potential sources of such substances are partly waste disposal facilities. It is therefore important for waste management operators to possess knowledge of how their activities affect the surrounding environment, which they can achieve by performing measurements and studies.</p><p>The waste disposal facility Högbytorp in Upplands-Bro is operated by Ragn-Sells Avfallsbehandling AB and handles reception, recycling and disposal of different types of waste. This project was aimed at supplementing parts of Ragn-Sell’s earlier characterization of landfill leachate at Högbytorp and giving them an improved supervision of their emissions regarding a number of determined organic pollutants. Examples of substances included were brominated flame retardants, polycyclic aromatic hydrocarbons and perfluoroalkylated substances, which are classified as priority substances within the water policy. The project would also provide information on the extent of reduction of these substances in various leachate treatment steps at Högbytorp. To achieve these objectives, sampling and analyzeshave been performed on Högbytorp’s leachate and on their soil plant system, but also on the surface water in the nearby recipient and on treated process water sent to the wastewater treatment plant Käppala. Measured concentrations in these sampling points have been evaluated by comparing them to relevant values like environmental quality standards. Massbalances of PFOS, perfluorooctane sulfonate, was performed to understand how the substanceis affected in different treatment steps and to estimate their purification efficiency. The project's delimitations consisted of the selected organic pollutants that have been analyzed and the particular sampling points.</p><p>The results of the evaluation of organic pollutants showed that depending on the sampling point some substances exceeded their comparative values while others measured below. However, levels of PFOS were particularly distinctive as they exceeded at least one comparative value in every sampling point. In general, higher concentrations were found in untreated leachate compared to treated leachate, indicating that Högbytorp's leachate treatment steps are able to reduce concentrations of organic pollutants in leachate. By comparing the concentration levels before and after the nitrification/denitrification treatment plant, the reduction of analyzed substances was classified as good to very good in this treatment step. Byusing mass balances regarding PFOS the purification efficiency of both the nitrification/denitrification treatment plant and the treatment ponds were estimated to be very good. A possible explanation for the reduction of PFOS was thought to be the ability of the substance to adsorb to sludge. Support to this hypothesis was given by the PFOS content measured in the sludge of the nitrification/denitrification plant.</p><p>Measurements in the soil plant system showed lower levels of naphthalene, anthracene and fluoranthene in the area where salix grows, which indicate that the plant has a reducing effecton polycyclic aromatic hydrocarbons in the soil. However, it was not possible to estimate the purification efficiency of the soil plant system using a mass balance due to uncertain assumptions and insufficient data. When comparing concentration levels in the recipient upstream and downstream the wastefacility, increased concentrations were detected downstream, which could be a result of the waste facility's operations. Evaluation of the concentration levels of substances downstream showed that the majority was measured below the environmental quality standards. Regardingthe evaluation of the treated process water led to the wastewater treatment plant Käppala, all substances evaluated fell below the risk criteria of REVAQ meaning that the impact on these wage sludge in the wastewater treatment plant should be considered tolerable with the assumption that the process water represents the total leachate flow to the wastewater treatment plant. With only regard to tolerable influence on sewage sludge, the connection of these waters can be maintained.</p><p>Due to the fact that most samples were taken only at one point, the results may be doubtful andtherefore misleading. In order to obtain more representative results, the sampling of this project should be supplemented with several samples taken over a longer period of time.</p><p>Based on the results of the project, several conclusions could be drawn and proposals forfurther studies could be given. For the reason that PFOS seems to adsorb to sludge, it is relevant to look over the management of sludge removal from the nitrification/denitrificationplant and the treatment ponds. Because of the high reduction of organic pollutants in thenitrification/denitrification plant it is important to avoid flow of leachate past the plant, whichmeans that the plant’s capacity should be investigated. Since concentrations of analyzed pollutants in the recipient were higher downstream the waste facility, it is likely that emissionsoccur from the facility. It is therefore a good idea to examine potential sources of emissions within the facility area, in order to prevent emissions to the recipient.</p>


corrected abstract:
<p>The establishment of the European Union Water Framework Directive (2000/60/EG) has created a common platform for the EU Member States regarding action in the field of water policy. In 2004 the directive was implemented in the Swedish legislation and it constitutes the foundation of the water management in Sweden. The responsibility of the water management is assigned to a water authority that consists of five County Administrative Boards. The objects of their work are to attain good quality of water environment and prevent further deterioration. In order to determine the quality status of a so-called surface water body, concentration of various chemicals substances in the water are measured. Some of these substances are toxic, persistent and bioaccumulative organic pollutants. Potential sources of such substances are partly waste disposal facilities. It is therefore important for waste management operators to possess knowledge of how their activities affect the surrounding environment, which they can achieve by performing measurements and studies.</p><p>The waste disposal facility Högbytorp in Upplands-Bro is operated by Ragn-Sells Avfallsbehandling AB and handles reception, recycling and disposal of different types of waste. This project was aimed at supplementing parts of Ragn-Sell’s earlier characterization of landfill leachate at Högbytorp and giving them an improved supervision of their emissions regarding a number of determined organic pollutants. Examples of substances included were brominated flame retardants, polycyclic aromatic hydrocarbons and perfluoroalkylated substances, which are classified as priority substances within the water policy. The project would also provide information on the extent of reduction of these substances in various leachate treatment steps at Högbytorp. To achieve these objectives, sampling and analyzes have been performed on Högbytorp’s leachate and on their soil plant system, but also on the surface water in the nearby recipient and on treated process water sent to the wastewater treatment plant Käppala. Measured concentrations in these sampling points have been evaluated by comparing them to relevant values like environmental quality standards. Mass balances of PFOS, perfluorooctane sulfonate, was performed to understand how the substance is affected in different treatment steps and to estimate their purification efficiency. The project's delimitations consisted of the selected organic pollutants that have been analyzed and the particular sampling points.</p><p>The results of the evaluation of organic pollutants showed that depending on the sampling point some substances exceeded their comparative values while others measured below. However, levels of PFOS were particularly distinctive as they exceeded at least one comparative value in every sampling point. In general, higher concentrations were found in untreated leachate compared to treated leachate, indicating that Högbytorp's leachate treatment steps are able to reduce concentrations of organic pollutants in leachate. By comparing the concentration levels before and after the nitrification/denitrification treatment plant, the reduction of analyzed substances was classified as good to very good in this treatment step. By using mass balances regarding PFOS the purification efficiency of both the nitrification/denitrification treatment plant and the treatment ponds were estimated to be very good. A possible explanation for the reduction of PFOS was thought to be the ability of the substance to adsorb to sludge. Support to this hypothesis was given by the PFOS content measured in the sludge of the nitrification/denitrification plant.</p><p>Measurements in the soil plant system showed lower levels of naphthalene, anthracene and fluoranthene in the area where salix grows, which indicate that the plant has a reducing effect on polycyclic aromatic hydrocarbons in the soil. However, it was not possible to estimate the purification efficiency of the soil plant system using a mass balance due to uncertain assumptions and insufficient data.</p><p>When comparing concentration levels in the recipient upstream and downstream the waste facility, increased concentrations were detected downstream, which could be a result of the waste facility's operations. Evaluation of the concentration levels of substances downstream showed that the majority was measured below the environmental quality standards. Regarding the evaluation of the treated process water led to the wastewater treatment plant Käppala, all substances evaluated fell below the risk criteria of REVAQ meaning that the impact on the sewage sludge in the wastewater treatment plant should be considered tolerable with the assumption that the process water represents the total leachate flow to the wastewater treatment plant. With only regard to tolerable influence on sewage sludge, the connection of these waters can be maintained.</p><p>Due to the fact that most samples were taken only at one point, the results may be doubtful and therefore misleading. In order to obtain more representative results, the sampling of this project should be supplemented with several samples taken over a longer period of time.</p><p>Based on the results of the project, several conclusions could be drawn and proposals for further studies could be given. For the reason that PFOS seems to adsorb to sludge, it is relevant to look over the management of sludge removal from the nitrification/denitrification plant and the treatment ponds. Because of the high reduction of organic pollutants in the nitrification/denitrification plant it is important to avoid flow of leachate past the plant, which means that the plant’s capacity should be investigated. Since concentrations of analyzed pollutants in the recipient were higher downstream the waste facility, it is likely that emissions occur from the facility. It is therefore a good idea to examine potential sources of emissions within the facility area, in order to prevent emissions to the recipient.</p>
----------------------------------------------------------------------
title: "Swedish biomedical text-miningand classification"
==>    "Swedish biomedical text-mining and classification"

In diva2:1451783 abstract is: <p>AbstractManual classification of text is both time consuming and expensive. However, it is anecessity within the field of biomedicine, for example to be able to quantify biomedical data.In this study, two different approaches were researched regarding the possibility of usingsmall amounts of training data, in order to create text classification models that are able tounderstand and classify biomedical texts. The study researched whether a specialized modelshould be considered a requirement for this purpose, or if a generic model might suffice. Thetwo models were based on publicly available versions, one specialized to understand Englishbiomedical texts, and the other to understand ordinary Swedish texts. The Swedish modelwas introduced to a new field of texts while the English model had to work on translatedSwedish texts.The results were quite low, but did however indicate that the method with the Swedish modelwas more reliable, performing almost twice as well as the English correspondence. The studyconcluded that there was potential in using general models as a base, and then tuning theminto more specialized fields, even with small amounts of data.KeywordsNLP, text-mining, biomedical texts, classification, labelling, models, BERT, machinelearning, FIC, ICF.</p>


corrected abstract:
<p>Manual classification of text is both time consuming and expensive. However, it is a necessity within the field of biomedicine, for example to be able to quantify biomedical data. In this study, two different approaches were researched regarding the possibility of using small amounts of training data, in order to create text classification models that are able to understand and classify biomedical texts. The study researched whether a specialized model should be considered a requirement for this purpose, or if a generic model might suffice. The two models were based on publicly available versions, one specialized to understand English biomedical texts, and the other to understand ordinary Swedish texts. The Swedish model was introduced to a new field of texts while the English model had to work on translated Swedish texts.</p><p>The results were quite low, but did however indicate that the method with the Swedish model was more reliable, performing almost twice as well as the English correspondence. The study concluded that there was potential in using general models as a base, and then tuning them into more specialized fields, even with small amounts of data.</p>
----------------------------------------------------------------------
In diva2:1260679 abstract is: <p>Pretreatment of biomass plays an important role in IGCC processes. Optimized operating conditions improve overall process eciency, product diversity and producergas composition. This project focuses on the pretreatment of spruce woodbiomass using high pressure superheated steam in a packed bed reactor with heated walls. The objective is to optimize the operation for ecient energy consumption and uniform thermal treatment of wood particles along the reactor. For this purpose,two models have been developed: Single-phase superheated steam in porous media where local thermal nonequilibrium condition prevails, Multi-phase ow in porous media.The rst model investigates the essential parameters to be taken into accountto ensure a uniform thermal treatment as the pyrolytic agent i.e. steam exchanges heat with solid particles. It is found that solid matrix and steam reach thermalequilibrium at early stages of the process however, sharp decrease in uid temperatureis observed. This trend is believed to be aected mostly by uid inlet velocity and solid initial temperature. The model suggests that the initial solid temperaturedoes not play an important role compared to the uid inlet velocity.The focus of the second model is optimization of reactor heat source to avoidthe complications attributed by a two-phase ow. The model can predict the phasesaturation of uid in the porous media along with the pressure and velocity prolesfor a given inlet ow condition and reactor heating power. A reactor with heatsource localized at the inlet is recommended to satisfy the thermal requirements ofthe system.Steam treatment of biomass at elevated pressures is a novel technology whichis believed to bring about many advantages for IGCC process. However, detailed optimization is necessary to benet from these advantages.</p>

w='eciency' val={'c': 'efficiency', 's': 'diva2:1260679', 'n': 'missing ligrature'}
w='ecient' val={'c': 'efficient', 's': 'diva2:1260679', 'n': 'missing ligrature'}
w='prolesfor' val={'c': 'profiles for', 's': 'diva2:1260679', 'n': 'missing ligature'}
w='benet' val={'c': 'benefit', 's': 'diva2:1260679', 'n': 'missing ligature'}
There is a error in the original as "process however" should probably be "process; however"
There were ligatures of "fl" missing in fluid and flow.

corrected abstract:
<p>Pretreatment of biomass plays an important role in IGCC processes. Optimized operating conditions improve overall process efficiency, product diversity and producer gas composition. This project focuses on the pretreatment of spruce wood biomass using high pressure superheated steam in a packed bed reactor with heated walls. The objective is to optimize the operation for efficient energy consumption and uniform thermal treatment of wood particles along the reactor. For this purpose, two models have been developed:<ul><li>Single-phase superheated steam in porous media where local thermal non-equilibrium condition prevails,</li><li>Multi-phase flow in porous media.</li></ul></p><p>The first model investigates the essential parameters to be taken into account to ensure a uniform thermal treatment as the pyrolytic agent i.e. steam exchanges heat with solid particles. It is found that solid matrix and steam reach thermal equilibrium at early stages of the process however, sharp decrease in fluid temperature is observed. This trend is believed to be affected mostly by fluid inlet velocity and solid initial temperature. The model suggests that the initial solid temperature does not play an important role compared to the fluid inlet velocity.</p><p>The focus of the second model is optimization of reactor heat source to avoid the complications attributed by a two-phase flow. The model can predict the phase saturation of fluid in the porous media along with the pressure and velocity profiles for a given inlet flow condition and reactor heating power. A reactor with heat source localized at the inlet is recommended to satisfy the thermal requirements of the system.</p><p>Steam treatment of biomass at elevated pressures is a novel technology which is believed to bring about many advantages for IGCC process. However, detailed optimization is necessary to benefit from these advantages.</p>
----------------------------------------------------------------------
In diva2:1889618 abstract is: <p>Background: In this thesis, it is investigated how different bathroom designs impact the useand effectiveness of assistive equipment by standardized patients and assistant nurses during homecare activities.</p><p>Aim: In this study, the aim is to quantitatively assess the impact of various ergonomicbathroom designs on the effectiveness of assistive equipment usage in home care settings.</p><p>Method: Through the analysis of video data from 42 experimental sessions, includingstandardized patients using walkers and wheelchairs across three different bathroom layouts, theeffectiveness of ergonomic designs in facilitating or hindering interactions with assistivetechnologies is examined.</p><p>Results: The experimental results indicate significantly higher frequency and duration ofhelpful interactions with assistive devices in the 'Equipped' bathroom layout during specific tasks,particularly for a standardized patient using walker. Although the results were not uniformlysignificant across all tasks and mobility levels, the data indicate a trend toward improved functionalindependence and safety in well-designed ergonomic environments.</p><p>Conclusion: Based on the findings, it is concluded that integrating ergonomic principles intohome care settings is critically important for enhancing the quality of life and care delivery. Thisstudy provides results emphasizing the necessity for thoughtful, user-centered designs in home careenvironments, particularly for aging populations. In this study, it is suggested that such designssignificantly impact the frequency and duration of assistive equipment usage, thereby promotingfunctional independence and safety for elderly users, and improving both caregiver efficiency andpatient autonomy.</p>

corrected abstract:
<p><strong>Background:</strong> In this thesis, it is investigated how different bathroom designs impact the use and effectiveness of assistive equipment by standardized patients and assistant nurses during home care activities.</p><p><strong>Aim:</strong> In this study, the aim is to quantitatively assess the impact of various ergonomic bathroom designs on the effectiveness of assistive equipment usage in home care settings.</p><p><strong>Method:</strong> Through the analysis of video data from 42 experimental sessions, including standardized patients using walkers and wheelchairs across three different bathroom layouts, the effectiveness of ergonomic designs in facilitating or hindering interactions with assistive technologies is examined.</p><p><strong>Results:</strong> The experimental results indicate significantly higher frequency and duration of helpful interactions with assistive devices in the 'Equipped' bathroom layout during specific tasks, particularly for a standardized patient using walker. Although the results were not uniformly significant across all tasks and mobility levels, the data indicate a trend toward improved functional independence and safety in well-designed ergonomic environments.</p><p><strong>Conclusion:</strong> Based on the findings, it is concluded that integrating ergonomic principles into home care settings is critically important for enhancing the quality of life and care delivery. This study provides results emphasizing the necessity for thoughtful, user-centered designs in home care environments, particularly for aging populations. In this study, it is suggested that such designs significantly impact the frequency and duration of assistive equipment usage, thereby promoting functional independence and safety for elderly users, and improving both caregiver efficiency and patient autonomy.</p>
----------------------------------------------------------------------
In diva2:1454420 abstract is: <p>Ouabain and other cardiotonic steroids are known to inhibit Na + ,K + -ATPase (NKA), theion pump responsible for the ionic gradient across the plasma membrane. These steroidsdisplay a selective toxicity towards several tumour cells in comparison to primary humancells, however, the mechanism behind this is not yet understood. Here, we examined theouabain toxicity in renal epithelial cells, proximal tubular cells (PTCs) of different origin. Weinvestigated the relative cytotoxicity in cancer cells (A-498) and papilloma virus-transformedPTCs (HK-2) as well as to primary human PTCs (hPTC) to validate key components in theeffect.</p><p>In exposure to ouabain, we examined the cell viability and density by MTT and CrystalViolet assays, and cell migration by a scratch assay. The cytotoxic effect was also studied invarious pH, glucose and potassium ion concentrations. In addition, apoptosis was examinedby the TUNEL assay, and if ouabain kills cancer cells through activation of thevolume-regulated anion channel VRAC channel via NKA.</p><p>We found that there is a decrease in viable cells when cells are exposed to ouabain ≥ 10nM, however, the effect was not seen to be selective towards cancer cells, nor due toapoptosis and the activation of VRAC. The cytotoxic effect was greater in more acidicextracellular pH ~6.8, but independent of glucose concentration in the medium. Interestingly,the effect was also reversed at an increased extracellular concentration of potassium, andouabain did selectively inhibit the cancer cells to migrate. Thus, there could be potential forouabain to act as an anti-cancer agent for renal cancer and to inhibit tumour metastasization.</p>


w='forouabain' val={'c': 'for ouabain', 's': 'diva2:1454420', 'n': 'correct in original'}
w='Na+ ,K + -ATPase' val={'c': 'Na<sup>+</sup>,K<sup>+</sup>-ATPase', 's': 'diva2:1454420', 'n': 'correct in original'}

corrected abstract:
<p>Ouabain and other cardiotonic steroids are known to inhibit Na<sup>+</sup>,K<sup>+</sup>-ATPase (NKA), the ion pump responsible for the ionic gradient across the plasma membrane. These steroids display a selective toxicity towards several tumour cells in comparison to primary human cells, however, the mechanism behind this is not yet understood. Here, we examined the ouabain toxicity in renal epithelial cells, proximal tubular cells (PTCs) of different origin. We investigated the relative cytotoxicity in cancer cells (A-498) and papilloma virus-transformed PTCs (HK-2) as well as to primary human PTCs (hPTC) to validate key components in the effect.</p><p>In exposure to ouabain, we examined the cell viability and density by MTT and Crystal Violet assays, and cell migration by a scratch assay. The cytotoxic effect was also studied in various pH, glucose and potassium ion concentrations. In addition, apoptosis was examined by the TUNEL assay, and if ouabain kills cancer cells through activation of the volume-regulated anion channel VRAC channel via NKA.</p><p>We found that there is a decrease in viable cells when cells are exposed to ouabain ≥ 10 nM, however, the effect was not seen to be selective towards cancer cells, nor due to apoptosis and the activation of VRAC. The cytotoxic effect was greater in more acidic extracellular pH ~6.8, but independent of glucose concentration in the medium. Interestingly, the effect was also reversed at an increased extracellular concentration of potassium, and ouabain did selectively inhibit the cancer cells to migrate. Thus, there could be potential for ouabain to act as an anti-cancer agent for renal cancer and to inhibit tumour metastasization.</p>
----------------------------------------------------------------------
In diva2:1689508 abstract is: <p>TRIM21 is a cytosolic ubiquitin ligase and an antibody receptor that providesa last line of defense against invading pathogens. By utilizing the diversity ofantibody repertoire to identify pathogens, TRIM21 serves as a link betweenintrinsic cellular defense and adaptive immunity. A variety of diseases havebeen linked to mutations of the TRIM family, including cancer, inflammatorydiseases, and autoimmune diseases. In this project, TRIM21 was producedand purified from Escherichia coli, (E.coli). Protein characterization wasperformed with SDS-PAGE, size exclusion chromatography and cryo-electronmicroscopy (cryo-EM). Previously TRIM21 has been shown to form a dimerwhen produced in SF9. Results from size exclusion chromatography show thatTRIM21 form a larger complex when expressed in E.coli. Cryo-EM resultsshow that the complex structure is more globular than previously thought.Purified TRIM21 was bound to the antibody IC100. SDS-PAGE and sizeexclusion chromatography results show much lower affinity to antibodies thanexpected.</p>


corrected abstract:
<p>TRIM21 is a cytosolic ubiquitin ligase and an antibody receptor that provides a last line of defense against invading pathogens. By utilizing the diversity of antibody repertoire to identify pathogens, TRIM21 serves as a link between intrinsic cellular defense and adaptive immunity. A variety of diseases have been linked to mutations of the TRIM family, including cancer, inflammatory diseases, and autoimmune diseases. In this project, TRIM21 was produced and purified from <em>Escherichia coli</em>, (<em>E.coli</em>). Protein characterization was performed with SDS-PAGE, size exclusion chromatography and cryo-electron microscopy (cryo-EM). Previously TRIM21 has been shown to form a dimer when produced in <em>SF9</em>. Results from size exclusion chromatography show that TRIM21 form a larger complex when expressed in <em>E.coli</em>. Cryo-EM results show that the complex structure is more globular than previously thought. Purified TRIM21 was bound to the antibody IC100. SDS-PAGE and size exclusion chromatography results show much lower affinity to antibodies than expected.</p>
----------------------------------------------------------------------
In diva2:1766059 abstract is: <p>Brain tumor is a disease characterized by uncontrolled growth of abnormal cells inthe brain. The brain is responsible for regulating the functions of all other organs,hence, any atypical growth of cells in the brain can have severe implications for itsfunctions. The number of global mortality in 2020 led by cancerous brains was estimatedat 251,329. However, early detection of brain cancer is critical for prompttreatment and improving patient’s quality of life as well as survival rates. Manualmedical image classification in diagnosing diseases has been shown to be extremelytime-consuming and labor-intensive. Convolutional Neural Networks (CNNs) hasproven to be a leading algorithm in image classification outperforming humans. Thispaper compares five CNN architectures namely: VGG-16, VGG-19, AlexNet, EffecientNetB7,and ResNet-50 in terms of performance and accuracy using transferlearning. In addition, the authors discussed in this paper the economic impact ofCNN, as an AI approach, on the healthcare sector. The models’ performance isdemonstrated using functions for loss and accuracy rates as well as using the confusionmatrix. The conducted experiment resulted in VGG-19 achieving best performancewith 97% accuracy, while EffecientNetB7 achieved worst performance with93% accuracy.</p>


corrected abstract:
<p>Brain tumor is a disease characterized by uncontrolled growth of abnormal cells in the brain. The brain is responsible for regulating the functions of all other organs, hence, any atypical growth of cells in the brain can have severe implications for its functions. The number of global mortality in 2020 led by cancerous brains was estimated at 251,329. However, early detection of brain cancer is critical for prompt treatment and improving patient’s quality of life as well as survival rates. Manual medical image classification in diagnosing diseases has been shown to be extremely time-consuming and labor-intensive. Convolutional Neural Networks (CNNs) has proven to be a leading algorithm in image classification outperforming humans. This paper compares five CNN architectures namely: VGG-16, VGG-19, AlexNet, EffecientNetB7, and ResNet-50 in terms of performance and accuracy using transfer learning. In addition, the authors discussed in this paper the economic impact of CNN, as an AI approach, on the healthcare sector. The models’ performance is demonstrated using functions for loss and accuracy rates as well as using the confusion matrix. The conducted experiment resulted in VGG-19 achieving best performance with 97% accuracy, while EffecientNetB7 achieved worst performance with 93% accuracy.</p>
----------------------------------------------------------------------
In diva2:1240203 abstract is: <p>In today’s paper and board production, quality control is made on a single cross direction (CD)sample from each tambour. As several different properties are analysed, only a limited number of measurement results are obtained for one property. Therefore, the measurement results might not be representative for the properties of the entire width of the tambour. The first objective of the project was to investigate variations of thickness, surface roughness and mechanical properties with a much higher resolution and number of measurements. The results of the measurement were compared with the routine quality control of the mill. The second objective of the project was to evaluate the influence of the wire shake unit in the centreply on the properties of the produced board. The measurements were performed on Iggesundpaperboard samples.The high-resolution measurements were performed using the STFI structural thicknessmeasurement device, an OptiTopo topography measurement device and a modified Autolinedevice at RISE Bioeconomy. The statistical evaluation of the results was performed in Matlab.Standard deviation, local variance and a frequency analysis were calculated for the thicknessmeasurements. Only standard deviation was considered for the topography data. For the mechanical properties, the distribution was evaluated using the Weibull distribution, since theresults had a single-sided distribution. In addition, the properties were analysed as a function of their location, for example to identify deterministic deviations in cross direction.The results of the first part of the project showed that the everyday control conducted in Iggesund is sufficient for most of the properties. Greatest difference was found at the edges ofthe samples, where Iggesund standard quality control does not detect a major variation inproperties, as no measurements are performed that close to the edge of the web. For example,at one edge, the high frequent measurements showed a significant drop in thickness which were not detected with the everyday quality control.In the second part of the project, the effect of a shake unit on the paper properties was evaluated. Here it was seen that the thickness variation were reduced, which also can be interpreted as an improvement of formation in the centre ply of the paperboard. As for thesurface roughness a slight improvement was found. Also for the mechanical properties, the shake unit appeared to improve the uniformity of the product</p>


corrected abstract:
<p>In today’s paper and board production, quality control is made on a single cross direction (CD) sample from each tambour. As several different properties are analysed, only a limited number of measurement results are obtained for one property. Therefore, the measurement results might not be representative for the properties of the entire width of the tambour. The first objective of the project was to investigate variations of thickness, surface roughness and mechanical properties with a much higher resolution and number of measurements. The results of the measurement were compared with the routine quality control of the mill. The second objective of the project was to evaluate the influence of the wire shake unit in the centre ply on the properties of the produced board. The measurements were performed on Iggesund paperboard samples.</p><p>The high-resolution measurements were performed using the STFI structural thickness measurement device, an OptiTopo topography measurement device and a modified Autoline device at RISE Bioeconomy. The statistical evaluation of the results was performed in Matlab. Standard deviation, local variance and a frequency analysis were calculated for the thickness measurements. Only standard deviation was considered for the topography data. For the mechanical properties, the distribution was evaluated using the Weibull distribution, since the results had a single-sided distribution. In addition, the properties were analysed as a function of their location, for example to identify deterministic deviations in cross direction.</p><p>The results of the first part of the project showed that the everyday control conducted in Iggesund is sufficient for most of the properties. Greatest difference was found at the edges of the samples, where Iggesund standard quality control does not detect a major variation in properties, as no measurements are performed that close to the edge of the web. For example, at one edge, the high frequent measurements showed a significant drop in thickness which were not detected with the everyday quality control.</p><p>In the second part of the project, the effect of a shake unit on the paper properties was evaluated. Here it was seen that the thickness variation were reduced, which also can be interpreted as an improvement of formation in the centre ply of the paperboard. As for the surface roughness a slight improvement was found. Also for the mechanical properties, the shake unit appeared to improve the uniformity of the product</p>
----------------------------------------------------------------------
In diva2:1859260 abstract is: <p>The purpose of this thesis was to validate a method for quantitatively determining PFAS in soil,sludge, and sediment. The validation involved a number of different tests to evaluate themethod's reporting limit, precision, accuracy, measurement uncertainty, and specificity. Excelwas used to calculate and summarize the results from the different tests. Blank and spikedsamples were analyzed to investigate the reporting limit. Control samples were analyzed toinvestigate precision and specificity. A variety of tests were performed to investigate accuracy;ISTD matching for some PFAS analytes that lacked a direct ISTD match, comparisons againstcertified reference material, analysis of previously performed tests that had already beenanalyzed at ALS Prague, and spiked sample matrices. The measurement uncertainty wasestimated and evaluated in consultation with the quality manager.</p><p>The results of the method validation confirm that the method has been shown to be wellvalidated. All tests performed during the validation process have consistently met and exceededthe established acceptance criteria for the validation. Criteria included, among others, relativestandard deviation (RSD), recovery, bias, and concentration for various test parameters. Thecomprehensive method validation provided strong evidence that the method is reliable and canbe used to generate reliable results in PFAS analysis.</p>


corrected abstract:
<p>The purpose of this thesis was to validate a method for quantitatively determining PFAS in soil, sludge, and sediment. The validation involved a number of different tests to evaluate the method's reporting limit, precision, accuracy, measurement uncertainty, and specificity. Excel was used to calculate and summarize the results from the different tests. Blank and spiked samples were analyzed to investigate the reporting limit. Control samples were analyzed to investigate precision and specificity. A variety of tests were performed to investigate accuracy; ISTD matching for some PFAS analytes that lacked a direct ISTD match, comparisons against certified reference material, analysis of previously performed tests that had already been analyzed at ALS Prague, and spiked sample matrices. The measurement uncertainty was estimated and evaluated in consultation with the quality manager.</p><p>The results of the method validation confirm that the method has been shown to be well validated. All tests performed during the validation process have consistently met and exceeded the established acceptance criteria for the validation. Criteria included, among others, relative standard deviation (RSD), recovery, bias, and concentration for various test parameters. The comprehensive method validation provided strong evidence that the method is reliable and can be used to generate reliable results in PFAS analysis.</p>
----------------------------------------------------------------------
In diva2:1455134 abstract is: <p>The numerous SNPs discovered in genome-wide association studies (GWAS) are predominantlylocated in non-coding regions. This project aimed to investigate and compare the effect of four rarevariants and four common variants on protein production. The variants were located in enhancers,regulatory and non-coding regions of the genome that affect the transcription by interacting withproteins bound to promoters. Foremost, eight primer pairs were designed to retrieve the SNPs from aselection of available DNA templates, by PCR. Thereafter, a series of transformations of E. coli wereperformed to obtain the inserts within the final luciferase vector, subsequently used for a luciferaseassay, where bioluminescence is utilized for assessment of the effect size of each correspondingvariant. The primary analytic methods used during the project for validation of the results weresequencing and gel electrophoresis. The result indicated a presence of both rare and common variants,with a heterogeneous contribution on gene expression. In conclusion, an effect from both rare andcommon variants have been confirmed in several earlier studies. However, it is desirable toinvestigate the contribution to effect size further and possibly obtain a quantitative measurement ofthe effect.</p>


corrected abstract:
<p>The numerous SNPs discovered in genome-wide association studies (GWAS) are predominantly located in non-coding regions. This project aimed to investigate and compare the effect of four rare variants and four common variants on protein production. The variants were located in enhancers, regulatory and non-coding regions of the genome that affect the transcription by interacting with proteins bound to promoters. Foremost, eight primer pairs were designed to retrieve the SNPs from a selection of available DNA templates, by PCR. Thereafter, a series of transformations of <em>E. coli</em> were performed to obtain the inserts within the final luciferase vector, subsequently used for a luciferase assay, where bioluminescence is utilized for assessment of the effect size of each corresponding variant. The primary analytic methods used during the project for validation of the results were sequencing and gel electrophoresis. The result indicated a presence of both rare and common variants, with a heterogeneous contribution on gene expression. In conclusion, an effect from both rare and common variants have been confirmed in several earlier studies. However, it is desirable to investigate the contribution to effect size further and possibly obtain a quantitative measurement of the effect.</p>
----------------------------------------------------------------------
title: "Are legal requirements enough forpreventing occupational accidents?"
==>    "Are legal requirements enough for preventing occupational accidents?"

In diva2:1775516 abstract is: <p>The increasing number of occupational health and safety issues is a problem. Legislationsthat are anchored in European law, such as “machinery directive 2006/42/EC”, the “Use ofwork equipment 2009/104/EC” and the Swedish AFS 2001:1 (Systematic Work EnvironmentManagement) are defined but still lack the power to stop accidents/ incidents fromhappening. When risks are being made conscious they are not stopped by the legalrequirements in place.</p><p>Scientific approaches such as the Swiss cheese model, safety management systems (SMS),and HTO (Human- Technology- Organization) explain how increased complexity inside asocio-technical system needs more attention. As the cases of accidents/ incidents in anoccupational setting still increase a need for solving this appears, with the help of sciencebasedtools.</p><p>In cooperation with the company AFRY, I conducted four interviews (n=4) and analyzed twoABRA (activity-based risk assessments) already conducted by the company. Using thecommon themes identified from the interviews to analyze the ABRA helped to identify twokey problems: unclear communication and insufficient knowledge.</p><p>With that in mind, I’m advocating for an increased emphasis on risk communication andresilience engineering. With the awareness that communication must be clearer and thatknowledge has to be increased, it is possible to work proactively on decreasing occupationalaccidents by mitigating the risks.</p>

corrected abstract:
<p>The increasing number of occupational health and safety issues is a problem. Legislations that are anchored in European law, such as “machinery directive 2006/42/EC”, the “Use of work equipment 2009/104/EC” and the Swedish AFS 2001:1 (Systematic Work Environment Management) are defined but still lack the power to stop accidents/ incidents from happening. When risks are being made conscious they are not stopped by the legal requirements in place.</p><p>Scientific approaches such as the Swiss cheese model, safety management systems (SMS), and HTO (Human- Technology- Organization) explain how increased complexity inside a socio-technical system needs more attention. As the cases of accidents/ incidents in an occupational setting still increase a need for solving this appears, with the help of science-based tools.</p><p>In cooperation with the company AFRY, I conducted four interviews (n=4) and analyzed two ABRA (activity-based risk assessments) already conducted by the company. Using the common themes identified from the interviews to analyze the ABRA helped to identify two key problems: unclear communication and insufficient knowledge.</p><p>With that in mind, I’m advocating for an increased emphasis on risk communication and resilience engineering. With the awareness that communication must be clearer and that knowledge has to be increased, it is possible to work proactively on decreasing occupational accidents by mitigating the risks.</p>
----------------------------------------------------------------------
In diva2:1217939 abstract is: <p>Banks internet activity have opened new possibilities for third party actors within the fintechindustry who can now create innovative economy and payment solutions by directlyinteracting with bank accounts. A problem for the fintech companies is that there is noavailable information about what technologies are available for them to use to get access tothe banks services and which technology is the most advantageous for their purpose.In this work, the four biggest swedish banks Nordea, Handelsbanken, SEB and Swedbankhave been examined to find out what options third party actors have to integrate with thebanks services. The examination showed that fintech companies will be able to or are able touse one of the three techniques SFTP, REST or web automation. A test environment for thethree technologies was developed to simulate real use cases for fintech companies. CPU-,RAM- and network usage as well as total operating time was measured for the threetechnologies. For sending information REST was the more effective technology when it cameto lower data volumes while SFTP was the most efficient with bigger data volumes. RESTwas the most efficient for retrieving information, no matter the data volume. Web automationwas the least efficient compared to both the other technologies.</p>


corrected abstract:
<p>Banks internet activity have opened new possibilities for third party actors within the fintech industry who can now create innovative economy and payment solutions by directly interacting with bank accounts. A problem for the fintech companies is that there is no available information about what technologies are available for them to use to get access to the banks services and which technology is the most advantageous for their purpose.</p><p>In this work, the four biggest swedish banks Nordea, Handelsbanken, SEB and Swedbank have been examined to find out what options third party actors have to integrate with the banks services. The examination showed that fintech companies will be able to or are able to use one of the three techniques SFTP, REST or web automation. A test environment for the three technologies was developed to simulate real use cases for fintech companies. CPU-, RAM- and network usage as well as total operating time was measured for the three technologies. For sending information REST was the more effective technology when it came to lower data volumes while SFTP was the most efficient with bigger data volumes. REST was the most efficient for retrieving information, no matter the data volume. Web automation was the least efficient compared to both the other technologies.</p>
----------------------------------------------------------------------
title: "Evaluating the Next Generation of Building Automation – IoT SmartBuildings"
==>    "Evaluating the Next Generation of Building Automation – IoT Smart Buildings"

In diva2:1451575 abstract is: <p>Abstract</p><p>Building automation systems typically use proprietary hardware and included softwarefor their automation which can make the systems vendor-locked. Establishedprogramming standards limits the freedom of companies to improve their automationsystems with the growth of the IT era. The purpose of the thesis is to investigateand evaluate IoT solutions and implement one of these methods as proof ofconcept and to elicit new aspects for analysis and discussion.</p><p>With the literature study three different methods was discovered followed bya comparative study. These methods include: Porting the existing software andmoving the automation process. Replacing the hardware with smaller computers.Adding a server as translator between the building and the cloud.</p><p>The methods have different use cases with the objective of integrating a cloudservice to create smarter building automation system to reduce energy consumptionin buildings. One of the methods was proven to be most suitable for implementationbased on requirements set by experts in the field. The method chosenwas porting a smaller portion of an existing BAS to a new programming language.</p><p>The final prototype was completed with a ported program, from IEC 61131-3 standard to Java and the automation was moved from a programmable logiccontroller to an edge unit. The discussion focuses on different ways of optimizingthe system, one of the optimization is to move the automation process to cloudcomputing. Energy managements are considered by collecting data and metadatain the cloud to create energy profiles for reduced energy consumption.</p><p>Keywords: internet of things, building management system, buildingautomation system, programmable logic controller, porting, legacy, energymanagement, cloud services</p>


corrected abstract:
<p>Building automation systems typically use proprietary hardware and included software for their automation which can make the systems vendor-locked. Established programming standards limits the freedom of companies to improve their automation systems with the growth of the IT era. The purpose of the thesis is to investigate and evaluate IoT solutions and implement one of these methods as proof of concept and to elicit new aspects for analysis and discussion.</p><p>With the literature study three different methods was discovered followed by a comparative study. These methods include: <em>Porting the existing software and moving the automation process.</em> <em>Replacing the hardware with smaller computers.</em> <em>Adding a server as translator between the building and the cloud.</em></p><p>The methods have different use cases with the objective of integrating a cloud service to create smarter building automation system to reduce energy consumption in buildings. One of the methods was proven to be most suitable for implementation based on requirements set by experts in the field. The method chosen was porting a smaller portion of an existing BAS to a new programming language.</p><p>The final prototype was completed with a ported program, from IEC 61131-3 standard to Java and the automation was moved from a programmable logic controller to an edge unit. The discussion focuses on different ways of optimizing the system, one of the optimization is to move the automation process to cloud computing. Energy managements are considered by collecting data and metadata in the cloud to create energy profiles for reduced energy consumption.</p>
----------------------------------------------------------------------
In diva2:1454998 abstract is: <p>Mesenchymal stromal cells (MSCs) possess immunomodulatory properties which make themattractive for the treatment of various inflammatory diseases. Upon intravascular administration,MSCs get trapped in the lungs due to their size. A few hours after their administration, the vast majorityof MSCs can neither be found in the lungs nor anywhere else in the body. To explain the long-termimmunomodulatory effect following MSC administration in concert with the rapid disappearance ofMSCs, it was hypothesized that monocytes phagocytose MSCs and on behalf of them induce observedimmunomodulatory effects. While the phagocytosis of MSCs derived from bone marrow and umbilicalcord by monocytes was already observed in vitro, this report aims to investigate whether this is alsotrue for MSCs derived from the human placental decidua basalis (DBMSCs).</p><p>Here, we report that CD14+ monocytes alone or monocytes within peripheral blood mononuclear cells(PBMCs) are capable of effectively phagocytosing DBMSCs. Kinetic studies showed that phagocytosisstarted after 2 hours of co-culture and reached a significant number of engulfed DBMSCs within24 hours of co-culture in vitro. Interestingly, a positive effect on monocyte survival could be observedfor monocytes co-cultured with DBMSCs. Further, the phagocytosis of DBMSCs by purifiedCD14+ monocytes resulted in an upregulation of cell surface markers CD163 and CD206 indicating thepolarization of monocytes towards an anti-inflammatory M2 phenotype. The generation ofCD16+CD163+CD206+ monocytes was significantly different if co-culture experiments were performedwith purified CD14+ monocytes, but not for monocytes mixed with other PBMCs, suggesting thatpolarization of monocytes by DBMSCs can be regulated by surrounding cells.</p>


corrected abstract:
<p>Mesenchymal stromal cells (MSCs) possess immunomodulatory properties which make them attractive for the treatment of various inflammatory diseases. Upon intravascular administration, MSCs get trapped in the lungs due to their size. A few hours after their administration, the vast majority of MSCs can neither be found in the lungs nor anywhere else in the body. To explain the long-term immunomodulatory effect following MSC administration in concert with the rapid dis appearance of MSCs, it was hypothesized that monocytes phagocytose MSCs and on behalf of them induce observed immunomodulatory effects. While the phagocytosis of MSCs derived from bone marrow and umbilical cord by monocytes was already observed <em>in vitro</em>, this report aims to investigate whether this is also true for MSCs derived from the human placental decidua basalis (DBMSCs).</p><p>Here, we report that CD14+ monocytes alone or monocytes within peripheral blood mononuclear cells (PBMCs) are capable of effectively phagocytosing DBMSCs. Kinetic studies showed that phagocytosis started after 2 hours of co-culture and reached a significant number of engulfed DBMSCs within 24 hours of co-culture <em>in vitro<em>. Interestingly, a positive effect on monocyte survival could be observed for monocytes co-cultured with DBMSCs. Further, the phagocytosis of DBMSCs by purifiedCD14+ monocytes resulted in an upregulation of cell surface markers CD163 and CD206 indicating the polarization of monocytes towards an anti-inflammatory M2 phenotype. The generation of CD16+CD163+CD206+ monocytes was significantly different if co-culture experiments were performed with purified CD14+ monocytes, but not for monocytes mixed with other PBMCs, suggesting that polarization of monocytes by DBMSCs can be regulated by surrounding cells.</p>
----------------------------------------------------------------------
In diva2:1455009 abstract is: <p>As the aging population is increasing worldwide, so is the prevalence of neurodegenerativediseases such as Alzheimer’s disease (AD), Parkinson’s disease (PD), frontotemporal dementia(FTD) and amyotrophic lateral sclerosis (ALS). Reliable biomarkers able to aid the diagnosis anddifferentiation of these diseases are needed in order to start the right treatment as early as possible.Due to its representative state of the central nervous system, cerebrospinal fluid (CSF) is afavorable sample material for biomarker discovery within neurodegenerative diseases. Alteredprotein levels of this body fluid might serve as a biomarker, but further validation of earlierfindings is needed. The aim of this project was to validate earlier studies suggesting potentialprotein biomarkers in CSF. From a list of 80 potential biomarkers in the CSF of patient samples,eight were chosen to be included in this validation effort. By utilizing a suspension bead array ina sandwich assay setup, 21 antibodies were tested in an initial screening. Antibody pairs that couldmeasure the protein levels in a dilution dependent manner was further optimized before individualpatient samples were analyzed. Sandwich assays targeting the three proteins Amphiphysin(AMPH), Chitotriosidase-1 (CHIT1) and Beta-synuclein (SNCB) were successfully developed andcorrelated to earlier generated data using a suspension bead array with a single binder setup.Therefore, the earlier findings of elevated levels of AMPH and SNCB in AD patients and CHIT1in ALS patients were successfully validated.</p>


corrected abstract:
<p>As the aging population is increasing worldwide, so is the prevalence of neurodegenerative diseases such as Alzheimer’s disease (AD), Parkinson’s disease (PD), frontotemporal dementia (FTD) and amyotrophic lateral sclerosis (ALS). Reliable biomarkers able to aid the diagnosis and differentiation of these diseases are needed in order to start the right treatment as early as possible. Due to its representative state of the central nervous system, cerebrospinal fluid (CSF) is a favorable sample material for biomarker discovery within neurodegenerative diseases. Altered protein levels of this body fluid might serve as a biomarker, but further validation of earlier findings is needed. The aim of this project was to validate earlier studies suggesting potential protein biomarkers in CSF. From a list of 80 potential biomarkers in the CSF of patient samples, eight were chosen to be included in this validation effort. By utilizing a suspension bead array in a sandwich assay setup, 21 antibodies were tested in an initial screening. Antibody pairs that could measure the protein levels in a dilution dependent manner was further optimized before individual patient samples were analyzed. Sandwich assays targeting the three proteins Amphiphysin (AMPH), Chitotriosidase-1 (CHIT1) and Beta-synuclein (SNCB) were successfully developed and correlated to earlier generated data using a suspension bead array with a single binder setup. Therefore, the earlier findings of elevated levels of AMPH and SNCB in AD patients and CHIT1 in ALS patients were successfully validated.</p>
----------------------------------------------------------------------
title: "Evaluation of tools for automatedacceptance testing of webapplications"
==>    "Evaluation of tools for automated acceptance testing of web applications"

In diva2:935212 abstract is: <p>Auddly provides a music management tool that gathers all information about a musical piece in oneplace. The acceptance testing on their web application is done manually, which has become bothtime and money consuming. To solve this problem, an evaluation on automated acceptance testingwas done to find a testing tool suitable for their web application. The evaluation was performed byfinding the current existing testing strategies to later compare the tools implementing these strategies.When analyzing the results it was found that two testing strategies were best suited for automatedacceptance testing. The Visual Recognition strategy that identifies components using screenshotsand the Record and Replay strategy that identifies them by their underlying ID. The choice betweenthem depends on which of these properties are modified more often.It was also found that automating acceptance testing is best applied for regression testing, otherwiseit should be performed with a manual approach.It was made clear that the Selenium tool, which uses the Record and Replay strategy, was best suitedfor Auddly’s acceptance testing. Selenium is able to test AJAX-calls with a manual modificationand is a free and open source tool with a large community.</p>


corrected abstract:
<p>Auddly provides a music management tool that gathers all information about a musical piece in one place. The acceptance testing on their web application is done manually, which has become both time and money consuming. To solve this problem, an evaluation on automated acceptance testing was done to find a testing tool suitable for their web application. The evaluation was performed by finding the current existing testing strategies to later compare the tools implementing these strategies.</p><p>When analyzing the results it was found that two testing strategies were best suited for automated acceptance testing. The Visual Recognition strategy that identifies components using screenshots and the Record and Replay strategy that identifies them by their underlying ID. The choice between them depends on which of these properties are modified more often. It was also found that automating acceptance testing is best applied for regression testing, otherwise it should be performed with a manual approach.</p><p>It was made clear that the Selenium tool, which uses the Record and Replay strategy, was best suited for Auddly’s acceptance testing. Selenium is able to test AJAX-calls with a manual modification and is a free and open source tool with a large community.</p>
----------------------------------------------------------------------
In diva2:1342590 abstract is: <p>Ionizing radiation is often used within medicine for diagnosis and treatments. Because ionizingradiation can be harmful to the body, it is important to know how it affects the tissue. Dosimetryis the study of how ionizing radiation deposits energy in a material. To measure how much ionizingradiation is deposited in the body, gas-filled detectors are often used. An ionization chamber isa type of gas-filled detector and exists in different shapes and sizes, depending on what kind ofmeasurements it is made for. Because ionization chambers are relatively expensive, it is often notpossible to buy one for each type of measurement that is to be done. This results in ionizationchambers being used for measurements they are not optimized for. This report evaluates thepossibility of 3D printing ionization chambers to make it easier to optimize them for specificmeasurements. The process included creating models of ionization chambers using CAD-software,slicing them and then 3D printing them. The 3D printed models were then brought to the SwedishRadiation Safety Authority for measurements. The ionization chambers were connected to highvoltage, and exposed to ionizing radiation in the form of high-intensity gamma-ray fields. Theoutput current of the ionization chamber was measured, which is proportional to the field intensity.The results were similar to those of a commercial ionization chamber. The conclusion is that it ispossible to 3D print ionization chambers. However, to get more accurate results, the design has tobe further optimized and more measurements need to be done.</p>


corrected abstract:
<p>Ionizing radiation is often used within medicine for diagnosis and treatments. Because ionizing radiation can be harmful to the body, it is important to know how it affects the tissue. Dosimetry is the study of how ionizing radiation deposits energy in a material. To measure how much ionizing radiation is deposited in the body, gas-filled detectors are often used. An ionization chamber is a type of gas-filled detector and exists in different shapes and sizes, depending on what kind of measurements it is made for. Because ionization chambers are relatively expensive, it is often not possible to buy one for each type of measurement that is to be done. This results in ionization chambers being used for measurements they are not optimized for. This report evaluates the possibility of 3D printing ionization chambers to make it easier to optimize them for specific measurements. The process included creating models of ionization chambers using CAD-software, slicing them and then 3D printing them. The 3D printed models were then brought to the Swedish Radiation Safety Authority for measurements. The ionization chambers were connected to high voltage, and exposed to ionizing radiation in the form of high-intensity gamma-ray fields. The output current of the ionization chamber was measured, which is proportional to the field intensity. The results were similar to those of a commercial ionization chamber. The conclusion is that it is possible to 3D print ionization chambers. However, to get more accurate results, the design has to be further optimized and more measurements need to be done.</p>
----------------------------------------------------------------------
In diva2:1456255 abstract is: <p>When a material is adhered onto a specific surface it is relevant to know how to make thematerial stay on the surface. By investigating different primers to use with a triazine-basedadhesive, further improvements to using the adhesive on metals can be achieved. This studyfound that an adhesive of (2,4,6-trioxo-1,3,5-triazinane-1,3,5-triyl)tris(ethane-2,1-diyl)tris(3-mercaptopropanoate) (TEMPIC) and 1,3,5-triallyl-1,3,5-triazinane-2,4,6-trione(TATATO) adhered to titanium and stainless steel, two clinically used metal surfaces. Itfurther found that between a phosphonic acid primer, a biomimetic catechol primer and acommercially available silane primer the phosphonic acid primer gave the best adhesion.These results could be because of a higher amount of crosslinking for the phosphonic acidprimer. For further testing increased pH and increased amount as well as increasedhydrolysation time for the catechol and silane primers respectively is suggested. Shearstrength testing was used to determine the adhesion strength. The shear strength testswere done with conditioning in phosphate buffer saline (PBS) solution for 24h beforehand</p>


corrected abstract:
<p>When a material is adhered onto a specific surface it is relevant to know how to make the material stay on the surface. By investigating different primers to use with a triazine-based adhesive, further improvements to using the adhesive on metals can be achieved. This study found that an adhesive of (2,4,6-trioxo-1,3,5-triazinane-1,3,5-triyl)tris(ethane-2,1-diyl) tris(3-mercaptopropanoate) (TEMPIC) and 1,3,5-triallyl-1,3,5-triazinane-2,4,6-trione (TATATO) adhered to titanium and stainless steel, two clinically used metal surfaces. It further found that between a phosphonic acid primer, a biomimetic catechol primer and a commercially available silane primer the phosphonic acid primer gave the best adhesion. These results could be because of a higher amount of crosslinking for the phosphonic acid primer. For further testing increased pH and increased amount as well as increased hydrolysation time for the catechol and silane primers respectively is suggested. Shear strength testing was used to determine the adhesion strength. The shear strength tests were done with conditioning in phosphate buffer saline (PBS) solution for 24h beforehand.</p>
----------------------------------------------------------------------
In diva2:1216523 abstract is: <p>The use of solar cells is continuously increasing in Sweden and the powergenerated by the solar cells is usually stored in lead acid batteries. These batterieshave a bad impact on the environment as much energy and environmentallyhazardous materials like lead and sulfuric acid are required to manufacture thesebatteries. Östersjökompaniet AB and many of its customers realize the importanceof sustainable thinking and were interested in knowing if it was possible tomaximize the lifetime of these batteries. During the course of the work, differentmethods of battery charging and discharging were analyzed that could affect thebatteries lifetime and how to take care of them to optimize them. A chargecontroller was used to optimize the charge of the battery. To calculate theremaining state of charge in the battery, the Extended voltmeter method was used.A prototype that was able to charge the batteries optimally, warn when the batterycapacity became too low, and a user-friendly application for battery monitoring wasdesigned. The calculated lifetime of a battery is not an exact science. According tostudies the lifetime of a battery can be doubled if it is c</p>

w='powergenerated' val={'c': 'power generated', 's': 'diva2:1216523', 'n': 'there was a newline between the words'}

Note that the abstract in DiVA was trunced, the version below also includes the missing text.
corrected abstract:
<p>The use of solar cells is continuously increasing in Sweden and the power generated by the solar cells is usually stored in lead acid batteries. These batteries have a bad impact on the environment as much energy and environmentally hazardous materials like lead and sulfuric acid are required to manufacture these batteries. Östersjökompaniet AB and many of its customers realize the importance of sustainable thinking and were interested in knowing if it was possible to maximize the lifetime of these batteries. During the course of the work, different methods of battery charging and discharging were analyzed that could affect the batteries lifetime and how to take care of them to optimize them. A charge controller was used to optimize the charge of the battery. To calculate the remaining state of charge in the battery, the Extended voltmeter method was used. A prototype that was able to charge the batteries optimally, warn when the battery capacity became too low, and a user-friendly application for battery monitoring was designed. The calculated lifetime of a battery is not an exact science. According to studies the lifetime of a battery can be doubled if it is charged and discharged in an optimal way compared to when it is fully discharged.</p>
----------------------------------------------------------------------
In diva2:1447001 abstract is: <p>In today’s society, with several environmental challenges such as global warming, the demand for cleanand renewable fuels is ever increasing. Since the aviation industry in Sweden is responsible for the sameamount of greenhouse gas emissions as the motor traffic, a change to a non-polluting energy source forflying vehicles would be considerable progress. Therefore, this project has designed a hybrid system of abattery and a fuel cell and investigated how different combinations of battery and fuel cell sizes perform ina drive cycle, through computer modelling. As batteries possess a high specific power but are heavy, thefuel cells with high specific energy complement them with a sustained and lightweight power supply,which makes the hybrid perfect for aviation. The bachelor thesis is a part of Project Green Raven, aninterdisciplinary collaboration with the institutions of Applied Electrochemistry, Mechatronics andEngineering Mechanics at KTH Royal Institute of Techology. The drive cycle simulations were done inSimulink, and several assumptions regarding the power profile, fuel cell measurements and power weremade. Three different energy management strategies were set up, determining the fuel cell powerdepending on hydrogen availability and state of charge of the battery. The strategies were called 35/65,20/80 and 0/100, and the difference between them was at which state of charge intervals the fuel cellchanged its power output. The best strategy proved to be 0/100, since it was the only option which causedno degradation of the fuel cell whatsoever.</p>

w='Techology' val={'c': 'Technology', 's': 'diva2:1447001', 'n': 'error in original'}

corrected abstract:
<p>In today’s society, with several environmental challenges such as global warming, the demand for clean and renewable fuels is ever increasing. Since the aviation industry in Sweden is responsible for the same amount of greenhouse gas emissions as the motor traffic, a change to a non-polluting energy source for flying vehicles would be considerable progress. Therefore, this project has designed a hybrid system of a battery and a fuel cell and investigated how different combinations of battery and fuel cell sizes perform in a drive cycle, through computer modelling. As batteries possess a high specific power but are heavy, the fuel cells with high specific energy complement them with a sustained and lightweight power supply, which makes the hybrid perfect for aviation. The bachelor thesis is a part of Project Green Raven, an interdisciplinary collaboration with the institutions of Applied Electrochemistry, Mechatronics and Engineering Mechanics at KTH Royal Institute of Techology. The drive cycle simulations were done in Simulink, and several assumptions regarding the power profile, fuel cell measurements and power were made. Three different energy management strategies were set up, determining the fuel cell power depending on hydrogen availability and state of charge of the battery. The strategies were called 35/65, 20/80 and 0/100, and the difference between them was at which state of charge intervals the fuel cell changed its power output. The best strategy proved to be 0/100, since it was the only option which caused no degradation of the fuel cell whatsoever.</p>
----------------------------------------------------------------------
title: "Compartmentmentalized immuno-sequencing (cI-Seq): identification of immune complex interactions"
==>    "Compartmentalized Immuno-Sequencing (cI-Seq): Identification of immune complex interactions"

In diva2:1049472 abstract is: <p>Today, a lot of proteomic research is aimed at discovering disease specific proteins. This requires theavailability of high-throughput, ultra-sensitive protein detection methods. Compartmentalized immunosequencing(cI-Seq) is a proximity-independent immuno-polymerase chain reaction (IPCR) based proteindetection method. Antigen recognition in cI-Seq is mediated by antibody pairs in which one of theantibodies is conjugated to a DNA-probe. The affinity recognition events occur in emulsion droplets inwhich the DNA-probes will be amplified through emulsion PCR (emPCR) and thereafter analyzed usingMassively Parallel Sequencing (MPS). The amplifiable nature of the DNA-probes improves the sensitivityof the detection, while the use of emulsion droplets and MPS increases the multiplex capacity andthroughput. Ultimately, cI-Seq enables analysis and detection even of lowly abundant proteins therebyincreasing the probability of discovering novel disease specific proteins.</p><p>In this project, conjugation of DNA probes to antibodies was performed through two different approaches;Covalent Conjugation and Conjugation using Biotin and NeutrAvidin. Both of these approaches showedadvantageous and disadvantageous features. However, neither of them succeeded in producing stableconjugates in an efficient and reproducible manner. After conjugation, the DNA-conjugated antibodieswere used in immune complex formation. However, the immune complexes either failed to form or wereformed in an inefficient manner.</p>


corrected abstract:
<p>Today, a lot of proteomic research is aimed at discovering disease specific proteins. This requires the availability of high-throughput, ultra-sensitive protein detection methods. Compartmentalized immunosequencing (cI-Seq) is a proximity-independent immuno-polymerase chain reaction (IPCR) based protein detection method. Antigen recognition in cI-Seq is mediated by antibody pairs in which one of the antibodies is conjugated to a DNA-probe. The affinity recognition events occur in emulsion droplets in which the DNA-probes will be amplified through emulsion PCR (emPCR) and thereafter analyzed using Massively Parallel Sequencing (MPS). The amplifiable nature of the DNA-probes improves the sensitivity of the detection, while the use of emulsion droplets and MPS increases the multiplex capacity and throughput. Ultimately, cI-Seq enables analysis and detection even of lowly abundant proteins thereby increasing the probability of discovering novel disease specific proteins.</p><p>In this project, conjugation of DNA probes to antibodies was performed through two different approaches; <em>Covalent Conjugation</em> and <em>Conjugation using Biotin and NeutrAvidin</em>. Both of these approaches showed advantageous and disadvantageous features. However, neither of them succeeded in producing stable conjugates in an efficient and reproducible manner. After conjugation, the DNA-conjugated antibodies were used in immune complex formation. However, the immune complexes either failed to form or were formed in an inefficient manner.</p>
----------------------------------------------------------------------
In diva2:1109024 abstract is: <p>This study has been analyzing if machine learning could be useful to work-relatedscheduling. The analysis was based on predictions generated by prototypes usingbusiness calendars. The business calendars were collected from two service and installationcompanies in the Stockholm region. An analysis was conducted regardingif the application could be practically applied to devices such as a smartphone. Theanalysis was based on tests regarding the prototypes required time to perform theirtasks.Three prototypes were developed with algorithms that made them predictive. Density-based Spatial Clustering of Applications with Noise (DBSCAN), Logistic Regressionand Weighted K-Nearest Neighbors (wKNN) were the implemented algorithms.DBSCAN was the best-performing algorithm according to the tests. However, a conclusioncould not be found concerning whether machine learning could be useful.The number of successful predictions did not exceed the number of available timeson concerned days, which was assumed as unsatisfying results. In addition, the prototypesneeded a significant amount of resources which could be a problem in practicaluse.</p>


corrected abstract:
<p>This study has been analyzing if machine learning could be useful to work-related scheduling. The analysis was based on predictions generated by prototypes using business calendars. The business calendars were collected from two service and installation companies in the Stockholm region. An analysis was conducted regarding if the application could be practically applied to devices such as a smartphone. The analysis was based on tests regarding the prototypes required time to perform their tasks.</p><p>Three prototypes were developed with algorithms that made them predictive. Density-based Spatial Clustering of Applications with Noise (DBSCAN), Logistic Regression and Weighted K-Nearest Neighbors (wKNN) were the implemented algorithms. DBSCAN was the best-performing algorithm according to the tests. However, a conclusion could not be found concerning whether machine learning could be useful. The number of successful predictions did not exceed the number of available times on concerned days, which was assumed as unsatisfying results. In addition, the prototypes needed a significant amount of resources which could be a problem in practical use.</p>
----------------------------------------------------------------------
title: "Ethical Hacking of an IoT-device:Threat Assessment andPenetration Testing"
==>    "Ethical Hacking of an IoT-device: Threat Assessment and Penetration Testing"

In diva2:1472577 abstract is: <p>Abstract</p><p>Internet of things (IoT) devices are becoming more prevalent.Due to a rapidly growing market of these appliances, impropersecurity measures lead to an expanding range of attacks. There isa devoir of testing and securing these devices to contribute to amore sustainable society. This thesis has evaluated the securityof an IoT-refrigerator by using ethical hacking, where a threatmodel was produced to identify vulnerabilities. Penetration testswere performed based on the threat model. The results from thepenetration tests did not find any exploitable vulnerabilities. Theconclusion from evaluating the security of this Samsungrefrigerator can say the product is secure and contributes to aconnected, secure, and sustainable society.</p><p>Keywords</p><p>Internet of things (IoT), device, security, penetration testing,threat assessment, vulnerabilities</p>

corrected abstract:
<p>Internet of things (IoT) devices are becoming more prevalent. Due to a rapidly growing market of these appliances, improper security measures lead to an expanding range of attacks. There is a devoir of testing and securing these devices to contribute to a more sustainable society. This thesis has evaluated the security of an IoT-refrigerator by using ethical hacking, where a threat model was produced to identify vulnerabilities. Penetration tests were performed based on the threat model. The results from the penetration tests did not find any exploitable vulnerabilities. The conclusion from evaluating the security of this <em>Samsung</em> refrigerator can say the product is secure and contributes to a connected, secure, and sustainable society.</p>
---------------------------------------------------------------------
In diva2:1528033 abstract is: <p>Security in applications, to protect users and information, is becoming increasingly importantas society is becoming more digitized. During the duration of this report two major informationleaks, of sensitive Swedish classified information, occurred. The aim of this report is to findwhat security risks exist for webapplications, general measures that can be implemented to address these risks and how The General Data Protection Regulation (GDPR) is related to securityin applications. In order to achieve these general measures a security evaluation method was alsoused to be able to evaluate how the various measures protect webapplications, and function asrequired. The results of this report provide a general list of actions that should be implementedin application backends, but to provide a similar for the frontend more wokrs is required, wherethe frontend action list in this report is minimal. The safety evaluation method also proved to bea part of the action lists in order to be able to test the security even on operational applications.The results of the GDPR survey showed that no specific requirements are set from GDPR,instead the law has the task of raising the priority by making the consequences that can arisefrom incorrect handling of users’ personal data more serious / costly to the organization</p>


w='wokrs' val={'c': 'works', 's': 'diva2:1528033', 'n': 'error in original'}

corrected abstract:
<p>Security in applications, to protect users and information, is becoming increasingly important as society is becoming more digitized. During the duration of this report two major information leaks, of sensitive Swedish classified information, occurred. The aim of this report is to find what security risks exist for webapplications, general measures that can be implemented to address these risks and how The General Data Protection Regulation (GDPR) is related to security in applications. In order to achieve these general measures a security evaluation method was also used to be able to evaluate how the various measures protect webapplications, and function as required. The results of this report provide a general list of actions that should be implemented in application backends, but to provide a similar for the <em>frontend</em> more wokrs is required, where the frontend action list in this report is minimal. The safety evaluation method also proved to be a part of the action lists in order to be able to test the security even on operational applications. The results of the GDPR survey showed that no specific requirements are set from GDPR, instead the law has the task of raising the priority by making the consequences that can arise from incorrect handling of users’ personal data more serious / costly to the organization.</p>
----------------------------------------------------------------------
title: "Promotion of Physical Activities of NightShift Nurses with Gamification: A Study of Investigating of Physical Activity among Night Shift Nurses and PromotingGamification"
==>    "Promotion of Physical Activities of Night Shift Nurses with Gamification: A Study of Investigating of Physical Activity among Night Shift Nurses and Promoting Gamification"

In diva2:1529843 abstract is: <p>This study shows the investigation of physical activeness among the nurses whowork in night shifts in hospitals and motivates them to do physical activities in theform of gamification in their working place. A qualitative approach is applied forgathering the data in the form of interviewing nurses to inquiry the problems andtried to find out the real opinions and scenarios related with physical inactivenessbased on the nurse’s interpretation. Working-time, working-loads, leisure-time,behaviors towards physical activity, laziness, less knowledge about voluntarymovements are coming out from the findings of this study. This paper suggests someactions such as walking, doing physical exercises, playing games etc. to encouragenurses to do more physical activities in a fun way in the workplace. A fun game,called ‘Healthy steps’ is designed based on the suggested actions in the form ofgamification is presented in this paper to promote physical activity at workplace andto encourage the nurses to participate lo lead a healthy lifestyle.</p>

w='lo' val={'c': 'to', 's': 'diva2:1529843', 'n': 'error in original - the text does not make sense - "lo lead a healthy lifestyle" - I think should be "to participate in living a healthy lifestyle." to match the Swedish'}

corrected abstract:
<p>This study shows the investigation of physical activeness among the nurses who work in night shifts in hospitals and motivates them to do physical activities in the form of gamification in their working place. A qualitative approach is applied for gathering the data in the form of interviewing nurses to inquiry the problems and tried to find out the real opinions and scenarios related with physical inactiveness based on the nurse’s interpretation. Working-time, working-loads, leisure-time, behaviors towards physical activity, laziness, less knowledge about voluntary movements are coming out from the findings of this study. This paper suggests some actions such as walking, doing physical exercises, playing games etc. to encourage nurses to do more physical activities in a fun way in the workplace. A fun game, called ‘Healthy steps’ is designed based on the suggested actions in the form of gamification is presented in this paper to promote physical activity at workplace and to encourage the nurses to participate lo lead a healthy lifestyle.</p>
----------------------------------------------------------------------
In diva2:1142521 abstract is: <p>The goal of this project was to investigate how elution buffer properties affectthe protein elution in the polishing purification step with a cation exchange chromatographyconsisting of POROS XS resin. Hence be able to better predict thebehavior of the resin, two model proteins were used for this investigation (K100and Y102). This project included lab experiments (small scale), and design ofexperiments using MODDE software to evaluate the optimal conditions. Thefactors pH, conductivity, sample load and buffer concentration were investigatedon antibody Y102 while only pH and conductivity were investigated as factorsfor antibody K100. The evaluated responses were the elution volume, aggregateamount and the amount of monomeric antibody (target protein). The results indicatedthat the elution behaviour for POROS XS resin was antibody-dependentand not general for the resin. The responses obtained different models for thetwo antibodies investigated. However, since the aggregate and monomeric proteinfor K100 had curvature indications more experiments should be performedto establish the different behaviour of aggregate and monomer for the respectiveproteins.</p>

corrected abstract:
<p>The goal of this project was to investigate how elution buffer properties affect the protein elution in the polishing purification step with a cation exchange chromatography consisting of POROS XS resin. Hence be able to better predict the behavior of the resin, two model proteins were used for this investigation (K100 and Y102). This project included lab experiments (small scale), and design of experiments using MODDE software to evaluate the optimal conditions. The factors pH, conductivity, sample load and buffer concentration were investigated on antibody Y102 while only pH and conductivity were investigated as factors for antibody K100. The evaluated responses were the elution volume, aggregate amount and the amount of monomeric antibody (target protein). The results indicated that the elution behaviour for POROS XS resin was antibody-dependent and not general for the resin. The responses obtained different models for the two antibodies investigated. However, since the aggregate and monomeric protein for K100 had curvature indications more experiments should be performed to establish the different behaviour of aggregate and monomer for the respective proteins.</p>
----------------------------------------------------------------------
In diva2:648355 abstract is: <p>Full migration to IPv6 brings the need to adjust datacommunication services for the new generationof IP protocols with maintained or expanded functionality. This thesis’ goals is to submitone or more solutions that meets requirements and the technical conditions that enables thecompany DGC:s to expand the service IP-VPN for IPv6. This includes address assignmenttechniques like prefix delegation and automatic address configuration in existing network infrastructure.Solutions are presented in six scenarios that have been investigated considering tests, analysis andexperienced problems. The investigation formed the criteria scalability, configuration complexity,compatibility, support by RFC:s and requirements stated by DGC that adds to the evaluationof the most suitable solution.The evaluation has resulted in a recommended scenario that is implementable according to givengoals.Techniques that may influence the choice of most suitable solution, but that is not yet available,are discussed and presented to point out what may needed to be considered in the future.</p>

w='DGC:s' val={'c': 'DGC', 's': 'diva2:648355', 'n': 'error in original'}
w='RFC:s' val={'c': 'RFCs', 's': 'diva2:648355', 'n': 'error in original'}

corrected abstract:
<p>Full migration to IPv6 brings the need to adjust datacommunication services for the new generation of IP protocols with maintained or expanded functionality. This thesis’ goals is to submit one or more solutions that meets requirements and the technical conditions that enables the company DGC:s to expand the service IP-VPN for IPv6. This includes address assignment techniques like prefix delegation and automatic address configuration in existing network infrastructure.</p><p>Solutions are presented in six scenarios that have been investigated considering tests, analysis and experienced problems. The investigation formed the criteria scalability, configuration complexity, compatibility, support by RFC:s and requirements stated by DGC that adds to the evaluation of the most suitable solution.</p><p>The evaluation has resulted in a recommended scenario that is implementable according to given goals.</p><p>Techniques that may influence the choice of most suitable solution, but that is not yet available, are discussed and presented to point out what may needed to be considered in the future.</p>
----------------------------------------------------------------------
In diva2:1244910 abstract is: <p>Motor intent and control rely on complex high-level and spinal networks. Untilnow, little is known about this system’s organization and mechanisms. Whilecognitive abilities play an essential role in planning movements, learning andmemorizing, their involvement during stereotyped tasks execution, aslocomotion, is still controversial. Recently, the relationship between cognitivefunctions and gait has received increasing attention.Here, a machine learning approach is used to investigate the engagement ofdi↵erent cortical areas during motor activity. In particular, data coming fromthree subjects with implanted electrodes have been analyzed in the frequencydomain to predict their tasks’ state. The choice of intracortical data hasallowed to elude motion artifacts’ presence and exploitation concern. Goodand satisfactory results have been achieved in the case of not highlystereotyped activity. During ambulation, an evidence of an engagement of thebrain has been shown even if with lower classification performances. Moreover,the cortical areas that have emerged in this analysis seem to be in line withthe relative functionality hypothesized in literature.</p>

corrected abstract:
<p>Motor intent and control rely on complex high-level and spinal networks. Until now, little is known about this system’s organization and mechanisms. While cognitive abilities play an essential role in planning movements, learning and memorizing, their involvement during stereotyped tasks execution, as locomotion, is still controversial. Recently, the relationship between cognitive functions and gait has received increasing attention. Here, a machine learning approach is used to investigate the engagement of different cortical areas during motor activity. In particular, data coming from three subjects with implanted electrodes have been analyzed in the frequency domain to predict their tasks’ state. The choice of intracortical data has allowed to elude motion artifacts’ presence and exploitation concern. Good and satisfactory results have been achieved in the case of not highly stereotyped activity. During ambulation, an evidence of an engagement of the brain has been shown even if with lower classification performances. Moreover, the cortical areas that have emerged in this analysis seem to be in line with the relative functionality hypothesized in literature.</p>
----------------------------------------------------------------------
In diva2:1455143 abstract is: <p>Saprolegnia parasitica, a eukaryotic oomycete, is a pathogen that infects freshwater aquaticorganisms by attaching itself to an injured part of the organism and forming mycelium patches onits skin. The mycelium patches severely weaken the epithelial tissue, leading to cellular necrosisand death by haemodilution. Currently, the lack of an effective drug against the infection of thepathogen has resulted in heavy losses of the global annual fish production.</p><p>In the current study, an in silico and in vitro analysis was performed. In silico, the predictedproteomes of S. parasitica, Homo sapiens, and 31 distinct fish species were compared, and thenon-homologous essential genes were extracted. The genes were then matched against drugbank databases to acquire FDA-approved drugs that could inhibit the non-homologous essentialproteins. Out of these, three genes were selected and expressed into four different type ofEscherichia coli (E. coli) competent cells: BL21, C41, C43, and Rosetta 2. Proteins which weresuccessfully expressed were purified and specific assays were designed to check their activities.</p><p>All three proteins were expressed but in low amounts. The identity of all expressed proteins wasalso confirmed by mass spectrometry (MS). Biochemical assay was performed for two of theselected proteins, though, further optimization and inhibitor testing needs to be performed forfurther characterization. Biochemical assay could not be performed for the third selected proteindue to the unavailability of the required substrate.</p>


corrected abstract:
<p><em>Saprolegnia parasitica</em>, a eukaryotic oomycete, is a pathogen that infects freshwater aquatic organisms by attaching itself to an injured part of the organism and forming mycelium patches on its skin. The mycelium patches severely weaken the epithelial tissue, leading to cellular necrosis and death by haemodilution. Currently, the lack of an effective drug against the infection of the pathogen has resulted in heavy losses of the global annual fish production.</p><p>In the current study, an <em>in silico</em> and <em>in vitro</em> analysis was performed. In silico, the predicted proteomes of <em>S. parasitica</em>, <em>Homo sapiens</em>, and 31 distinct fish species were compared, and the non-homologous essential genes were extracted. The genes were then matched against drug bank databases to acquire FDA-approved drugs that could inhibit the non-homologous essential proteins. Out of these, three genes were selected and expressed into four different type of <em>Escherichia coli</em> (<em>E. coli</em>) competent cells: BL21, C41, C43, and Rosetta 2. Proteins which were successfully expressed were purified and specific assays were designed to check their activities.</p><p>All three proteins were expressed but in low amounts. The identity of all expressed proteins was also confirmed by mass spectrometry (MS). Biochemical assay was performed for two of the selected proteins, though, further optimization and inhibitor testing needs to be performed for further characterization. Biochemical assay could not be performed for the third selected protein due to the unavailability of the required substrate.</p>
----------------------------------------------------------------------
In diva2:1664916 abstract is: <p>Collecting and deploying online games made by inexperienced developers can behard. This is something KTH (Royal Institute of Technology) has a problem withpertaining to a course involving SDL and SDL_Net programming. A good solutionto this problem is to host these games on a website. An easy-to-use way of compilingand deploying multiplayer games and game-servers written in C as web applicationsand web servers was needed. Specifically for games written in C using SDL andSDL_net libraries. The compiler toolchain Emscripten was used to compile gameand server code from C to WebAssembly, that could then be used through the generated JavaScript functions. Communication between the client and the server washandled by WebSockets. As much of the Emscripten specific functions were to behidden behind C libraries, emulating the format of SDL_Net. The finished solutionsthat emulated the format of SDL_Net, consisted of two new libraries, one for theserver and the other for the client. The libraries successfully emulated the TCP partsof SDL_Net library. The browsers event scheduler necessitates applications to beable to return control back to it. This meant that the game codes endlessly loopingfunctions had to be rewritten to enable rendering in the browser. </p>

corrected abstract:
<p>Collecting and deploying online games made by inexperienced developers can be hard. This is something KTH (Royal Institute of Technology) has a problem with pertaining to a course involving SDL and SDL_Net programming. A good solution to this problem is to host these games on a website. An easy-to-use way of compiling and deploying multiplayer games and game-servers written in C as web applications and web servers was needed. Specifically for games written in C using SDL and SDL_net libraries. The compiler toolchain Emscripten was used to compile game and server code from C to WebAssembly, that could then be used through the generated JavaScript functions. Communication between the client and the server was handled by WebSockets. As much of the Emscripten specific functions were to be hidden behind C libraries, emulating the format of SDL_Net. The finished solutions that emulated the format of SDL_Net, consisted of two new libraries, one for the server and the other for the client. The libraries successfully emulated the TCP parts of SDL_Net library. The browsers event scheduler necessitates applications to be able to return control back to it. This meant that the game codes endlessly looping functions had to be rewritten to enable rendering in the browser.</p>
----------------------------------------------------------------------
In diva2:1562805 abstract is: <p>Railway interlocking is used for safe control of train traffic. A computer based interlockingsystem ensures that trains can be driven on a railway line by locking train paths and preventingunauthorized combinations of train paths. The connection between the different parts of a yard(objects) and the interlocking system is performed with the help of Object Controllers (OC).The Swedish name for OC is “utdelar”. Track circuits are railway objects used for thedetection of trains on a specific railway section.</p><p>EBILOCK 950 will replace EBILOCK 850 at Östra Station. Through a literature study and asystem analysis, this work will investigate the effects this has on delays in the 75 Hz AC trackcircuit (SASIB) and the audio frequency track circuit DigiCode, together with the delays fromthe Object Controller Systems, JZU840 and OCS950. This was done with the aim of findingout how the cycle time in EBILOCK 950 is affected by the delays from the track circuits andOC. The cycle time is the time it takes for the interlocking system to receive a status updatefrom OC, until the interlocking system sends new commands to be executed by the objects.</p><p>The reason for the additional delays for the track circuits is a consequence of contact bounces.The contact bounces occur when the track circuit relay change position and means that thearmature in the relay does not stop immediately but bounces and the stabilizes. If the relay inthe track circuits can guarantee an absolute change of position more quickly, it is possible toreduce the cycle time and increase the capacity at Östra Station.</p>


corrected abstract:
<p>Railway interlocking is used for safe control of train traffic. A computer based interlocking system ensures that trains can be driven on a railway line by locking train paths and preventing unauthorized combinations of train paths. The connection between the different parts of a yard (objects) and the interlocking system is performed with the help of Object Controllers (OC). The Swedish name for OC is “utdelar”. Track circuits are railway objects used for the detection of trains on a specific railway section.</p><p>EBILOCK 950 will replace EBILOCK 850 at Östra Station. Through a literature study and a system analysis, this work will investigate the effects this has on delays in the 75 Hz AC track circuit (SASIB) and the audio frequency track circuit DigiCode, together with the delays from the Object Controller Systems, JZU840 and OCS950. This was done with the aim of finding out how the cycle time in EBILOCK 950 is affected by the delays from the track circuits and OC. The cycle time is the time it takes for the interlocking system to receive a status update from OC, until the interlocking system sends new commands to be executed by the objects.</p><p>The reason for the additional delays for the track circuits is a consequence of contact bounces. The contact bounces occur when the track circuit relay change position and means that the armature in the relay does not stop immediately but bounces and the stabilizes. If the relay in the track circuits can guarantee an absolute change of position more quickly, it is possible to reduce the cycle time and increase the capacity at Östra Station.</p>
----------------------------------------------------------------------

In diva2:801739 abstract is: <p>The existing seasonal influenza vaccine does not provide broad long-term protection against seasonal influenza and must be remanufactured yearly due to frequens mutations and reassortment of theinfluenza genes. A universal influenza vaccine with the ability to raise long lasting immunity is the focus of several studies, including the Edufluvac project. Edufluvac is based on virus-likeparticles, a modern recombinant platform wellsuited for vaccinatin applications. Redbiotec's rePAX® technology allows the generation of multivalent recombinant baculovirus which generatesvirus-like particles presenting multiple proteins on the surface in insect cell culture. Any effects oninsect cell culture protein expression brought on by the regulatory elements controlling each gene in the baculovirus, or by the genome position of the baculovirus genes, could affect the composition of the virus-like  particles. The ai of this thesis was to elicit a better understanding of the protein expression by analysing multi-protein influenza virus-like particles and virus-like particles encoding a reporter gene regulated by different promoter and terminator combinations. Different bivalent and tetravalent influenza gene bacmids were cloned as well as seven bacmids encoding a YFP gene regulated by different promoter and terminator combinations. Spodopera frugiperda cells weretransfected with the bacmids and harvested recombinant baculovirus was used to perform testexpressions in High-Five™ cells. The resulting protein expression levels from the bivalent andtetravalent recombinant baculovirus were analyzed and compared by Western blots and ELISA assays. The expression of YFP in infected Spodoptera and High-Five™ cells was monitored byfluorescence microscopy and measured with FACS to quantify protein expression differencesbetween the seven promoter and terminator combinations. Analysis of the bivalent constructs indicated that the order of the genes in a recombinant baculovirus does not affect the protein expression in High-Five™ cells. The analysis of the tetravalent constructs revelaed positionalvariations in expressin of the H1 and M1 genes, but the number of test expressions and recombinant baculovirus construct clones included in the analysis were not hogh enough to allow a definitive conclusion. Of the different promoter and terminator constructs highest mean fluorescence intensity was reached with the reference combination. The early promoter yielded mean fluorescent intensitites that were close to the values of the negative control in both cell lines. </p>

w='frequens' val={'c': 'frequent', 's': 'diva2:801739', 'n': 'correct in original'}
w='hogh' val={'c': 'high', 's': 'diva2:801739', 'n': 'correct in the original'}
w='revelaed' val={'c': 'revealed', 's': 'diva2:801739', 'n': 'correct in original'}
w='vaccinatin' val={'c': 'vaccination', 's': 'diva2:801739', 'n': 'correct in original'}
w='intensitites' val={'c': 'intensities', 's': 'diva2:801739'}
w='Spodopera' val={'c': 'Spodoptera', 's': 'diva2:801739', 'n': 'correct in original'}

corrected abstract:
<p>The existing seasonal influenza vaccine does not provide broad long-term protection against seasonal influenza and must be remanufactured yearly due to frequent mutations and reassortment of the influenza genes [1,2]. A universal influenza vaccine with the ability to raise long lasting immunity is the focus of several studies, including the Edufluvac project [3]. Edufluvac is based on virus-like particles, a modern recombinant platform well suited for vaccination applications. Redbiotec’s rePAX® technology allows the generation of multivalent recombinant baculovirus which generates virus-like particles presenting multiple proteins on the surface in insect cell culture. Any effects on insect cell culture protein expression brought on by the regulatory elements controlling each gene in the baculovirus, or by the genome position of the baculovirus genes, could affect the composition of the virus-like particles. The aim of this thesis was to elicit a better understanding of the protein expression by analyzing multi-protein influenza virus-like particles and virus-like particles encoding a reporter gene regulated by different promoter and terminator combinations. Different bivalent and tetravalent influenza gene bacmids were cloned as well as seven bacmids encoding a YFP gene regulated by different promoter and terminator combinations. <em>Spodoptera frugiperda</em> cells were transfected with the bacmids and harvested recombinant baculovirus was used to perform test expressions in High-Five™ cells. The resulting protein expression levels from the bivalent and tetravalent recombinant baculovirus were analyzed and compared by Western blots and ELISA assays. The expression of YFP in infected <em>Spodoptera</em> and High-Five™ cells was monitored by fluorescence microscopy and measured with FACS to quantify protein expression differences between the seven promoter and terminator combinations. Analysis of the bivalent constructs indicated that the order of the genes in a recombinant baculovirus does not affect the protein expression in High-Five™ cells. The analysis of the tetravalent constructs revealed positional variations in expression of the H1 and M1 genes, but the number of test expressions and recombinant baculovirus construct clones included in the analysis were not high enough to allow a definitive conclusion. Of the different promoter and terminator constructs highest mean fluorescence intensity was reached with the reference combination. The early promoter yielded mean fluorescent intensities that were close to the values of the negative control in both cell lines.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-25
======================================================================
In diva2:1451767 abstract is: <p>AbstractThe starting point for this thesis is to determine how well a chatbot can be adapted for emailconversations. If chatbots can be customized for email conversations, the need for humanwork in monotonous and predictable email conversations could be reduced. An existingchatbot framework has been implemented and a dialogue decision system for responseconstruction based on the classified intents has been developed. The whole system is called aconversational unit. The goal of the conversation unit is to achieve a precision value of 0.90,which was achieved with an implementation of Watson assistant. The precision value is usedto measure how well the conversation unit can be adapted for mail conversations. Thedialogue decision system should be able to give different answers depending on what intentsare classified and answers that depend on the existence of other intents should be independentof the order the intents were classified. The developed dialogue decision system achieves theset goals.KeywordsChatbot, NLU, Mailbot, Dialog decision system4</p>


corrected abstract:
<p>The starting point for this thesis is to determine how well a chatbot can be adapted for email conversations. If chatbots can be customized for email conversations, the need for human work in monotonous and predictable email conversations could be reduced. An existing chatbot framework has been implemented and a dialogue decision system for response construction based on the classified intents has been developed. The whole system is called a conversational unit. The goal of the conversation unit is to achieve a precision value of 0.90, which was achieved with an implementation of Watson assistant. The precision value is used to measure how well the conversation unit can be adapted for mail conversations. The dialogue decision system should be able to give different answers depending on what intents are classified and answers that depend on the existence of other intents should be independent of the order the intents were classified. The developed dialogue decision system achieves the set goals.</p>
----------------------------------------------------------------------
In diva2:1454819 abstract is: <p>In this project, we investigate the functional role of rare variants in common diseasecontext. The field of common disease genomics has been stalled mainly due to theslow pace of translating genetic variant information into clinical care. In particular,the role of rare variants in modulating common disease risk is not well explored.</p><p>Using Capture Hi-C, Sahlén Lab has previously mapped promoter-enhancerinteractions of aortic endothelial cells taken from six individuals. This enabled theisolation of private variants that overlapped with private interactions for eachindividual. Here, we have cloned eight enhancer sequences containing such rarevariants into pGL3 reporter vectors and transfected them into a human cell line. Wethen performed a luciferase assay to measure the effect size of the chosen variants.</p><p>Our results display the numerous steps necessary to prepare genomic DNA regionsfor a luciferase assay. The assay showed that 2 alternative alleles, rs185055141 andrs147010901, altered luciferase expression and therefore may have an effect on generegulation.</p><p>This study will hopefully serve as a guide for future research efforts in evaluating theeffect of rare variants in enhancer sequences. It is very relevant in today’s society toincrease our understanding of the genetic landscape of complex diseases.</p>


corrected abstract:
<p>In this project, we investigate the functional role of rare variants in common disease context. The field of common disease genomics has been stalled mainly due to the slow pace of translating genetic variant information into clinical care. In particular, the role of rare variants in modulating common disease risk is not well explored.</p><p>Using Capture Hi-C, Sahlén Lab has previously mapped promoter-enhancer interactions of aortic endothelial cells taken from six individuals. This enabled the isolation of private variants that overlapped with private interactions for each individual. Here, we have cloned eight enhancer sequences containing such rare variants into pGL3 reporter vectors and transfected them into a human cell line. We then performed a luciferase assay to measure the effect size of the chosen variants.</p><p>Our results display the numerous steps necessary to prepare genomic DNA regions for a luciferase assay. The assay showed that 2 alternative alleles, rs185055141 and rs147010901, altered luciferase expression and therefore may have an effect on gene regulation.</p><p>This study will hopefully serve as a guide for future research efforts in evaluating the effect of rare variants in enhancer sequences. It is very relevant in today’s society to increase our understanding of the genetic landscape of complex diseases.</p>
----------------------------------------------------------------------
In diva2:1889754 abstract is: <p>At SHL Medical in Nacka, Sweden, autoinjectors are developed and during their developmentstage, the autoinjectors are tested on a soft tissue model made of gel. The gel however haschallenges with robustness and customization. To eliminate these challenges, a soft tissuemodel of silicone was created. The aim of this project was to determine whether the twomaterials could generate the same results and, if therefore, the gel model was replaceable.</p><p>Four different autoinjector models were chosen to be tested on four different silicone modelswith different geometry and stiffness. This was done to obtain a broad observation of how thesilicone performed during testing. All autoinjectors were tested on all different siliconemodels and the gel model to compare their results.</p><p>We could conclude that the silicone model generates similar results to the gel model and thatthe gel model can be replaced by the silicone model. Additionally, it was concluded whichstiffness and geometry of the silicone model generates the most similar results to the gelmodel. Apart from this, the project gave guidance of how the silicone model can further bedeveloped to better match the gel model. </p>


corrected abstract:
<p>At SHL Medical in Nacka, Sweden, autoinjectors are developed and during their development stage, the autoinjectors are tested on a soft tissue model made of gel. The gel however has challenges with robustness and customization. To eliminate these challenges, a soft tissue model of silicone was created. The aim of this project was to determine whether the two materials could generate the same results and, if therefore, the gel model was replaceable.</p><p>Four different autoinjector models were chosen to be tested on four different silicone models with different geometry and stiffness. This was done to obtain a broad observation of how the silicone performed during testing. All autoinjectors were tested on all different silicone models and the gel model to compare their results.</p><p>We could conclude that the silicone model generates similar results to the gel model and that the gel model can be replaced by the silicone model. Additionally, it was concluded which stiffness and geometry of the silicone model generates the most similar results to the gel model. Apart from this, the project gave guidance of how the silicone model can further be developed to better match the gel model.</p>
----------------------------------------------------------------------
In diva2:1665259 abstract is: <p>The application of instruments in the practice of explosive sports, to measure an athlete's force production, can be used to determine how well an athlete performs. Present instruments on the marketare most often applied to sprint starts and function to capture an athlete on film, and link the eventto the data retrieved from the instrument. The disadvantages with present day systems are that theyare either too expensive or lack flexibility and portability as they are usually delivered with a lot ofperipherals.The purpose of this thesis was to develop a system of a mobile application and logical code to a microcontroller that register horizontal forces at sprint starts from an analog source. The collected datawas calculated and transmitted using BLE communication to the mobile application which presentsthe force data to the user.Moreover, the purpose was also to have the acquired force data time-synchronized with a mobile device to make it possible to evaluate events against external sources such as IMU and high-speed film.The result for the logic developed throughout this work demonstrate that it is possible to retrievesampled force data from a microcontroller via BLE communication. It was possible to present thecalculated force data visually to an end user with a mobile application and have the events time synchronized using time synchronization algorithms. However, the results can be further improved bydevelopment of the system.</p>


corrected abstract:
<p>The application of instruments in the practice of explosive sports, to measure an athlete's force production, can be used to determine how well an athlete performs. Present instruments on the market are most often applied to sprint starts and function to capture an athlete on film, and link the event to the data retrieved from the instrument. The disadvantages with present day systems are that they are either too expensive or lack flexibility and portability as they are usually delivered with a lot of peripherals.</p><p>The purpose of this thesis was to develop a system of a mobile application and logical code to a microcontroller that register horizontal forces at sprint starts from an analog source. The collected data was calculated and transmitted using BLE communication to the mobile application which presents the force data to the user.</p><p>Moreover, the purpose was also to have the acquired force data time-synchronized with a mobile device to make it possible to evaluate events against external sources such as IMU and high-speed film.</p><p>The result for the logic developed throughout this work demonstrate that it is possible to retrieve sampled force data from a microcontroller via BLE communication. It was possible to present the calculated force data visually to an end user with a mobile application and have the events time synchronized using time synchronization algorithms. However, the results can be further improved by development of the system.</p>
----------------------------------------------------------------------
In diva2:1520078 abstract is: <p>Due to increase use of plastics, bio-based polymers as packaging materials have garneredmuch attention in recent years due to environmental concerns. Several protein materials, e.g.wheat gluten, have been in focus for significant research towards new biobased plastics andresults are promising. Bio-based and environmentally friendly plastics gather much interestand attention today due to the green-house generating effects of conventional petroleum-basedplastics.</p><p>A protein material was investigated here for its plastic material properties. The protein puritywas ca. 65 %. The films were created by first grinding the protein flakes to a fine powder andmixing it with glycerol. The glycerol content was 30 %. The material was subsequently hotpressed.To test the plastic, multiple techniques and methods were used. TGA, DSC, FT-IR, WVTR,OTR and tensile testing. In general, the material was relatively weak. As most protein plasticsit had also poor water barrier properties, however it had relatively good oxygen-barrierproperties.</p><p>In conclusion it is a material that could have a bright future as it is made from biomass insteadof petroleum, which means that it is more environmentally friendly. With modification andimprovement, it can be a good plastic for several applications in future.</p>


corrected abstract:
<p>Due to increase use of plastics, bio-based polymers as packaging materials have garnered much attention in recent years due to environmental concerns. Several protein materials, e.g. wheat gluten, have been in focus for significant research towards new biobased plastics and results are promising. Bio-based and environmentally friendly plastics gather much interest and attention today due to the green-house generating effects of conventional petroleum-based plastics.</p><p>A protein material was investigated here for its plastic material properties. The protein purity was ca. 65 %. The films were created by first grinding the protein flakes to a fine powder and mixing it with glycerol. The glycerol content was 30 %. The material was subsequently hot-pressed.</p><p>To test the plastic, multiple techniques and methods were used. TGA, DSC, FT-IR, WVTR, OTR and tensile testing. In general, the material was relatively weak. As most protein plastics it had also poor water barrier properties, however it had relatively good oxygen-barrier properties.</p><p>In conclusion it is a material that could have a bright future as it is made from biomass instead of petroleum, which means that it is more environmentally friendly. With modification and improvement, it can be a good plastic for several applications in future.</p>
----------------------------------------------------------------------
In diva2:1322258 abstract is: <p>Following circumstances such as mergers and acquisitions, the IT systemsassociated with the participating organisations may need to share access towardsservices and systems with eachother. Access towards systems and services is oftencontrolled using secret information such as passwords or keys. This implies thatsharing access between IT systems is achieved by sharing secret information.</p><p>This thesis proposes new methods for automatic synchronization of secretsbetween different secret management systems that may not be natively compatiblewith one another. After examining how the already existing secret managementsystems function as well as created a data centric threat model, a system design wasproposed. A secret proxy connects to each secret management system which in turnconnects to a central secret distributor that handles and updates the other proxies.</p><p>The results indicate that such a system can be implemented and securely distributesecrets automatically. By synchronizing secrets automatically, the work involvedwith supporting several secret management systems in parallel which all needaccess to some common secrets could be reduced.</p>


In the actual abstract "eachother" is set as one word.

corrected abstract:
<p>Following circumstances such as mergers and acquisitions, the IT systems associated with the participating organisations may need to share access towards services and systems with eachother. Access towards systems and services is often controlled using secret information such as passwords or keys. This implies that sharing access between IT systems is achieved by sharing secret information.</p><p>This thesis proposes new methods for automatic synchronization of secrets between different secret management systems that may not be natively compatible with one another. After examining how the already existing secret management systems function as well as created a data centric threat model, a system design was proposed. A secret proxy connects to each secret management system which in turn connects to a central secret distributor that handles and updates the other proxies.</p><p>The results indicate that such a system can be implemented and securely distribute secrets automatically. By synchronizing secrets automatically, the work involved with supporting several secret management systems in parallel which all need access to some common secrets could be reduced.</p>
----------------------------------------------------------------------
In diva2:1260917 abstract is: <p>CHO (Chinese hamster ovary) cells have become the workhorse for industrial production of biologics.Deciphering the internal metabolism of these cells will be a valuable information to design new medium and achieve advanced process control. In this study, the observations pertaining toamino acid metabolism of CHO cells in response to varied amino acid balance in the medium wasobserved and recorded. Pseudo perfusion method formed the basis in conducting the experimentsto determine the uptake and production rates of all amino acids in different culture medium together with other important cell culture metabolites like glucose, ammonia, lactate, product titer,biomass and viability, etc. Cells showed different metabolic response to different tested balanceof amino acids. The response generated were recorded for using it with mathematical tools infuture. The above objective was achieved by conducting a series of 3 experiments. Experiment 01;to determine how low a commercial medium can be diluted to still have acceptable growth and performance in terms of productivity, viability, etc. Experiment 02; to record response of cells tovaried amino acid balance in the medium. This experiment was later redesigned to amplify themetabolic response seen in the experiment. Conducting a similar experiment at higher cell density was thought to help us see amplified metabolic response and this was done as Experiment 03. Theresults of this experiment in terms of amino acid uptake and production rates is yet to be performedto see if we obtained an amplified response.</p>


corrected abstract:
<p>CHO (Chinese hamster ovary) cells have become the workhorse for industrial production of biologics. Deciphering the internal metabolism of these cells will be a valuable information to design new medium and achieve advanced process control. In this study, the observations pertaining to amino acid metabolism of CHO cells in response to varied amino acid balance in the medium was observed and recorded. Pseudo perfusion method formed the basis in conducting the experiments to determine the uptake and production rates of all amino acids in different culture medium to gether with other important cell culture metabolites like glucose, ammonia, lactate, product titer, biomass and viability, etc. Cells showed different metabolic response to different tested balance of amino acids. The response generated were recorded for using it with mathematical tools in future. The above objective was achieved by conducting a series of 3 experiments. Experiment 01;to determine how low a commercial medium can be diluted to still have acceptable growth and performance in terms of productivity, viability, etc. Experiment 02; to record response of cells to varied amino acid balance in the medium. This experiment was later redesigned to amplify the metabolic response seen in the experiment. Conducting a similar experiment at higher cell density was thought to help us see amplified metabolic response and this was done as Experiment 03. The results of this experiment in terms of amino acid uptake and production rates is yet to be performed to see if we obtained an amplified response.</p>
----------------------------------------------------------------------
In diva2:1775282 abstract is: <p>Around 17% of the population in Stockholm suffers from speech or voice disorders. In case ofsuch disorders, it is common to visit a speech therapist for medical examination. During theexamination, the speech therapist uses VoiceJournal that is developed by NeoviusSignalsystem och Data AB, which includes different programs for voice recording andanalysis. Although the programs have been operating efficiently, there are areas where thatcould be improved. Prior to a new release of VoiceJournal, the company wanted to get rid ofan annoyance in the form of a dialogue box that should not occur. Additionally, the possibilityto play the recording by remote playback would be further examined given that it did notfunctioned properly. Lastly, a new feature would be implemented that enables the speechtherapist to undo various actions made to the file. This was done by modifying and addingcode. Out of these three programming tasks, the first two were finalized while the third onewas completed to some extent since all the various actions on the recording could not beundone one or multiple times.</p>

corrected abstract:
<p>Around 17% of the population in Stockholm suffers from speech or voice disorders. In case of such disorders, it is common to visit a speech therapist for medical examination. During the examination, the speech therapist uses VoiceJournal that is developed by Neovius Signalsystem och Data AB, which includes different programs for voice recording and analysis. Although the programs have been operating efficiently, there are areas where that could be improved. Prior to a new release of VoiceJournal, the company wanted to get rid of an annoyance in the form of a dialogue box that should not occur. Additionally, the possibility to play the recording by remote playback would be further examined given that it did not functioned properly. Lastly, a new feature would be implemented that enables the speech therapist to undo various actions made to the file. This was done by modifying and adding code. Out of these three programming tasks, the first two were finalized while the third one was completed to some extent since all the various actions on the recording could not be undone one or multiple times.</p>
----------------------------------------------------------------------
In diva2:842556 abstract is: <p>Hearing-impaired individuals that use hearing aids often experience problemswhen exposed to daily situations in life. The di↵erent environments include forexample classrooms, offices and public areas. For the hearing impaired, the correctadjustments of the hearing aids are of great importance. For these settings to beproper, a measurement called the Speech Intelligibility Index (SII), is questionedto be implemented in the clinical hearing care. To answer this question of issuewhether feasibility lay in the implementation of SII in the clinical hearing carewith the interface of hearing aids, a literature study ordered by the Hearing- &amp;Balance Clinic (H&amp;B) at Karolinska University Hospital situated at RosenlundHospital, was initiated. The focus of the study was primarily held in concern ofSII as a hearing aid validation measurement and later extended to SII as a roomacoustic measurement disregarding the e↵ect of hearing aids. The result of thestudy showed that the use of SII in the clinical hearing care is troubled and furtherthat the implementation as a room acoustic measurement is probable but has tobe additionally investigated.</p>

Note severl mishandled ligatures.

corrected abstract:
<p>Hearing-impaired individuals that use hearing aids often experience problems when exposed to daily situations in life. The different environments include for example classrooms, offices and public areas. For the hearing impaired, the correct adjustments of the hearing aids are of great importance. For these settings to be proper, a measurement called the Speech Intelligibility Index (SII), is questioned to be implemented in the clinical hearing care. To answer this question of issue whether feasibility lay in the implementation of SII in the clinical hearing care with the interface of hearing aids, a literature study ordered by the Hearing- &amp; Balance Clinic (H&amp;B) at Karolinska University Hospital situated at Rosenlund Hospital, was initiated. The focus of the study was primarily held in concern of SII as a hearing aid validation measurement and later extended to SII as a room acoustic measurement disregarding the effect of hearing aids. The result of the study showed that the use of SII in the clinical hearing care is troubled and further that the implementation as a room acoustic measurement is probable but has to be additionally investigated.</p>
----------------------------------------------------------------------
In diva2:744710 abstract is: <p>Motivation: The advent of clinical exome sequencing will require new tools to handlecoverage data and making it relevant to clinicians. That means genes over targets, smartsoftware over BED-files, and full stack, automated solutions from BAM-files to genetic testreport. Fresh ideas can also provide new insights into the factors that cause certain regionsof the exome to receive poor coverage.Results: A novel coverage analysis tool for analyzing clinical exome sequencing data has beendeveloped. Named Chanjo, it’s capable of converting between different elements such astargets and exons, supports custom annotations, and provides powerful statistics andplotting options. A coverage investigation using Chanjo linked both extreme GC content andlow sequence complexity to poor coverage. High bait density was shown to increasereliability of exome capture but not improve coverage of regions that had already proventricky. To improve coverage of especially very G+C rich regions, developing new ways toamplify rather than enrich DNA will likely make the biggest difference.</p>


corrected abstract:
<p><strong>Motivation</strong>: The advent of clinical exome sequencing will require new tools to handle coverage data and making it relevant to clinicians. That means genes over targets, smart software over BED-files, and full stack, automated solutions from BAM-files to genetic test report. Fresh ideas can also provide new insights into the factors that cause certain regions of the exome to receive poor coverage.</p><p><strong>Results</strong>: A novel coverage analysis tool for analyzing clinical exome sequencing data has been developed. Named Chanjo, it’s capable of converting between different elements such as targets and exons, supports custom annotations, and provides powerful statistics and plotting options. A coverage investigation using Chanjo linked both extreme GC content and low sequence complexity to poor coverage. High bait density was shown to increase reliability of exome capture but not improve coverage of regions that had already proven tricky. To improve coverage of especially very G+C rich regions, developing new ways to amplify rather than enrich DNA will likely make the biggest difference.</p><p><strong>Availability</strong>: The source code for Chanjo is freely available at <a href="https://github.com/robinandeer/chanjo">https://github.com/robinandeer/chanjo</a>.</p>
----------------------------------------------------------------------
In diva2:1353473 abstract is: <p>In this thesis the solar-powered lightning are investigated as lightning options forwalk and cycle paths in countryside. Rarely used walk and bicycle paths in thecountryside are left without lighting by municipalities and roads associations.Therefore, it is desirable to offer a lightning technology that has zero electricitycosts and no cable routing.</p><p>The study’s hypothesis was that solar-powered lightning installations can be recommendedas street lightning for walk and cycle paths in the countryside. resultsshow that solar radiation in Sweden is low which makes it difficult for Sweden’smunicipalities and associations willing to invest. To increase energy savings insolar lighting system, advanced presence-controlled technology is needed.</p><p>The results show that presence-controlled solar lightning can provide an economicalsaving of 58 % compared to a LED network connected lightning. The savingvary depending on the type of solar lightning used. Solar LED street lighting technologyalso reduces carbon dioxide emissions.</p><p></p>


corrected abstract:
<p>In this thesis the solar-powered lightning are investigated as lightning options for walk and cycle paths in countryside. Rarely used walk and bicycle paths in the countryside are left without lighting by municipalities and roads associations. Therefore, it is desirable to offer a lightning technology that has zero electricity costs and no cable routing.</p><p>The study’s hypothesis was that solar-powered lightning installations can be recommended as street lightning for walk and cycle paths in the countryside. results show that solar radiation in Sweden is low which makes it difficult for Sweden’s municipalities and associations willing to invest. To increase energy savings in solar lighting system, advanced presence-controlled technology is needed.</p><p>The results show that presence-controlled solar lightning can provide an economical saving of 58 % compared to a LED network connected lightning. The saving vary depending on the type of solar lightning used. Solar LED street lighting technology also reduces carbon dioxide emissions.</p>
----------------------------------------------------------------------
In diva2:1095512 the thesis has a rather unique "Graphical abstract" - it would be interesting to know how to include this in the DiVA abstract metadata.


In diva2:1095512 abstract is: <p>The task of medicinal chemists in a drug discoveryproject is to synthesize/design analogues to the screening hits, simultaneouslyincreasing target potency and optimizing the pharmacological properties.  This requires a wide selection of moleculesto be synthesized, where both synthetic feasibility and price of startingmaterials are of great importance. In this work, a synthetic pathway from cheapand readily available starting materials to highly modifiable 2,4-disubstitutedpyrrolidines is demonstrated. Previously reported procedures to similarpyrrolidines use expensive catalysts, requires harsh conditions and requiresnon-commercially available starting materials. The suggested pathway herein has demonstrated great possibility forvariation in the 4-position, including fluoro, difluoro, nitrile and alcoholfunctional groups. There are several areas in which the synthesis can beimproved and expanded upon. Improvements can be made by optimizing thedescribed reaction conditions and further expansion of possible modificationsin both 2- and 4-position could be explored.</p>

corrected abstract:
<p>The task of medicinal chemists in a drug discovery project is to synthesize/design analogues to the screening hits, simultaneously increasing target potency and optimizing the pharmacological properties. This requires a wide selection of molecules to be synthesized, where both synthetic feasibility and price of starting materials are of great importance. In this work, a synthetic pathway from cheap and readily available starting materials to highly modifiable 2,4-disubstituted pyrrolidines is demonstrated. Previously reported procedures to similar pyrrolidines use expensive catalysts, requires harsh conditions and requires non-commercially available starting materials. The suggested pathway herein has demonstrated great possibility for variation in the 4-position, including fluoro, difluoro, nitrile and alcohol functional groups. There are several areas in which the synthesis can be improved and expanded upon. Improvements can be made by optimizing the described reaction conditions and further expansion of possible modifications in both 2- and 4-position could be explored.</p>
----------------------------------------------------------------------
In diva2:1454862 abstract is: <p>Nanobodies, or VHHs, are derived from heavy chain-only antibodies (hcAbs) found in camelids.They overcome some of the inherent limitations of monoclonal antibodies (mAbs) and derivativesthereof, due to their smaller molecular size and higher stability, and thus present an alternative tomAbs therapeutically. Two nanobodies, Nb23 and Nb24, have been shown to similarly inhibit theself-aggregation of a very amyloidogenic variant of β2-microglobulin, D76N β2-m, which hasimplications in hereditary systemic amyloidosis. Here, the structure of Nb23 was modeled withthe Chemical Shift (CS)-Rosetta server using chemical shift assignments from nuclear magneticresonance (NMR) spectroscopy experiments, and partially validated using the already determinedstructure of Nb24, NMR chemical shifts, circular dichroism (CD), and nuclear Overhauser effect(NOE) spectroscopy. Further, the binding properties of Nb23 and Nb24 with the antigen wereinvestigated with isothermal titration calorimetry (ITC), showing that Nb23 has slightly loweraffinity to the antigen than Nb24. The structural analysis indicated that Nb23 has a much moredynamic CDR3 loop with respect to Nb24, which could be a contributing factor to its loweraffinity.</p>

corrected abstract:
<p>Nanobodies, or VHHs, are derived from heavy chain-only antibodies (hcAbs) found in camelids. They overcome some of the inherent limitations of monoclonal antibodies (mAbs) and derivatives thereof, due to their smaller molecular size and higher stability, and thus present an alternative to mAbs therapeutically. Two nanobodies, Nb23 and Nb24, have been shown to similarly inhibit the self-aggregation of a very amyloidogenic variant of β2-microglobulin, D76N β2-m, which has implications in hereditary systemic amyloidosis. Here, the structure of Nb23 was modeled with the Chemical Shift (CS)-Rosetta server using chemical shift assignments from nuclear magnetic resonance (NMR) spectroscopy experiments, and partially validated using the already determined structure of Nb24, NMR chemical shifts, circular dichroism (CD), and nuclear Overhauser effect (NOE) spectroscopy. Further, the binding properties of Nb23 and Nb24 with the antigen were investigated with isothermal titration calorimetry (ITC), showing that Nb23 has slightly lower affinity to the antigen than Nb24. The structural analysis indicated that Nb23 has a much more dynamic CDR3 loop with respect to Nb24, which could be a contributing factor to its lower affinity.</p>
----------------------------------------------------------------------
In diva2:1272665 abstract is: <p>Igelbäcken is a 10 kilometer long watercourse in northwestern Stockholm, starting in Säbysjön,Järfälla and passing Sollentuna, Sundbyberg, Stockholm and Solna where it flows into Edsviken,which is the outlet. The water passes industries, buildings and roads. The aim of this work is toanalyze how Igelbäcken is affected by the surroundings. The work contains a field study andlaboratory work.</p><p>The results show that the water is contaminated. pH, temperature, conductivity and flow weremeasured in the field and COD, alkalinity, chlorine, total phosphorus, color, odour, suspension ofcolloidal particles and precipitate were analyzed in the laboratory. The result shows some veryhigh values. The conductivity and chloride are high at all places, indicating that the water isaffected by roads. The COD is above the acceptable value except in two places. Alkalinity variesbetween 2 mM and 4 mM except in two places. The phosphorus content is also high. Based on theresults, more efforts are required to make Igelbäcken cleaner.</p>


corrected abstract:
<p>Igelbäcken is a 10 kilometer long watercourse in northwestern Stockholm, starting in Säbysjön, Järfälla and passing Sollentuna, Sundbyberg, Stockholm and Solna where it flows into Edsviken, which is the outlet. The water passes industries, buildings and roads. The aim of this work is to analyze how Igelbäcken is affected by the surroundings. The work contains a field study and laboratory work.</p><p>The results show that the water is contaminated. pH, temperature, conductivity and flow were measured in the field and COD, alkalinity, chlorine, total phosphorus, color, odour, suspension of colloidal particles and precipitate were analyzed in the laboratory. The result shows some very high values. The conductivity and chloride are high at all places, indicating that the water is affected by roads. The COD is above the acceptable value except in two places. Alkalinity varies between 2 mM and 4 mM except in two places. The phosphorus content is also high. Based on the results, more efforts are required to make Igelbäcken cleaner.</p>
----------------------------------------------------------------------
In diva2:1449212 abstract is: <p>Medical image analysis is both time consuming and requires expertise. In thisreport, a 2.5D version of the U-net convolution network adapted for automatedkidney segmentation is further developed. Convolution neural networks havepreviously shown expert level performance in image segmentation. Training datafor the network was created by manually segmenting MRI images of kidneys.The 2.5D U-Net network was trained with 64 kidney segmentations fromprevious work. Volume analysis on the network’s kidney segmentation proposalsof 38,000 patients showed that the ammount of segmented voxels that are notpart of the kidneys was 0.35%. After the addition of 56 of our segmentations, itdecreased to just 0.11%, indicating a reduction of about 68%. This is a majorimprovement of the network and an important step towards the development ofpractical applications of automated segmentation.</p>

w='ammount' val={'c': 'amount', 's': 'diva2:1449212', 'n': 'error in original'}

corrected abstract:
<p>Medical image analysis is both time consuming and requires expertise. In this report, a 2.5D version of the U-net convolution network adapted for automated kidney segmentation is further developed. Convolution neural networks have previously shown expert level performance in image segmentation. Training data for the network was created by manually segmenting MRI images of kidneys. The 2.5D U-Net network was trained with 64 kidney segmentations from previous work. Volume analysis on the network’s kidney segmentation proposals of 38,000 patients showed that the ammount of segmented voxels that are not part of the kidneys was 0.35%. After the addition of 56 of our segmentations, it decreased to just 0.11%, indicating a reduction of about 68%. This is a major improvement of the network and an important step towards the development of practical applications of automated segmentation.</p>
----------------------------------------------------------------------
In diva2:908846 abstract is: <p>Each year million of babies are born pre-term, some of these pre-term births occur due to the motherhaving a too soft cervix which can not withstand the forces the baby exposes it to. The aim of thisstudy was to implement and evaluate a programmable shear wave elastography ultrasound system forcervical applications and investigate the optimal settings of shear wave elastography push voltage andshear wave elastography push focus depth. Shear wave elastography is an ultrasound based imagingmodality aiming to evaluate the tissue elasticity by using acoustic radiation forces to induce shear waves.The propagation of the shear waves through the tissue is then tracked in order to calculate the shearwave velocity which is related to the tissue elasticity. B-mode imaging, pushing sequence and planewave imaging have been implemented and measurements have been conducted on four cervical polyvinylalcohol phantoms. The acquired data has been post-processed using Loupas 2D-autocorrector to gainthe axial displacement and enabling tracking of the shear waves to allow evaluation and optimizationof the implemented method. The implemented shear wave technique showed to be able to distinguishcervical phantoms of dierent elasticity and a high pushing voltage and shallow focus push depth havebeen found to produce the most reliable results.</p>

corrected abstract:
<p>Each year million of babies are born pre-term, some of these pre-term births occur due to the mother having a too soft cervix which can not withstand the forces the baby exposes it to. The aim of this study was to implement and evaluate a programmable shear wave elastography ultrasound system for cervical applications and investigate the optimal settings of shear wave elastography push voltage and shear wave elastography push focus depth. Shear wave elastography is an ultrasound based imaging modality aiming to evaluate the tissue elasticity by using acoustic radiation forces to induce shear waves. The propagation of the shear waves through the tissue is then tracked in order to calculate the shear wave velocity which is related to the tissue elasticity. B-mode imaging, pushing sequence and plane wave imaging have been implemented and measurements have been conducted on four cervical polyvinyl alcohol phantoms. The acquired data has been post-processed using Loupas 2D-autocorrector to gain the axial displacement and enabling tracking of the shear waves to allow evaluation and optimization of the implemented method. The implemented shear wave technique showed to be able to distinguish cervical phantoms of different elasticity and a high pushing voltage and shallow focus push depth have been found to produce the most reliable results.</p>
----------------------------------------------------------------------
In diva2:1229241 abstract is: <p>Aliphatic polyesters such as poly(lactide) (PLA), poly(glycolide) (PGA), poly(Ɛ-caprolactone)(PCL) and their copolymers have gained an intense interest for their implementation for tissueregeneration and medical applications because of their unique properties such asbiocompatibility, degradability and easy processing. This master thesis was focused on thefabrication of degradable porous scaffolds for soft tissue regeneration. For this purpose, a twostepprocess was optimized using PLA and PCL: melt-extrusion following 3D printing.In the melt-extrusion optimization, filaments with a specific diameter were fabricated by alab extruder and in-house made winder. To achieve this, extrusion parameters wereadjusted according to the thermal behavior of the polymer. Filaments with a regular diameter forfeeding a commercial 3D printer were produced using PLA and PCL. The printability wasassessed using the produced filaments as well as commercially available filaments. Twodifferent scaffold designs were 3D printed to check how the mechanical properties can be varied.Molecular weight, mechanical and thermal properties were respectively characterized bysize exclusion chromatography (SEC), tensile tests, differential scanning calorimetry (DSC)and thermogravimetric analysis (TGA) for the filaments and scaffoldsfabricated.</p>


corrected abstract:
<p>Aliphatic polyesters such as poly(lactide) (PLA), poly(glycolide) (PGA), poly(Ɛ-caprolactone)(PCL) and their copolymers have gained an intense interest for their implementation for tissue regeneration and medical applications because of their unique properties such as biocompatibility, degradability and easy processing. This master thesis was focused on the fabrication of degradable porous scaffolds for soft tissue regeneration. For this purpose, a two step process was optimized using PLA and PCL: melt-extrusion following 3D printing. In the melt-extrusion optimization, filaments with a specific diameter were fabricated by a lab extruder and in-house made winder. To achieve this, extrusion parameters were adjusted according to the thermal behavior of the polymer. Filaments with a regular diameter for feeding a commercial 3D printer were produced using PLA and PCL. The printability was assessed using the produced filaments as well as commercially available filaments. Two different scaffold designs were 3D printed to check how the mechanical properties can be varied. Molecular weight, mechanical and thermal properties were respectively characterized by size exclusion chromatography (SEC), tensile tests, differential scanning calorimetry (DSC) and thermogravimetric analysis (TGA) for the filaments and scaffolds fabricated.</p>
----------------------------------------------------------------------
In diva2:1045581 abstract is: <p>Strawberry plants (Fragaria x ananassa) and Scots pine (Pinus sylvestris) represent species, withinagriculture and forestry respectively, that are traditionally protected by utilization of pesticidesincluding neurotoxic insecticides. More environmentally friendly protection strategies are thereforehighly desirable. Treating plants with specific metabolites naturally occurring in their tissues might alterepigenetic mechanisms, which in turn may strengthen plants self-defense against diseases and weevilattacks. F. x ananassa and P. sylvestris seeds were treated with 2,5 mM nicotinamide and 2,5 mMnicotinic acid in order to investigate possible epigenetical effects by analyzing changes in the level ofthe DNA methylation. The epigenetic changes, for both plants, were analyzed on the global DNA level.Reduction in the DNA methylation level in strawberry leaves as well as the DNA methylation increase inpine needles were observed by means of LUMA-analysis when HpaII restriction enzyme was used in theanalysis. Further investigation is required in order to understand if NIC and NIA may have a significantimpact on pathogen attack in strawberry plants and Scots pine. More research may also unveil ifnicotinamide and nicotinic acid can play a potential role in more sustainable defense strategies ofplants.</p>


corrected abstract:
<p>Strawberry plants (<em>Fragaria x ananassa</em>) and Scots pine (<em>Pinus sylvestris</em>) represent species, within agriculture and forestry respectively, that are traditionally protected by utilization of pesticides including neurotoxic insecticides. More environmentally friendly protection strategies are therefore highly desirable. Treating plants with specific metabolites naturally occurring in their tissues might alter epigenetic mechanisms, which in turn may strengthen plants self-defense against diseases and weevil attacks. <em>F. x ananassa</em> and <em>P. sylvestris</em> seeds were treated with 2,5 mM nicotin amide and 2,5 mM nicotinic acid in order to investigate possible epigenetical effects by analyzing changes in the level of the DNA methylation. The epigenetic changes, for both plants, were analyzed on the global DNA level. Reduction in the DNA methylation level in strawberry leaves as well as the DNA methylation increase in pine needles were observed by means of LUMA-analysis when HpaII restriction enzyme was used in the analysis. Further investigation is required in order to understand if NIC and NIA may have a significant impact on pathogen attack in strawberry plants and Scots pine. More research may also unveil if nicotinamide and nicotinic acid can play a potential role in more sustainable defense strategies of plants.</p>
----------------------------------------------------------------------
In diva2:1331767 abstract is: <p>The aim of the project was to compare the workflow for an Elekta Linac with and without the surfacescanning system Catalyst and describe pros and cons with both workflows. The findings in the reportcan be used as decision support in development of Elekta products and workflow improvements.</p><p>The method for the project was to do interviews, observations and time measurements at Södersjukhuset(not using Catalyst) and Sundsvalls sjukhus (using Catalyst). The workflows were graded in an as-sessment protocol covering time efficiency, comfort, noise, resources, reliability, cost, dosage and sideeffects. Different workflow scenarios were simulated in AnyLogic.</p><p>The result of the project was that, according to our protocol, the workflow with Catalyst was ratedhigher than without it. The simulations in Anylogic showed that minimizing gaps in the treatment sched-ule generated the same number of patients treated per day, if the positioning could not be done faster.The simulations also showed that removing position verification with cone beam computer tomography(CBCT), an imaging system which is used in addition to the Catalyst system, would increase the numberof treated patients with approximately 33%.</p><p>The conclusion was that there were no great differences in time efficiency between the workflows. How-ever, considering the higher reliability and comfort for the patient, optical surface scanning can improvethe positioning for Elekta Linac and is therefore worth implementing. Minimizing treatment gaps wouldnot improve the workflow. Removing the use of CBCT would increase the number of treated patientsper day.</p>

w='as-sessment' val={'c': 'as-sessment', 's': 'diva2:1331767'}
w='How-ever' val={'c': 'However', 's': 'diva2:1331767'}

corrected abstract:
<p>The aim of the project was to compare the workflow for an Elekta Linac with and without the surface scanning system Catalyst and describe pros and cons with both workflows. The findings in the report can be used as decision support in development of Elekta products and workflow improvements.</p><p>The method for the project was to do interviews, observations and time measurements at Södersjukhuset (not using Catalyst) and Sundsvalls sjukhus (using Catalyst). The workflows were graded in an assessment protocol covering time efficiency, comfort, noise, resources, reliability, cost, dosage and side effects. Different workflow scenarios were simulated in AnyLogic.</p><p>The result of the project was that, according to our protocol, the workflow with Catalyst was rated higher than without it. The simulations in Anylogic showed that minimizing gaps in the treatment schedule generated the same number of patients treated per day, if the positioning could not be done faster. The simulations also showed that removing position verification with cone beam computer tomography (CBCT), an imaging system which is used in addition to the Catalyst system, would increase the number of treated patients with approximately 33%.</p><p>The conclusion was that there were no great differences in time efficiency between the workflows. However, considering the higher reliability and comfort for the patient, optical surface scanning can improve the positioning for Elekta Linac and is therefore worth implementing. Minimizing treatment gaps would not improve the workflow. Removing the use of CBCT would increase the number of treated patients per day.</p>
----------------------------------------------------------------------
In diva2:1353232 abstract is: <p>The work done in this report as requested by H&amp;D Wireless is performed by using an already developed real time positioning system called Griffin Enterprise Positioning Service and integratingit with ultrasound sensors for presence detection in order to enable assets to park in a visualized environment being actualized by tagging the asset with radio technology hardware. The testing environment was deployed with radio technology hardware equipped for transmitting and receiving radio signals for position estimation of tagged objects where hardware tags emits radio signals being received by sensing radio technology chips called sensepoints which also serves as communication links for further data processing.The thesis focus is on evaluating how different radio technologies combined with different positioning techniques perform in terms of accuracy and precision in positioning tests to assess eachones positioning performance characteristic and the technologies upsides and downsides.This was firstly evaluated by comparing three different technology positioning techniques based on one for Bluetooth Low Energy and two using Ultra Wideband technology being subject to generic tests including a static, dynamic and a walking positioning test for each technology.These initial tests were utilized as a foreground to evaluate which of the two positioning techniques based on Ultra Wideband technology that would compete in the parking tests alongside Bluetooth Low Energy that would serve as the primary objective to accomplish in the thesis.A final implication on parking tags between the two technologies is that Bluetooth Low Energy had to be implemented with higher requirement restrictions for parking due to insufficient relative accuracy and precision in parking positioning which also limited its ability to be parked in alternative manners explored but with power efficiency as a highly valuable aspect for consideration of this technology. Parking tag using Ultra Wideband technology proved highly successful as it saw large distance margins to be allowed parking in all test cases as well as exhibiting sufficient positioning performance to be considered for alternative parking methods without risk of exposure for failed attempts of parking.</p>

corrected abstract:
<p>The work done in this report as requested by H&amp;D Wireless is performed by using an already developed real time positioning system called Griffin Enterprise Positioning Service and integrating it with ultrasound sensors for presence detection in order to enable assets to park in a visualized environment being actualized by tagging the asset with radio technology hardware. The testing environment was deployed with radio technology hardware equipped for transmitting and receiving radio signals for position estimation of tagged objects where hardware tags emits radio signals being received by sensing radio technology chips called sensepoints which also serves as communication links for further data processing.</p><p>The thesis focus is on evaluating how different radio technologies combined with different positioning techniques perform in terms of accuracy and precision in positioning tests to assess each ones positioning performance characteristic and the technologies upsides and downsides.</p><p>This was firstly evaluated by comparing three different technology positioning techniques based on one for Bluetooth Low Energy and two using Ultra Wideband technology being subject to generic tests including a static, dynamic and a walking positioning test for each technology. These initial tests were utilized as a foreground to evaluate which of the two positioning techniques based on Ultra Wideband technology that would compete in the parking tests alongside Bluetooth Low Energy that would serve as the primary objective to accomplish in the thesis.</p><p>A final implication on parking tags between the two technologies is that Bluetooth Low Energy had to be implemented with higher requirement restrictions for parking due to insufficient relative accuracy and precision in parking positioning which also limited its ability to be parked in alternative manners explored but with power efficiency as a highly valuable aspect for consideration of this technology. Parking tag using Ultra Wideband technology proved highly successful as it saw large distance margins to be allowed parking in all test cases as well as exhibiting sufficient positioning performance to be considered for alternative parking methods without risk of exposure for failed attempts of parking.</p>
----------------------------------------------------------------------
In diva2:1242939 abstract is: <p>Spatial Transcriptomics (ST) is a microarray-based RNA sequencing technology that allows for genome-wide transcriptome profiling of tissue sections with spatialresolution, which was published in Science by Ståhl and Salmén et al in 2016. Polyadenylated transcripts are captured on a microarray surface through hybridizationwith a barcoded DNA oligo that carries the information necessary to infer spatialposition and transcript uniqueness from the output sequencing reads. The ST protocolutilizes Unique Molecular Identifiers (UMIs) to remove PCR duplicates and obtain reliable estimates of gene counts. However, errors gradually accumulate in the UMIpool as a consequence of enzymatic conversion steps and these errors ought to be addressed computationally in data processing steps to produce reliable gene countestimates.In this thesis, we used ERCC reference RNAs and a set of custom-made DNA oligosas spike-ins in the ST protocol to explore sources of technical variation under controlled experimental conditions. An exploratory analysis of the spike-in data gave new insights into the chemistry which could guide future improvements of the protocol. We also developed a new strategy to bin read alignments before gene quantification and showed that this strategy produces a more reliable output. Finally, we developed an in silicosimulation of the ST library preparation to provide a framework that can be used toevaluate the performance of various computational processing strategies. The simulation was then used to benchmark a set of duplicate removal algorithms used toquantify gene expression.This master thesis project was carried out at SciLifeLab at the division of GeneTechnology under supervision of José Fernandez Navarro.</p>

corrected abstract:
<p>Spatial Transcriptomics (ST) is a microarray-based RNA sequencing technology that allows for genome-wide transcriptome profiling of tissue sections with spatial resolution, which was published in Science by Ståhl and Salmén et al in 2016. Polyadenylated transcripts are captured on a microarray surface through hybridization with a barcoded DNA oligo that carries the information necessary to infer spatial position and transcript uniqueness from the output sequencing reads. The ST protocol utilizes Unique Molecular Identifiers (UMIs) to remove PCR duplicates and obtain reliable estimates of gene counts. However, errors gradually accumulate in the UMI pool as a consequence of enzymatic conversion steps and these errors ought to be addressed computationally in data processing steps to produce reliable gene count estimates.</p><p>In this thesis, we used ERCC reference RNAs and a set of custom-made DNA oligos as spike-ins in the ST protocol to explore sources of technical variation under controlled experimental conditions. An exploratory analysis of the spike-in data gave new insights into the chemistry which could guide future improvements of the protocol. We also developed a new strategy to bin read alignments before gene quantification and showed that this strategy produces a more reliable output. Finally, we developed an <em>in silico</em> simulation of the ST library preparation to provide a framework that can be used to evaluate the performance of various computational processing strategies. The simulation was then used to benchmark a set of duplicate removal algorithms used to quantify gene expression.</p><p>This master thesis project was carried out at SciLifeLab at the division of Gene Technology under supervision of José Fernandez Navarro.</p>
----------------------------------------------------------------------
In diva2:1217278 abstract is: <p>This report investigates which components a portable wireless Bluetooth speakerconsists of and the design parameters a prospective designer should take into consideration.The characteristics of six commercially available speaker models andthe design solutions used in these were examined. The design parameters wereanalyzed to investigate if and how these could be improved upon. Simulations ofspeaker prototypes were done and prototypes of speakers and an amplifier werebuilt. Measurements were made to be able to compare and verify the audio performanceof the prototypes. Moreover, measurements of music were made to investigatehow the RMS-value of music impacts the power of an amplifier in general andin active systems in particular. Design recommendations on how to build a Bluetoothspeaker are given in the end of the report together with suggestions on alternatetechnologies to look into for implementation in future Bluetooth speakers.</p>

corrected abstract:
<p>This report investigates which components a portable wireless Bluetooth speaker consists of and the design parameters a prospective designer should take into consideration. The characteristics of six commercially available speaker models and the design solutions used in these were examined. The design parameters were analyzed to investigate if and how these could be improved upon. Simulations of speaker prototypes were done and prototypes of speakers and an amplifier were built. Measurements were made to be able to compare and verify the audio performance of the prototypes. Moreover, measurements of music were made to investigate how the RMS-value of music impacts the power of an amplifier in general and in active systems in particular. Design recommendations on how to build a Bluetooth speaker are given in the end of the report together with suggestions on alternate technologies to look into for implementation in future Bluetooth speakers.</p>
----------------------------------------------------------------------
In diva2:1278530 abstract is: <p>Today, in the automotive industry, many of the interior parts in the car are made of ABS and PC/ABS polymeric blend. These materials are used in the areas for example: instrument panels, tunnel consoles and door panels. The extensive use of these materials means that it is important to gain in-depth knowledge about the materials,their properties; and also their behaviour when in contact with different chemicals andat different conditions.This study aims to address the potential problem of the polymers used in the interiorof the car - ABS and PC/ABS cracking due to environmental factors. This study proposes to introduce a low-cost test method to compare the polymeric materials and choose the best one for future purposes with the environmental circumstances in mind for materials to have a good service life.During the thesis project, ABS and PC/ABS samples were tested for environmental stress cracking to compare the strained materials against PEG 400 and an assemblyfluid chemical. These tests were conducted at three different temperature levels.Differential Scanning Calorimetry (DSC) was used to verify the polymeric materialsthat the samples were made of. Optical microscope and FTIR were employed to analyzethe samples for crazes / cracks and degradation of material, respectively.This thesis helped in establishing a good starting point for ESC testing of different materials for the organization. The test method was used to test the failure of material sin ESC. It was observed that the chemicals used for the testing were aggressive and accelerated the cracking process in the materials rapidly. Another observation of the tests was that high strain also caused the materials to fail quickly. While comparing the materials, PC/ABS polymer blend was more resistant than ABS materials to cracking when exposed to same strain level during the creep rupture test (test in absence ofchemicals acting as a reference test for ESC).</p>

corrected abstract:
<p>Today, in the automotive industry, many of the interior parts in the car are made of ABS and PC/ABS polymeric blend. These materials are used in the areas for example: instrument panels, tunnel consoles and door panels. The extensive use of these materials means that it is important to gain in-depth knowledge about the materials, their properties; and also their behaviour when in contact with different chemicals and at different conditions.</p><p>This study aims to address the potential problem of the polymers used in the interior of the car - ABS and PC/ABS cracking due to environmental factors. This study proposes to introduce a low-cost test method to compare the polymeric materials and choose the best one for future purposes with the environmental circumstances in mind for materials to have a good service life.</p><p>During the thesis project, ABS and PC/ABS samples were tested for environmental stress cracking to compare the strained materials against PEG 400 and an assembly fluid chemical. These tests were conducted at three different temperature levels. Differential Scanning Calorimetry (DSC) was used to verify the polymeric materials that the samples were made of. Optical microscope and FTIR were employed to analyze the samples for crazes / cracks and degradation of material, respectively.</p><p>This thesis helped in establishing a good starting point for ESC testing of different materials for the organization. The test method was used to test the failure of materials in ESC. It was observed that the chemicals used for the testing were aggressive and accelerated the cracking process in the materials rapidly. Another observation of the tests was that high strain also caused the materials to fail quickly. While comparing the materials, PC/ABS polymer blend was more resistant than ABS materials to cracking when exposed to same strain level during the creep rupture test (test in absence of chemicals acting as a reference test for ESC).</p>
----------------------------------------------------------------------
In diva2:1451795 abstract is: <p>Abstract</p><p>With the rapid increase of bandwidth and complexity in the network of communicationsservice providers, there is need for effective troubleshooting to maintain troublefree networks and in turn high customer satisfaction. An important tool in troubleshootingis visualisation of call flows and messages in the network. This thesis setout to compare different charts and user interfaces for the purpose of displaying callflow data in a network for communication services. Two high-fidelity mock-ups weremade for visualising call flows and two for visualising messages. The mock-ups wereevaluated using user-based evaluation and heuristic evaluation. The mock-ups werethen revised based on the information gathered from the evaluations. The result ofthis thesis suggests that an implementation should use a sequence diagram for visualisationof call flows together with a hierarchical view for visualisation of messages.</p><p>Keywords</p><p>Call flow, visualisation, user interface, communication services, heuristics, sequencediagram, hierarchical view, table view, tree diagram, mock-up</p>


corrected abstract:
<p>With the rapid increase of bandwidth and complexity in the network of communications service providers, there is need for effective troubleshooting to maintain trouble free networks and in turn high customer satisfaction. An important tool in troubleshooting is visualisation of call flows and messages in the network. This thesis set out to compare different charts and user interfaces for the purpose of displaying call flow data in a network for communication services. Two high-fidelity mock-ups were made for visualising call flows and two for visualising messages. The mock-ups were evaluated using user-based evaluation and heuristic evaluation. The mock-ups were then revised based on the information gathered from the evaluations. The result of this thesis suggests that an implementation should use a sequence diagram for visualisation of call flows together with a hierarchical view for visualisation of messages.</p>
----------------------------------------------------------------------
In diva2:1763859 abstract is: <p>The work describes the opportunities to perform preventive maintenance with the help of informationfrom COMTRADE disturbance files. A software algorithm was developed which collects disturbancedata and gives indications if equipment are not working within optimal conditions.Using thisinformation preventive maintenance can be performed based on need instead of scheduling to savetime and money.Together with supervisors from involved companies a software was developed to be used as a supportfor preventive maintenance. The software can extract disturbance times and handle multiple scenariosbased on information collected from disturbance files. A user has access to a algoritm that createsautomatical analysis of the COMTRADE file and a manual tool for extensive analysis when the algoritmdoes not give proper results.Trends over time can be analysed with the algortim, this do require a larger amount of data than whatwas available during the work. </p>


corrected abstract:
<p>The work describes the opportunities to perform preventive maintenance with the help of information from COMTRADE disturbance files. A software algorithm was developed which collects disturbance data and gives indications if equipment are not working within optimal conditions. Using this information preventive maintenance can be performed based on need instead of scheduling to save time and money.</p><p>Together with supervisors from involved companies a software was developed to be used as a support for preventive maintenance. The software can extract disturbance times and handle multiple scenarios based on information collected from disturbance files. A user has access to a algoritm that creates automatical analysis of the COMTRADE file and a manual tool for extensive analysis when the algoritm does not give proper results.</p><p>Trends over time can be analysed with the algortim, this do require a larger amount of data than what was available during the work.</p>
----------------------------------------------------------------------
The PDF file has a bizzare character encoding and one cannot easily extract the text.

In diva2:724099 abstract is: <p>After the extension of the boiler 6 at Högdalenverket during 2010, there has been some problems in the form of increased carbon monoxide levels. There may be several explanations for this problem, which means that complete combustion doesent take place. There have also been problems with the ash-handling system and the sand returning systemt, this results in higher operating costs because it requires a higher consumption of inert material.The project was first divided into two phases, the first phase was to develop material and energy balances for the boiler but also introduce the parameters that we wanted to investigate further in order to possibly identify the causes of the problems that the boiler had. In the second phase experiments were going to be designed in consultation with the contact staff at Fortum to explore these parameters.During the project changes have been made in the project description, as it will require more time and more accurate planning to perform  the desired tests on the boiler. Three proposals on parameters that could be the cause of carbon monoxide problem was presented at the end of phase one and it was found that two of the proposals was not possible to carry out during the project because it would have affected economically and the availibility of the boilier.The third parameter, the bed quality effect on combustion experiment was designed in consultation with those responsible at Fortum.This experiment could also not be performed as there was some operational difficulties with the boiler during the project time. Instead, a description of the design for the experiment has been added to this project, which may at a later stage be used to investigate the bed quality impact on the carbon monoxide issues.Recent changes to the project description meant that purpose instead was to further develop the computational program created during the project's first phase and at the end of the project shall be submitted to the operators at Fortum. This calculation program takes into account the significant parameters that the boiler regulator is governed by today. The simulation program is developed using Microsoft Excel and is based on material and energy balances. This program can be used internally by Fortum to get an idea of ​​how the different mass flow rates would vary at different operating conditions and how the energy balance for the boiler would look like during the changes.When compared with materials and energy made by the boiler supplier revealed that the deviations are very small compared to what the program come up with and is caused mainly due to the assumptions made. These assumptions can be eliminated by performing experiments and collecting more data.The results gained by the calculation program has been compared with the results that was presented by the supplier during their boiler mapping, comparisons have been made regarding flue gas flows, fuel flows, combustion air flow, ash production and energy balances. It has apperad that the deviations are relatively small and the presented scheme provides a theoretical overall perspective of the boiler in line with reality.Although discussions have taken into consideration for the problem of elevated levels of carbon monoxide, and whether the measurement program can be developed further in the future.</p>

w='boilier' val={'c': 'boiler', 's': 'diva2:724099'}
w='doesent' val={'c': "doesn't", 's': 'diva2:724099', 'n': 'error in original'}
w='apperad' val={'c': 'appeared', 's': 'diva2:724099'}
w='availibility' val={'c': 'availability', 's': 'diva2:724099', 'n': 'error in original'}

corrected abstract:
<p>After the extension of the boiler 6 at Högdalenverket during 2010, there has been some problems in the form of increased carbon monoxide levels. There may be several explanations for this problem, which means that complete combustion doesent take place. There have also been problems with the ash-handling system and the sand returning systemt, this results in higher operating costs because it requires a higher consumption of inert material.</p><p>The project was first divided into two phases, the first phase was to develop material and energy balances for the boiler but also introduce the parameters that we wanted to investigate further in order to possibly identify the causes of the problems that the boiler had. In the second phase experiments were going to be designed in consultation with the contact staff at Fortum to explore these parameters.<br>During the project changes have been made in the project description, as it will require more time and more accurate planning to perform the desired tests on the boiler. Three proposals on parameters that could be the cause of carbon monoxide problem was presented at the end of phase one and it was found that two of the proposals was not possible to carry out during the project because it would have affected economically and the availibility of the boiler.<br>The third parameter, the bed quality effect on combustion experiment was designed in consultation with those responsible at Fortum. This experiment could also not be performed as there was some operational difficulties with the boiler during the project time. Instead, a description of the design for the experiment has been added to this project, which may at a later stage be used to investigate the bed quality impact on the carbon monoxide issues. Recent changes to the project description meant that purpose instead was to further develop the computational program created during the project's first phase and at the end of the project shall be submitted to the operators at Fortum. This calculation program takes into account the significant parameters that the boiler regulator is governed by today. The simulation program is developed using Microsoft Excel and is based on material and energy balances. This program can be used internally by Fortum to get an idea of ​​how the different mass flow rates would vary at different operating conditions and how the energy balance for the boiler would look like during the changes.</p><p>When compared with materials and energy made by the boiler supplier revealed that the deviations are very small compared to what the program come up with and is caused mainly due to the assumptions made. These assumptions can be eliminated by performing experiments and collecting more data. The results gained by the calculation program has been compared with the results that was presented by the supplier during their boiler mapping, comparisons have been made regarding flue gas flows, fuel flows, combustion air flow, ash production and energy balances. It has appeared that the deviations are relatively small and the presented scheme provides a theoretical overall perspective of the boiler in line with reality.<br>Although discussions have taken into consideration for the problem of elevated levels of carbon monoxide, and whether the measurement program can be developed further in the future.</p>
----------------------------------------------------------------------
In diva2:1178039 abstract is: <p>Overwork of muscle can be a problem for the double poling cross country skier potentially resultingin lower efficiency. An assignment was established – on behalf of Johnny Nilsson at Gymnastik- ochIdrottshögskolan (GIH) in Stockholm – in order to build a prototype able to monitor and record datafrom musculus trapezius (m. trapezius) through the use of electromyography (EMG). The EMG wasmade using the open source hardware Arduino. The prototype was able to record bilateralmeasurements with the use of EMG-shields, where surface-electrodes were attached to m. trapezius.By creating a prototype based on a rotary potentiometer attached to the elbow joint a reference ofmovement was established by measuring the extension and flexion angle of the elbow. Arduino’s ownIDE was used to program the hardware of the prototype and data was post-processed and presented inMatlab. Data was transferred with the use of an SD-card reader installed on the microcontroller. Withthe help of Peter Arfert at KTH, a 3D-printed model was made for the prototype. The final prototypewas attached to an elite level cross-country skier and tested on a professional treadmill at the LIVIlaboratory in Falun, Sweden. Raw-data was successfully recorded during these trials.</p>

corrected abstract:
<p>Overwork of muscle can be a problem for the double poling cross country skier potentially resulting in lower efficiency. An assignment was established – on behalf of Johnny Nilsson at Gymnastik- och Idrottshögskolan (<em>GIH</em>) in Stockholm – in order to build a prototype able to monitor and record data from musculus trapezius (m. trapezius) through the use of electromyography (<em>EMG</em>). The EMG was made using the open source hardware Arduino. The prototype was able to record bilateral measurements with the use of EMG-shields, where surface-electrodes were attached to m. trapezius. By creating a prototype based on a rotary potentiometer attached to the elbow joint a reference of movement was established by measuring the extension and flexion angle of the elbow. Arduino’s own IDE was used to program the hardware of the prototype and data was post-processed and presented in Matlab. Data was transferred with the use of an SD-card reader installed on the microcontroller. With the help of Peter Arfert at KTH, a 3D-printed model was made for the prototype. The final prototype was attached to an elite level cross-country skier and tested on a professional treadmill at the LIVI laboratory in Falun, Sweden. Raw-data was successfully recorded during these trials.</p>
----------------------------------------------------------------------
In diva2:1698780 abstract is: <p>Parkinson’s Disease (PD) is a neurodegenerative disorder, within this categoryof diseases it is among the most prevalent worldwide. The etiology of PD isbased in progressive deterioration of neural tissue in the basal ganglia (neuronalnuclei located at the base of the cerebrum) and their related structures. Current research is focusing on treatment approaches to either enhance or replaceexisting pharmaceutical treatment approaches, such as dopamine replacementtherapy. In this project, the focus was on finding correlates between movementdata and neurological signals to provide insight into potential biomarkers forcomplex motor symptoms of PD. This will in turn provide a starting point forspecifically targeted closed-loop neural stimulation that alleviates these symptoms. Although the data available at the time of this thesis did not providesufficient insight to derive a conclusion on the neural correlates, a pipeline wasdeveloped, which analyzes and synchronizes kinematic and neural data and willenable further exploration as additional data is obtained.</p>

corrected abstract:
<p>Parkinson’s Disease (PD) is a neurodegenerative disorder, within this category of diseases it is among the most prevalent worldwide. The etiology of PD is based in progressive deterioration of neural tissue in the basal ganglia (neuronal nuclei located at the base of the cerebrum) and their related structures. Current research is focusing on treatment approaches to either enhance or replace existing pharmaceutical treatment approaches, such as dopamine replacement therapy. In this project, the focus was on finding correlates between movement data and neurological signals to provide insight into potential biomarkers for complex motor symptoms of PD. This will in turn provide a starting point for specifically targeted closed-loop neural stimulation that alleviates these symptoms. Although the data available at the time of this thesis did not provide sufficient insight to derive a conclusion on the neural correlates, a pipeline was developed, which analyzes and synchronizes kinematic and neural data and will enable further exploration as additional data is obtained.</p>
----------------------------------------------------------------------
In diva2:1242473 abstract is: <p>Biomass is a viable option to supply the need of renewable energy inthe future. However, the high cost of biomass processing makes the process not viable economically. For that reason, two one dimensionalparticle models that describe the mass and heat transfer of biomasswere developed and solved using the method of lines. The two modelsare based on the previous work of Park et al (Combustion and flame,2010). The first model neglects the importance of the convective fluxterm in the heat and mass balances. The second model considers the convective term, but a velocity expression that assumes constant pressurewas used. The results of the simulation show that the convectiveterm is insignificant for the mass loss and temperature profiles of athick particle. Experiments of biomass spheres were done to furthervalidate the results of the conduction and convection model. The experimentaldata is compatible with the results of the conduction and convection model.</p>

Note there is not a period after "al" in the actual abstract.
corrected abstract:
<p>Biomass is a viable option to supply the need of renewable energy in the future. However, the high cost of biomass processing makes the process not viable economically. For that reason, two one dimensional particle models that describe the mass and heat transfer of biomass were developed and solved using the method of lines. The two models are based on the previous work of Park et al (Combustion and flame, 2010). The first model neglects the importance of the convective flux term in the heat and mass balances. The second model considers the convective term, but a velocity expression that assumes constant pressure was used. The results of the simulation show that the convective term is insignificant for the mass loss and temperature profiles of a thick particle. Experiments of biomass spheres were done to further validate the results of the conduction and convection model. The experimental data is compatible with the results of the conduction and convection model.</p>
----------------------------------------------------------------------
In diva2:1684046 abstract is: <p>IEC 60601 electrical safety tests are still used in many medical engineeringdepartments in hospitals for testing equipment in connection with arrival controland repair. This report investigates the possibility of producing a basis for starting toimplement IEC 62353 as an electrical safety test standard instead of IEC 60601.The study focuses on whether Karolinska University Hospital should apply IEC62353 as its electrical safety test standard. Furthermore, the differences between thetwo above standards are discussed. This is done by finding out how the KarolinskaUniversity Hospital responds to the question and the collection of information, aswell as how the IEC 62353 standard is applied in industry.The information presented in the results after contact with several hospitals showstwo different aspects. One is an independent application of IEC 62353 as an electricalsafety test standard, the other is to follow the manufacturer's instructions. Futurestudies that can provide clearer guidance from the manufacturer's point of view arerecommended. </p>


corrected abstract:
<p>IEC 60601 electrical safety tests are still used in many medical engineering departments in hospitals for testing equipment in connection with arrival control and repair. This report investigates the possibility of producing a basis for starting to implement IEC 62353 as an electrical safety test standard instead of IEC 60601.</p><p>The study focuses on whether Karolinska University Hospital should apply IEC 62353 as its electrical safety test standard. Furthermore, the differences between the two above standards are discussed. This is done by finding out how the Karolinska University Hospital responds to the question and the collection of information, as well as how the IEC 62353 standard is applied in industry.</p><p>The information presented in the results after contact with several hospitals shows two different aspects. One is an independent application of IEC 62353 as an electrical safety test standard, the other is to follow the manufacturer's instructions. Future studies that can provide clearer guidance from the manufacturer's point of view are recommended.</p>
----------------------------------------------------------------------
In diva2:1866444 abstract is: <p>This bachelor thesis has been carried out in cooperation with Luvly AB, a light urban vehiclestartup working out of Stockholm. The project’s goal has been to develop a theory regardingcomputerized troubleshooting of the CAN-bus in three domains, the messages, the signals,and the electrical characteristics, which is of importance for testing in commercial applications. The result of this report consists of two parts, a software program applying the messagedomain and a theoretical part explaining how the two other domains can be implemented.These three domains together give an intuitive and complete troubleshooting theory thatcould be automated as well as built into the CAN-bus itself in industrial and vehicular applications.The developed program is available for download on the Royal Institute of Technology’sOneDrive and on MediaFire.</p>

corrected abstract:
<p>This bachelor thesis has been carried out in cooperation with Luvly AB, a light urban vehicle startup working out of Stockholm. The project’s goal has been to develop a theory regarding computerized troubleshooting of the CAN-bus in three domains, the messages, the signals, and the electrical characteristics, which is of importance for testing in commercial applications. The result of this report consists of two parts, a software program applying the message domain and a theoretical part explaining how the two other domains can be implemented.</p><p>These three domains together give an intuitive and complete troubleshooting theory that could be automated as well as built into the CAN-bus itself in industrial and vehicular applications.</p><p>The developed program is available for download on the Royal Institute of Technology’s OneDrive and on MediaFire.</p>
----------------------------------------------------------------------
title: "Effect of nutrient limitation in chemostat cultures on amino acid excretion in Clostridium thermocellum"
==>    "Effect of nutrient limitation in chemostat cultures on amino acid excretion in <em>Clostridium thermocellum</em>"

In diva2:1297420 abstract is: <p>Introduction: Clostridium thermocellum is considered a model organism for consolidated bioprocessing, due to its ability to hydrolyze lignocellulosic biomass more efficiently than many other organisms and to produce ethanol.In order to meet the industrial requirements of ethanol yield and titer, metabolic engineering efforts have been made resulting in a strain that successfully displays increased ethanol yield with reduced amount of some byproducts.However, the ethanol yield in this engineered strain still does not meet the industrial requirements and significant amounts of amino acids are still produced. To attempt to decrease the level of amino acid excretion intended to improve the ethanol yield in C. thermocellum, it is essential to understand its metabolism and how it is affected by different cultivation conditions and mediumcompositions. This study aimed to gain an insight in how carbon- and nitrogenlimitation affect amino acid excretion in C. thermocellum, with the hypothesisthat excess of carbon and nitrogen yields more amino acid excretion. Methods: Mass-balance based calculations of rates and yields were used to analyze the metabolism of a wild-type of C. thermocellum (DSM 1313) grownanaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbonmedium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as the limiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0and 400 RPM shaking at a dilution rate of 0.1 h-1.Conclusions: Considering yields of total amino acids excreted in both limitations, it was hypothesized that C. thermocellum exploited the amino acid excretion to maintain carbon balance around the pyruvate node caused by excess of the carbon. Based on yield of valine excreted in particular, it was hypothesized that amino acid excretion was used to maintain redox balance in the metabolism of C. thermocellum, where malate shunt could play a major role.However, results of the Carbon-limitation did not allow any conclusion of nitrogen excess having an effect on amino acid excretion in C. thermocellum.</p>


corrected abstract:
<p><strong>Introduction:</strong> <em>Clostridium thermocellum</em> is considered a model organism for consolidated bioprocessing, due to its ability to hydrolyze lignocellulosic biomass more efficiently than many other organisms and to produce ethanol. In order to meet the industrial requirements of ethanol yield and titer, metabolic engineering efforts have been made resulting in a strain that successfully displays increased ethanol yield with reduced amount of some byproducts. However, the ethanol yield in this engineered strain still does not meet the industrial requirements and significant amounts of amino acids are still produced. To attempt to decrease the level of amino acid excretion intended to improve the ethanol yield in <em>C. thermocellum</em>, it is essential to understand its metabolism and how it is affected by different cultivation conditions and medium compositions. This study aimed to gain an insight in how carbon- and nitrogen limitation affect amino acid excretion in <em>C. thermocellum</em>, with the hypothesis that excess of carbon and nitrogen yields more amino acid excretion.</p><p><strong>Methods:</strong> Mass-balance based calculations of rates and yields were used to analyze the metabolism of a wild-type of <em>C. thermocellum</em> (DSM 1313) grown anaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbon medium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as the limiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0 and 400 RPM shaking at a dilution rate of 0.1 h<sup>-1</sup>.</p><p><strong>Conclusions:</strong> Considering yields of total amino acids excreted in both limitations, it was hypothesized that <em>C. thermocellum</em> exploited the amino acid excretion to maintain carbon balance around the pyruvate node caused by excess of the carbon. Based on yield of valine excreted in particular, it was hypothesized that amino acid excretion was used to maintain redox balance in the metabolism of <em>C. thermocellum</em>, where malate shunt could play a major role. However, results of the Carbon-limitation did not allow any conclusion of nitrogen excess having an effect on amino acid excretion in <em>C. thermocellum</em>.</p>
----------------------------------------------------------------------
In diva2:1139906 abstract is: <p>Nowadays surgery simulations are aiming to apply not just visual effects but forcefeedback as well. To carry out force feedback, haptic devices are utilized that are mostlycommercial products for general purposes. Some of the haptic device features are moreimportant than others in case of surgery simulator use. The precision of the output forcemagnitude is one such property. The specifications provided by haptic devicemanufacturers are lacking details on device characteristics, known to cause difficulties inplanning of accurate surgery simulations.This project shows the design of a testbed that is capable of measuring the precision ofoutput forces within the haptic devices’ workspace. With the testbed, a set ofmeasurements can be run on different haptic devices, giving as a result a betterknowledge of the utilized device. This knowledge aids the design of more precise andrealistic surgery simulations.</p>


corrected abstract:
<p>Nowadays surgery simulations are aiming to apply not just visual effects but force feedback as well. To carry out force feedback, haptic devices are utilized that are mostly commercial products for general purposes. Some of the haptic device features are more important than others in case of surgery simulator use. The precision of the output force magnitude is one such property. The specifications provided by haptic device manufacturers are lacking details on device characteristics, known to cause difficulties in planning of accurate surgery simulations.</p><p>This project shows the design of a testbed that is capable of measuring the precision of output forces within the haptic devices’ workspace. With the testbed, a set of measurements can be run on different haptic devices, giving as a result a better knowledge of the utilized device. This knowledge aids the design of more precise and realistic surgery simulations.</p>
----------------------------------------------------------------------
In diva2:1864545 abstract is: <p>In this thesis, the Purdue model and its applicability within network security forindustrial control systems under the framework of Industry 4.0 are analyzed.Through a literature review, the model's structure and function are examined inrelation to the new challenges that have emerged due to increased digitization andintegration of IIoT technologies. The study identifies both strengths and weaknessesin the traditional Purdue model. In the results section, a modified version of thePurdue model is introduced, designed to enhance network security and increase thesystems' ability to handle cyber threats and adapt to technological changes in theindustrial sector. This adaptation has been achieved by incorporating additionalsecurity standards and tools aimed at improving the model's efficiency andrelevance.</p>

corrected abstract:
<p>In this thesis, the Purdue model and its applicability within network security for industrial control systems under the framework of Industry 4.0 are analyzed. Through a literature review, the model's structure and function are examined in relation to the new challenges that have emerged due to increased digitization and integration of IIoT technologies. The study identifies both strengths and weaknesses in the traditional Purdue model. In the results section, a modified version of the Purdue model is introduced, designed to enhance network security and increase the systems' ability to handle cyber threats and adapt to technological changes in the industrial sector. This adaptation has been achieved by incorporating additional security standards and tools aimed at improving the model's efficiency and relevance.</p>
----------------------------------------------------------------------
In diva2:1458899 abstract is: <p>Immune-orchestrating biomaterials that precisely modulate the immune reac-tion to the host could lead the way for improving the implantation outcomein the transplantation field, in comparison to passive biomaterials. The lab-oratory of Dr. Thomas Crouzier has shown hydrogels derived from mucinsare capable of orchestrating the immune response mediated by foreign bodyresponse (FBR), as a result of evading fibrosis. Further, a recent study fromhis group showed sialic acid on mucin hydrogels is essential for the immuno-logical activity of those materials. Mucin glycans transiently activated thendampened macrophages, important orchestrators for material-mediated FBR,in a sialic acid-dependent manner for the majority of cytokines followed. Thematerial properties such as rheological properties, self-healing capacity, andstability, can be governed by the crosslinking chemistry used and have a drasticimpact on the functionalities of the materials. In this project, various cross-linking strategies are applied to tune the hydrogel properties. We show thatthe robust cross-linking formed mucin hydrogels having a 1.5% (wt/v) bettersupported insulin-secreting cells form islet-like organoids, compared to 2.5%mucin hydrogels. We then investigate the self-healing properties of the newmucin hydrogels and their interactions with various cell systems.</p>

Note that the actual abstract some time sets "cross-lining" or "crosslinking".
corrected abstract:
<p>Immune-orchestrating biomaterials that precisely modulate the immune reaction to the host could lead the way for improving the implantation outcome in the transplantation field, in comparison to passive biomaterials. The laboratory of Dr. Thomas Crouzier has shown hydrogels derived from mucins are capable of orchestrating the immune response mediated by foreign body response (FBR), as a result of evading fibrosis. Further, a recent study from his group showed sialic acid on mucin hydrogels is essential for the immunological activity of those materials. Mucin glycans transiently activated then dampened macrophages, important orchestrators for material-mediated FBR, in a sialic acid-dependent manner for the majority of cytokines followed. The material properties such as rheological properties, self-healing capacity, and stability, can be governed by the crosslinking chemistry used and have a drastic impact on the functionalities of the materials. In this project, various cross-linking strategies are applied to tune the hydrogel properties. We show that the robust cross-linking formed mucin hydrogels having a 1.5% (wt/v) better supported insulin-secreting cells form islet-like organoids, compared to 2.5% mucin hydrogels. We then investigate the self-healing properties of the new mucin hydrogels and their interactions with various cell systems.</p>
----------------------------------------------------------------------
In diva2:1223357 abstract is: <p>Dietary fatty acids (FAs) have been extensively studied in terms of preventing cardiovascular disease (CVD) through a healthy diet. Certain FA patterns in plasma lipids have been related to increased risk to develop CVD whereas others seems to be preventative (e.g. high linoleic acid [LA], and low palmitoleic acid [16:1n-7]).However, the mechanism behind how FAs are involved in CVD remains unclear. It is believed to involve inflammation and cholesterol metabolism, conditions in which both FAs and a large variety of cardiovascular(CV) proteins are involved. The aim of this project was to investigate whether there are any linkage between plasma fatty acid patterns related to CVD and CV proteins. A cross-sectional analysis was performed in two independent population-based cohorts. The Prospective Investigation of the Vasculature in Uppsala Seniors(PIVUS) cohort was used as a discovery sample and The Prospective investigation of Obesity, Energy andMetabolism (POEM) cohort in Uppsala was used as a replicate sample. A factor analysis of FAs measured in plasma was conducted in respective cohort to identify fatty acid patterns related to CVD. Linear regressionmodels between FA patterns and 84 CV proteins measured in respective cohort (samples) were performed.The 81 proteins being found in both samples were meta-analyzed. Traditional CVD risk factors (sex, BMI,diabetes, systolic blood pressure, LDL and HDL-cholesterol and smoking) were adjusted for. Two factors(Low-LA-factor, Fatty-fish-Lipid-factor) (eigenvalue&gt;1) built up by the same patterns of FAs in both samples and constituted of FAs previously related to CVD were disclosed. The Low-LA-factor was constituted of highloadings of palmitoleic acid (16:1n-7) and oleic acid (18:1n-9) and low loadings of linoleic acid (LA) (18:2n-6),previously recognized as a CVD risk pattern. The Fatty-fish-lipid-factor was constituted of high loading’s ofeicosapentaenoic acid (EPA) (20:5n-3) and docosahexaenoic acid (DHA) (22:6-n-3), previously recognized as cardio preventative FA pattern. In the linear regression meta-analysis, the Low-LA-factor was significantly positively related to 3 plasma proteins. The Fatty-fish-lipid-factor was significantly related negatively with 16plasma proteins and positively with one plasma protein. The majority of these CV proteins have previously been related to CVD but not connected with plasma fatty acid composition. Thus, these novel results confirmthat there are relationships between dietary FA patterns and proteins involved in CVD that potentially partially mediates the association between certain FA patterns and CVD risk. With further research oncausality between FAs and these proteins as well as potential mechanisms, these links may be used as a basisfor future and more tailored dietary strategies to prevent CVD in high-risk individuals.</p>


corrected abstract:
<p>Dietary fatty acids (FAs) have been extensively studied in terms of preventing cardiovascular disease (CVD) through a healthy diet. Certain FA patterns in plasma lipids have been related to increased risk to develop CVD whereas others seems to be preventative (e.g. high linoleic acid [LA], and low palmitoleic acid [16:1n-7]). However, the mechanism behind how FAs are involved in CVD remains unclear. It is believed to involve inflammation and cholesterol metabolism, conditions in which both FAs and a large variety of cardiovascular (CV) proteins are involved. The aim of this project was to investigate whether there are any linkage between plasma fatty acid patterns related to CVD and CV proteins. A cross-sectional analysis was performed in two independent population-based cohorts. The Prospective Investigation of the Vasculature in Uppsala Seniors (PIVUS) cohort was used as a discovery sample and The Prospective investigation of Obesity, Energy and Metabolism (POEM) cohort in Uppsala was used as a replicate sample. A factor analysis of FAs measured in plasma was conducted in respective cohort to identify fatty acid patterns related to CVD. Linear regression models between FA patterns and 84 CV proteins measured in respective cohort (samples) were performed. The 81 proteins being found in both samples were meta-analyzed. Traditional CVD risk factors (sex, BMI, diabetes, systolic blood pressure, LDL and HDL-cholesterol and smoking) were adjusted for. Two factors (Low-LA-factor, Fatty-fish-Lipid-factor) (eigenvalue &gt; 1) built up by the same patterns of FAs in both samples and constituted of FAs previously related to CVD were disclosed. The Low-LA-factor was constituted of high loadings of palmitoleic acid (16:1n-7) and oleic acid (18:1n-9) and low loadings of linoleic acid (LA) (18:2n-6), previously recognized as a CVD risk pattern. The Fatty-fish-lipid-factor was constituted of high loading’s ofeicosapentaenoic acid (EPA) (20:5n-3) and docosahexaenoic acid (DHA) (22:6-n-3), previously recognized as cardio preventative FA pattern. In the linear regression meta-analysis, the Low-LA-factor was significantly positively related to 3 plasma proteins. The Fatty-fish-lipid-factor was significantly related negatively with 16 plasma proteins and positively with one plasma protein. The majority of these CV proteins have previously been related to CVD but not connected with plasma fatty acid composition. Thus, these novel results confirm that there are relationships between dietary FA patterns and proteins involved in CVD that potentially partially mediates the association between certain FA patterns and CVD risk. With further research on causality between FAs and these proteins as well as potential mechanisms, these links may be used as a basis for future and more tailored dietary strategies to prevent CVD in high-risk individuals.</p>
----------------------------------------------------------------------
In diva2:1536745 abstract is: <p>This study shows that stress is one of the important issues which may have a negative impactboth in physical and mental effects on a nurse's working life. Poor ergonomics of the hospitaland a poor behavior approach of nurses can cause stress. To reduce these stress factors, thisstudy suggests solutions based on gamification that imply activities to create a good workingenvironment and to reduce the stress to ensure the good health and well-being of the nurseswho work different shifts especially in night shifts. To achieve the purpose, qualitative researchis used as a method. For this study, nurses who work in hospitals were interviewed in differentshifts especially in night shifts with some open-ended questions related to stress andgamification. Based on their answers, trying to evaluate and analyze the problem and findings.Based on the findings, a gamification concept was developed to provide recommendations tohandle the stress and to be motivated and engaged to develop the well-being of life.</p>

corrected abstract:
<p>This study shows that stress is one of the important issues which may have a negative impact both in physical and mental effects on a nurse's working life. Poor ergonomics of the hospital and a poor behavior approach of nurses can cause stress. To reduce these stress factors, this study suggests solutions based on gamification that imply activities to create a good working environment and to reduce the stress to ensure the good health and well-being of the nurses who work different shifts especially in night shifts. To achieve the purpose, qualitative research is used as a method. For this study, nurses who work in hospitals were interviewed in different shifts especially in night shifts with some open-ended questions related to stress and gamification. Based on their answers, trying to evaluate and analyze the problem and findings. Based on the findings, a gamification concept was developed to provide recommendations to handle the stress and to be motivated and engaged to develop the well-being of life.</p>
----------------------------------------------------------------------
In diva2:936117 abstract is: <p>This paper presents the possibility to transfer Musical Instrument Digital Interface messages overBluetooth Low Energy. The main problem was to transmit the messages between two computers inless than 10 milliseconds. Anything above 10 milliseconds could be noticed as a delay by the personplaying or listening to the music. A prototype was written which could transfer Musical InstrumentDigital Interface messages over Bluetooth Low Energy between two Linux-computers together with atesting framework which was used to make measurements. The prototype was written in the languageC++ with the BlueZ library. The time it took for one packet to travel back and forth from the computerswas clocked to get an estimation of the time it took for a packet to travel from one computer to theother. The measured results showed that it was possible to reach the desired time of 10 milliseconds.The results can also be used when considering development of other kind of equipment and/or applicationsthat implements the use of Bluetooth Low Energy.</p>

corrected abstract:
<p>This paper presents the possibility to transfer Musical Instrument Digital Interface messages over Bluetooth Low Energy. The main problem was to transmit the messages between two computers in less than 10 milliseconds. Anything above 10 milliseconds could be noticed as a delay by the person playing or listening to the music. A prototype was written which could transfer Musical Instrument Digital Interface messages over Bluetooth Low Energy between two Linux-computers together with a testing framework which was used to make measurements. The prototype was written in the language C++ with the BlueZ library. The time it took for one packet to travel back and forth from the computers was clocked to get an estimation of the time it took for a packet to travel from one computer to the other. The measured results showed that it was possible to reach the desired time of 10 milliseconds. The results can also be used when considering development of other kind of equipment and/or applications that implements the use of Bluetooth Low Energy.</p>
----------------------------------------------------------------------
In diva2:935124 abstract is: <p>Today, a steel roll is cut within the steel industry in a cutting factory, the steel rollsare divided to smaller part bands with poor control of the burr height quality. Samplesis taken manually, the amount of samples is too low to know the quality of thesteel roll, the steel rolls can be divided up to 150 times and the length will be 30 kilometers. A whole resend for one steel roll costs up against a million SEK and has anegative climatic impact. One software prototype for detection of burr heights witha reference line was programmed. The prototype contained one light sensor, two motors,a PC and one prototype construction. Each task in the software was allocatedan own thread. Operating systems, threads and algorithms was performance testedfor measurement of execution times and period times. The result showed that a burrheight detector where possible to implement. The algorithm could detect burrheights that were too large related to its reference line.</p>

Note: The actual abstract has "kilo meters" - with a line break between the two words.

corrected abstract:
<p>Today, a steel roll is cut within the steel industry in a cutting factory, the steel rolls are divided to smaller part bands with poor control of the burr height quality. Samples is taken manually, the amount of samples is too low to know the quality of the steel roll, the steel rolls can be divided up to 150 times and the length will be 30 kilo meters. A whole resend for one steel roll costs up against a million SEK and has a negative climatic impact. One software prototype for detection of burr heights with a reference line was programmed. The prototype contained one light sensor, two motors, a PC and one prototype construction. Each task in the software was allocated an own thread. Operating systems, threads and algorithms was performance tested for measurement of execution times and period times. The result showed that a burr height detector where possible to implement. The algorithm could detect burr heights that were too large related to its reference line.</p>
----------------------------------------------------------------------
In diva2:1881766 abstract is: <p>The purpose of the project is to improve the management of aids, accessories and spare parts forMedicinteknisk apparatur i hemmet (MAH) by replacing existing Word-files with a user-friendlydatabase application. The goal is to facilitate MAH employees in finding the right accessories andspare parts for various aids with the help of an application.</p><p>The application was developed in Visual Studio Code using the C programming language and wasdesigned to be user-friendly with pushable buttons and easy updating of information. A user testwas performed to ensure usability. The test showed positive feedback, and suggestions forimprovements which included adding images and the ability to add multiple articles at once. Thenew application proved to be more structured and user-friendly than the previous solutions.</p><p>A comparison between other existing databases was carried out, which showed that the newsolution was the most effective for MAH's needs. The application has streamlined the work ofMAH's employees and offers a better management of data, with future improvements proposed tofurther increase functionality. </p>


corrected abstract:
<p>The purpose of the project is to improve the management of aids, accessories and spare parts for Medicinteknisk apparatur i hemmet (MAH) by replacing existing Word-files with a user-friendly database application. The goal is to facilitate MAH employees in finding the right accessories and spare parts for various aids with the help of an application.</p><p>The application was developed in Visual Studio Code using the C programming language and was designed to be user-friendly with pushable buttons and easy updating of information. A user test was performed to ensure usability. The test showed positive feedback, and suggestions for improvements which included adding images and the ability to add multiple articles at once. The new application proved to be more structured and user-friendly than the previous solutions.</p><p>A comparison between other existing databases was carried out, which showed that the new solution was the most effective for MAH's needs. The application has streamlined the work of MAH's employees and offers a better management of data, with future improvements proposed to further increase functionality.</p>
----------------------------------------------------------------------
In diva2:852471 abstract is: <p>In transcriptomics, Cap Analysis of Gene Expression (CAGE) has been recognized as a powerful tool to identify locations of transcription to start sites and to investigate gene transcription activity and promoter usage on the genome-wide scale. However, despite the substantial efforts from the FANTOM consortium, CAGE has not yet become the leding method of choice for the gene expressions investigations, with data processing and analysis methods awaiting further developments in order to be easily and commonly applicable as for instance the currently popular RNA-seq method. In this thesis, we aimed at developing such CAGE-oriented methods with the ultimate goal of enabling CAGE studies and broadening their applicability. In particular, wefocused on the computational prediction of the location of active enhancers and on application of CAGE to extending the reference transcriptome for a non-model species for which the referencegenome is not yet known.</p><p>In terms of enchancer prediction, we predicted the genomic locations of the active enhancerstranscribed in the white adipose tissues (WAT) using two methods termed Enhancer Intersecttion (EI) and Enhancer Prediction (EP). Both methods were developed and optimized around a previously published study which documented enhancer properties of transcribed bidirectional capped RNAs that can be measured via CAGE. Following the eliminiation of the false positivesvia a set of designed filters, 5.976 uniquely identified enhancer candidates were obtained andassessed in the biological context of obesity in WAT. In Terms of reference transcriptome, also two methods of extending the reference transcirptome of the red spotted salamnder (Notoph-thalmus viridescens) were developed and tested, i.e. de novo contigs-based assembly (DMA) and mapping assembly (MA). The two methods yielded comparable results of ca. 4% of the reference transcripts being extended with DMA method outperforming slightly the MA methodin terms of the newly added bases. Alongside the reference transcriptome extension, the lowerthan standard amount of TNA for preparing CAGE library were tested, showing that as low as100 ng total RNA could work well for the library preparation as well as the subsequen data processing and analysis.</p><p>Together, this thesis presents methods and their application results that could be viewed as new directions of the applicability of the CAGE technique  in genome-wide gene expression and regulation studies. The limited size of the available testing data, unfortunately, does not allow drawing statistically valid conclusions, yet the results clearly highlight the potential of CAGE to computationally predict the location of enhancer candidates and to extend the reference transcriptome when the reference genome is not known. We hope that this work will increase awareness of CAGE and will direct its future application to investigate gene regulation via enhancers and to address genomic questions in the studies involving non-model species.</p>


w='transcirptome' val={'c': 'transcriptome', 's': 'diva2:852471'}
w='eliminiation' val={'c': 'elimination', 's': 'diva2:852471', 'n': 'no full text'}
w='enchancer' val={'c': 'enhancer', 's': 'diva2:852471', 'n': 'no full text'}
w='leding' val={'c': 'leading', 's': 'diva2:852471', 'n': 'no full text'}
w='subsequen' val={'c': 'subsequent', 's': 'diva2:852471', 'n': 'no full text'}
w='salamnder' val={'c': 'salamander', 's': 'diva2:852471', 'n': 'no full text'}
w='Intersecttion' val={'c': 'Intersection', 's': 'diva2:852471', 'n': 'no full text'}

I assumed the usual italics for species and Latin words.

corrected abstract:
<p>In transcriptomics, Cap Analysis of Gene Expression (CAGE) has been recognized as a powerful tool to identify locations of transcription to start sites and to investigate gene transcription activity and promoter usage on the genome-wide scale. However, despite the substantial efforts from the FANTOM consortium, CAGE has not yet become the leading method of choice for the gene expressions investigations, with data processing and analysis methods awaiting further developments in order to be easily and commonly applicable as for instance the currently popular RNA-seq method. In this thesis, we aimed at developing such CAGE-oriented methods with the ultimate goal of enabling CAGE studies and broadening their applicability. In particular, we focused on the computational prediction of the location of active enhancers and on application of CAGE to extending the reference transcriptome for a non-model species for which the reference genome is not yet known.</p><p>In terms of enhancer prediction, we predicted the genomic locations of the active enhancers transcribed in the white adipose tissues (WAT) using two methods termed Enhancer Intersection (EI) and Enhancer Prediction (EP). Both methods were developed and optimized around a previously published study which documented enhancer properties of transcribed bidirectional capped RNAs that can be measured via CAGE. Following the elimination of the false positives via a set of designed filters, 5.976 uniquely identified enhancer candidates were obtained and assessed in the biological context of obesity in WAT. In Terms of reference transcriptome, also two methods of extending the reference transcriptome of the red spotted salamander (<em>Notoph-thalmus viridescens</em>) were developed and tested, i.e. <em>de novo</em> contigs-based assembly (DMA) and mapping assembly (MA). The two methods yielded comparable results of ca. 4% of the reference transcripts being extended with DMA method outperforming slightly the MA method in terms of the newly added bases. Alongside the reference transcriptome extension, the lower than standard amount of TNA for preparing CAGE library were tested, showing that as low as 100 ng total RNA could work well for the library preparation as well as the subsequent data processing and analysis.</p><p>Together, this thesis presents methods and their application results that could be viewed as new directions of the applicability of the CAGE technique  in genome-wide gene expression and regulation studies. The limited size of the available testing data, unfortunately, does not allow drawing statistically valid conclusions, yet the results clearly highlight the potential of CAGE to computationally predict the location of enhancer candidates and to extend the reference transcriptome when the reference genome is not known. We hope that this work will increase awareness of CAGE and will direct its future application to investigate gene regulation via enhancers and to address genomic questions in the studies involving non-model species.</p>
----------------------------------------------------------------------
In diva2:1044172 abstract is: <p>Several epidemiological, in vitro and animal studies using full ERβ knockout(fERβKO) mice suggest that ERβ can be a possible target for colorectal cancer (CRC) prevention andtreatment but the mechanisms are not fully understood. In the present study, intestine specific ERβknockout (iERβKO) mice lacking ERβ specifically in the intestinal epithelial cells have been comparedto mice expressing ERβ, and fERβKO mice completely lacking ERβ, before and after azoxymethane(AOM)/dextran sodium sulfate (DSS) carcinogenic treatment. Alcian blue staining, which specificallystains goblet cells, immunohistochemistry (IHC) with an antibody against the proliferative markerBromodeoxyuridine (BrdU), quantitative PCR (qPCR) and an enzyme-linked immunosorbent assay(ELISA) for the detection of inflammatory markers have been performed. The percentage ofproliferative cells increased as a result of intestinal loss of ERβ in the AOM/DSS model and thenumber of goblet cells did show a trend of increase for the fERβKO mice. These results indicate thatERβ in colon epithelial cells is important for cell proliferation/cancer progression and ERβ in othercells, e.g. immune cells is important for inflammation.</p>


I assumed the usual italics for Latin words.

corrected abstract:
<p>Several epidemiological, <em>in vitro</em> and animal studies using full ERβ knockout (fERβKO) mice suggest that ERβ can be a possible target for colorectal cancer (CRC) prevention and treatment but the mechanisms are not fully understood. In the present study, intestine specific ERβ knockout (iERβKO) mice lacking ERβ specifically in the intestinal epithelial cells have been compared to mice expressing ERβ, and fERβKO mice completely lacking ERβ, before and after azoxymethane (AOM)/dextran sodium sulfate (DSS) carcinogenic treatment. Alcian blue staining, which specifically stains goblet cells, immunohistochemistry (IHC) with an antibody against the proliferative marker Bromodeoxyuridine (BrdU), quantitative PCR (qPCR) and an enzyme-linked immunosorbent assay (ELISA) for the detection of inflammatory markers have been performed. The percentage of proliferative cells increased as a result of intestinal loss of ERβ in the AOM/DSS model and the number of goblet cells did show a trend of increase for the fERβKO mice. These results indicate that ERβ in colon epithelial cells is important for cell proliferation/cancer progression and ERβ in other cells, e.g. immune cells is important for inflammation.</p>
----------------------------------------------------------------------
In diva2:895264 abstract is: <p>Central Venous Catheters (CVCs) are used to provide safe administration of chemotherapeutic drugsduring breast cancer treatment. This study focused on how the exposure of the medical drugs influences the material properties providing possible alterations of the inner surface of one type of CVCs,the Subcutaneous Venous Access Port (SVAP).The SVAP is completely implanted under the skin and is associated with fewer complications and restrictions in regards to physical activity for the patient compared  to other eves on the market.</p><p>The polyurethane catheters from the SVAP examined in this study were obtained from a previous study. The samples were exposed to chemotherapy treatments, containing the cytostatic drugs FE100C + Taxotere, following the treatment protocol for patients, over a prolonged period of time, i.e. 18 weeks. One reference sample exposed to NaCl solution according to the same procedure was obtained from another study conducted earlier.</p><p>The surface degradation on the inner catheter surface was analyzed by the Field Emission Scanning Electron Microscopy (FE-SEM). An increase in porosity and crazing of the material were observed throughout the duration of the treatment. This is caused by the leaching of additives/particlesfrom the material, resulting in alteration of the mechanical and chemical properties.</p><p>The effect on the mechanical properties was analyzed by tensile test. After exposure to chemotherapy treatment a decrease in strength and increase in ductility of the material were observed. The uncertainty in the characterization method of the dynamic contact anglemeasurements made it difficult to conclude a prominent trend for the changes in hydrophobicityofthe material. The analysis, however, indicated that exposure to chemotherapy treatment affectedthe surfaceroughness.</p><p>The results in this study clearly indicate the degradation of the polyurethane catheters afterlong­term exposure to chemotherapy drugs,however,further research is required to estimate the extent of the material deterioration.</p>


w='FE100C' val={'c': 'Fe<sub>100</sub>', 's': 'diva2:895264', 'n': 'no full text - there seems to be a 4 and 6 potentially in front of the formula'}

Is "eves" supposed to be "CVCs"?


corrected abstract:
<p>Central Venous Catheters (CVCs) are used to provide safe administration of chemotherapeutic drugs during breast cancer treatment. This study focused on how the exposure of the medical drugs influences the material properties providing possible alterations of the inner surface of one type of CVCs, the Subcutaneous Venous Access Port (SVAP). The SVAP is completely implanted under the skin and is associated with fewer complications and restrictions in regards to physical activity for the patient compared  to other eves on the market.</p><p>The polyurethane catheters from the SVAP examined in this study were obtained from a previous study. The samples were exposed to chemotherapy treatments, containing the cytostatic drugs Fe<sub>100</sub> + Taxotere, following the treatment protocol for patients, over a prolonged period of time, i.e. 18 weeks. One reference sample exposed to NaCl solution according to the same procedure was obtained from another study conducted earlier.</p><p>The surface degradation on the inner catheter surface was analyzed by the Field Emission Scanning Electron Microscopy (FE-SEM). An increase in porosity and crazing of the material were observed throughout the duration of the treatment. This is caused by the leaching of additives/particles from the material, resulting in alteration of the mechanical and chemical properties.</p><p>The effect on the mechanical properties was analyzed by tensile test. After exposure to chemotherapy treatment a decrease in strength and increase in ductility of the material were observed. The uncertainty in the characterization method of the dynamic contact angle measurements made it difficult to conclude a prominent trend for the changes in hydrophobicity of the material. The analysis, however, indicated that exposure to chemotherapy treatment affected the surface roughness.</p><p>The results in this study clearly indicate the degradation of the polyurethane catheters after long­term exposure to chemotherapy drugs, however, further research is required to estimate the extent of the material deterioration.</p>
----------------------------------------------------------------------
In diva2:1463898 abstract is: <p>This study investigated the architecture and composition of eight lower limb musclesin typically developing children using diffusion tensor imaging and mDixontechniques, respectively. Moreover, the correlation between intramuscular fat fractionand force generation capacity was studied. It was observed that intramuscularfat fraction differed in the considered muscles. A positive correlation was observedbetween the maximum voluntary contraction and the intramuscular fat fraction ofgastrocnemius, soleus and tibialis anterior in four subjects, implying that maximumvoluntary contraction increases proportionally with intramuscular fat fraction.This outcome disproves the primary hypothesis which states that lower intramuscularfat fraction corresponds to a higher amount of produced force. It wasconcluded that intramuscular fat fractions do not affect force generation capacityin typically developing children.</p>

corrected abstract:
<p>This study investigated the architecture and composition of eight lower limb muscles in typically developing children using diffusion tensor imaging and mDixon techniques, respectively. Moreover, the correlation between intramuscular fat fraction and force generation capacity was studied. It was observed that intramuscular fat fraction differed in the considered muscles. A positive correlation was observed between the maximum voluntary contraction and the intramuscular fat fraction of gastrocnemius, soleus and tibialis anterior in four subjects, implying that maximum voluntary contraction increases proportionally with intramuscular fat fraction. This outcome disproves the primary hypothesis which states that lower intramuscular fat fraction corresponds to a higher amount of produced force. It was concluded that intramuscular fat fractions do not affect force generation capacity in typically developing children.</p>
----------------------------------------------------------------------
In diva2:1217156 abstract is: <p>In commercial vehicles, where the driver overnights with the engine turned off whilestill consuming electricity, it is important to know how much the battery can bedischarged before reliable engine starting is at risk. The vehicle’s ability to crank theengine, i.e. startability, changes with the vehicle’s ambient temperature and thebatteries state of charge. The aim of this project is therefore to test the startability ofa commercial vehicle and its cranking system’s behaviour at different ambienttemperatures and battery state of charge. Physical startability tests were planned andperformed on a commercial vehicle at different temperatures inside a climatechamber. The results of these tests show the torque of the vehicle’s powertrainincreasing with lowering temperature while the cranking system’s performancedecreases. This decrease in the cranking system’s performance is a result of thebattery’s lowering ability to supply power at lower temperatures.</p>


corrected abstract:
<p>In commercial vehicles, where the driver overnights with the engine turned off while still consuming electricity, it is important to know how much the battery can be discharged before reliable engine starting is at risk. The vehicle’s ability to crank the engine, i.e. startability, changes with the vehicle’s ambient temperature and the batteries state of charge. The aim of this project is therefore to test the startability of a commercial vehicle and its cranking system’s behaviour at different ambient temperatures and battery state of charge. Physical startability tests were planned and performed on a commercial vehicle at different temperatures inside a climate chamber. The results of these tests show the torque of the vehicle’s powertrain increasing with lowering temperature while the cranking system’s performance decreases. This decrease in the cranking system’s performance is a result of the battery’s lowering ability to supply power at lower temperatures.</p>
----------------------------------------------------------------------
title: "Production of ganglioside biosynthetic membrane enzymes for biochemical and functional studies: Expression, purification and crystallization optimization of Thermococcus onnurineus Dolicho l-phosphate mannose synthase, Homosapiens and Branchiostoma floridae Glucosylceramide synthase"
==>    "Production of ganglioside biosynthetic membrane enzymes for biochemical and functional studies:
Expression, purification and crystallization optimization of <em>Thermococcus onnurineus</em> Dolichol-phosphate mannose synthase, <em>Homo sapiens</em> and <em>Branchiostoma floridae</em> Glucosylceramide synthase"


In diva2:1219274 abstract is: <p>Glycolipids play important roles in the biology of prokaryotes and eukaryotes, including humans, and although theyare found on the cell-membrane surface of all eukaryotic cells, not much is known about their biosynthesis. The aim ofthis project was to characterize two enzymes: glucosylceramide synthase (GCS) which is involved in the biosynthesisof glycolipids such as gangliosides that are abundant in the membranes of nerve cells; and dolicholphosphate mannosesynthase (DPMS), involved in the synthesis precursor for protein glycosylation. Both GCS and DPMS have been shown play a role in cancer as well as in congenital disorders of glycosylation, and are therefore interesting targets tostudy from a therapeutic perspective.With the goal to identify a suitable expression system for GCS, the genes coding for GCS from lancelet (Branchiostoma floridae) and human (Homo sapiens) were cloned and tested for expression in Escherichia coliBL21(DE3)T1 and C41(DE3) using different vectors. Cloning into three different vectors was successful and initial expression testing was performed. SDS-PAGE analysis confirmed initial expression of proteins. Although the correctsize of the protein could be confirmed by Western blot, no fluorescence of the GFP-fusion protein could be detected.DPMS from Thermococcus onnurineus (ToDP) was expressed in E. coli C41(DE3) and purified by immobilized metal ion affinity chromatography and gel filtration. Crystallization optimization was performed for ToDP produced from the vector pNIC28-Bsa4 and plate-like crystals were obtained. X-ray intensity data analysis indicated that thesecrystals contained lipid rather than protein. Crystallization screening for ToDP produced from the vector pNIC-CTHO construct was successful. Crystallization screening using the commercially available MemGold-HT96 crystallization kit resulted in initial crystallization that yielded protein crystals that diffracted to 10 °A resolution.</p>

w='°A' val={'c': 'Å', 's': 'diva2:1219274', 'n': 'correct in original'}

corrected abstract:
<p>Glycolipids play important roles in the biology of prokaryotes and eukaryotes, including humans, and although they are found on the cell-membrane surface of all eukaryotic cells, not much is known about their biosynthesis. The aim of this project was to characterize two enzymes: glucosylceramide synthase (GCS) which is involved in the biosynthesis of glycolipids such as gangliosides that are abundant in the membranes of nerve cells; and dolicholphosphate mannose synthase (DPMS), involved in the synthesis precursor for protein glycosylation. Both GCS and DPMS have been shown play a role in cancer as well as in congenital disorders of glycosylation, and are therefore interesting targets to study from a therapeutic perspective.</p><p>With the goal to identify a suitable expression system for GCS, the genes coding for GCS from lancelet (<em>Branchiostoma floridae</em>) and human (<em>Homo sapiens</em>) were cloned and tested for expression in <em>Escherichia coli</em> BL21(DE3)T1 and C41(DE3) using different vectors. Cloning into three different vectors was successful and initial expression testing was performed. SDS-PAGE analysis confirmed initial expression of proteins. Although the correct size of the protein could be confirmed by Western blot, no fluorescence of the GFP-fusion protein could be detected.</p><p>DPMS from <em>Thermococcus onnurineus</em> (ToDP) was expressed in <em>E. coli</em> C41(DE3) and purified by immobilized metal ion affinity chromatography and gel filtration. Crystallization optimization was performed for ToDP produced from the vector pNIC28-Bsa4 and plate-like crystals were obtained. X-ray intensity data analysis indicated that these crystals contained lipid rather than protein. Crystallization screening for ToDP produced from the vector pNIC-CTHO construct was successful. Crystallization screening using the commercially available MemGold-HT96 crystallization kit resulted in initial crystallization that yielded protein crystals that diffracted to 10 Å resolution.</p>
----------------------------------------------------------------------
In diva2:1353874 abstract is: <p>Positioning systems have a vital role in securing safe movement of trains. There are many differenttypes of positioning systems. This thesis is about how axle counters, communications-based traincontrol (CBTC) and different kinds of track circuits operate. It also contains an analysis of AC-trackcircuits and axle counters with RAMS-parameters (Reliability, Availability, Maintenance and Safety)as guide points to make a conclusion of what type of system that best suits for Stockholm PublicTransports railway tracks. Through interviews with experienced persons within the railway industryin Stockholm knowledge of pros and cons of different systems was obtained. A fault tree analyses(FTA) was made for axle counters and track circuits to visualize potentially hazardous situations.Failure statistics were produced to show failure frequency for one track with axle counters and onetrack with track circuits. A clear result was not shown but it can be concluded that sources of failurethat are prone to track circuit systems can be avoided using axle counters. What became evident isthat the management need to standardize to a fewer amount of different positioning systems. Itwould make it easier to find available personnel with the required skills for doing maintenance. Thiswould also have a benefit when securing maintenance supplies.</p>

corrected abstract:
<p>Positioning systems have a vital role in securing safe movement of trains. There are many different types of positioning systems. This thesis is about how axle counters, communications-based train control (CBTC) and different kinds of track circuits operate. It also contains an analysis of AC-track circuits and axle counters with RAMS-parameters (Reliability, Availability, Maintenance and Safety) as guide points to make a conclusion of what type of system that best suits for Stockholm Public Transports railway tracks. Through interviews with experienced persons within the railway industry in Stockholm knowledge of pros and cons of different systems was obtained. A fault tree analyses (FTA) was made for axle counters and track circuits to visualize potentially hazardous situations. Failure statistics were produced to show failure frequency for one track with axle counters and one track with track circuits. A clear result was not shown but it can be concluded that sources of failure that are prone to track circuit systems can be avoided using axle counters. What became evident is that the management need to standardize to a fewer amount of different positioning systems. It would make it easier to find available personnel with the required skills for doing maintenance. This would also have a benefit when securing maintenance supplies.</p>
----------------------------------------------------------------------
In diva2:1873924 abstract is: <p>Atopic dermatitis (AD) and other atopic diseases are strongly related to skinbarrier dysfunction, a biomeasure which has limited and unsatisfactory assessmenttechniques. Machine learning (ML) powered electrical impedance spectroscopy(EIS) has been shown to differentiate defective barrier function in mice and adults. Techniques such as principal component analysis (PCA) andsupport vector classifiers (SVC) can be used as ML tools to evaluate EIS measurements.</p><p>EIS measurements taken on unaffected skin of children aged between 4 monthsand 3 years were collected and analysed using the children’s AD status. Measurements were grouped into one of four groups based on this AD status; No AD (No AD was developed up until 2 years of age), Pre AD (measurements takenbefore the onset of AD), AD remission (measurements taken after the onset ofAD) and AD flare (measurements taken during active AD). A SVC model was trained on the raw EIS measurement data to distinguish measurements from twoof the binarized AD status groups; AD flare and No AD. An additional SVC model was trained on the No AD group, distinguishing measurements based onbinarized age groups: measurements taken at 4 months against measurements taken at 3 years of age.</p><p>The AD model tested on AD flare against No AD within the test set yielded AUC of 0.92, with a sensitivity and specificity of 89.29% and 88.89% respectively. When testing on all groups of the test set, Pre AD and AD remission groups had group means between the AD flare and No AD groups. No data bias against age was detected in the model. The results of the age model showed that age could be chronologically identified by the age model.</p><p>The AD model was able to differentiate active AD children from children never experiencing AD symptoms on visually unaffected skin and in turn detecting skin barrier dysfunction. Separate studies would need to be conducted to test the predictive power and external validity of the model. Age is a significant factor to consider when designing ML models using EIS data in children, with proper balance in the training set a data bias within the model can be avoided. EIS is a versatile technique due to its data rich nature.</p><p>Machine learning powered electrical impedance spectroscopy measurements are able to detect skin barrier dysfunction. Age is a significant factor when measuring EIS on children, but can be managed.</p>


corrected abstract:
<p>Atopic dermatitis (AD) and other atopic diseases are strongly related to skin barrier dysfunction, a biomeasure which has limited and unsatisfactory assessment techniques. Machine learning (ML) powered electrical impedance spectroscopy (EIS) has been shown to differentiate defective barrier function in mice and adults. Techniques such as principal component analysis (PCA) and support vector classifiers (SVC) can be used as ML tools to evaluate EIS measurements.</p><p>EIS measurements taken on unaffected skin of children aged between 4 months and 3 years were collected and analysed using the children’s AD status. Measurements were grouped into one of four groups based on this AD status; No AD (No AD was developed up until 2 years of age), Pre AD (measurements taken before the onset of AD), AD remission (measurements taken after the onset of AD) and AD flare (measurements taken during active AD). A SVC model was trained on the raw EIS measurement data to distinguish measurements from two of the binarized AD status groups; AD flare and No AD. An additional SVC model was trained on the No AD group, distinguishing measurements based on binarized age groups: measurements taken at 4 months against measurements taken at 3 years of age.</p><p>The AD model tested on AD flare against No AD within the test set yielded AUC of 0.92, with a sensitivity and specificity of 89.29% and 88.89% respectively. When testing on all groups of the test set, Pre AD and AD remission groups had group means between the AD flare and No AD groups. No data bias against age was detected in the model. The results of the age model showed that age could be chronologically identified by the age model.</p><p>The AD model was able to differentiate active AD children from children never experiencing AD symptoms on visually unaffected skin and in turn detecting skin barrier dysfunction. Separate studies would need to be conducted to test the predictive power and external validity of the model. Age is a significant factor to consider when designing ML models using EIS data in children, with proper balance in the training set a data bias within the model can be avoided. EIS is a versatile technique due to its data rich nature.</p><p>Machine learning powered electrical impedance spectroscopy measurements are able to detect skin barrier dysfunction. Age is a significant factor when measuring EIS on children, but can be managed.</p>
----------------------------------------------------------------------
In diva2:1451599 abstract is: <p>Abstract</p><p>Dating apps are continuously becoming a larger part of the social media market.Like any social media app, dating apps utilize a large amount of personaldata. This thesis analyzes two dating apps and how they handle personal informationfrom a security and privacy standpoint. This was done by conceptualizinga threat model and then validating the threat through penetration testingon both of the apps in an attempt to find security vulnerabilities. This analysisproves that there is a substantial difference in whether or not app developerstake security seriously or not. Itwas found that in one of the two apps analyzed,gaining access to personal data was particularly more trivial than expected, asTLS or other encryption were not implemented and server-side authorizationwas lacking in important app features like the one-to-one user chat.</p><p>Keywords – Penetration testing, ethical hacking, dating apps, Android, reverseengineering, threat modeling, risk rating</p>

corrected abstract:
<p>Dating apps are continuously becoming a larger part of the social media market. Like any social media app, dating apps utilize a large amount of personal data. This thesis analyzes two dating apps and how they handle personal information from a security and privacy standpoint. This was done by conceptualizing a threat model and then validating the threat through penetration testing on both of the apps in an attempt to find security vulnerabilities. This analysis proves that there is a substantial difference in whether or not app developers take security seriously or not. It was found that in one of the two apps analyzed, gaining access to personal data was particularly more trivial than expected, as TLS or other encryption were not implemented and server-side authorization was lacking in important app features like the one-to-one user chat.</p>
----------------------------------------------------------------------
In diva2:1108979 abstract is: <p>This thesis has been carried out on behalf of the consulting company Seavus to re-duce cable usage in their office. Smart hubs (i.e. a central control unit) are made tomake usage of devices more effective.One problem is to decide between Bluetooth and Wi-Fi for communication betweenthe user and the smart hub. The choice may depend upon several factors such asbandwidth (throughput), jitter (variation in delay) and packet loss, which are im-portant parameters for assessing the quality of the communication channel.The HW-platform Raspberry Pi and compatible software was used as a measure-ment tool to test Bluetooth and Wi-Fi in different environments.The result showed that Wi-Fi is best suited for communication systems that requirehigh bandwidth and low jitter, and where high amount of packet loss is tolerable.Bluetooth is best suited for communication systems where low bandwidth and highjitter is tolerable, and minimal packet losses preferred.</p>


corrected abstract:
<p>This thesis has been carried out on behalf of the consulting company Seavus to reduce cable usage in their office. Smart hubs (i.e. a central control unit) are made to make usage of devices more effective.</p><p>One problem is to decide between Bluetooth and Wi-Fi for communication between the user and the smart hub. The choice may depend upon several factors such as bandwidth (throughput), jitter (variation in delay) and packet loss, which are important parameters for assessing the quality of the communication channel.</p><p>The HW-platform Raspberry Pi and compatible software was used as a measurement tool to test Bluetooth and Wi-Fi in different environments.</p><p>The result showed that Wi-Fi is best suited for communication systems that require high bandwidth and low jitter, and where high amount of packet loss is tolerable. Bluetooth is best suited for communication systems where low bandwidth and high jitter is tolerable, and minimal packet losses preferred.</p>
----------------------------------------------------------------------
In diva2:1451552 abstract is: <p>Abstract</p><p>A prototype for a smart shoe has been developed at the Royal Institute ofTechnology. It consisted of an Internet of Things (IoT) device which was builtand integrated with a radio module to wirelessly transmit packets. With thissmart-shoe as a base, this thesis attempts to create a system that has the ability torecognize certain foot gestures, and utilizing them to trigger a radio transmissionwhen faced with a danger to hopefully receive the aid that is necessary. Thedangers could for instance be in the event of abuse, kidnapping or robbery. Inorder to accomplish this, machine learning (ML) models were used to predictwhat activity or gesture was executed. For the radio communication, LoRa(Long Range) was utilized as the modulation technique and enabled wirelesstransmissions.</p><p>A system was developed that can detect two gestures with a high accuracy.Whenever a gesture is performed, the radio module is activated and generatesa transmission to another unit.</p><p>Keywords</p><p>Internet of Things, embedded systems, radio communication, LoRa, machinelearning, foot gestures, recognitioniv</p>

w='recognitioniv' val={'c': 'recognition', 's': 'diva2:1451552', 'n': 'correct in original'}

corrected abstract:
<p>A prototype for a smart shoe has been developed at the Royal Institute of Technology. It consisted of an Internet of Things (IoT) device which was built and integrated with a radio module to wirelessly transmit packets. With this smart-shoe as a base, this thesis attempts to create a system that has the ability to recognize certain foot gestures, and utilizing them to trigger a radio transmission when faced with a danger to hopefully receive the aid that is necessary. The dangers could for instance be in the event of abuse, kidnapping or robbery. In order to accomplish this, machine learning (ML) models were used to predict what activity or gesture was executed. For the radio communication, LoRa (Long Range) was utilized as the modulation technique and enabled wireless transmissions.</p><p>A system was developed that can detect two gestures with a high accuracy. Whenever a gesture is performed, the radio module is activated and generates a transmission to another unit.</p>
----------------------------------------------------------------------
In diva2:1438165 abstract is: <p>Plastics and composites have been a growing industry for decades, and the always growing worldand new technologies demand even greater amounts of polymeric composites for a variety ofapplications. Nowadays the negative environmental aspects of polymer production and plastic waste are known, but despite that crude oil is still the primary material for most polymers. Finding biobased materials with sufficient properties to replace the fully synthetic ones is crucial in sustainable development. This master’s thesis studies both glass fiber and microcrystalline cellulose reinforcedbio-oil based polyamide, how they could be compatibilized, the mechanical properties and applicability for additive manufacturing. Compatibilization is an important aspect when two compounds are mixed to make a composite. A proper compatibilizer will enhance the interfacial adhesion between the reinforcement and matrix, thus increasing the mechanical properties of the material. The glass fiber/polyamide11 composite was compatibilized with vinyltrimethoxysilane, and the microcrystalline cellulose/polyamide11 composite was compatibilized with 4,4'-diphenylmethane diisocyanate. Both composites were analyzed to obtain information about thermal, mechanical, and rheological properties. The surface and fracture morphology are examined, as well.The results indicate that reinforcing resulted to enhanced mechanical properties, even though the desired compatibilization was not acquired in the experiment. The most encouraging result was that the bio-based cellulose reinforcement enhanced mechanical properties, by visual examination thefully bio-based polymeric composite was found to be more ductile than the glass fiber reinforcedone. For future prospect, there are few issues to be addressed and overcome for these materials to bemade suitable for additive manufacturing. The key is finding a compatibilizer that can withstand high processing temperatures repeatedly. Maintaining uniform properties requires proper dispersion of reinforcement, which is achieved by optimizing the manufacturing method. In addition, cellulose is prone to thermal degradation, so the processing temperatures for both reinforcement and matrix should be considered carefully.</p>


corrected abstract:
<p>Plastics and composites have been a growing industry for decades, and the always growing world and new technologies demand even greater amounts of polymeric composites for a variety of applications. Nowadays the negative environmental aspects of polymer production and plastic waste are known, but despite that crude oil is still the primary material for most polymers. Finding biobased materials with sufficient properties to replace the fully synthetic ones is crucial in sustainable development. This master’s thesis studies both glass fiber and microcrystalline cellulose reinforced bio-oil based polyamide, how they could be compatibilized, the mechanical properties and applicability for additive manufacturing.</p><p>Compatibilization is an important aspect when two compounds are mixed to make a composite. A proper compatibilizer will enhance the interfacial adhesion between the reinforcement and matrix, thus increasing the mechanical properties of the material. The glass fiber/polyamide11 composite was compatibilized with vinyltrimethoxysilane, and the microcrystalline cellulose/polyamide11 composite was compatibilized with 4,4'-diphenylmethane diisocyanate. Both composites were analyzed to obtain information about thermal, mechanical, and rheological properties. The surface and fracture morphology are examined, as well.</p><p>The results indicate that reinforcing resulted to enhanced mechanical properties, even though the desired compatibilization was not acquired in the experiment. The most encouraging result was that the bio-based cellulose reinforcement enhanced mechanical properties, by visual examination the fully bio-based polymeric composite was found to be more ductile than the glass fiber reinforced one.</p><p>For future prospect, there are few issues to be addressed and overcome for these materials to be made suitable for additive manufacturing. The key is finding a compatibilizer that can withstand high processing temperatures repeatedly. Maintaining uniform properties requires proper dispersion of reinforcement, which is achieved by optimizing the manufacturing method. In addition, cellulose is prone to thermal degradation, so the processing temperatures for both reinforcement and matrix should be considered carefully.</p>
----------------------------------------------------------------------
In diva2:1801810 abstract is: <p>This Bachelor thesis presents an in-depth investigation into the effects of RogueAccess Point interference within Internet of Things networks. The study focuses onthe impact of rogue APs on the modulation and coding scheme indices, round triptime, and overall network performance. The presence of a rogue AP was found toshift devices from dual-stream to single-stream operation, causing a decrease in themodulation and coding scheme indices and data rates. Additionally, a significantincrease in round trip time was observed, emphasizing the detrimental impact ofrogue AP interference on network latency. The insights gained from this researchcontribute to a greater understanding of the challenges posed by rogue APinterference. This deeper comprehension paves the way for devising effectivestrategies to mitigate these impacts, thereby enhancing the reliability, security, andperformance of IoT networks.</p>


corrected abstract:
<p>This Bachelor thesis presents an in-depth investigation into the effects of Rogue Access Point interference within Internet of Things networks. The study focuses on the impact of rogue APs on the modulation and coding scheme indices, round trip time, and overall network performance. The presence of a rogue AP was found to shift devices from dual-stream to single-stream operation, causing a decrease in the modulation and coding scheme indices and data rates. Additionally, a significant increase in round trip time was observed, emphasizing the detrimental impact of rogue AP interference on network latency. The insights gained from this research contribute to a greater understanding of the challenges posed by rogue AP interference. This deeper comprehension paves the way for devising effective strategies to mitigate these impacts, thereby enhancing the reliability, security, and performance of IoT networks.</p>
----------------------------------------------------------------------
In diva2:1686451 abstract is: <p>When removal of primary tumors is performed by surgery, the surgeon andthe patient would benefit from using intraoperative feedback to evaluate thetumor resection margin. If intraoperative feedback is not provided there isadditional risk of treatment of the tumor. With improvements to the fixationtime of soft tissue, phase-contrast computed tomography (PC-CT) could allow3D intraoperative feedback to the surgeons.The focus of this project has been to develop a method for measuring fixationtime on soft tissue samples and examine ways to decrease the required fixationtime. Furthermore, evaluation of fixatives to achieve image contrast that wouldbe sufficient to accomplish the possibility of intraoperative tumor feedback, inPC-CT images has been performed in this project.</p>

corrected abstract:
<p>When removal of primary tumors is performed by surgery, the surgeon and the patient would benefit from using intraoperative feedback to evaluate the tumor resection margin. If intraoperative feedback is not provided there is additional risk of treatment of the tumor. With improvements to the fixation time of soft tissue, phase-contrast computed tomography (PC-CT) could allow 3D intraoperative feedback to the surgeons.</p><p>The focus of this project has been to develop a method for measuring fixation time on soft tissue samples and examine ways to decrease the required fixation time. Furthermore, evaluation of fixatives to achieve image contrast that would be sufficient to accomplish the possibility of intraoperative tumor feedback, in PC-CT images has been performed in this project.</p>
----------------------------------------------------------------------
In diva2:1080331 abstract is: <p>This diploma work has been carried out on behalf of Stegia AB. Stegia manufacturesand sells linear motors that are tested in a test bench before shipped to thecustomer. Today there is a need for a test bench that collects test results from theperformed tests. The company would like to lower the risk of falsely made tests,and to make the test bench easier for the production staff to use. At this moment,the test of the linear motors is made up of several manual tasks before the test iscompleted. This increases the risk of wrong results because of incorrectly madetests, with no traceability of the results. The goal with this diploma work is to lowerthe risk of incorrect test result by automating the testing process for the productionstaff. The result is presented as a prototype, that should increase the reliability ofthe test result of the motors. The result is then validated and analyzed in this report.</p>

corrected abstract:
<p>This diploma work has been carried out on behalf of Stegia AB. Stegia manufactures and sells linear motors that are tested in a test bench before shipped to the customer. Today there is a need for a test bench that collects test results from the performed tests. The company would like to lower the risk of falsely made tests, and to make the test bench easier for the production staff to use. At this moment, the test of the linear motors is made up of several manual tasks before the test is completed. This increases the risk of wrong results because of incorrectly made tests, with no traceability of the results. The goal with this diploma work is to lower the risk of incorrect test result by automating the testing process for the production staff. The result is presented as a prototype, that should increase the reliability of the test result of the motors. The result is then validated and analyzed in this report.</p>
----------------------------------------------------------------------
In diva2:1738689 abstract is: <p>The Network Operation Center (NOC) at Transtema Network Service monitors andfilters incoming alarms from different types of networks. When a disturbance occursin a network, among the measures taken is to inform customers about it. This processis not automated, which means that people working in the NOC must do it manually.At the beginning of this study, a literature study of previous work has been done inorder to collect data and be able to increase knowledge about how the automation ofthe process for sending information to a customer is carried out. Interviews and existing documentation helped to collect data and to understand the process. Afterthat, a software prototype was developed to automate the process of sending information to a customer about the current state of the network. In addition, pre-automation and post-automation tests of the process were carried out to compare themand analyze the results, which showed that the NOC-department can increase theefficiency and benefit of both the company and the customers with the automationof the process</p>

corrected abstract:
<p>The Network Operation Center (NOC) at Transtema Network Service monitors and filters incoming alarms from different types of networks. When a disturbance occurs in a network, among the measures taken is to inform customers about it. This process is not automated, which means that people working in the NOC must do it manually. At the beginning of this study, a literature study of previous work has been done in order to collect data and be able to increase knowledge about how the automation of the process for sending information to a customer is carried out. Interviews and existing documentation helped to collect data and to understand the process. After that, a software prototype was developed to automate the process of sending information to a customer about the current state of the network. In addition, pre-automation and post-automation tests of the process were carried out to compare them and analyze the results, which showed that the NOC-department can increase the efficiency and benefit of both the company and the customers with the automation of the process.</p>
----------------------------------------------------------------------
In diva2:801742 abstract is: <p>Alzheimer's disease (AD) and Lewy body dementia (DLB) are mong the most common form of dementia. Neurodegenerative diseases affect many people worldwide, and due to the lack in therapeutic targets, as well as biomarkers for diagnostics, there is a need for novel biomarkers for monitoring disease progression and especially early diagnose. A list of disease associated, brain enriched genes served as a starting point for the initiative phase in the search of a protein as a potntial biomarker. The method used was multiplex- immunofluorescence, in which a limitationtoday is the availability of primary antibodies raised in different host species. Here, heatinactivation as elution to eliminate cross-reaction of antibodies was investigated using tyramide signal amplification in an automated stainer. As proof of principle, sequential staining wasperformed on human cortex, with four rabbit antibodies targeting different morphological structures. There was no cross-rection between secondary antibodies and the protocol designed was used throughout the study. Protein distribution pattern was analysed on a tissue microarray(TMA) composed of human temporal cortex tissue obtained from 29 subjects; ten from patientsdiagnosed with AD, ten with DLB and nine controls, two cores from each case. IF was applied on proteins selected from the list provided, to assess how they relate to disease progression and evaluate TMA as approach. TMA was regarded as a useful tool for initiative step in screening for proteins as biomarkers, as long as representative tissue for cores is carefully selected.There are challenges important to address in using the sequential staining method and the TMA, but as a whole, a new approach for qualitative analysis of protein distribtuion was proposed. With thisapproach, the full potential of the antibodies generated within the HPA project, and other primary antibodies derived from the same species, can be combined with desired fluorophore within the same staining protocol, in a high throughput screening for proteins as a biomarker.</p>

w='distribtuion' val={'c': 'distribution', 's': 'diva2:801742', 'n': 'no full text'}
w='cross-rection' val={'c': 'cross-reaction', 's': 'diva2:801742', 'n': 'no full text'}
w='mong' val={'c': 'among', 's': 'diva2:801742', 'n': 'no full text'}
w='potntial' val={'c': 'potential', 's': 'diva2:801742', 'n': 'no full text'}

corrected abstract:
<p>Alzheimer's disease (AD) and Lewy body dementia (DLB) are among the most common form of dementia. Neurodegenerative diseases affect many people worldwide, and due to the lack in therapeutic targets, as well as biomarkers for diagnostics, there is a need for novel biomarkers for monitoring disease progression and especially early diagnose. A list of disease associated, brain enriched genes served as a starting point for the initiative phase in the search of a protein as a potential biomarker. The method used was multiplex- immunofluorescence, in which a limitation today is the availability of primary antibodies raised in different host species. Here, heatin activation as elution to eliminate cross-reaction of antibodies was investigated using tyramide signal amplification in an automated stainer. As proof of principle, sequential staining was performed on human cortex, with four rabbit antibodies targeting different morphological structures. There was no cross-reaction between secondary antibodies and the protocol designed was used throughout the study. Protein distribution pattern was analysed on a tissue microarray (TMA) composed of human temporal cortex tissue obtained from 29 subjects; ten from patients diagnosed with AD, ten with DLB and nine controls, two cores from each case. IF was applied on proteins selected from the list provided, to assess how they relate to disease progression and evaluate TMA as approach. TMA was regarded as a useful tool for initiative step in screening for proteins as biomarkers, as long as representative tissue for cores is carefully selected. There are challenges important to address in using the sequential staining method and the TMA, but as a whole, a new approach for qualitative analysis of protein distribution was proposed. With this approach, the full potential of the antibodies generated within the HPA project, and other primary antibodies derived from the same species, can be combined with desired fluorophore within the same staining protocol, in a high throughput screening for proteins as a biomarker.</p>

----------------------------------------------------------------------
In diva2:854019 abstract is: <p>Glycosyltransferases (GT) are enzymes that synthesise vital complexes consisting of one or more sugar molecule units. A large number of GTs are integral membrane proteins. This means they can be found embedded in the many different membrane structures of cells. One example is the bacterial glycosyltransferase XCGT.</p><p>Few threee-dimensional structures have been solved for membrane GTs because of the technical difficultires that arie in work with integral membrane proteins. As a result there is only limited information available on the interplay of structure and function for membrane GTs. The aim of thisproject is to characterise XCGT biochemically and optimise conditions for increased protein stability and well.ordered, crystallisation. This should pave the road to a successfully determined crystalstructure. This thesis reports the project before publication in a peer-reviewed journal and for this reason some information is not disclosed. Yet, specific protein characteristics and methodological details are presented and supported by theory and experimental data.</p><p>The thesis opens with a theoretical review of the family of glycosyltransferases and methodology for work with membrane proteins. In addition, a background is provided for later discussion of the effect of certaindetergents on protein stability. Choice of detergent, additives and crystallisation format are shown to have critical influence on crystallisation. An outlook on alternative techniques that may improve the chances to solve XCGT's crystal structure forms the closing of the thesis.</p><p>Throughout the project, methods have been refined and results were obtained that will havesignificant importance for the continued efforts to determine the crystal structure of XGCT, and themethodological findings can even be extrapolated to the work with membrane proteins in general.</p>


w='arie' val={'c': 'are', 's': 'diva2:854019', 'n': 'no full text'}
w='difficultires' val={'c': 'difficulties', 's': 'diva2:854019', 'n': 'no full text'}
w='threee-dimensional' val={'c': 'three-dimensional', 's': 'diva2:854019', 'n': 'no full text'}

corrected abstract:
<p>Glycosyltransferases (GT) are enzymes that synthesise vital complexes consisting of one or more sugar molecule units. A large number of GTs are integral membrane proteins. This means they can be found embedded in the many different membrane structures of cells. One example is the bacterial glycosyltransferase XCGT.</p><p>Few three-dimensional structures have been solved for membrane GTs because of the technical difficulties that are in work with integral membrane proteins. As a result there is only limited information available on the interplay of structure and function for membrane GTs. The aim of this project is to characterise XCGT biochemically and optimise conditions for increased protein stability and well. ordered, crystallisation. This should pave the road to a successfully determined crystal structure. This thesis reports the project before publication in a peer-reviewed journal and for this reason some information is not disclosed. Yet, specific protein characteristics and methodological details are presented and supported by theory and experimental data.</p><p>The thesis opens with a theoretical review of the family of glycosyltransferases and methodology for work with membrane proteins. In addition, a background is provided for later discussion of the effect of certain detergents on protein stability. Choice of detergent, additives and crystallisation format are shown to have critical influence on crystallisation. An outlook on alternative techniques that may improve the chances to solve XCGT's crystal structure forms the closing of the thesis.</p><p>Throughout the project, methods have been refined and results were obtained that will have significant importance for the continued efforts to determine the crystal structure of XGCT, and the methodological findings can even be extrapolated to the work with membrane proteins in general.</p>
----------------------------------------------------------------------
In diva2:721188 abstract is: <p>Acid catalyzed liquefaction of brown paper handtowel was performed using a mixture of diethylene glycol and glycerol andaddition of minute amount of p-toluenesulphonic acid as a catalyst. TheLiquefied brown paper was used as a polyhydroxy alcohol during the estersynthesis as large number of hydroxyl groups is available in liquefied brownpaper products. Esterification was performed by using hexanoic acid along withdibutyl tin (IV) oxide as a catalyst. The product was characterized by usingFTIR, SEC and SEM. PVC films were prepared by solution casting. They wereplasticized by the prepared liquefied brown paper ester and by traditionaldiisooctyl phthalate. Comparison of both plasticizers on the basis ofmechanical, thermal properties and plasticizer migration was elaborated.</p>


I think that it should be "dibutyltin (IV) oxide" and not have a space before "tin". See for example: https://www.sigmaaldrich.com/SE/en/product/aldrich/183083

corrected abstract:
<p>Acid catalyzed liquefaction of brown paper hand towel was performed using a mixture of diethylene glycol and glycerol and addition of minute amount of p-toluenesulphonic acid as a catalyst. The Liquefied brown paper was used as a polyhydroxy alcohol during the ester synthesis as large number of hydroxyl groups is available in liquefied brownpaper products. Esterification was performed by using hexanoic acid along with dibutyltin (IV) oxide as a catalyst. The product was characterized by using FTIR, SEC and SEM. PVC films were prepared by solution casting. They were plasticized by the prepared liquefied brown paper ester and by traditional diisooctyl phthalate. Comparison of both plasticizers on the basis of mechanical, thermal properties and plasticizer migration was elaborated.</p>
----------------------------------------------------------------------
In diva2:1763068 abstract is: <p>This report analyzes and tests various methods aimed at distinguishinghuman-generated solutions to tasks and texts from those generated by artificialintelligence. Recently the use of artificial intelligence has seen a significantincrease, especially among students. The purpose of this study is to determinewhether it is currently possible to detect if a college student in electricalengineering is using AI to cheat. In this report, solutions to tasks and textsgenerated by the program ChatGPT are tested using a general methodology andexternal AI-based tools. The research covers the areas of mathematics,programming and written text. The results of the investigation suggest that it is notpossible to detect cheating with the help of an AI in the subjects of mathematicsand programming. In the case of text, cheating by using an AI can be detected tosome extent.</p>

corrected abstract:
<p>This report analyzes and tests various methods aimed at distinguishing human-generated solutions to tasks and texts from those generated by artificial intelligence. Recently the use of artificial intelligence has seen a significant increase, especially among students. The purpose of this study is to determine whether it is currently possible to detect if a college student in electrical engineering is using AI to cheat. In this report, solutions to tasks and texts generated by the program ChatGPT are tested using a general methodology and external AI-based tools. The research covers the areas of mathematics, programming and written text. The results of the investigation suggest that it is not possible to detect cheating with the help of an AI in the subjects of mathematics and programming. In the case of text, cheating by using an AI can be detected to some extent.</p>
----------------------------------------------------------------------
In diva2:1383922 abstract is: <p>The interior of heavy goods vehicles (HGVs) differs from passenger cars. Both the steering wheel and the occupant are positioned differently in a HGV and increases the risk of steering wheel rim impacts. Such impact scenarios are relatively unexplored compared to passenger car safety studies that are more prevalent within the field of injury biomechanics. The idea with using human body models (HBMs) is to complement current crash test dummies with biomechanical data. Furthermore, the biofidelity of a crash dummy for loading similar to a steering wheel rimimpact is relatively unstudied and especially to different rib levels. Therefore, the aim with this thesis was to evaluate HGV occupant thoracic response between THUMS v4.0 and Hybrid III (H3) during steering wheel rim impacts with respect to different rib levels (level 1-2, 3-4, 6-7, 7-8, 9-10) with regards to ribs, aorta, liver, and spleen.</p><p>To the author’s best knowledge, use of local injury risk functions for thoracic injuries is fairly rare compared to the predominant usage of global injury criteria that mainly predicts the most commonthoracic injury risk, i.e. rib fractures. Therefore, local injury criteria using experimental test datahave been developed for the ribs and the organs. The measured parameters were chest deflectionand steering wheel to thorax contact force on a global level, whilst 1st principal Green-Lagrangestrains was assessed for the rib and the organ injury risk. The material models for the liver and</p><p>the spleen were remodelled using an Ogden material model based on experimental stress-strain data to account for hyperelasticity. Rate-dependency was included by iteration of viscoelastic parameters. The contact modelling of the organs was changed from a sliding contact to a tied contact to minimize unrealistic contact separations during impact.</p><p>The results support previous findings that H3 needs additional instrumentation to accurately</p><p>register chest deflection for rib levels beyond its current range, namely at ribs 1-2, 7-8, and 9-10. For THUMS, the chest deflection were within reasonable values for the applied velocities, but there were no definite injury risk. Fact is, the global injury criteria might overpredict the AIS3 injury risk (rib fractures) for rib level 1-2, 7-8, and 9-10. The rib strains could not be correlated with the measured chest deflections. This was explained by the unique localized loading characterized by pure steering wheel rim impact that mainly affected the sternum and the rib cartilage while minimizing rib deformation. The organ strains indicate some risk of rupture where the spleen deforms the most at rib levels 3-4 and 6-7, and the liver and the aorta at rib levels 6-7 and 7-8.</p><p>This study provides a framework for complementing H3 with THUMS for HGV occupant safety</p><p>with emphasis on the importance of using local injury criteria for functional injury prediction,</p><p>i.e. prediction of injury risk using parameters directly related to rib fracture or organ rupture.</p><p>Local injury criteria are thus a powerful safety assessment tool as it is independent on exterior loading such as airbag, steering wheel hub, or seat belt loading. It was noticed that global injury criteria with very localized impacts such as rim impacts have not been studied and will affect rib fracture risk differently than what has been studied using airbag or seat belt restraints. However, improvements are needed to accurately predict thoracic injury risk at a material level by finding more data for the local injury risk functions.</p><p>Conclusively, it is clear that Hybrid III has insufficient instrumentation and is in need of upgrades to register chest deflections at multiple rib levels. Furthermore, the following are needed: better understanding of global injury criteria specific for HGV occupant safety evaluation, more data for age-dependent (ribs) and rate-dependent (organs) injury risk functions, a tiebreak contact with tangential sliding for better organ kinematics during impacts, and improving the biofidelity of the material models using data from tissue level experiments.</p>

corrected abstract:
<p>The interior of heavy goods vehicles (HGVs) differs from passenger cars. Both the steering wheel and the occupant are positioned differently in a HGV and increases the risk of steering wheel rim impacts. Such impact scenarios are relatively unexplored compared to passenger car safety studies that are more prevalent within the field of injury biomechanics. The idea with using human body models (HBMs) is to complement current crash test dummies with biomechanical data. Furthermore, the biofidelity of a crash dummy for loading similar to a steering wheel rim impact is relatively unstudied and especially to different rib levels. Therefore, the aim with this thesis was to evaluate HGV occupant thoracic response between THUMS v4.0 and Hybrid III (H3) during steering wheel rim impacts with respect to different rib levels (level 1-2, 3-4, 6-7, 7-8, 9-10) with regards to ribs, aorta, liver, and spleen.</p><p>To the author’s best knowledge, use of local injury risk functions for thoracic injuries is fairly rare compared to the predominant usage of global injury criteria that mainly predicts the most common thoracic injury risk, i.e. rib fractures. Therefore, local injury criteria using experimental test data have been developed for the ribs and the organs. The measured parameters were chest deflection and steering wheel to thorax contact force on a global level, whilst 1st principal Green-Lagrange strains was assessed for the rib and the organ injury risk. The material models for the liver and the spleen were remodelled using an Ogden material model based on experimental stress-strain data to account for hyperelasticity. Rate-dependency was included by iteration of viscoelastic parameters. The contact modelling of the organs was changed from a sliding contact to a tied contact to minimize unrealistic contact separations during impact.</p><p>The results support previous findings that H3 needs additional instrumentation to accurately register chest deflection for rib levels beyond its current range, namely at ribs 1-2, 7-8, and 9-10. For THUMS, the chest deflection were within reasonable values for the applied velocities, but there were no definite injury risk. Fact is, the global injury criteria might overpredict the AIS3 injury risk (rib fractures) for rib level 1-2, 7-8, and 9-10. The rib strains could not be correlated with the measured chest deflections. This was explained by the unique localized loading characterized by pure steering wheel rim impact that mainly affected the sternum and the rib cartilage while minimizing rib deformation. The organ strains indicate some risk of rupture where the spleen deforms the most at rib levels 3-4 and 6-7, and the liver and the aorta at rib levels 6-7 and 7-8.</p><p>This study provides a framework for complementing H3 with THUMS for HGV occupant safety with emphasis on the importance of using local injury criteria for functional injury prediction, i.e. prediction of injury risk using parameters directly related to rib fracture or organ rupture. Local injury criteria are thus a powerful safety assessment tool as it is independent on exterior loading such as airbag, steering wheel hub, or seat belt loading. It was noticed that global injury criteria with very localized impacts such as rim impacts have not been studied and will affect rib fracture risk differently than what has been studied using airbag or seat belt restraints. However, improvements are needed to accurately predict thoracic injury risk at a material level by finding more data for the local injury risk functions.</p><p>Conclusively, it is clear that Hybrid III has insufficient instrumentation and is in need of upgrades to register chest deflections at multiple rib levels. Furthermore, the following are needed: better understanding of global injury criteria specific for HGV occupant safety evaluation, more data for age-dependent (ribs) and rate-dependent (organs) injury risk functions, a tiebreak contact with tangential sliding for better organ kinematics during impacts, and improving the biofidelity of the material models using data from tissue level experiments.</p>
----------------------------------------------------------------------
In diva2:572065 abstract is: <p>Voice over IP (VoIP) is a relatively new technology that enables voice calls over data networks.With VoIP it is possible to lower expenses, and increase functionality and flexibility. FromSwedish Armed Forces point of view, the security issue is of great importance, why the focus inthis report is on the security aspect of the two most common open-source VoIP-protocols H.323and SIP, some of the most common attacks, and counter-measures for those attacks.Because of the level of complexity with a network running H.323 or SIP, and the fact that it hasyet to stand the same level of trial as of traditional telephony, a VoIP-system includes manyknown security-issues, and probably at present many unknown security flaws.</p><p>The conclusion is that it takes great knowledge and insight about a VoIP-network based onH.323 or SIP to make the network satisfyingly safe as it is today, and is therefore perhaps not asuitable solution for the Swedish Armed Forces today for their more sensitive communications.</p>

corrected abstract:
<p>Voice over IP (VoIP) is a relatively new technology that enables voice calls over data networks. With VoIP it is possible to lower expenses, and increase functionality and flexibility. From Swedish Armed Forces point of view, the security issue is of great importance, why the focus in this report is on the security aspect of the two most common open-source VoIP-protocols H.323 and SIP, some of the most common attacks, and counter-measures for those attacks. Because of the level of complexity with a network running H.323 or SIP, and the fact that it has yet to stand the same level of trial as of traditional telephony, a VoIP-system includes many known security-issues, and probably at present many unknown security flaws.</p><p>The conclusion is that it takes great knowledge and insight about a VoIP-network based on H.323 or SIP to make the network satisfyingly safe as it is today, and is therefore perhaps not a suitable solution for the Swedish Armed Forces today for their more sensitive communications.</p>
----------------------------------------------------------------------
In diva2:1454860 abstract is: <p>Implementing the string method with swarms of trajectories, to model transitions between protein states,is laborious work. As it is also repetitive with small di↵erences in between repetitions, it suits itself wellfor automation. This project’s aim was to produce software, developed in Python3, implementing the GromacsPython API, capable of automating a generalised implementation of the string method with swarmsof trajectories. The final software was able to adapt one collective variable (CV) configuration file and twoprotein configuration files representing start and end states of the protein in preparation of, and subsequentlyrun MD simulations implementing the string method with swarms of trajectories. The resulting software hasgarnered interest from both the TCB-labs at Science for Life Laboratories who wish to use the software forthe implementation of the string method, and the Gromacs development team for further insights into theusability of the API.</p>

corrected abstract:
<p>Implementing the string method with swarms of trajectories, to model transitions between protein states, is laborious work. As it is also repetitive with small di↵erences in between repetitions, it suits itself well for automation. This project’s aim was to produce software, developed in Python3, implementing the Gromacs Python API, capable of automating a generalised implementation of the string method with swarms of trajectories. The final software was able to adapt one collective variable (CV) configuration file and two protein configuration files representing start and end states of the protein in preparation of, and subsequently run MD simulations implementing the string method with swarms of trajectories. The resulting software has garnered interest from both the TCB-labs at Science for Life Laboratories who wish to use the software for the implementation of the string method, and the Gromacs development team for further insights into the usability of the API.</p>
----------------------------------------------------------------------
In diva2:1144116 abstract is: <p>This thesis has been carried out on behalf of KTH the school for medicaltechnology, who saw the need to use diaphanography, an medical imaging methodto detect breastmilk and diseases in women. In this thesis, diaphanography is usedto detect breastmilk in breastfeeding women.Breast engorgement is a problem for women who breastfeed. When the motherproduces more milk than the baby uses, pain occurs. To reduce engorgementdiaphanography can be used to analyze and see if there is any milk produced.Two methods, reflectance and penetration was used to examine the amount ofmilk.The results showed that both methods could detect milk, but only the penetrationcould estimate the amount of milk.</p>


corrected abstract:
<p>This thesis has been carried out on behalf of KTH the school for medical technology, who saw the need to use diaphanography, an medical imaging method to detect breastmilk and diseases in women. In this thesis, diaphanography is used to detect breastmilk in breastfeeding women.</p><p>Breast engorgement is a problem for women who breastfeed. When the mother produces more milk than the baby uses, pain occurs. To reduce engorgement diaphanography can be used to analyze and see if there is any milk produced.</p><p>Two methods, reflectance and penetration was used to examine the amount of milk.</p><p>The results showed that both methods could detect milk, but only the penetration could estimate the amount of milk.</p>
----------------------------------------------------------------------
title: "Analyzing Changes inIntra-OperativeSignals UsingMachine Learning"
==>    "Analyzing Changes in Intra-Operative Signals Using Machine Learning"

In diva2:1874237 abstract is: <p>Non-cardiac surgeries conducted globally each year often lead to cardiac complications,with myocardial injury commonly occurring within 30 days post-surgery. This thesis investigates the correlation between intra-operative signals and myocardial injuryusing machine learning models. The study focuses on analyzing intra-operative STelevation, heart rate, diastolic blood pressure, and pulse pressure variation. Multiplemachine learning models, including decision tree and random forest classifiers, weredeveloped and evaluated using two approaches: the sequence of event times and comprehensive event features.The results indicate that intra-operative physiological signals are valuable predictorsof myocardial injury, with random forest models generally outperforming decision treemodels. However, limited and unbalanced data posed challenges, affecting model performance variability. This research establishes a framework for enhancing predictivemodels and monitoring strategies to improve patient outcomes.</p>

partal corrected: diva2:1874237: <p>Non-cardiac surgeries conducted globally each year often lead to cardiac complications, with myocardial injury commonly occurring within 30 days post-surgery. This thesis investigates the correlation between intra-operative signals and myocardial injury using machine learning models. The study focuses on analyzing intra-operative ST elevation, heart rate, diastolic blood pressure, and pulse pressure variation. Multiple machine learning models, including decision tree and random forest classifiers, were developed and evaluated using two approaches: the sequence of event times and comprehensive event features. The results indicate that intra-operative physiological signals are valuable predictors of myocardial injury, with random forest models generally outperforming decision tree models. However, limited and unbalanced data posed challenges, affecting model performance variability. This research establishes a framework for enhancing predictivemodels and monitoring strategies to improve patient outcomes.</p>

corrected abstract:
<p>Non-cardiac surgeries conducted globally each year often lead to cardiac complications, with myocardial injury commonly occurring within 30 days post-surgery. This thesis investigates the correlation between intra-operative signals and myocardial injury using machine learning models. The study focuses on analyzing intra-operative ST-elevation, heart rate, diastolic blood pressure, and pulse pressure variation. Multiple machine learning models, including decision tree and random forest classifiers, were developed and evaluated using two approaches: the sequence of event times and comprehensive event features.</p><p>The results indicate that intra-operative physiological signals are valuable predictors of myocardial injury, with random forest models generally outperforming decision tree models. However, limited and unbalanced data posed challenges, affecting model performance variability. This research establishes a framework for enhancing predictive models and monitoring strategies to improve patient outcomes.</p>
----------------------------------------------------------------------
title: "Optimeringsanalys av en modern,systemintegrerad operationssal: Jämförelse av tre styrningsfall"
==>    "Optimeringsanalys av en modern, systemintegrerad operationssal: Jämförelse av tre styrningsfall"

In diva2:1389960 abstract is: <p>Healthcare system complexity can be simplified by equipping hospital operatingtheatres with an integrated system solution. The goal is to improve the hospitalworkflow by enabling maneuvering of all the equipment from a common unit.Today there is no data to support the claim that the system offered feature of usingpreset scenes provides any improvements.</p><p>The aim of this study is to demonstrate the benefits of the integrated operatingsystem. This is done by performing a workflow analysis based on three differentcases (with preset scenes, maneuvered through the applications and without use ofthe integrated system) and three different parameters (clicks, steps, and time). Thepurpose is to clarify the, from a user perspective, most beneficial case.</p><p>The measured result shows an advantage when using the system with presetscenes. All of the assumptions in the survey, including the limitations ofquantitative data, are considered in the study.</p><p>The result is validated by the analysis. A combination of the case with preset scenesand a distributed control between the users are therefore the best alternative for anoptimized workflow.</p>

partal corrected: diva2:1389960: <p>Healthcare system complexity can be simplified by equipping hospital operatingtheatres with an integrated system solution. The goal is to improve the hospitalworkflow by enabling maneuvering of all the equipment from a common unit. Today there is no data to support the claim that the system offered feature of using preset scenes provides any improvements.</p><p>The aim of this study is to demonstrate the benefits of the integrated operatingsystem. This is done by performing a workflow analysis based on three different cases (with preset scenes, maneuvered through the applications and without use of the integrated system) and three different parameters (clicks, steps, and time). The purpose is to clarify the, from a user perspective, most beneficial case.</p><p>The measured result shows an advantage when using the system with presetscenes. All of the assumptions in the survey, including the limitations of quantitative data, are considered in the study.</p><p>The result is validated by the analysis. A combin ation of the case with preset scenes and a distributed control between the users are therefore the best alternative for an optimized workflow.</p>

corrected abstract:
<p>Healthcare system complexity can be simplified by equipping hospital operating theatres with an integrated system solution. The goal is to improve the hospital workflow by enabling maneuvering of all the equipment from a common unit. Today there is no data to support the claim that the system offered feature of using preset scenes provides any improvements.</p><p>The aim of this study is to demonstrate the benefits of the integrated operating system. This is done by performing a workflow analysis based on three different cases (with preset scenes, maneuvered through the applications and without use of the integrated system) and three different parameters (clicks, steps, and time). The purpose is to clarify the, from a user perspective, most beneficial case.</p><p>The measured result shows an advantage when using the system with preset scenes. All of the assumptions in the survey, including the limitations of quantitative data, are considered in the study.</p><p>The result is validated by the analysis. A combination of the case with preset scenes and a distributed control between the users are therefore the best alternative for an optimized workflow.</p>
----------------------------------------------------------------------
In diva2:1849768 abstract is: <p>This thesis explored the development of advanced machine learning models to improve autonomous transportation systems. By focusing on the identification and classification of traffic light signals, the work contributes to the safety and efficiency of self-driving vehicles. Areview of models such as the Single Shot MultiBox Detector (SSD), as an object detectionmodel, and InceptionV3 and VGG16, as classification models, was conducted, with particular emphasis on their training and testing processes.The results, in terms of validation accuracy and validation loss, showed that the InceptionV3model performed well across various parameters. This model proved to be robust and adaptable, making it a good choice for the project's goal of accurate and reliable classification oftraffic light signals.On the other hand, the VGG16 model showed varying results. While it performed well undercertain conditions, it proved to be less robust at certain parameter settings, especially at higherbatch sizes, which led to lower validation accuracy and higher validation loss.</p>


corrected abstract:
<p>This thesis explored the development of advanced machine learning models to improve autonomous transportation systems. By focusing on the identification and classification of traffic light signals, the work contributes to the safety and efficiency of self-driving vehicles. A review of models such as the Single Shot MultiBox Detector (SSD), as an object detection model, and InceptionV3 and VGG16, as classification models, was conducted, with particular emphasis on their training and testing processes.</p><p>The results, in terms of validation accuracy and validation loss, showed that the InceptionV3 model performed well across various parameters. This model proved to be robust and adaptable, making it a good choice for the project's goal of accurate and reliable classification of traffic light signals.</p><p>On the other hand, the VGG16 model showed varying results. While it performed well under certain conditions, it proved to be less robust at certain parameter settings, especially at higher batch sizes, which led to lower validation accuracy and higher validation loss.</p>
----------------------------------------------------------------------
In diva2:1217609 abstract is: <p>As a result of the new General Data Protection Regulation (GDPR) in the EU, there arestricter requirements for handling personal data. For the first time, companies risk sanc-tions if they fail to handle personal data properly, giving rise to a wide spectrum of im-pacts. In the IT sector, an analysis must be undertaken to determine which data will beaffected by the introduction of GDPR and how this data can be managed in current IT sys-tems in order to meet the new requirements. Against this backdrop, this study was con-ducted at Primona, a purchasing and electronic trade company located in Stockholm.A proposed solution was developed by studying the GDPR, related works and the resultsfrom the interviews which was conducted in this study. The proposed solution was thentested on a selected part of one of the company´s systems. Furthermore, this study pre-sents an economic analysis to determine the significance of implementing of this solution,which points to a need for such a solution to be prioritized by the company.Overall, the proposed solution proves to have a positive effect with respect to complyingwith GDPR and can be reused with relatively few resources.</p>

w='pre-sents' val={'c': 'presents', 's': 'diva2:1217609', 'n': 'hyphen at end of line in original'}
w='sanc-tions' val={'c': 'sanctions', 's': 'diva2:1217609', 'n': 'hyphen at end of line in original'}

corrected abstract:
<p>As a result of the new General Data Protection Regulation (GDPR) in the EU, there are stricter requirements for handling personal data. For the first time, companies risk sanctions if they fail to handle personal data properly, giving rise to a wide spectrum of impacts. In the IT sector, an analysis must be undertaken to determine which data will be affected by the introduction of GDPR and how this data can be managed in current IT systems in order to meet the new requirements. Against this backdrop, this study was conducted at Primona, a purchasing and electronic trade company located in Stockholm.</p><p>A proposed solution was developed by studying the GDPR, related works and the results from the interviews which was conducted in this study. The proposed solution was then tested on a selected part of one of the company´s systems. Furthermore, this study presents an economic analysis to determine the significance of implementing of this solution, which points to a need for such a solution to be prioritized by the company.</p><p>Overall, the proposed solution proves to have a positive effect with respect to complying with GDPR and can be reused with relatively few resources.</p>
----------------------------------------------------------------------
In diva2:826657 abstract is: <p>In Sweden, 379 children and adolescents under the age of 19 was diagnosed withcancer in 2012. In workplaces where staff were not sufficiently informed they couldobserve a higher exposure in patients to cytotoxic drugs in urine. Workers at theDepartment of Oncology are regularly exposed to the toxic substances. In the longterm, it means an occupational exposure to low doses of chemotherapy drugs. Inconclusion, a probable exposure occurs through the skin and not through the air and atthis time we do not know the consequences of chemotherapy drugs in the sewage.This is the result of too little research, and what impact these substances have on theenvironment. However, there are clear statutes and laws for handling and waste thatcan help to reduce exposure for the staff.</p>


corrected abstract:
<p>In Sweden, 379 children and adolescents under the age of 19 was diagnosed with cancer in 2012. In workplaces where staff were not sufficiently informed they could observe a higher exposure in patients to cytotoxic drugs in urine. Workers at the Department of Oncology are regularly exposed to the toxic substances. In the long term, it means an occupational exposure to low doses of chemotherapy drugs. In conclusion, a probable exposure occurs through the skin and not through the air and at this time we do not know the consequences of chemotherapy drugs in the sewage. This is the result of too little research, and what impact these substances have on the environment. However, there are clear statutes and laws for handling and waste that can help to reduce exposure for the staff.</p>
----------------------------------------------------------------------
title: "Evaluation of antimicrobial properties of cellulosemycelium biocomposite"
==>    "Evaluation of antimicrobial properties of cellulose-mycelium biocomposite"

In diva2:1454422 abstract is: <p>A type of biocomposite material, based on fungal mycelium and taking advantage of itsproperty to bind cellulosic biomass has recently found popularity as, among other things,renewable and biodegradable packaging and construction material. Because fungi are knownto commonly have antibacterial properties, and because this could be useful for applicationssuch as wound dressing or packaging, the antibacterial surface properties of such a materialwere investigated. A composite bio-based material was formed from cellulosic material andmycelium and turned into papers. A mass balance was done on the material and the cellulosic% determined. The possible antibacterial activity of similar biomaterials was evaluated fromliterature, and an antibacterial assay was done with the papers on E. coli and B. subtilis,showing little antibacterial effect under the assay conditions, but concluding that furthertesting should be done.</p>


corrected abstract:
<p>A type of biocomposite material, based on fungal mycelium and taking advantage of its property to bind cellulosic biomass has recently found popularity as, among other things, renewable and biodegradable packaging and construction material. Because fungi are known to commonly have antibacterial properties, and because this could be useful for applications such as wound dressing or packaging, the antibacterial surface properties of such a material were investigated. A composite bio-based material was formed from cellulosic material and mycelium and turned into papers. A mass balance was done on the material and the cellulosic % determined. The possible antibacterial activity of similar biomaterials was evaluated from literature, and an antibacterial assay was done with the papers on <em>E. coli</em> and <em>B. subtilis</em>, showing little antibacterial effect under the assay conditions, but concluding that further testing should be done.</p>
----------------------------------------------------------------------
title: "Evaluation of push/pull based loadbalancing in a distributed loggingenvironment"
==>    "Evaluation of push/pull based load balancing in a distributed logging environment"

In diva2:936217 abstract is: <p>This report compares the characteristics of push/pull load balancing techniques usedin the context of a logging system. The logging system is expected to handle a largevolume of events. The load balancing techniques are evaluated with focus onthroughput during high load. The testing scenarios includes the use of a traditionalload balancer (push-based) and the use of messaging queues (pull-based and indirectlycontext aware) in its place. The ultimate goal of the report is to determine the feasibilityof using a messaging queue rather than a traditional load balancer in a distributedlogging system. Tests were conducted measuring the throughput of multiple setupswith different load balancers. The conclusion of this report is that both messagingqueues and load balancing are equally feasible in a logging context.</p>

corrected abstract:
<p>This report compares the characteristics of push/pull load balancing techniques used in the context of a logging system. The logging system is expected to handle a large volume of events. The load balancing techniques are evaluated with focus on throughput during high load. The testing scenarios includes the use of a traditional load balancer (push-based) and the use of messaging queues (pull-based and indirectly context aware) in its place. The ultimate goal of the report is to determine the feasibility of using a messaging queue rather than a traditional load balancer in a distributed logging system. Tests were conducted measuring the throughput of multiple setups with different load balancers. The conclusion of this report is that both messaging queues and load balancing are equally feasible in a logging context.</p>
----------------------------------------------------------------------
In diva2:1361820 abstract is: <p>With the continuous update of dental materials, hundreds of new materials have been invented and put into use, greatly enriching the selection of clinical applications. Among them, resin-based materials are one of the most popular dental restoration materials. In recent years, high-performance composites based ontriazine-trione (TATO) monomers polymerized via light-initiated thiol-ene coupling(TEC) or thiol-yne coupling (TYC) have become the most promising candidates to replace the methacrylate composites used today in dental restoration application.This study reports the large scale synthesis of important click-based TATO monomers, the formulation of reactive TEC/TYC monomers together with fillers as well as their transformation into crosslinked networks and finally the evaluation of their mechanical properties. The most promising of these systems are the TATATO/TMTATO (TEC) and octadiyne/TMTATO (TYC) mixtures. Compared with traditional methacrylate composites, the flexure strength of the TEC composites with 70wt% filler content is 1.1 times, the modulus is 0.7 times, and the shrinkage stressis 0.5 times, meanwhile, the flexure strength of the TEC composites with 70wt% filler content is 1.1 times, the modulus is 0.7 times, and the shrinkage stress is 0.9 times. In addition, their shear stress values are similar to those of commercial materials.Therefore, with better mechanical properties as well as lower shrinkage stress,TEC/TYC composites have successfully demonstrated their commercial potential in dental restoration application.</p>


corrected abstract:
<p>With the continuous update of dental materials, hundreds of new materials have been invented and put into use, greatly enriching the selection of clinical applications. Among them, resin-based materials are one of the most popular dental restoration materials. In recent years, high-performance composites based on triazine-trione (TATO) monomers polymerized via light-initiated thiol-ene coupling (TEC) or thiol-yne coupling (TYC) have become the most promising candidates to replace the methacrylate composites used today in dental restoration application. This study reports the large scale synthesis of important click-based TATO monomers, the formulation of reactive TEC/TYC monomers together with fillers as well as their transformation into crosslinked networks and finally the evaluation of their mechanical properties. The most promising of these systems are the TATATO/TMTATO (TEC) and octadiyne/TMTATO (TYC) mixtures. Compared with traditional methacrylate composites, the flexure strength of the TEC composites with 70wt% filler content is 1.1 times, the modulus is 0.7 times, and the shrinkage stress is 0.5 times, meanwhile, the flexure strength of the TEC composites with 70wt% filler content is 1.1 times, the modulus is 0.7 times, and the shrinkage stress is 0.9 times. In addition, their shear stress values are similar to those of commercial materials. Therefore, with better mechanical properties as well as lower shrinkage stress, TEC/TYC composites have successfully demonstrated their commercial potential in dental restoration application.</p>
----------------------------------------------------------------------
In diva2:1287489 abstract is: <p>The interactions between the divalent salts CaCl2, MnCl2 and NiCl2 with the carboxylicacid groups in an arachidic acid Langmuir monolayer are studied using vibrational sumfrequency spectroscopy (VSFS). At low salt concentrations the charging of the monolayer is shown to behave in accordance with classical theories of the electrical doublelayer. However, deviations from classical theories are readily apparent at concentrations starting from 1 μM and shown to differ depending on the nature of the cation, indicating an ion specific effect. From the analysis of the carboxylate symmetric vibration at least two different types of ion pair interactions between the divalent cations and the carboxylicacid moiety are detected: a solvent-separated and two (or more) possible contact ion-pairs and/or complexes. By using different VSFS polarization schemes the molecular orientation of the vibration is elucidated and further insight into the state of the carboxylic acidis gained. The ordering of water molecules in the diffuse layer as a result of the surfacepotential of the charged monolayer is also probed and found, at low ionic strengths, to bein agreement with the behaviour predicted by the Gouy-Chapman model. At high concentrations the hydration within the Stern layer can be detected and found to differ with the nature of the electrolyte. Additionally, the importance of the purity grade of the salts used for correctly interpreting the results is shown theoretically and verified experimentally,where trivalent ions affect the surface at concentrations several orders of magnitude below those from mono and divalent ions. Finally, diffusion is shown to play a major rolein the kinetics of deprotonation of the monolayer at submicromolar concentrations, while higher valency cations are also shown to deprotonate the surface at a quicker rate through electroflux.</p>


corrected abstract:
<p>The interactions between the divalent salts CaCl<sub>2</sub>, MnCl<sub>2</sub> and NiCl<sub>2</sub> with the carboxylic acid groups in an arachidic acid Langmuir monolayer are studied using vibrational sum frequency spectroscopy (VSFS). At low salt concentrations the charging of the monolayer is shown to behave in accordance with classical theories of the electrical double layer. However, deviations from classical theories are readily apparent at concentrations starting from 1 μM and shown to differ depending on the nature of the cation, indicating an ion specific effect. From the analysis of the carboxylate symmetric vibration at least two different types of ion pair interactions between the divalent cations and the carboxylic acid moiety are detected: a solvent-separated and two (or more) possible contact ion-pairs and/or complexes. By using different VSFS polarization schemes the molecular orientation of the vibration is elucidated and further insight into the state of the carboxylic acid is gained. The ordering of water molecules in the diffuse layer as a result of the surface potential of the charged monolayer is also probed and found, at low ionic strengths, to be in agreement with the behaviour predicted by the Gouy-Chapman model. At high concentrations the hydration within the Stern layer can be detected and found to differ with the nature of the electrolyte. Additionally, the importance of the purity grade of the salts used for correctly interpreting the results is shown theoretically and verified experimentally, where trivalent ions affect the surface at concentrations several orders of magnitude below those from mono and divalent ions. Finally, diffusion is shown to play a major role in the kinetics of deprotonation of the monolayer at submicromolar concentrations, while higher valency cations are also shown to deprotonate the surface at a quicker rate through electroflux.</p>
----------------------------------------------------------------------
In diva2:1242516 abstract is: <p>The photosynthetic abilities of cyanobacteria have forged hopes of using them as productionorganisms for biofuels, using only sunlight and CO2 to produce a sustainable alternative tofossil fuels. Science for Life Laboratories has genetically-engineered a strain capable of excreting butanol, a compound with valuable fuel properties. This work is an attempt of getting an un derstanding of the future potentials of this biofuel through an analysis of its environmental impacts and the cumulative energy demand.A cradle-to-grave environmental life cycle assessment of a hypothetical production plant innorthern Sweden was performed, assessing a range of scenarios showing that the technology isstill young and that further research in metabolic engineering and cultivation technology isneeded to create an energetically and environmentally viable process. However, several scenarios tested give results that would induce more than 60% greenhouse gas emission savings compared to fossil fuel, implying that there is future potential for the technology in a world where the demand for climate change mitigating solutions is increasingly pressing.</p>

w='derstanding' val={'c': 'understanding', 's': 'diva2:1242516', 'n': 'no full text'}

corrected abstract:
<p>The photosynthetic abilities of cyanobacteria have forged hopes of using them as production organisms for biofuels, using only sunlight and CO2 to produce a sustainable alternative to fossil fuels. Science for Life Laboratories has genetically-engineered a strain capable of excreting butanol, a compound with valuable fuel properties. This work is an attempt of getting an understanding of the future potentials of this biofuel through an analysis of its environmental impacts and the cumulative energy demand. A cradle-to-grave environmental life cycle assessment of a hypothetical production plant in northern Sweden was performed, assessing a range of scenarios showing that the technology is still young and that further research in metabolic engineering and cultivation technology is needed to create an energetically and environmentally viable process. However, several scenarios tested give results that would induce more than 60% greenhouse gas emission savings compared to fossil fuel, implying that there is future potential for the technology in a world where the demand for climate change mitigating solutions is increasingly pressing.</p>
----------------------------------------------------------------------
In diva2:1447074 abstract is: <p>It is estimated that 15% of all couples worldwide suffer from infertility. Roughly half is male-factor infertility and 40% of these cases cannot be explained. Thus, current methods for diagnosing male infertility are not enough and further techniques are needed. To have a successful fertilisation event, it is required that the sperm expresses membrane surface-protein Izumo1 which must recognise its counterpart protein Juno, located at the surface of the egg membrane. The recognition step between Juno and Izumo1 is essential in mammalian fertilisation for the gametes to bind and start the creation of a new distinct organism, but the molecular mechanism is still unknown.A start-up company named Spermosens want to measure the Juno-Izumo1 interaction in a new diagnostic device designed to diagnose male infertility. The idea is to have Juno immobilised on gold nanoparticles and measure the interaction between Juno and various semen samples. The new device is supposed to help couples pin-point the procreation issue which would help in the selection of suitable assisted reproductive technology. In the development of the new device, it had to be established that the Juno used in the device will bind correctly to human Izumo1. Therefore, the interactions between the human recombinant proteins Juno and Izumo1 had to be measured and characterized.The objectives of this project were to develop a method to immobilise Juno on gold nanoparticles and then measure the interactions with Izumo1 using UV-vis spectroscopy. This is theoretically possible since the gold nanoparticles exhibit a phenomenon called localized surface plasmon resonance that vary depending on the size of the gold nanoparticle-complex. The immobilisation procedure was a process involving several steps that were designed, polished and improved along the way. Dithiobis(C2-NTA) was conjugated to the gold surface and a cobalt ion was conjugated to the NTA. The last step involving conjugation of Juno to the cobalt through a His-tag was not succeeded, and the interactions could therefore not be measured this way.Instead, the protein-protein interaction was measured through SPR-measurements using Biacore, an instrument that is based on surface plasmon resonance as well. Interactions between Izumo1 and Juno could be detected using Juno produced in E. coli and in mammalian cells. The dissociation constant (Kd) could be calculated to 7-33 nM which can be compared to a previously published Kd of 48 nM. A more precise Kd could not be established, possibly due to that the regeneration of the sensor surface with NaOH varied in efficiency, leading to changing surface conditions during the measurements. The two Juno proteins, that were produced in different hosts, showed two different affinity profiles with Izumo1, which contributes to the suggestion that the glycosylation plays a role in the binding mechanism between Juno and Izumo1.</p>

w='Kd' val={'c': 'K<sub>d</sub>', 's': ['diva2:922823', 'diva2:1447074'], 'n': 'no full text for diva2:922823 but correct in source for diva2:1447074'}

corrected abstract:
<p>It is estimated that 15% of all couples worldwide suffer from infertility. Roughly half is male-factor infertility and 40% of these cases cannot be explained. Thus, current methods for diagnosing male infertility are not enough and further techniques are needed. To have a successful fertilisation event, it is required that the sperm expresses membrane surface-protein Izumo1 which must recognise its counterpart protein Juno, located at the surface of the egg membrane. The recognition step between Juno and Izumo1 is essential in mammalian fertilisation for the gametes to bind and start the creation of a new distinct organism, but the molecular mechanism is still unknown.</p><p>A start-up company named Spermosens want to measure the Juno-Izumo1 interaction in a new diagnostic device designed to diagnose male infertility. The idea is to have Juno immobilised on gold nanoparticles and measure the interaction between Juno and various semen samples. The new device is supposed to help couples pin-point the procreation issue which would help in the selection of suitable assisted reproductive technology. In the development of the new device, it had to be established that the Juno used in the device will bind correctly to human Izumo1. Therefore, the interactions between the human recombinant proteins Juno and Izumo1 had to be measured and characterized.</p><p>The objectives of this project were to develop a method to immobilise Juno on gold nanoparticles and then measure the interactions with Izumo1 using UV-vis spectroscopy. This is theoretically possible since the gold nanoparticles exhibit a phenomenon called localized surface plasmon resonance that vary depending on the size of the gold nanoparticle-complex. The immobilisation procedure was a process involving several steps that were designed, polished and improved along the way. Dithiobis(C2-NTA) was conjugated to the gold surface and a cobalt ion was conjugated to the NTA. The last step involving conjugation of Juno to the cobalt through a His-tag was not succeeded, and the interactions could therefore not be measured this way.</p><p>Instead, the protein-protein interaction was measured through SPR-measurements using Biacore, an instrument that is based on surface plasmon resonance as well. Interactions between Izumo1 and Juno could be detected using Juno produced in E. coli and in mammalian cells. The dissociation constant (K<sub>d</sub>) could be calculated to 7-33 nM which can be compared to a previously published K<sub>d</sub> of 48 nM. A more precise K<sub>d</sub> could not be established, possibly due to that the regeneration of the sensor surface with NaOH varied in efficiency, leading to changing surface conditions during the measurements. The two Juno proteins, that were produced in different hosts, showed two different affinity profiles with Izumo1, which contributes to the suggestion that the glycosylation plays a role in the binding mechanism between Juno and Izumo1.</p>
----------------------------------------------------------------------
In diva2:1642811 abstract is: <p>As the need for crops escalates, the application of chemicals against plant pathogens drastically increases, swaying the ecosystem balance. Biocontrol utilizing the bacteria strain <em>Chitinophaga pinensis</em> as an alternative provides a more sustainable solution to prevent fungal and oomycete diseases from attacking vulnerable crops. The project goal is to further investigate and quantify actual antagonizing effects of <em>C. pinensis</em>, which has been shown in preliminary data to be capable of some pathogen antagonism. This project also plans to develop a conceptual mechanism to explain the antagonism, including the role of cell wall degrading enzymes. Though being a pilot study, this project lays a firm groundwork for future development at all aspects with results obtained.</p><p>The clear pattern of Inhibition Effects (IE) exhibited by <em>C. pinensis</em> regarding thethree pathogens of choice, <em>Aphanomyces cochlioides</em>, <em>Aphanomyces euteiches</em>, and <em>Phytophthora capsici</em>, was shown from Agar Plate Inhibition Test (APIT) and enzyme activity assay results. There is high likelihood that the IE observed in these labscale microbiological experiments can be translated into an efficient protection strategy for plants. The extracellular secretome of <em>C. pinensis</em> appeared inhibitory as expected,which aligned well with previous proteomic predictions. The supplementation of different Carbon Source (CS)s did seemingly induce different enzymes and differences in bacterial growth rate, where future mass spectrometry analyses can reveal the actual enzyme composition to help perfect the model of Cell Wall (CW) degradation. Nonetheless, the profound IE observed with living <em>C. pinensis</em> cells and some isolated secretomes suggested that growth on certain CS could prime the bacterium for a stronger biocontrol response especially in CW degradation that contributes to pathogen inhibition. The <em>C. pinensis</em> growth rate in precultivation is thus found to be partly predictive of IE.</p><p>More research is essential to finely put together this delicate theoretical model. Both in <em>planta</em> pathogen IE and mycological fluorescent microscopy for closeup oomycete-bacteria interaction are also recommended for, intriguingly, some bacteria contaminants in APIT were found thriving on Oomycete Cell Wall (OCW) substances presented, depicting the complexity of this welltuned microcommunity. Efforts should be spent to ultimately promote biocontrol to markets, where a resilient soil microstatus is created via bacterial supplementation, suppressing pathogen invasion with symbiosis rather than additional chemicals.</p><p>In future food industrial application, the biocontrol model stands for sustainability as well as reempowers the natural resilience, through which the food justice highlighted in Sustainable Development Goals (SDGs) 2: Zero Hunger, can be embodied with the expected higher crops yields.</p>


corrected abstract:
<p>As the need for crops escalates, the application of chemicals against plant pathogens drastically increases, swaying the ecosystem balance. Biocontrol utilizing the bacteria strain <em>Chit inophaga pinensis</em> as an alternative provides a more sustain able solution to prevent fungal and oomycete diseases from attacking vulnerable crops. The project goal is to further investigate and quantify actual antagonizing effects of <em>C. pinensis</em>, which has been shown in preliminary data to be capable of some pathogen antagonism. This project also plans to develop a conceptual mechanism to explain the antagonism, including the role of cell wall degrading enzymes. Though being a pilot study, this project lays a firm groundwork for future development at all aspects with results obtained.</p><p>The clear pattern of Inhibition Effects (IE) exhibited by <em>C. pinensis</em> regarding the three pathogens of choice, <em>Aphanomyces cochlioides</em>, <em>Aphanomyces euteiches</em>, and <em>Phytophthora capsici</em>, was shown from Agar Plate Inhibition Test (APIT) and enzyme activity assay results. There is high likelihood that the IE observed in these lab scale microbiological experiments can be translated into an efficient protection strategy for plants. The extracellular secretome of <em>C. pinensis</em> appeared inhibitory as expected, which aligned well with previous proteomic predictions. The supplementation of different Carbon Source (CS)s did seemingly induce different enzymes and differences in bacterial growth rate, where future mass spectrometry analyses can reveal the actual enzyme composition to help perfect the model of Cell Wall (CW) degradation. Nonetheless, the profound IE observed with living <em>C. pinensis</em> cells and some isolated secretomes suggested that growth on certain CS could prime the bacterium for a stronger biocontrol response especially in CW degradation that contributes to pathogen inhibition. The <em>C. pinensis</em> growth rate in precultivation is thus found to be partly predictive of IE.</p><p>More research is essential to finely put to gether this delicate theoretical model. Both in <em>planta</em> pathogen IE and mycological fluorescent microscopy for closeup oomycete-bacteria interaction are also recommended for, intriguingly, some bacteria contamin ants in APIT were found thriving on Oomycete Cell Wall (OCW) substances presented, depicting the complexity of this welltuned microcommunity. Efforts should be spent to ultimately promote biocontrol to markets, where a resilient soil microstatus is created via bacterial supplementation, suppressing pathogen invasion with symbiosis rather than additional chemicals.</p><p>In future food industrial application, the biocontrol model stands for sustain ability as well as reempowers the natural resilience, through which the food justice highlighted in Sustain able Development Goals (SDGs) 2: Zero Hunger, can be embodied with the expected higher crops yields.</p>
----------------------------------------------------------------------
In diva2:1886087 abstract is: <p>Adeno-associated virus (AAV) based gene therapy is the therapeutic treatment of introducing geneticmaterial into a cell, with the help of a non-pathogenic vector. However, there are still challenges of the technique to be considered for optimization. Empty capsids are being formed during production andoff-target infection and insufficient targeting leads to high costs and immunogenicity for the patient. Fusing the affinity protein Affibody, with affinity towards a certain cell surface receptor, to the AAV can increase targeting. In this project, the length of the linker between the Affibody and thecapsid is being investigated through genetic modifications and evaluated in terms of bindingand transduction capacity. Eight different Affibody-AAVs with different linker lengths weresuccessfully produced. All constructs were able to bind the its specific receptor in vitro. One construct showed highest transduction efficiency when tested on receptor-expressing ExpiCHO-cell lines. Ion exchange chromatography was also explored to separate full AAV capsids from empty ones, which could in a therapeutic setting increase the therapeutic efficiency of AAVs. An initial attempt was achieved, with a successful binding of AAVs to the column. A separation of AAVs was also achieved, however with unknown fractions of empty versus full capsids. These results make up the basis for further development and optimization to reach a two-capsid population separation. Overall, this project results in efforts with promising outcomes to enhance specificity and effectiveness of AAV-based gene therapy for therapeutic applications.</p>

I assumed the usual italics for Latin words.

corrected abstract:
<p>Adeno-associated virus (AAV) based gene therapy is the therapeutic treatment of introducing genetic material into a cell, with the help of a non-pathogenic vector. However, there are still challenges of the technique to be considered for optimization. Empty capsids are being formed during production and off-target infection and insufficient targeting leads to high costs and immunogenicity for the patient. Fusing the affinity protein Affibody, with affinity towards a certain cell surface receptor, to the AAV can increase targeting. In this project, the length of the linker between the Affibody and the capsid is being investigated through genetic modifications and evaluated in terms of binding and transduction capacity. Eight different Affibody-AAVs with different linker lengths were successfully produced. All constructs were able to bind the its specific receptor <em>in vitro</em>. One construct showed highest transduction efficiency when tested on receptor-expressing ExpiCHO-cell lines. Ion exchange chromatography was also explored to separate full AAV capsids from empty ones, which could in a therapeutic setting increase the therapeutic efficiency of AAVs. An initial attempt was achieved, with a successful binding of AAVs to the column. A separation of AAVs was also achieved, however with unknown fractions of empty versus full capsids. These results make up the basis for further development and optimization to reach a two-capsid population separation. Overall, this project results in efforts with promising outcomes to enhance specificity and effectiveness of AAV-based gene therapy for therapeutic applications.</p>
----------------------------------------------------------------------
In diva2:1446137 abstract is: <p>There currently exists no commercialized method for rapid sampling and analysis of trace tar ingas streams. Solid phase microextraction (SPME) with a polydimethylsiloxane (PDMS) solidphase has been previously investigated as a possible candidate due to its solvent-free natureand reusability. This project set out to deliver a proof of concept study to test whether SPMEcan be sufficiently tuned to analyse trace tar content in syngas below the concentration of 0.1mg/Nm 3 . Due to complications that arose from the Covid-19 pandemic, it was unfeasible tocarry out the practical elements of the project. Instead a concept design for carrying out such astudy has been successfully developed. This design envisions a two-chamber setup able tosample syngas directly from a gasifier at 60 °C and 125 °C respectively and is illustrated in thetext. It utilizes commercially available solvent tubes to cross-check and verify the SPME results.</p>


corrected abstract:
<p>There currently exists no commercialized method for rapid sampling and analysis of trace tar in gas streams. Solid phase microextraction (SPME) with a polydimethylsiloxane (PDMS) solid phase has been previously investigated as a possible candidate due to its solvent-free nature and reusability. This project set out to deliver a proof of concept study to test whether SPME can be sufficiently tuned to analyse trace tar content in syngas below the concentration of 0.1 mg/Nm<sup>3</sup>. Due to complications that arose from the Covid-19 pandemic, it was unfeasible to carry out the practical elements of the project. Instead a concept design for carrying out such a study has been successfully developed. This design envisions a two-chamber setup able to sample syngas directly from a gasifier at 60 °C and 125 °C respectively and is illustrated in the text. It utilizes commercially available solvent tubes to cross-check and verify the SPME results.</p>
----------------------------------------------------------------------
In diva2:1282178 abstract is: <p>Currently, there is a worldwide need for non-hormonal contraceptives, as many women suffer from hormone-related side effects. Pain, bleeding, mood swings, and weightgain, as well as the risk of venous thromboembolism and cervix adenocarcinoma, are only some of the adverse effects that these women have come to accept as a part of their daily lives. This master thesis project is focused on investigating the possibilityof creating a non-hormonal contraceptive by crosslinking cervical mucus to prevent the passage of spermatozoa. For this task, the versatile polysaccharide chitosan was chosen. A biocompatible biopolymer with mucoadhesive properties, which enables the crosslinking of mucins and subsequently strengthens the cervical mucus network. Creating a mechanical barrier that immobilizes the sperm without damaging them.To use chitosan as a contraceptive it is desired to introduce it into a biological system. Therefore, it is of importance to understand the interaction of chitosan during human intercourse. This study focused on investigating the chitosan interaction with mucus and spermatozoa. The study’s four main objectives were to: 1) investigate thecomplexation between different types of chitosans and a model mucin, pig gastric mucin(PGM), 2) investigate the accumulation and penetration of chitosan into cervical mucus,3) investigate whether the addition of excipients to the chitosan formulation changes the chitosan accumulation and diffusion distance in cervical mucus and 4) investigate whether chitosan shows signs of cytotoxicity towards spermatozoa. Complexation of chitosan with PGM was quantified by fluorescence spectroscopy, comparing samples with and without addition of mucins. The accumulation and diffusion distance of chitosanswere investigated by fluorescence microscopy of labeled chitosan diffusing in capillaries filled with cervical mucus. Chitosans’ cytotoxicity analysis was divided into three assays,investigating spermatozoa membrane integrity, Reactive Oxygen Species production andDNA-damage.</p>


corrected abstract:
<p>Currently, there is a worldwide need for non-hormonal contraceptives, as many women suffer from hormone-related side effects. Pain, bleeding, mood swings, and weight gain, as well as the risk of venous thromboembolism and cervix adenocarcinoma, are only some of the adverse effects that these women have come to accept as a part of their daily lives. This master thesis project is focused on investigating the possibility of creating a non-hormonal contraceptive by crosslinking cervical mucus to prevent the passage of spermatozoa. For this task, the versatile polysaccharide chitosan was chosen. A biocompatible biopolymer with mucoadhesive properties, which enables the crosslinking of mucins and subsequently strengthens the cervical mucus network. Creating a mechanical barrier that immobilizes the sperm without damaging them. To use chitosan as a contraceptive it is desired to introduce it into a biological system. Therefore, it is of importance to understand the interaction of chitosan during human intercourse. This study focused on investigating the chitosan interaction with mucus and spermatozoa. The study’s four main objectives were to: 1) investigate the complexation between different types of chitosans and a model mucin, pig gastric mucin (PGM), 2) investigate the accumulation and penetration of chitosan into cervical mucus, 3) investigate whether the addition of excipients to the chitosan formulation changes the chitosan accumulation and diffusion distance in cervical mucus and 4) investigate whether chitosan shows signs of cytotoxicity towards spermatozoa. Complexation of chitosan with PGM was quantified by fluorescence spectroscopy, comparing samples with and without addition of mucins. The accumulation and diffusion distance of chitosans were investigated by fluorescence microscopy of labeled chitosan diffusing in capillaries filled with cervical mucus. Chitosans’ cytotoxicity analysis was divided into three assays, investigating spermatozoa membrane integrity, Reactive Oxygen Species production and DNA-damage.</p>
-------------------------------------------------------------------------------
In diva2:1127754 abstract is: <p>Advances of magnetic resonance imaging (MRI) techniques enable visualguidance to identify the anatomical target of interest during the image guidedintervention(IGI). Non-rigid image registration is one of the crucial techniques,aligning the target tissue with the MRI preoperative image volumes. As thegrowing demand for the real-time interaction in IGI, time used for intraoperativeregistration is increasingly important. This work implements 3D diffeomorphicdemons algorithm on Nvidia GeForce GTX 1070 GPU in C++ based on CUDA8.0.61 programming environment, using which the average registration time hasaccelerated to 5s. We have also extensively evaluated GPU accelerated 3D diffeomorphicregistration against both CPU implementation and Matlab codes, and theresults show that GPU implementation performs a much better algorithm efficiency.</p>

corrected abstract:
<p>Advances of magnetic resonance imaging (MRI) techniques enable visual guidance to identify the anatomical target of interest during the image guided intervention (IGI). Non-rigid image registration is one of the crucial techniques, aligning the target tissue with the MRI preoperative image volumes. As the growing demand for the real-time interaction in IGI, time used for intraoperative registration is increasingly important. This work implements 3D diffeomorphic demons algorithm on Nvidia GeForce GTX 1070 GPU in C++ based on CUDA 8.0.61 programming environment, using which the average registration time has accelerated to 5s. We have also extensively evaluated GPU accelerated 3D diffeomorphic registration against both CPU implementation and Matlab codes, and the results show that GPU implementation performs a much better algorithm efficiency.</p>
----------------------------------------------------------------------
title: "Indexing and Search Algorithmsfor Web shops:"
==>    "Indexing and Search Algorithms for Web shops"

In diva2:1014931 abstract is: <p></p><p></p><p>Web shops today needs to be more and more responsive, where one part of this responsivenessis fast product searches. One way of getting faster searches are by searching against anindex instead of directly against a database.</p><p>Network Expertise Sweden AB (Net Exp) wants to explore different methods of implementingan index in their future web shop, building upon the open-source web shop platformSmartStore.NET. Since SmartStore.NET does all of its searches directly against itsdatabase, it will not scale well and will wear more on the database. The aim was thereforeto find different solutions to offload the database by using an index instead.</p><p>A prototype that retrieved products from a database and made them searchable through anindex was developed, evaluated and implemented. The prototype indexed the data with aninverted index algorithm, and was made searchable with a search algorithm that mixed typeboolean queries with normal queries.</p><p></p>

corrected abstract:
<p>Web shops today needs to be more and more responsive, where one part of this responsiveness is fast product searches. One way of getting faster searches are by searching against an index instead of directly against a database.</p><p>Network Expertise Sweden AB (Net Exp) wants to explore different methods of implementing an index in their future web shop, building upon the open-source web shop platform SmartStore.NET. Since SmartStore.NET does all of its searches directly against its database, it will not scale well and will wear more on the database. The aim was therefore to find different solutions to offload the database by using an index instead.</p><p>A prototype that retrieved products from a database and made them searchable through an index was developed, evaluated and implemented. The prototype indexed the data with an inverted index algorithm, and was made searchable with a search algorithm that mixed type boolean queries with normal queries.</p>
----------------------------------------------------------------------
In diva2:1451804 abstract is: <p>Abstract</p><p>Regionarkivet Stockholm is responsible for archiving and facilitating records to the public. This processincludes looking through records and identifying sensitive information that requires masking.This is a time-consuming process when done by a human, due to the fact that they have to readthrough a large amount of text. Regionarkivet has requested a solution that would automate this process.A possible solution is to implement a language model that uses Named Entity Recognition (NER)to identify specific data in text and tag it for easy identification. This paper set out to evaluate severallanguage models on their performance on Swedish NER-tasks. The language models selected for evaluationall use the Transformer architecture. From the evaluation it could be seen that a languagemodel, in this case a BERT model, pre-trained on a Swedish corpus gives the best performance. Theevaluation also shows the effect of fine-tuning a language model on a corpus that does not accuratelyrepresent the data used for evaluating the language models.</p><p>Keywords</p><p>Natural Language Processing, Named Entity Recognition, Transformers, BERT, DistilBERT, ALBERT,XLM-R</p>


corrected abstract:
<p>Regionarkivet Stockholm is responsible for archiving and facilitating records to the public. This process includes looking through records and identifying sensitive information that requires masking. This is a time-consuming process when done by a human, due to the fact that they have to read through a large amount of text. Regionarkivet has requested a solution that would automate this process. A possible solution is to implement a language model that uses Named Entity Recognition (NER) to identify specific data in text and tag it for easy identification. This paper set out to evaluate several language models on their performance on Swedish NER-tasks. The language models selected for evaluation all use the Transformer architecture. From the evaluation it could be seen that a language model, in this case a BERT model, pre-trained on a Swedish corpus gives the best performance. The evaluation also shows the effect of fine-tuning a language model on a corpus that does not accurately represent the data used for evaluating the language models.</p>
----------------------------------------------------------------------
In diva2:1880538 abstract is: <p>Hearing impairment is a widespread issue among the elderly, leading to reduced quality of life and social isolation. The increasing digitalization offers both opportunitiesand challenges, particularly in the use of mobile phones. This thesis investigates a technical application aimed at assisting elderly individuals with hearing impairment inlocating their phones when they ring. Through extensive research and analysis, theessential features and characteristics that make the application user-friendly and effective were identified. The results indicate that a combination of visual and auditorysignals, ease of use, and customizable functions are crucial to meeting the needs of thetarget group. The application aims to enhance independence and improve the qualityof life for elderly individuals with hearing impairment.</p>


corrected abstract:
<p>Hearing impairment is a widespread issue among the elderly, leading to reduced quality of life and social isolation. The increasing digitalization offers both opportunities and challenges, particularly in the use of mobile phones. This thesis investigates a technical application aimed at assisting elderly individuals with hearing impairment in locating their phones when they ring. Through extensive research and analysis, the essential features and characteristics that make the application user-friendly and effective were identified. The results indicate that a combination of visual and auditory signals, ease of use, and customizable functions are crucial to meeting the needs of the target group. The application aims to enhance independence and improve the quality of life for elderly individuals with hearing impairment.</p>
----------------------------------------------------------------------
In diva2:1886084 abstract is: <p>ObjectiveThis thesis aims to evaluate and characterize a multi-disease serological assay to study immune responses and antibody levels for various respiratory and child vaccine program pathogens in a longitudinal cohort of children and young adults. The research explores the dynamics of immune responses per age, sex and lung function.</p><p>MethodsThe study involves a collaboration between SciLifeLab, KTH Royal Institute of Technology and the ”Barn, Allergi, Miljö, Stockholm och Epidemiologi” (BAMSE) project. A bead-based multi-disease serological assay was used to measure the presence of antibodies towards 63 antigens in a total of 604 samples. Included in the assay are antigens from 20 diseases with some being purely respiratory such as influenza, SARS−CoV−2 and metapneumovirus but also vaccine connected, diphtheria, pertussis and human papillomavirus (HPV). Applied in this assay are samples from the BAMSE cohort which is a longitudinal cohort including four samples per individual.</p><p>ResultsThe study successfully generated a comprehensive data set with analysis focused on a few selected antigens, including HPV and influenza. Correlation analysis reveals intriguing patterns of immune response, with notable differences observed between age groups, sex and antigens within the same disease. HPV type 6 antigen seems to be connected to females at age groups 16 and 24. For influenza it was found that H1N1_Wisc588_HA correlates to age groups 16 and 24. Longitudinal trajectory analysis elucidates individual variations in immune response patterns over time for the two most significant antigens.</p><p>ConclusionThe findings from this study indicate the possibility to use a multi-disease serological assay while incorporating various antigens. This approach offers a robust method for serological testing across multiple diseases allowing for antibody detection. These observations show the potential of how multi-disease serological panels can be used for diagnostic applications and infectious disease research and preparation.</p>


corrected abstract:
<p><strong>Objective:</strong> This thesis aims to evaluate and characterize a multi-disease serological assay to study immune responses and antibody levels for various respiratory and child vaccine program pathogens in a longitudinal cohort of children and young adults. The research explores the dynamics of immune responses per age, sex and lung function.</p><p><strong>Methods:</strong> The study involves a collaboration between SciLifeLab, KTH Royal Institute of Technology and the ”Barn, Allergi, Miljö, Stockholm och Epidemiologi” (BAMSE) project. A bead-based multi-disease serological assay was used to measure the presence of antibodies towards 63 antigens in a total of 604 samples. Included in the assay are antigens from 20 diseases with some being purely respiratory such as influenza, SARS−CoV−2 and metapneumovirus but also vaccine connected, diphtheria, pertussis and human papillomavirus (HPV). Applied in this assay are samples from the BAMSE cohort which is a longitudinal cohort including four samples per individual.</p><p><strong>Results:</strong> The study successfully generated a comprehensive data set with analysis focused on a few selected antigens, including HPV and influenza. Correlation analysis reveals intriguing patterns of immune response, with notable differences observed between age groups, sex and antigens within the same disease. HPV type 6 antigen seems to be connected to females at age groups 16 and 24. For influenza it was found that H1N1_Wisc588_HA correlates to age groups 16 and 24. Longitudinal trajectory analysis elucidates individual variations in immune response patterns over time for the two most significant antigens.</p><p><strong>Conclusion:</strong> The findings from this study indicate the possibility to use a multi-disease serological assay while incorporating various antigens. This approach offers a robust method for serological testing across multiple diseases allowing for antibody detection. These observations show the potential of how multi-disease serological panels can be used for diagnostic applications and infectious disease research and preparation.</p>
----------------------------------------------------------------------
In diva2:1259563 abstract is: <p>Data visualization is an essential methodology for bioinformatics studies. Spatial Transcriptomics(ST) is a method that aims at measuring the transcriptome of tissue sections while maintaining its spacial information. Finally, the study of biological pathway focuses on a series of biochemical reactions that take place in organisms. As these studies generate a large number of datasets, this thesis attempts to combine the ST’s data with pathwayinformation and visualize it in an intuitive way to assist user comprehension and insight.In this thesis, Python was used for integrating the dataset and JavaScript libraries wereused for building the visualization. The processing of ST pathway data together with the data visualization interface are the outcomes of this thesis. The data visualization can show the regulation of pathways in the ST data and can be accessed by modern browsers. These outcomes can help users navigate the ST and pathway datasets more effectively.</p>


corrected abstract:
<p>Data visualization is an essential methodology for bioinformatics studies. Spatial Transcriptomics (ST) is a method that aims at measuring the transcriptome of tissue sections while maintaining its spacial information. Finally, the study of biological pathway focuses on a series of biochemical reactions that take place in organisms. As these studies generate a large number of datasets, this thesis attempts to combine the ST’s data with pathway information and visualize it in an intuitive way to assist user comprehension and insight.</p><p>In this thesis, Python was used for integrating the dataset and JavaScript libraries were used for building the visualization. The processing of ST pathway data together with the data visualization interface are the outcomes of this thesis. The data visualization can show the regulation of pathways in the ST data and can be accessed by modern browsers. These outcomes can help users navigate the ST and pathway datasets more effectively.</p>
----------------------------------------------------------------------
In diva2:1560846 abstract is: <p>Tourism is an important factor for economic growth. Unfortunately, the on going COVID-19 pandemic has struck hard on the tourism sector due to the lockdowns and travel restrictions. The lockdowns have also led to an increasing isolation among people which in the long term can lead to a decline in people’s psychological wellbeing.Together with Cybercom Group AB, an idea to solve this problem was to developan application with the intention to nurture the tourism sector and get people out of their homes while keeping the human interactions at a satisfactory level.</p><p>The main feature of the application developed was a scheduler that carefully planned out people’s daily activities depending how crowded a specific location was. An application such as the one developed could lead to an increase in foot traffic while simultaneously decreasing the amount of physical contact between people.</p><p>The result of this thesis mainly focuses on the developed application but more specifically the developed algorithms to schedule your day using crowd data. The algorithmdeveloped, the Optimal Time Slot Algorithm, averaged a crowding value of18,8% while the average of the best possible crowding value was 17,8%.</p>

corrected abstract:
<p>Tourism is an important factor for economic growth. Unfortunately, the ongoing COVID-19 pandemic has struck hard on the tourism sector due to the lockdowns and travel restrictions. The lockdowns have also led to an increasing isolation among people which in the long term can lead to a decline in people’s psychological well-being. Together with Cybercom Group AB, an idea to solve this problem was to develop an application with the intention to nurture the tourism sector and get people out of their homes while keeping the human interactions at a satisfactory level.</p><p>The main feature of the application developed was a scheduler that carefully planned out people’s daily activities depending how crowded a specific location was. An application such as the one developed could lead to an increase in foot traffic while simultaneously decreasing the amount of physical contact between people.</p><p>The result of this thesis mainly focuses on the developed application but more specifically the developed algorithms to schedule your day using crowd data. The algorithm developed, the Optimal Time Slot Algorithm, averaged a crowding value of 18,8% while the average of the best possible crowding value was 17,8%.</p>
----------------------------------------------------------------------
In diva2:1519510 abstract is: <p>Cancer is a number of diseases where the cells in the body start growing and dividing out ofcontrol. It is a widespread health issue, which requires novel treatments to combat effectively.Some novel strategies involve activating the body’s immune system and letting it attack thecancerous cells. This has been achieved through the use of engineered antibodies, normally a part of the immune system. Antibodies bind to different antigens and the areas that facilitate the binding are well known, allowing for manipulation of what the antibodies bind to. To activate the immune system, cell-surface receptors of T cells and B cells have been targeted. There has been some promise shown, but issues include systemic side effects, some of them severe. A proposed way to circumvent this is to try and only locally activate the immune system through the use of bispecific proteins. The idea behind this is to have specificity towards an activation target and another specificity towards some target of interest on cancer cells. One such class of bispecifics is AffimAbs, monoclonal antibodies (mAbs) with an affibody conjugated to them. An affibody is an affinity molecule derived from the staphylococcal protein A, with its main advantage being its small size of 7 kDa compared to an antibody at 150 kDa. This study investigates the ability of an AffimAb with specificity towards the cell-surface receptor CD40 expressed on B cells and specificity towards PDGFR-β, a cell-surface receptor found to be overexpressed in some types of cancer, to locally activate B cells. To investigate this a co-culture activation assay was designed and the separate parts validated. This included finding proper seeding density for PDGFR-β-expressing cells, establishing a protocol for isolating primary B cells, and validating binding ofconstructs towards targets. The results from the co-culture activation assay show that there is a potential higher activation for the AffimAb compared to a mAb, but the small scale of this study precludes it from being statistically significant. Further, larger assays need to be performed to show these results to be significant.</p>

corrected abstract:
<p>Cancer is a number of diseases where the cells in the body start growing and dividing out of control. It is a widespread health issue, which requires novel treatments to combat effectively. Some novel strategies involve activating the body’s immune system and letting it attack the cancerous cells. This has been achieved through the use of engineered antibodies, normally a part of the immune system. Antibodies bind to different antigens and the areas that facilitate the binding are well known, allowing for manipulation of what the antibodies bind to. To activate the immune system, cell-surface receptors of T cells and B cells have been targeted. There has been some promise shown, but issues include systemic side effects, some of them severe. A proposed way to circumvent this is to try and only locally activate the immune system through the use of bispecific proteins. The idea behind this is to have specificity towards an activation target and another specificity towards some target of interest on cancer cells. One such class of bispecifics is AffimAbs, monoclonal antibodies (mAbs) with an affibody conjugated to them. An affibody is an affinity molecule derived from the staphylococcal protein A, with its main advantage being its small size of 7 kDa compared to an antibody at 150 kDa. This study investigates the ability of an AffimAb with specificity towards the cell-surface receptor CD40 expressed on B cells and specificity towards PDGFR-β, a cell-surface receptor found to be overexpressed in some types of cancer, to locally activate B cells. To investigate this a co-culture activation assay was designed and the separate parts validated. This included finding proper seeding density for PDGFR-β-expressing cells, establishing a protocol for isolating primary B cells, and validating binding of constructs towards targets. The results from the co-culture activation assay show that there is a potential higher activation for the AffimAb compared to a mAb, but the small scale of this study precludes it from being statistically significant. Further, larger assays need to be performed to show these results to be significant.</p>
----------------------------------------------------------------------
In diva2:826706 abstract is: <p>Q-Med is a medical device company that manufactures medical devices, for instance a product that is a gel containing hyaluronic acid. The gel is placed in a syringe component containing a stopper made of rubber that is in contact with the gel.Today, increasingly more polymeric materials for various applications are used, for example medical devices and pharmaceutical packaging in the pharmaceutical industry. The products of Q-Med are considered as medical devices in many markets. Medical devices and pharmaceutical packaging often contains materials composed of polymers, for instance rubber and plastics. For these materials there are different requirements so that they do not contaminate the product that they are in contact with. Contamination may lead to negative consequences, such as toxic reactions and a reduced effectiveness of the drug/product.Since it is polymeric materials that are used there might be some potential impurities in the material, these pollutants are called leachables and extractables. Organic and inorganic compounds are examples of what can leach out of the rubber.In this report the rubber (bromobutyl rubber) that is in contact with a gel prototype (Prototype A and Prototype A-L) is studied to see if the rubber leaches out some compounds of the perspective leachables/extractbales. To know if the rubber leaches out some contaminates the rubber was exposed to various conditions. For instance; different pH, temperature, solvent, placebo solution and contact with the product. In order to analyze which pollutants leached out of the rubber, Solid Phase Micro Extraction (SPME) and GC-MS where used as analytical methods. Three SPME-fibers with different polarity and selectivity where used for the collection of extractable compounds.The results show that the rubber leaches out different compounds. Many of these have a low probability compared to the spectra of masses in the used reference library. This makes it difficult to determine if there is special compound that leaches out or if it is a similar compound that leaches out. When compared, more compounds were found in the gas phase than the liquid phase. This is partly because there are more compounds that are willing to get up in the gas phase, because of the compounds affinity to the sample, for example different polarity. The two main compounds, which are found in the most headspace analyses, are 1-Bromo-3- (2-bromoethyl) heptane and butylated hydroxytoulene.The conclusion is that some compounds, for instance 1-Bromo-3- (2-bromoethyl) heptane and butylated hydroxytoulene leaches out in the gas phase. For other compounds it’s hard to determine if they come from the rubber or from another source from it’s surrounding. This is because the SPME-method is a sensitive method that can absorb a number of compounds from its surroundings. It is therefore considered that more studies must be done in this area but with another method that give more reliable results.</p>


w='leachables/extractbales' val={'c': 'leachables/extractables', 's': 'diva2:826706', 'n': 'error in original'}

corrected abstract:
<p>Q-Med is a medical device company that manufactures medical devices, for instance a product that is a gel containing hyaluronic acid. The gel is placed in a syringe component containing a stopper made of rubber that is in contact with the gel.</p><p>Today, increasingly more polymeric materials for various applications are used, for example medical devices and pharmaceutical packaging in the pharmaceutical industry. The products of Q-Med are considered as medical devices in many markets. Medical devices and pharmaceutical packaging often contains materials composed of polymers, for instance rubber and plastics. For these materials there are different requirements so that they do not contaminate the product that they are in contact with. Contamination may lead to negative consequences, such as toxic reactions and a reduced effectiveness of the drug/product. Since it is polymeric materials that are used there might be some potential impurities in the material, these pollutants are called leachables and extractables. Organic and inorganic compounds are examples of what can leach out of the rubber.</p><p>In this report the rubber (bromobutyl rubber) that is in contact with a gel prototype (Prototype A and Prototype A-L) is studied to see if the rubber leaches out some compounds of the perspective leachables/extractbales. To know if the rubber leaches out some contaminates the rubber was exposed to various conditions. For instance; different pH, temperature, solvent, placebo solution and contact with the product. In order to analyze which pollutants leached out of the rubber, Solid Phase Micro Extraction (SPME) and GC-MS where used as analytical methods. Three SPME-fibers with different polarity and selectivity where used for the collection of extractable compounds.</p><p>The results show that the rubber leaches out different compounds. Many of these have a low probability compared to the spectra of masses in the used reference library. This makes it difficult to determine if there is special compound that leaches out or if it is a similar compound that leaches out. When compared, more compounds were found in the gas phase than the liquid phase. This is partly because there are more compounds that are willing to get up in the gas phase, because of the compounds affinity to the sample, for example different polarity. The two main compounds, which are found in the most headspace analyses, are 1-Bromo-3- (2-bromoethyl) heptane and butylated hydroxytoulene.</p><p>The conclusion is that some compounds, for instance 1-Bromo-3- (2-bromoethyl) heptane and butylated hydroxytoulene leaches out in the gas phase. For other compounds it’s hard to determine if they come from the rubber or from another source from it’s surrounding. This is because the SPME-method is a sensitive method that can absorb a number of compounds from its surroundings. It is therefore considered that more studies must be done in this area but with another method that give more reliable results.</p>
----------------------------------------------------------------------
In diva2:1045594 abstract is: <p>Antifouling coats are used to prevent fouling, such as barnacles and algae, on boat hulls. With the use of antifouling coatings, fouling can be prevented chemically (with biocides),biologically, physically or mechanically. Coatings containing biocides are counted as pesticides and have to be approved by the Swedish Chemicals Agency before they can besold or used in Sweden. Due to the low concentration of salt, there are fewer problems withfouling in Lake Mälaren than in the water on the west coast of Sweden. This means that antifouling products containing biocides used on leisure boats in Lake Mälaren should be questioned.</p><p>Bo Olson at Innovation and Chemical Industries in Sweden created the idea for this thesis. The goal was to analyze the copper leakage from boat hulls covered with antifouling coatings containing copper, to research if using different biocide-free coatings can reduce the leakage and to discuss the pros and cons of blasting as a method for removing the biocides.</p><p>In order to answer how much copper the antifouling colors release into the freshwater and whether the leakages can be reduced by antifouling sealers, glass jars were painted on the inside with different color combinations and was filled with water from Lake Mälaren. After a month, the water samples were analyzed with a flame atomic absorption spectrometer. The results showed a leakage of 271 mg/m2, month for the color VC 17 and 24,3 mg/m2,month for the color Antifouling Copper Plus. From a medium sized leisure boat, with a wetted surface area of 24 m2, between 580 and 6500 mg copper leaks into the freshwater of lake Malaren in one month. The results from the analysis also showed a significant decrease of the copper leakage for both of the studied covering techniques, with 60–80 % for Antifouling Sealer and 94–96 % for the laminating epoxy. Even though the epoxy greatly reduced the copper leakage, Antifouling Sealer is recommended because of the presence of the endocrine disruptor bisphenol A in epoxy plastics. The cost of the amount of Antifouling Sealer needed for a medium sized leisure boat was estimated at SEK 1700. Another possible alternative to antifouling sealers is to blast the boat free from antifouling coatings, cover the boat with, for example, a fouling release coating and then clean it with a pressure washer when needed. Although blasting eliminates the problem with biocides for good, another environmental issue is created when the hazardous waste has to be deposited. The economical cost that comes with blasting is approximately SEK 16 000 excluding VAT for a 24m2 boat, and is significantly higher than if the Antifouling Sealer is applied instead. With financial aids from the County Administrative Board, the cost for the boat owners could however be lowered. Another interesting solution is to use coatings containing enzymes that break down the adhesives the organisms need when attaching to a surface.</p><p>The conclusion of this thesis is that it is difficult to determine the most suitable method forreducing the copper leakage to Lake Mälaren from an environmental point of view. Each method may result in new problems that need to be investigated further. Therefore, the precautionary method is vital to avoid the increasing stress on the environment. The interest to improve the marine environment is however big and the range of environmental friendly alternatives are constantly increasing.</p>

w='Malaren' val={'c': 'Mälaren', 's': 'diva2:1045594', 'n': 'error in original'}

corrected abstract:
<p>Antifouling coats are used to prevent fouling, such as barnacles and algae, on boat hulls. With the use of antifouling coatings, fouling can be prevented chemically (with biocides), biologically, physically or mechanically. Coatings containing biocides are counted as pesticides and have to be approved by the Swedish Chemicals Agency before they can be sold or used in Sweden. Due to the low concentration of salt, there are fewer problems with fouling in Lake Mälaren than in the water on the west coast of Sweden. This means that antifouling products containing biocides used on leisure boats in Lake Mälaren should be questioned.</p><p>Bo Olson at Innovation and Chemical Industries in Sweden created the idea for this thesis. The goal was to analyze the copper leakage from boat hulls covered with antifouling coatings containing copper, to research if using different biocide-free coatings can reduce the leakage and to discuss the pros and cons of blasting as a method for removing the biocides.</p><p>In order to answer how much copper the antifouling colors release into the freshwater and whether the leakages can be reduced by antifouling sealers, glass jars were painted on the inside with different color combinations and was filled with water from Lake Mälaren. After a month, the water samples were analyzed with a flame atomic absorption spectrometer. The results showed a leakage of 271 mg/m<sup>2</sup>, month for the color VC 17 and 24,3 mg/m<sup>2</sup>, month for the color Antifouling Copper Plus. From a medium sized leisure boat, with a wetted surface area of 24 m<sup>2</sup>, between 580 and 6500 mg copper leaks into the freshwater of lake Malaren in one month. The results from the analysis also showed a significant decrease of the copper leakage for both of the studied covering techniques, with 60–80 % for Antifouling Sealer and 94–96 % for the laminating epoxy. Even though the epoxy greatly reduced the copper leakage, Antifouling Sealer is recommended because of the presence of the endocrine disruptor bisphenol A in epoxy plastics. The cost of the amount of Antifouling Sealer needed for a medium sized leisure boat was estimated at SEK 1700. Another possible alternative to antifouling sealers is to blast the boat free from antifouling coatings, cover the boat with, for example, a fouling release coating and then clean it with a pressure washer when needed. Although blasting eliminates the problem with biocides for good, another environmental issue is created when the hazardous waste has to be deposited. The economical cost that comes with blasting is approximately SEK 16 000 excluding VAT for a 24m<sup>2</sup> boat, and is significantly higher than if the Antifouling Sealer is applied instead. With financial aids from the County Administrative Board, the cost for the boat owners could however be lowered. Another interesting solution is to use coatings containing enzymes that break down the adhesives the organisms need when attaching to a surface.</p><p>The conclusion of this thesis is that it is difficult to determine the most suitable method for reducing the copper leakage to Lake Mälaren from an environmental point of view. Each method may result in new problems that need to be investigated further. Therefore, the precautionary method is vital to avoid the increasing stress on the environment. The interest to improve the marine environment is however big and the range of environmental-friendly alternatives are constantly increasing.</p>
----------------------------------------------------------------------
In diva2:1463598 abstract is: <p>This study was conducted in order to see further combinations for lanosol andwhether it’s flame retardancy properties can be utilised, biochar was also used in thestudy as a flame-retardant delivery system between lanosol and the polymer matrix. Lanosol and biochar were used as additives in a polylactic acid, PLA, matrix or apolypropylene, PP, matrix with different percentages. The properties which were investigated in this study were the following: the fire properties, the mechanical properties, the thermal properties and the morphology. Several different methods were used in order to conduct the study, to investigate thefire properties the micro-scale combustion calorimetry, MCC, was used. Themechanical properties were tested using the INSTRON tensile testing machine. To investigate the thermal properties two analysis methods, the Thermogravimetric analysis, TGA and Differential Scanning Calorimetry, DSC were used. Themorphology was studied with the Scanning Electron Microscopy, SEM.</p>

corrected abstract:
<p>This study was conducted in order to see further combinations for lanosol and whether it’s flame retardancy properties can be utilised, biochar was also used in the study as a flame-retardant delivery system between lanosol and the polymer matrix. Lanosol and biochar were used as additives in a polylactic acid, PLA, matrix or a polypropylene, PP, matrix with different percentages. The properties which were investigated in this study were the following: the fire properties, the mechanical properties, the thermal properties and the morphology. Several different methods were used in order to conduct the study, to investigate the fire properties the micro-scale combustion calorimetry, MCC, was used. The mechanical properties were tested using the INSTRON tensile testing machine. To investigate the thermal properties two analysis methods, the Thermogravimetric analysis, TGA and Differential Scanning Calorimetry, DSC were used. The morphology was studied with the Scanning Electron Microscopy, SEM.</p>
----------------------------------------------------------------------
In diva2:1094126 abstract is: <p>In this master'sthesis,a strategy to synthesize amphiphilicpeptide-polymerconjugates is presented. The amphiphilic molecule was obtained in two separate steps, first a protocol for synthesizing hydrophobic polymers was developed followed by covalently conjugationtoa chelating peptide.</p><p>These nanomaterials are inspired by the naturally marine occurring siderophores, compounds which marine bacteria excrete for iron chelation. A synthetic material with similar applications could be applicable in, for instance,removing toxic metals from water or starving bacteria of metal ions required for the irmetabolism.</p><p>The synthesis of oligomeric-hydrophobic polymer blocks was performed via atom transfer radical polymerization (ATRP) with a protected maleimide initiator. Efforts to enhance uniform dispersity, obtain reproducibility and gain control over the polymerization were performed by kinetic experimental work. The main focus was put towards styrene polymerization and work up procedures of the obtained product, including purification , dehalogenation anddeprotection.</p><p>A protocol for tunable styrene oligomers was developed and optimized to fit the requirements of the conjugation step, before being coupled to  a commercially  available (His)5-  p eptide  via thiol-ene  chemistry. Experiments were conducted to conjugate the peptide to a novel synthetic fatty-acidtail utilizing the same method . Both studies show successful formation of a mphiphilic compounds which were analyzed by high performance liquid chromatography (HPLC), 1 H-nuclear magnetic resonance spectroscopy(1H-NMR) and matrix-assisted laser desorption/ionization(MALDI).</p><p>Future research aims to use these conjugates to explore self-assembly behavior and stimuli-responsiveness  of  the nanomaterials.</p>


w='eptide' val={'c': 'peptide', 's': 'diva2:1094126', 'n': 'no full text'}
w='irmetabolism' val={'c': 'iron metabolism', 's': 'diva2:1094126', 'n': 'no full text'}
w='mphiphilic' val={'c': 'amphiphilic', 's': 'diva2:1094126', 'n': 'no full text'}

corrected abstract:
<p>In this master's thesis, a strategy to synthesize a amphiphilic peptide-polymer conjugates is presented. The amphiphilic molecule was obtained in two separate steps, first a protocol for synthesizing hydrophobic polymers was developed followed by covalently conjugation to a chelating peptide.</p><p>These nanomaterials are inspired by the naturally marine occurring siderophores, compounds which marine bacteria excrete for iron chelation. A synthetic material with similar applications could be applicable in, for instance, removing toxic metals from water or starving bacteria of metal ions required for the iron metabolism.</p><p>The synthesis of oligomeric-hydrophobic polymer blocks was performed via atom transfer radical polymerization (ATRP) with a protected maleimide initiator. Efforts to enhance uniform dispersity, obtain reproducibility and gain control over the polymerization were performed by kinetic experimental work. The main focus was put towards styrene polymerization and work up procedures of the obtained product, including purification, dehalogenation and deprotection.</p><p>A protocol for tunable styrene oligomers was developed and optimized to fit the requirements of the conjugation step, before being coupled to a commercially available (His)5-peptide  via thiolene  chemistry. Experiments were conducted to conjugate the peptide to a novel synthetic fatty-acid tail utilizing the same method. Both studies show successful formation of a amphiphilic compounds which were analyzed by high performance liquid chromatography (HPLC), <sup>1</sup>H-nuclear magnetic resonance spectroscopy (1H-NMR) and matrix-assisted laser desorption/ionization(MALDI).</p><p>Future research aims to use these conjugates to explore self-assembly behavior and stimuli-responsiveness  of  the nanomaterials.</p>
----------------------------------------------------------------------
title: "Production of Biochar Through Slow Pyrolysis of Biomass: Peat,Straw, Horse Manure and Sewage Sludge"
==>    "Production of Biochar Through Slow Pyrolysis of Biomass: Peat, Straw, Horse Manure and Sewage Sludge"

In diva2:1295394 abstract is: <p>With a growing concern of climate change due to increased levels of CO2 in the atmosphere, carbon sequestration has been suggested as a possible solution for climate change mitigation. Biochar,a highly carbonaceous product produced through pyrolysis, is considered a viable option due to its content of stable carbon. This work covers the investigation of the possibility to produce biocharfrom four different feedstocks, namely peat, straw, horse manure and sewage sludge. The study includes a literature study and a five-week trial period at a 500 kW pilot plant, PYREG 500, in Högdalen. The thermal behaviour of the feedstocks, including garden waste, was investigated using thermogravimetric analysis (TGA). The TGA results were used to decide the optimal pyrolysis temperature for peat and straw at the pilot plant. The TGA results showed that the feedstocks behave differently when pyrolysed; the mass loss rate as well as the final mass loss varied. Physiochemical characterisation of the biochar was completed and the results were in agreement with previous studies. The produced biochar from straw and two types of peat had a C content above50 wt.% (76.6, 80.7, 79.2 wt.%) and low molar ratios of H/C (0.33, 0.36, 0.38) and O/C (0.032,0.023, 0.024). The pH increased as a consequence of pyrolysis and the biochars were alkaline (pH10.1, 8.5, 8.3). Polycyclic aromatic hydrocarbons (PAHs) were found in biochar from both strawand peat (8.26, 1.03, 5.83 mg/kg). In general, nutrients and heavy metals were concentrated in the biochar, except for Cd which decreased and Hg which could not be determined. The specific surface area of biochar from straw was considered small (21 m2/g) while biochar from peat had a higher specific surface area with a greater span (102-247 m2/g). The properties of the produced biochar were compared to the criteria included in the European Biochar Certificate and some of them were fulfilled, including the content of C, PAH and heavy metals. A flue gas analysis was completed when operating the pilot plant on straw pellets and it was showed that several emissions were released, including NO2, SOX, HCl and particulates, however, solely the emissions of NO2 exceed the regulations which will be applied in 2020. Regarding process design of a future pyrolysis plant, it is suggested that the means of material transport, particle separation, temperature control and quenching of biochar should be improved.</p>

w='SOX' val={'c': 'SO<sub>X</sub>', 's': 'diva2:1295394', 'n': 'correct in original'}

corrected abstract:
<p>With a growing concern of climate change due to increased levels of CO<sub>2</sub> in the atmosphere, carbon sequestration has been suggested as a possible solution for climate change mitigation. Biochar, a highly carbonaceous product produced through pyrolysis, is considered a viable option due to its content of stable carbon. This work covers the investigation of the possibility to produce biochar from four different feedstocks, namely peat, straw, horse manure and sewage sludge. The study includes a literature study and a five-week trial period at a 500 kW pilot plant, PYREG 500, in Högdalen. The thermal behaviour of the feedstocks, including garden waste, was investigated using thermogravimetric analysis (TGA). The TGA results were used to decide the optimal pyrolysis temperature for peat and straw at the pilot plant. The TGA results showed that the feedstocks behave differently when pyrolysed; the mass loss rate as well as the final mass loss varied. Physiochemical characterisation of the biochar was completed and the results were in agreement with previous studies. The produced biochar from straw and two types of peat had a C content above 50 wt.% (76.6, 80.7, 79.2 wt.%) and low molar ratios of H/C (0.33, 0.36, 0.38) and O/C (0.032, 0.023, 0.024). The pH increased as a consequence of pyrolysis and the biochars were alkaline (pH 10.1, 8.5, 8.3). Polycyclic aromatic hydrocarbons (PAHs) were found in biochar from both straw and peat (8.26, 1.03, 5.83 mg/kg). In general, nutrients and heavy metals were concentrated in the biochar, except for Cd which decreased and Hg which could not be determined. The specific surface area of biochar from straw was considered small (21 m<sup>2</sup>/g) while biochar from peat had a higher specific surface area with a greater span (102-247 m<sup>2</sup>/g). The properties of the produced biochar were compared to the criteria included in the European Biochar Certificate and some of them were fulfilled, including the content of C, PAH and heavy metals. A flue gas analysis was completed when operating the pilot plant on straw pellets and it was showed that several emissions were released, including NO<sub>2</sub>, SO<sub>X</sub>, HCl and particulates, however, solely the emissions of NO<sub>2</sub> exceed the regulations which will be applied in 2020. Regarding process design of a future pyrolysis plant, it is suggested that the means of material transport, particle separation, temperature control and quenching of biochar should be improved.</p>
----------------------------------------------------------------------
In diva2:1673978 abstract is: <p>Glycoside hydrolases (GHs) are hydrolytic enzymes that catalyse the hydrolysis of glycosidic linkages in complex carbohydrates like polysaccharides. In this project, an enzyme from GH family 30, subfamily 3, has been characterized. For GH30, enzyme activities established are glucosylceramidase, β-glucosidase, β-xylosidase, and endo-β-1,6-glucanase. Previously, only two bacterial GH30s from subfamily 3 had been characterized. These GHs have shown activity on the hydrolysis of β-1,6-glucan, a polysaccharide found in the cell wall of some fungi and oomycetes. Since the enzyme investigated in this project was also a bacterial GH30 from subfamily 3, it was predicted to be a β-1,6-glucanase.</p><p>The aim of this project was to investigate if a new family of CBMs being characterized in the McKee group could improve activity and/or thermostability of the predicted β-1,6-glucanase. Carbohydrate binding modules (CBMs) are non-catalytic protein domains that can be found attached to carbohydrate-active enzymes in microorganisms. It has been discovered that CBMs can confer certain features to enzymes, such as high activity and thermostability. If the CBM could transfer higher activity and/or thermostability to the GH30 investigated here, it would be of interest for industrial applications such as biorefinery since this would reduce the cost of the processes. A possible application could therefore be biorefinery using fungi from agricultural waste as biomass.</p><p>Thus, in this project, a gene encoding the predicted β-1,6 glucanase from the bacterium Mucilaginibacter rubeus was investigated by over-expression of recombinant forms of the fulllength protein (GH30 + CBM) and a truncated protein (GH30) without the CBM domain. As far as we know, this is the third β-1,6 glucanase from bacteria ever to be characterized. We found that this enzyme had endo-β-1,6 glucanase activity with high activity on Pustulan. We also found that GH30 + CBM had higher catalytic activity than GH30 alone, and GH30 + CBM could degrade the substrate for up to 12 days with increasing hydrolysis of products, compared to GH30 for which the activity only lasted for 7 days. Furthermore, CBM appended to GH30 increased the binding affinity to the substrate ligands of Pustulan and Scleroglucan. However, CBM appended to GH30 did not increase enzyme thermostability to any noticeable effect, so we conclude that the primary role of the CBM in this multi-modular protein is to increase reaction rate and catalytic efficiency. Future explorations of this enzyme could focus on the aforementioned biorefinery concept or investigate the potential for enzymesdegrading fungal cell wall components to be used in bio-pesticides.</p>

corrected abstract:
<p>Glycoside hydrolases (GHs) are hydrolytic enzymes that catalyse the hydrolysis of glycosidic linkages in complex carbohydrates like polysaccharides. In this project, an enzyme from GH family 30, subfamily 3, has been characterized. For GH30, enzyme activities established are glucosylceramidase, β-glucosidase, β-xylosidase, and endo-β-1,6-glucanase. Previously, only two bacterial GH30s from subfamily 3 had been characterized. These GHs have shown activity on the hydrolysis of β-1,6-glucan, a polysaccharide found in the cell wall of some fungi and oomycetes. Since the enzyme investigated in this project was also a bacterial GH30 from subfamily 3, it was predicted to be a β-1,6-glucanase.</p><p>The aim of this project was to investigate if a new family of CBMs being characterized in the McKee group could improve activity and/or thermostability of the predicted β-1,6-glucanase. Carbohydrate binding modules (CBMs) are non-catalytic protein domains that can be found attached to carbohydrate-active enzymes in microorganisms. It has been discovered that CBMs can confer certain features to enzymes, such as high activity and thermostability. If the CBM could transfer higher activity and/or thermostability to the GH30 investigated here, it would be of interest for industrial applications such as biorefinery since this would reduce the cost of the processes. A possible application could therefore be biorefinery using fungi from agricultural waste as biomass.</p><p>Thus, in this project, a gene encoding the predicted β-1,6 glucanase from the bacterium <em>Mucilaginibacter rubeus</em> was investigated by over-expression of recombinant forms of the full length protein (GH30 + CBM) and a truncated protein (GH30) without the CBM domain. As far as we know, this is the third β-1,6 glucanase from bacteria ever to be characterized. We found that this enzyme had endo-β-1,6 glucanase activity with high activity on Pustulan. We also found that GH30 + CBM had higher catalytic activity than GH30 alone, and GH30 + CBM could degrade the substrate for up to 12 days with increasing hydrolysis of products, compared to GH30 for which the activity only lasted for 7 days. Furthermore, CBM appended to GH30 increased the binding affinity to the substrate ligands of Pustulan and Scleroglucan. However, CBM appended to GH30 did not increase enzyme thermostability to any noticeable effect, so we conclude that the primary role of the CBM in this multi-modular protein is to increase reaction rate and catalytic efficiency. Future explorations of this enzyme could focus on the afore mentioned biorefinery concept or investigate the potential for enzymes degrading fungal cell wall components to be used in bio-pesticides.</p>
----------------------------------------------------------------------
In diva2:442685 abstract is: <p><strong>Summary</strong>The area of study in this project is in Bolivia, South America. The specific area is theMunicipality of Tiquipaya, situated in Cochabamba valley. Cochabamba valley consists of a gently sloping plain bounded by steep slopes of the Cordillera. The vegetation in the Cordillera is mostly high steppe and high mountain prairie. The average annual precipitation is 600-1000 mm/ year. When it rains in the mountain range the rivers flow down to the valley with high velocity and cause great problems with erosion. In the valley the average annual precipitation is 500 mm/year and the land use is mostly agricultural. Because of the intense agriculture and the rapid population growth, Cochabamba Central Valley suffers a constant shortage of water. Different irrigation systems and groundwater from the aquifer supplies Tiquipaya with water.The aquifer in Tiquipaya is one out of three thick sedimentary bodies in the Central Valley. The aquifer is an alluvial fan delta and the thickness is approximately 500 m. It is recharged through infiltration from the mountain rivers, precipitation on the valley area and by irrigation. The discharge from the aquifer consists of wells and springs. There are 37 flowing springs in Tiquipaya and they have been in use for hundreds of years. During the last 10 years the habitants have noticed a decreasing amount of flow in the springs. This indicates that the groundwater level is sinking. In order to find out whether or not the aquifer reserve is decreasing a water balance is calculated. To estimate the recharge to the aquifer we had access to precipitation data, but we had to estimate the evapotranspiration and the percentage which infiltrates. Concerning the discharge we had access to specific spring data over two areas and a list of registered wells. We know that there are more wells then the ones registered, but we did not know how many and what flow rate they have. Based on the existing data and our assumptions we have estimated a possible discharge for the Tiquipaya area. To get a truthful picture of the reality, we have made three water balances based on different data and variations. The one we find most likely show that the aquifer is decreasing.The aquifer is under a lot of environmental threats. The town of Tiquipaya is situated on the aquifer and the town’s residual water is used as irrigation water without any treatment. Also the river beds, which are very permeable recharge zones, are being used as garbage heaps. Concerning the aquifer the most impending problems are the use of residual water and the garbage handling. To obtain a sustainable development the habitants' residual water and garbage have to be taken care of.As a conclusion we do not think that the actual situation in Tiquipaya is sustainable. The groundwater reserve is decreasing and can not supply enough water for the municipality. If the agriculture stays as intensive as it is today the municipality needs to supplement with water from watersheds outside of the Cochabamba Valley. This would also give the aquifer a chance to restore its reserve. The completion of the Misicuni project will hopefully provide Tiquipaya with the water supplement needed. In order to determine the exact extension and capacity of the aquifer more reliable data is needed. We recommend geophysical tests, drilling and field investigations to obtain this data.</p>

corrected abstract:
<p>The area of study in this project is in Bolivia, South America. The specific area is the Municipality of Tiquipaya, situated in Cochabamba valley. Cochabamba valley consists of a gently sloping plain bounded by steep slopes of the Cordillera. The vegetation in the Cordillera is mostly high steppe and high mountain prairie. The average annual precipitation is 600-1000 mm/ year. When it rains in the mountain range the rivers flow down to the valley with high velocity and cause great problems with erosion.</p><p>In the valley the average annual precipitation is 500 mm/year and the land use is mostly agricultural. Because of the intense agriculture and the rapid population growth, Cochabamba Central Valley suffers a constant shortage of water. Different irrigation systems and groundwater from the aquifer supplies Tiquipaya with water.</p><p>The aquifer in Tiquipaya is one out of three thick sedimentary bodies in the Central Valley. The aquifer is an alluvial fan delta and the thickness is approximately 500 m. It is recharged through infiltration from the mountain rivers, precipitation on the valley area and by irrigation.</p><p>The discharge from the aquifer consists of wells and springs. There are 37 flowing springs in Tiquipaya and they have been in use for hundreds of years. During the last 10 years the habitants have noticed a decreasing amount of flow in the springs. This indicates that the groundwater level is sinking. In order to find out whether or not the aquifer reserve is decreasing a water balance is calculated. To estimate the recharge to the aquifer we had access to precipitation data, but we had to estimate the evapotranspiration and the percentage which infiltrates. Concerning the discharge we had access to specific spring data over two areas and a list of registered wells. We know that there are more wells then the ones registered, but we did not know how many and what flow rate they have. Based on the existing data and our assumptions we have estimated a possible discharge for the Tiquipaya area. To get a truthful picture of the reality, we have made three water balances based on different data and variations. The one we find most likely show that the aquifer is decreasing.</p><p>The aquifer is under a lot of environmental threats. The town of Tiquipaya is situated on the aquifer and the town’s residual water is used as irrigation water without any treatment. Also the river beds, which are very permeable recharge zones, are being used as garbage heaps. Concerning the aquifer the most impending problems are the use of residual water and the garbage handling. To obtain a sustainable development the habitants' residual water and garbage have to be taken care of.</p><p>As a conclusion we do not think that the actual situation in Tiquipaya is sustainable. The groundwater reserve is decreasing and can not supply enough water for the municipality.</p><p>If the agriculture stays as intensive as it is today the municipality needs to supplement with water from watersheds outside of the Cochabamba Valley. This would also give the aquifer a chance to restore its reserve. The completion of the Misicuni project will hopefully provide Tiquipaya with the water supplement needed.</p><p>In order to determine the exact extension and capacity of the aquifer more reliable data is needed. We recommend geophysical tests, drilling and field investigations to obtain this data.</p>
----------------------------------------------------------------------
In diva2:1801804 abstract is: <p>The first Metatarsophalangeal (MTP) joint is essential for foot biomechanics and weight-bearing activities. Osteoarthritis in this joint can lead to pain, discomfort, and limited mobility. In order to treat this, Episurf Medical is working to produce individualized implants based on 3D segmentations of the joint. As manual segmentations are both time- and cost-consuming, and susceptible to human errors, automatic approaches are preferred. This thesis uses U-Net and DeepEdit as deep-learning based methods for segmentation of the MTP joint, with the latter being evaluated with and without user interactions. The dataset used in this study consisted of 38 CT images, where each model was trained on 30 images, and the remaining images were used as a test set. The final models were evaluated and compared with regards to the Dice Similarity Coefficient (DSC), precision, and recall. The U-Net model achieved DSC 0.944, precision 0.961, and recall 0.929. The automatic DeepEdit approach obtained DSC of 0.861, precision of 0.842, and recall of 0.891, while the interactive DeepEdit approach resulted in DSC of 0.918, precision of 0.912, and recall of 0.928. All pairwise comparisons in terms of precision and DSC showed significant differences (p&lt;0.05), where U-Net had the highest performance, while the difference in recall was not found to be significant (p&gt;0.05) for any comparison. The lower performances of DeepEdit compared to U-Net could be due to lower spatial resolution in the segmentations. Nevertheless, DeepEdit remains a promising method, and further investigations of unexplored areas could be addressed as future work.</p>


corrected abstract:
<p>The first Metatarsophalangeal (MTP) joint is essential for foot biomechanics and weight-bearing activities. Osteoarthritis in this joint can lead to pain, discomfort, and limited mobility. In order to treat this, Episurf Medical is working to produce individualized implants based on 3D segmentations of the joint. As manual segmentations are both time- and cost-consuming, and susceptible to human errors, automatic approaches are preferred. This thesis uses U-Net and DeepEdit as deep-learning based methods for segmentation of the MTP joint, with the latter being evaluated with and without user interactions. The dataset used in this study consisted of 38 CT images, where each model was trained on 30 images, and the remaining images were used as a test set. The final models were evaluated and compared with regards to the Dice Similarity Coefficient (DSC), precision, and recall. The U-Net model achieved DSC 0.944, precision 0.961, and recall 0.929. The automatic DeepEdit approach obtained DSC of 0.861, precision of 0.842, and recall of 0.891, while the interactive DeepEdit approach resulted in DSC of 0.918, precision of 0.912, and recall of 0.928. All pairwise comparisons in terms of precision and DSC showed significant differences (p&lt; 0.05), where U-Net had the highest performance, while the difference in recall was not found to be significant (p&gt; 0.05) for any comparison. The lower performances of DeepEdit compared to U-Net could be due to lower spatial resolution in the segmentations. Nevertheless, DeepEdit remains a promising method, and further investigations of unexplored areas could be addressed as future work.</p>
----------------------------------------------------------------------
In diva2:1802066 abstract is: <p>Workplace safety, particularly in manual handling tasks, is a critical concern that hasbeen increasingly addressed using advanced risk assessment tools. However, pre-senting the complex results of these assessments in an easily digestible format re-mains a challenge. This thesis focused on designing and developing a user-friendlyweb application to visualise risk assessment data effectively. Grounded in a robusttheoretical framework that combines user experience principles, and data visualisa-tion techniques. The study employed an iterative, user-centric design process to de-velop the web application. Multiple visualisation methods, such as pie charts for vis-ualising risk distribution, bar chart, and line chart for time-based analysis, were eval-uated for their effectiveness through usability testing. The application's primary con-tribution lies in its efficient data visualisation techniques, aimed at simplifying com-plex datasets into actionable insights. This work lays the groundwork enabling futuredevelopment by pinpointing areas for improvement like enhanced interactivity andaccessibility.</p>

w='con-tribution' val={'c': 'contribution', 's': 'diva2:1802066'}
w='com-plex' val={'c': 'complex', 's': ['diva2:1864417', 'diva2:1802066']}
w='de-velop' val={'c': 'develop', 's': 'diva2:1802066'}

corrected abstract:
<p>Workplace safety, particularly in manual handling tasks, is a critical concern that has been increasingly addressed using advanced risk assessment tools. However, presenting the complex results of these assessments in an easily digestible format remains a challenge. This thesis focused on designing and developing a user-friendly web application to visualise risk assessment data effectively. Grounded in a robust theoretical framework that combines user experience principles, and data visualisation techniques. The study employed an iterative, user-centric design process to develop the web application. Multiple visualisation methods, such as pie charts for visualising risk distribution, bar chart, and line chart for time-based analysis, were evaluated for their effectiveness through usability testing. The application's primary contribution lies in its efficient data visualisation techniques, aimed at simplifying complex datasets into actionable insights. This work lays the groundwork enabling future development by pinpointing areas for improvement like enhanced interactivity and accessibility.</p>
----------------------------------------------------------------------
In diva2:1343214 abstract is: <p>The increasing encumbrance on Emergency Rooms in Sweden contributes to a vigorous flow to theX-ray Clinics. This leads to an increasing work load for the personnel on the X-ray Clinic, and theywould benefit from having a screen that shows the flow of patients from the Emergency Room tothe X-ray Clinic. The purpose of this study was to investigate possibilities to implement such an aidon the X-ray Clinic at Södersjukhuset. The method consisted of an observation of the X-ray Clinicand analyses of manuals and interviews with personnel associated to different database systems,such as TakeCare, RIS and PACS. The results showed that a solution exists in TakeCare. However,the production of this solution would result in a long and advanced process. This is due to the different requirements on medical products, partially because of the law regarding patient data security.</p>

corrected abstract:
<p>The increasing encumbrance on Emergency Rooms in Sweden contributes to a vigorous flow to the X-ray Clinics. This leads to an increasing work load for the personnel on the X-ray Clinic, and they would benefit from having a screen that shows the flow of patients from the Emergency Room to the X-ray Clinic. The purpose of this study was to investigate possibilities to implement such an aid on the X-ray Clinic at Södersjukhuset. The method consisted of an observation of the X-ray Clinic and analyses of manuals and interviews with personnel associated to different database systems, such as TakeCare, RIS and PACS. The results showed that a solution exists in TakeCare. However, the production of this solution would result in a long and advanced process. This is due to the different requirements on medical products, partially because of the law regarding patient data security.</p>
----------------------------------------------------------------------
In diva2:744734 abstract is: <p>BillerudKorsnäs is a manufacturer of fiber-based cartonboard and liquid packaging board. Microbial growth occurs at several steps in cartonboard production due to favourable environment and the good access to nutrients from the raw material, and additives such as starch. Vegetative bacteria are usually not harmful in the production and die in the hot drying end of the cartonboard machine. The most abundant microflora at paper- and cartonboard factories consists largely of sporeforming microorganisms from the genera Bacillus and Paenibacillus. The endospores are highly resistant and can stay in the final end product, which is undesirable. Levels of endospores from these species at BillerudKorsnäs production unit KM5 are usually low, but an occational increase can be seen when a new cartonboard product, KW1 is produced. Today, the method used for controlling the microbiology is by adding biocides to broke towers. This has shown to be both expensive and non-effective at KM5. A new method is needed for controlling the microbiology at KM5 that is more effective, costbeneficial and environmental friendly.The aim of this project was to test a hypothesis for spore formation at a paper board factory in lab-scale experiments. A suggestion of a technical change in the process would be made that could minimize spore formation and the use of biocides at KM5. A model organism Bacillus licheniformis (E-022052) was used to study effects of environmental conditions on spore formation. Experiments were also performed in controlled bioreactor trials, where methods to minimize spore formation were tested.The experiments showed that nutrient deficiency of a primary carbon source was the major reason for spore formation and should be avoided at KM5. Further, the experiments showed that oxygen limitation significantly decreases the endospore formation.The conclusion reached, was that spore formation could be minimized by a feed addition of glucose to Broke tower 1 during the few days production of KW1. A second alternative includes using a feed of concentrated pulp that could be used to minimize spore formation without the use of biocides and without the need for rebuilding of the mill.</p>

w='occational' val={'c': 'occasional', 's': 'diva2:744734', 'n': 'error in original'}

corrected abstract:
<p>BillerudKorsnäs is a manufacturer of fiber-based cartonboard and liquid packaging board. Microbial growth occurs at several steps in cartonboard production due to favourable environment and the good access to nutrients from the raw material, and additives such as starch. Vegetative bacteria are usually not harmful in the production and die in the hot drying end of the cartonboard machine. The most abundant microflora at paper- and cartonboard factories consists largely of sporeforming microorganisms from the genera <em>Bacillus</em> and <em>Paenibacillus</em>. The endospores are highly resistant and can stay in the final end product, which is undesirable. Levels of endospores from these species at BillerudKorsnäs production unit KM5 are usually low, but an occational increase can be seen when a new cartonboard product, KW1 is produced. Today, the method used for controlling the microbiology is by adding biocides to broke towers. This has shown to be both expensive and non-effective at KM5. A new method is needed for controlling the microbiology at KM5 that is more effective, costbeneficial and environmental friendly.</p><p>The aim of this project was to test a hypothesis for spore formation at a paper board factory in lab-scale experiments. A suggestion of a technical change in the process would be made that could minimize spore formation and the use of biocides at KM5. A model organism <em>Bacillus licheniformis</em> (E-022052) was used to study effects of environmental conditions on spore formation. Experiments were also performed in controlled bioreactor trials, where methods to minimize spore formation were tested.</p><p>The experiments showed that nutrient deficiency of a primary carbon source was the major reason for spore formation and should be avoided at KM5. Further, the experiments showed that oxygen limitation significantly decreases the endospore formation.</p><p>The conclusion reached, was that spore formation could be minimized by a feed addition of glucose to Broke tower 1 during the few days production of KW1. A second alternative includes using a feed of concentrated pulp that could be used to minimize spore formation without the use of biocides and without the need for rebuilding of the mill.</p>
----------------------------------------------------------------------
In diva2:1449246 abstract is: <p>Pharamacovigilance relates to activities involving drug safety monitoring in the post-marketing phase of the drug development life-cycle. Despite rigorous trials and experiments that drugs undergo before they are available in the market, they can still cause previously unobserved side-effects (also known as adverse events) due to drug–drug interaction, genetic, physiological or demographic reasons. The Uppsala Monitoring Centre (UMC) is the custodian of the global reporting system, VigiBase, for adverse drug reactions in collaboration with the World Health Organization (WHO). VigiBase houses over 20 million case reports of suspected adverse drug reactions from all around the world. However, not all case reports that the UMC receives pertains to adverse reactions that are novel in the safety profile of the drugs. In fact, many of the reported reactions found in the database are known adverse events for the reported drugs. With more than 3 million potential associations between all possible drugs and all possible adverse events present in the database, identifying associations that are likely to represent previously unknown safety concerns requires powerful statistical methods and knowledge of the known safety profiles of the drugs. Therefore, there is a need for a knowledge base with mappings of drugs to their known adverse reactions. To-date, such a knowledge base does not exist.</p><p>The purpose of this thesis is to develop a deep-learning model that learns to extract adverse reactions from product labels — regulatory documents providing the current state of knowledge of the safety profile of a given product — and map them to a standardized terminology with high precision. To achieve this, I propose a two-phase algorithm, with a first scanning phase aimed at finding regions of the text representing adverse reactions, and a second mapping phase aiming at normalizing the detected text fragments into Medical Dictionary for Regulatory Activities (MedDRA) terms, the terminology used at the UMC to represent adverse reactions. A previous dictionary-based algorithm developed at the UMC achieved a scanning F1 of 0.42 (0.31 precision, 0.66 recall) and mapping macro-averaged F1 of 0.43 (0.39 macro-averaged precision, 0.64 macro-averaged recall). State-of-the-art methods achieve F1 above 0.8 and above 0.7 for the scanning and mapping problems respectively. To develop algorithms for adverse reaction extraction, I use the 2019 ADE Evaluation Challenge data, a dataset made by the FDA with 100 product labels annotated for adverse events and their mappings to MedDRA. This thesis explores three architectures for the scanning problem: 1) a Bidirectional Long Short-Term Memory (BiLSTM) encoder followed by a softmax classifier, 2) a BiLSTM encoder with Conditional Random Field (CRF) classifier and finally, 3) a BiLSTM encoder with CRF classifier with Embeddings from Language Model (ELMo) embeddings. For the mapping problem, I explore Information Retrieval techniques using the search engines whoosh and Solr, as well as a Learning to Rank algorithm.</p><p>The BiLSTM encoder with CRF gave the highest performance on finding the adverse events in the texts, with an F1 of 0.67 (0.75 precision, 0.61 recall), representing a 0.06 absolute increase in F1 over the simpler BiLSTM encoder with softmax. Using the ELMo embeddings was proven detrimental and lowered the F1 to 0.62. Error analysis revealed the adopted Inside, Beginning, Outside (IOB2) labelling scheme to be poorly adapted for denoting discontinuous and compound spans while introducing ambiguity in the training data. Based on the gold standard annotated mappings, I also evaluated the whoosh and Solr search engines, with and without Learning to Rank. The best performing search engine on this data was Solr, with a macro-averaged F1 of 0.49 compared to the macro-averaged F1 of 0.47 for the whoosh search engine. Adding a Learning to Rank algorithm on top of each engine did not improve mapping performance, as both macro-averaged F1 dropped by over 0.1 when using the re-ranking approach. Finally, the best performing scanning and mapping algorithms beat the aforementioned dictionary-based baseline F1 by 0.25 in the scanning phase and 0.06 in the mapping phase. A large source of error for the Solr search engine came from tokenisation issues, which had a detrimental impact on the performance of the entire pipeline.</p><p>In conclusion, modern Natural Language Processing (NLP) techniques can significantly improve the performance of adverse event detection from free-formtext compared to dictionary-based approaches, especially in cases where context is important.</p>


corrected abstract:
<p>Pharamacovigilance relates to activities involving drug safety monitoring in the post-marketing phase of the drug development life-cycle. Despite rigorous trials and experiments that drugs undergo before they are available in the market, they can still cause previously unobserved side-effects (also known as adverse events) due to drug–drug interaction, genetic, physiological or demographic reasons. The Uppsala Monitoring Centre (UMC) is the custodian of the global reporting system, VigiBase, for adverse drug reactions in collaboration with the World Health Organization (WHO). VigiBase houses over 20 million case reports of suspected adverse drug reactions from all around the world. However, not all case reports that the UMC receives pertains to adverse reactions that are novel in the safety profile of the drugs. In fact, many of the reported reactions found in the database are known adverse events for the reported drugs. With more than 3 million potential associations between all possible drugs and all possible adverse events present in the database, identifying associations that are likely to represent previously unknown safety concerns requires powerful statistical methods and knowledge of the known safety profiles of the drugs. Therefore, there is a need for a knowledge base with mappings of drugs to their known adverse reactions. To-date, such a knowledge base does not exist.</p><p>The purpose of this thesis is to develop a deep-learning model that learns to extract adverse reactions from product labels — regulatory documents providing the current state of knowledge of the safety profile of a given product — and map them to a standardized terminology with high precision. To achieve this, I propose a two-phase algorithm, with a first scanning phase aimed at finding regions of the text representing adverse reactions, and a second mapping phase aiming at normalizing the detected text fragments into Medical Dictionary for Regulatory Activities (MedDRA) terms, the terminology used at the UMC to represent adverse reactions. A previous dictionary-based algorithm developed at the UMC achieved a scanning F1 of 0.42 (0.31 precision, 0.66 recall) and mapping macro-averaged F1 of 0.43 (0.39 macro-averaged precision, 0.64 macro-averaged recall). State-of-the-art methods achieve F1 above 0.8 and above 0.7 for the scanning and mapping problems respectively.</p><p>To develop algorithms for adverse reaction extraction, I use the 2019 ADE Evaluation Challenge data, a dataset made by the FDA with 100 product labels annotated for adverse events and their mappings to MedDRA. This thesis explores three architectures for the scanning problem: 1) a Bidirectional Long Short-Term Memory (BiLSTM) encoder followed by a softmax classifier, 2) a BiLSTM encoder with Conditional Random Field (CRF) classifier and finally, 3) a BiLSTM encoder with CRF classifier with Embeddings from Language Model (ELMo) embeddings. For the mapping problem, I explore Information Retrieval techniques using the search engines whoosh and Solr, as well as a <em>Learning to Rank</em> algorithm.</p><p>The BiLSTM encoder with CRF gave the highest performance on finding the adverse events in the texts, with an F1 of 0.67 (0.75 precision, 0.61 recall), representing a 0.06 absolute increase in F1 over the simpler BiLSTM encoder with softmax. Using the ELMo embeddings was proven detrimental and lowered the F1 to 0.62. Error analysis revealed the adopted Inside, Beginning, Outside (IOB2) labelling scheme to be poorly adapted for denoting discontinuous and compound spans while introducing ambiguity in the training data. Based on the gold standard annotated mappings, I also evaluated the whoosh and Solr search engines, with and without <em>Learning to Rank</em>. The best performing search engine on this data was Solr, with a macro-averaged F1 of 0.49 compared to the macro-averaged F1 of 0.47 for the whoosh search engine. Adding a <em>Learning to Rank</em> algorithm on top of each engine did not improve mapping performance, as both macro-averaged F1 dropped by over 0.1 when using the re-ranking approach. Finally, the best performing scanning and mapping algorithms beat the aforementioned dictionary-based baseline F1 by 0.25 in the scanning phase and 0.06 in the mapping phase. A large source of error for the Solr search engine came from tokenisation issues, which had a detrimental impact on the performance of the entire pipeline.</p><p>In conclusion, modern Natural Language Processing (NLP) techniques can significantly improve the performance of adverse event detection from free-form text compared to dictionary-based approaches, especially in cases where context is important.</p>
----------------------------------------------------------------------
In diva2:1131146 abstract is: <p>The Elekta Synergy is a linear accelerator used in radiation therapy. It is constructed fromvarious mechanical components, including a set of 112.5° bending magnets, which need tobe aligned correctly in order to produce an efficient treatment beam. The current alignmentprocess is outdated and results in inconsistencies, which leads to prolonged assembly timesand the need to correct alignment errors. This paper details the study, design andconstruction of a new bending magnet alignment approach and thereby serves to introducea replacement for the current system. The proposed solution consists of an optimizedmechanical jig in conjunction with image processing software.</p>

corrected abstract:
<p>The Elekta Synergy is a linear accelerator used in radiation therapy. It is constructed from various mechanical components, including a set of 112.5° bending magnets, which need to be aligned correctly in order to produce an efficient treatment beam. The current alignment process is outdated and results in inconsistencies, which leads to prolonged assembly times and the need to correct alignment errors. This paper details the study, design and construction of a new bending magnet alignment approach and thereby serves to introduce a replacement for the current system. The proposed solution consists of an optimized mechanical jig in conjunction with image processing software.</p>
----------------------------------------------------------------------
In diva2:1865420 abstract is: <p>This thesis investigates the optimization of user interfaces (UI) for enhancing userexperiences (UX), particularly focusing on the last-minute ticket page of the flightcomparison website Flygresor.se. The core objective is to improve user engagementand satisfaction through the implementation of effective UI/UX design principles.Using a literature study, semi-structured interviews and iterative prototyping thisresearch identifies key UI/UX improvements. Prototypes were developed and refined partly based on insights gained through the literature review, but prominentlyon feedback from participants in semi-structured interviews, focusing on navigability, readability, and interaction efficiency. The study highlights the significant impactof thoughtful UI/UX design on participants satisfaction and highlights practical recommendations for UI enhancements. The findings emphasize the necessity of continuous UI updates and the potential of user-centric approaches in real-world applications to improve user engagement and operational success. </p>


corrected abstract:
<p>This thesis investigates the optimization of user interfaces (UI) for enhancing user experiences (UX), particularly focusing on the last-minute ticket page of the flight comparison website Flygresor.se. The core objective is to improve user engagement and satisfaction through the implementation of effective UI/UX design principles. Using a literature study, semi-structured interviews and iterative prototyping this research identifies key UI/UX improvements. Prototypes were developed and refined partly based on insights gained through the literature review, but prominently on feedback from participants in semi-structured interviews, focusing on navigability, readability, and interaction efficiency. The study highlights the significant impact of thoughtful UI/UX design on participants satisfaction and highlights practical recommendations for UI enhancements. The findings emphasize the necessity of continuous UI updates and the potential of user-centric approaches in real-world applications to improve user engagement and operational success.</p>
----------------------------------------------------------------------
In diva2:1824429 abstract is: <p>Flue gas desulphurisation (FGD) is a crucial method to minimise the SO<sub>2</sub> emissions from industrial processes. The FGD system utilise an alkaline sorbent to remove SO<sub>2</sub> from the flue gases. Calcium in form of limestone is a commonly used sorbent where gypsum is produced asa by-product. However, the limestone reactivity, along with impurities within the sorbent, can significantly influence the effectiveness of SO<sub>2</sub> removal and the quality of the by-products.</p><p>At Heidelberg materials Cement Sverige an intermediate product, raw meal (RM) 8, is used assorbent in the FGD and gypsum is used as setting retarder in the cement. The aim with this project is to examine if raw meal 7 or A-sten is a better sorbent than raw meal 8 with respect to consumption rate, gypsum quality and its effect on the cement properties, and economic viability. To accomplish this a theoretical study was performed along with data analysis.</p><p>RM 7 and A-sten are both purer than RM 8 but the raw material cost for RM 7 is about 1.24 times the raw material cost for RM 8 and the production costs and transportation costs are greater for RM 7 than for RM 8. The raw material cost for A-sten is about 0.45 times the cost for RM 8 but is not produced on site. All sorbents contain magnesium which can react with sulphur and precipitate as epsomite or hexahydrite, which also acts as retarders, where epsomite retard the cement setting time significantly compared to gypsum.</p><p>To calculate the consumption rate of raw meal and A-sten into the scrubber three different methods were used. The difference between the methods lies in the consideration of how the calcium and magnesium species in the sorbent react with sulphur, with all calcium and magnesium reacting with sulphur in method 1, method 2 take the mass fraction of sulphurcontaining species into consideration, and method 3 incorporating mole fractions of calcium and magnesium species as well as sulphur from the sorbent.</p><p>The pH and SO2 emissions were analysed for two different time periods where the first analysed period shows a correlation between low pH and high SO<sub>2</sub> emissions, while the latter analysed period lacks a clear pH-SO<sub>2</sub> correlation. A correlation between a lower pH and a lower percentage of MgSO<sub>4</sub>⸱6H<sub>2</sub>O and MgSO<sub>4</sub>⸱7H<sub>2</sub>O in the slurry could be made. The calculated rawmeal flow rate for RM 8 is between 520 and 554 kg/h, depending on the method used. To achieve the same desulphurisation efficiency with RM 7 the flow was calculated to 499-538kg/h and 427-464 kg/h for A-sten.</p><p>Given the comparable mass flow rates of RM 8 and RM 7 it is advisable to retain RM 8 as asorbent in the scrubber regardless of the higher magnesium content due the higher cost associated with RM 7. However, the epsomite content in the slurry should be considered when optimising sulphur in cement production. Using A-sten as sorbent would minimise the rawmaterial costs and result in purer gypsum slurry with a lower epsomite content. Operating with a purer sorbent can also enhance the efficiency of the FGD process, leading to lower SO<sub>2 </sub>emissions.</p><p>The calculations in the report assume that calcium and magnesium in the different sorbents react similarly, further analysis of their reactivity is recommended for more accurate results.</p>


Note there is not always a space between RM and 8 in the actual abstract.
corrected abstract:
<p>Flue gas desulphurisation (FGD) is a crucial method to minimise the SO<sub>2</sub> emissions from industrial processes. The FGD system utilise an alkaline sorbent to remove SO<sub>2</sub> from the flue gases. Calcium in form of limestone is a commonly used sorbent where gypsum is produced as a by-product. However, the limestone reactivity, along with impurities within the sorbent, can significantly influence the effectiveness of SO<sub>2</sub> removal and the quality of the by-products.</p><p>At Heidelberg materials Cement Sverige an intermediate product, raw meal (RM) 8, is used as sorbent in the FGD and gypsum is used as setting retarder in the cement. The aim with this project is to examine if raw meal 7 or A-sten is a better sorbent than raw meal 8 with respect to consumption rate, gypsum quality and its effect on the cement properties, and economic viability. To accomplish this a theoretical study was performed along with data analysis.</p><p>RM 7 and A-sten are both purer than RM 8 but the raw material cost for RM 7 is about 1.24 times the raw material cost for RM 8 and the production costs and transportation costs are greater for RM 7 than for RM 8. The raw material cost for A-sten is about 0.45 times the cost for RM 8 but is not produced on site. All sorbents contain magnesium which can react with sulphur and precipitate as epsomite or hexahydrite, which also acts as retarders, where epsomite retard the cement setting time significantly compared to gypsum.</p><p>To calculate the consumption rate of raw meal and A-sten into the scrubber three different methods were used. The difference between the methods lies in the consideration of how the calcium and magnesium species in the sorbent react with sulphur, with all calcium and magnesium reacting with sulphur in method 1, method 2 take the mass fraction of sulphur-containing species into consideration, and method 3 incorporating mole fractions of calcium and magnesium species as well as sulphur from the sorbent.</p><p>The pH and SO2 emissions were analysed for two different time periods where the first analysed period shows a correlation between low pH and high SO<sub>2</sub> emissions, while the latter analysed period lacks a clear pH-SO<sub>2</sub> correlation. A correlation between a lower pH and a lower percentage of MgSO<sub>4</sub>⸱6H<sub>2</sub>O and MgSO<sub>4</sub>⸱7H<sub>2</sub>O in the slurry could be made. The calculated raw meal flow rate for RM 8 is between 520 and 554 kg/h, depending on the method used. To achieve the same desulphurisation efficiency with RM 7 the flow was calculated to 499-538 kg/h and 427-464 kg/h for A-sten.</p><p>Given the comparable mass flow rates of RM 8 and RM 7 it is advisable to retain RM 8 as a sorbent in the scrubber regardless of the higher magnesium content due the higher cost associated with RM 7. However, the epsomite content in the slurry should be considered when optimising sulphur in cement production. Using A-sten as sorbent would minimise the raw material costs and result in purer gypsum slurry with a lower epsomite content. Operating with a purer sorbent can also enhance the efficiency of the FGD process, leading to lower SO<sub>2 </sub> emissions.</p><p>The calculations in the report assume that calcium and magnesium in the different sorbents react similarly, further analysis of their reactivity is recommended for more accurate results.</p>
----------------------------------------------------------------------
In diva2:1737426 abstract is: <p>MicroRNAs (miRNAs) are non-coding RNAs important for post-transcriptional regulation of protein-coding genes. Furthermore, they impact many cellular processes, such as differentiation. The miRNA biosynthesis pathway involves proteins which are themselves regulated by miRNAs, acting through feedback loops. Additionally, miRNA expression may not directly predict target mRNA regulation. Consequently, parallel profiling of miRNAs, mRNAs and specific proteins over a differentiation process may provide new insights into miRNA function and regulation.</p><p>This study developed Parallel Analysis of miRNAs, Proteins and RNAs (PAMPaR) for bulk-cell co-profiling of miRNAs by small RNA-sequencing, mRNAs by RNA-sequencing and select proteins by proximity extension assay (PEA). The PEA setup was advanced from a previous study using qPCR as final readout to using Next Generation Sequencing (NGS). For this purpose, concentrations of PEA-detection antibodies conjugated to protein-specific oligonucleotides were validated for antibody specificity prior to PEA-NGS. PAMPaR was applied to mouse embryonic stem cells (mESCs) differentiating into neural precursor cells (NPCs). miRNA-profiling showed NPC-downregulation of pluripotency-related miR-290-family members and upregulation of the brain-related miR-9 as well as let-7f and miR-99a; miRNAs involved in differentiation. mRNA-profiling similarly showed expected NPC-upregulation of neuronal-differentiation-related transcripts and downregulation of transcripts involved in embryonic development. miRNA-mRNA correlation analysis indicated several types of co-expression patterns; both expected such as mRNA upregulation in NPCs as targeting miRNAs were downregulated, and unexpected such as upregulation of both miRNAs and target mRNAs in NPCs. Hence, it was concluded that the substantial effects observed on mRNA expression during differentiation are rather driven by transcription factors than by miRNA-expression changes and that deeper computational data analyses are needed for more insights. Furthermore, miRNAs and mRNAs could be profiled at additional timepoints during differentiation to detect gradual changes. PEA-NGS data was generated quantifying miRNA biosynthesis pathway as well as pluripotency proteins. However, correlation to miRNA and mRNA expression data was not possible within the study time frame.</p><p>Taken together, applying PAMPaR to mESCs and NPCs provided insight into miRNA-mediated regulation during neuronal differentiation. Moreover, PAMPaR is a promising multi-omics approach that also functions as a stepping stone for scaling down PEA analyses to the single-cell level.</p>

corrected abstract:
<p>MicroRNAs (miRNAs) are non-coding RNAs important for post-transcriptional regulation of protein-coding genes. Furthermore, they impact many cellular processes, such as differentiation. The miRNA biosynthesis pathway involves proteins which are themselves regulated by miRNAs, acting through feedback loops. Additionally, miRNA expression may not directly predict target mRNA regulation. Consequently, parallel profiling of miRNAs, mRNAs and specific proteins over a differentiation process may provide new insights into miRNA function and regulation.</p><p>This study developed Parallel Analysis of miRNAs, Proteins and RNAs (PAMPaR) for bulk-cell co-profiling of miRNAs by small RNA-sequencing, mRNAs by RNA-sequencing and select proteins by proximity extension assay (PEA). The PEA setup was advanced from a previous study using qPCR as final readout to using Next Generation Sequencing (NGS). For this purpose, concentrations of PEA-detection antibodies conjugated to protein-specific oligonucleotides were validated for antibody specificity prior to PEA-NGS. PAMPaR was applied to mouse embryonic stem cells (mESCs) differentiating into neural precursor cells (NPCs). miRNA-profiling showed NPC-downregulation of pluripotency-related miR-290-family members and upregulation of the brain-related miR-9 as well as let-7f and miR-99a; miRNAs involved in differentiation. mRNA-profiling similarly showed expected NPC-upregulation of neuronal-differentiation-related transcripts and downregulation of transcripts involved in embryonic development. miRNA-mRNA correlation analysis indicated several types of co-expression patterns; both expected such as mRNA upregulation in NPCs as targeting miRNAs were downregulated, and unexpected such as upregulation of both miRNAs and target mRNAs in NPCs. Hence, it was concluded that the substantial effects observed on mRNA expression during differentiation are rather driven by transcription factors than by miRNA-expression changes and that deeper computational data analyses are needed for more insights. Furthermore, miRNAs and mRNAs could be profiled at additional timepoints during differentiation to detect gradual changes. PEA-NGS data was generated quantifying miRNA biosynthesis pathway as well as pluripotency proteins. However, correlation to miRNA and mRNA expression data was not possible within the study time frame.</p><p>Taken together, applying PAMPaR to mESCs and NPCs provided insight into miRNA-mediated regulation during neuronal differentiation. Moreover, PAMPaR is a promising multi-omics approach that also functions as a stepping stone for scaling down PEA analyses to the single-cell level.</p>
----------------------------------------------------------------------
In diva2:1838025 abstract is: <p>Flygresor.se, a leading flight comparison platform, uses machine learning to rankflights based on their likelihood of being clicked. The main goal of this project was toimprove this flight sorting to obtain a better user experience. The platform's existingmodel is based on a neural network approach and a limited set of features. The solution involved developing and comparing two machine learning models, Random Forest and XGBoost besides using a set of existing and newly created features. TheXGBoost model demonstrated superior performance by significantly improving theprediction of clicked flights by 4.18% while also achieving a remarkable increase inefficiency by being 125 times faster than the existing model.</p>

corrected abstract:
<p>Flygresor.se, a leading flight comparison platform, uses machine learning to rank flights based on their likelihood of being clicked. The main goal of this project was to improve this flight sorting to obtain a better user experience. The platform's existing model is based on a neural network approach and a limited set of features. The solution involved developing and comparing two machine learning models, Random Forest and XGBoost besides using a set of existing and newly created features. The XGBoost model demonstrated superior performance by significantly improving the prediction of clicked flights by 4.18% while also achieving a remarkable increase in efficiency by be ing 125 times faster than the existing model.</p>
----------------------------------------------------------------------
In diva2:1184399 abstract is: <p>This thesis is performed on behalf of the department of Systemkonstruktion El at Forsmarks Kraftgrupp AB.This report deals with the task of making an inventory for miniature circuit break-ers (MCB). The aim of the graduation work is to design a quick reference so that the choice of cable and in some cases even MCB’s can be performed by the designer in connection with electrical design. This involves reporting a calculation method for selecting MCB’s in different applications. MCB’s in both AC voltage and DC voltage systems are affected.As reference for Forsmark, this report deals with the system 519.The problem lies in in delimiting the work since every reactor at Forsmark consists of approximately 4000 MCB’s of different characteristics and applications. A very huge importance is also given to parameters specified in SS-EN standards for load capacity.The thesis began with a pilot study of the pros and cons of MBC’s in different appli-cations, which rules control the dimensioning and how the design is carried out today, etc. An analysis was performed based on formulas and calculations present-ed in Swedish Standard such as SS 424 14 03, resulted in a quick reference.</p>

w='MBC' val={'c': 'MCB', 's': 'diva2:1184399', 'n': 'error in original'}

corrected abstract:
<p>This thesis is performed on behalf of the department of Systemkonstruktion El at Forsmarks Kraftgrupp AB.</p><p>This report deals with the task of making an inventory for miniature circuit breakers (MCB). The aim of the graduation work is to design a quick reference so that the choice of cable and in some cases even MCB’s can be performed by the designer in connection with electrical design. This involves reporting a calculation method for selecting MCB’s in different applications. MCB’s in both AC voltage and DC voltage systems are affected.</p><p>As reference for Forsmark, this report deals with the system 519.</p><p>The problem lies in in delimiting the work since every reactor at Forsmark consists of approximately 4000 MCB’s of different characteristics and applications. A very huge importance is also given to parameters specified in SS-EN standards for load capacity.</p><p>The thesis began with a pilot study of the pros and cons of MBC’s in different applications, which rules control the dimensioning and how the design is carried out today, etc. An analysis was performed based on formulas and calculations presented in Swedish Standard such as SS 424 14 03, resulted in a quick reference.</p>
----------------------------------------------------------------------
In diva2:1437409 abstract is: <p>This study was about performing an LCA on a portable flash unit, C1, made by Profoto. The goal was to get an overview of the environmental impact of the C1 and to investigate hotspots in the production and/or use phase of the C1. The results from the LCA would then be compared by LCAs made on similar products such as the C1, and in this LCA study the comparison was made on two different lamps, LED and CFL lamps. The reason for this is because no other LCA could be found on portable flash units and therefore LED and CFL lamps were chosen instead being the second closest product with the same function as the C1, to create light. The functional unit chosen in this LCA was 1 lumen-hour, which was the same functional unit used by the LED/CFL lamps, and CML 2001 method was used for life cycle impact assessment. In this study only the production and use phase were analyzed and no further investigation were made on the end of life phase of the C1.The whole LCA system has been analyzed and designed by using the GaBi LCA software, and literature studies on both other LCAs and datasheets was used to gather key manufacturing steps of each component of the C1 and rescaled to fit inside the given system boundaries.  From the results given, a conclusion can be made that the battery and the reflector, being two components found in the C1, had the highest contribution in environmental impacts, which is mainly due to the fact that these two components had the highest consumption of electricity. The production phase was the phase with the highest impact in all the chosen impact categories, and stood for 88-99% of the total impacts, while the use phase had an overall low contribution.A scenario analysis was also made where the use phase was changed to three different countries, changing the electricity grid mix used during the use phase. This was done to see whether these changes would influence the overall results of the LCA, and to see whether the use phase still had a low impact compared to the production phase. The results achieved showed a very small change in the overall results in all the chosen impact categories for this study, only increasing the results with 1-3%. With this a conclusion was made that changing the use phase would not affect the overall results of this LCA, the fact being that the production phase stood for the higher contribution in all of the impact categories.Looking at the comparison part, the C1 had a lower impact in 3 out of the 5 chosen impact categories, compared to the LED/CFL lamps. Here the C1 had a lower impact in the Global Warming Potential- (GWP 100), Eutrophication Potential-(EP) and Ozone layer Depletion Potential-(ODP) impact category, while having a higher impact in the Human Toxicity Potential- (HTP) and Acidification Potential- (AP) impact categories, concluding that the C1 has an overall lower impact compared to CFL/LED lamps. The reason for the higher impacts in these two categories was mainly because of the usage of the aluminum reflector, which was the reason for the high impact in the HTP impact category.For future studies Profoto could look over the possibility to exchange the reflector, which was made out of aluminum, for another material, since this could reduce both the cost and environmental impact of the C1. The possibility of exchanging the battery would also be a possible future investigation, since it is the battery which decides the life span of the C1. Being able to exchange the battery would improve the life span of the C1. Investigations regarding the end-of-life phase of the C1 would also be recommended, since many of the components are made out of plastic, which could be recycled. Profoto should also continue making LCAs on their other products and compare the results to the C1, since this would give a better comparison to the C1 with products more similar to itself.  </p>


corrected abstract:
<p>This study was about performing an LCA on a portable flash unit, C1, made by Profoto. The goal was to get an overview of the environmental impact of the C1 and to investigate hotspots in the production and/or use phase of the C1. The results from the LCA would then be compared by LCAs made on similar products such as the C1, and in this LCA study the comparison was made on two different lamps, LED and CFL lamps. The reason for this is because no other LCA could be found on portable flash units and therefore LED and CFL lamps were chosen instead being the second closest product with the same function as the C1, to create light. The functional unit chosen in this LCA was 1 lumen-hour, which was the same functional unit used by the LED/CFL lamps, and CML 2001 method was used for life cycle impact assessment. In this study only the production and use phase were analyzed and no further investigation were made on the end of life phase of the C1.</p><p>The whole LCA system has been analyzed and designed by using the GaBi LCA software, and literature studies on both other LCAs and datasheets was used to gather key manufacturing steps of each component of the C1 and rescaled to fit inside the given system boundaries.</p><p>From the results given, a conclusion can be made that the battery and the reflector, being two components found in the C1, had the highest contribution in environmental impacts, which is mainly due to the fact that these two components had the highest consumption of electricity. The production phase was the phase with the highest impact in all the chosen impact categories, and stood for 88-99% of the total impacts, while the use phase had an overall low contribution.</p><p>A scenario analysis was also made where the use phase was changed to three different countries, changing the electricity grid mix used during the use phase. This was done to see whether these changes would influence the overall results of the LCA, and to see whether the use phase still had a low impact compared to the production phase. The results achieved showed a very small change in the overall results in all the chosen impact categories for this study, only increasing the results with 1-3%. With this a conclusion was made that changing the use phase would not affect the overall results of this LCA, the fact being that the production phase stood for the higher contribution in all of the impact categories.</p><p>Looking at the comparison part, the C1 had a lower impact in 3 out of the 5 chosen impact categories, compared to the LED/CFL lamps. Here the C1 had a lower impact in the Global Warming Potential- (GWP 100), Eutrophication Potential-(EP) and Ozone layer Depletion Potential-(ODP) impact category, while having a higher impact in the Human Toxicity Potential- (HTP) and Acidification Potential- (AP) impact categories, concluding that the C1 has an overall lower impact compared to CFL/LED lamps. The reason for the higher impacts in these two categories was mainly because of the usage of the aluminum reflector, which was the reason for the high impact in the HTP impact category.</p><p>For future studies Profoto could look over the possibility to exchange the reflector, which was made out of aluminum, for another material, since this could reduce both the cost and environmental impact of the C1. The possibility of exchanging the battery would also be a possible future investigation, since it is the battery which decides the life span of the C1. Being able to exchange the battery would improve the life span of the C1. Investigations regarding the end-of-life phase of the C1 would also be recommended, since many of the components are made out of plastic, which could be recycled. Profoto should also continue making LCAs on their other products and compare the results to the C1, since this would give a better comparison to the C1 with products more similar to itself.</p>
----------------------------------------------------------------------
In diva2:1260715 abstract is: <p>The Swedish Transport Administration, which is the contractor of this study, has, after the first years of delivering BIM surveys, experienced that their expectations for the delivery of BIMsurveys have not been met. The purpose of this study is to investigate what factors may cause the contractor and consultants to fail to reach a consensus on the process and how to synchronise the consultant's process with the contractor so that the quality of the deliveries becomes moresatisfactory based on the contractor's requirements.</p><p>The purpose of the study is to analyse, from a sociotechnical perspective, and identify the factorsrequired to obtain an effective BIM survey process.</p><p>The questions are what expectations and experiences the Swedish Transport Administration hasas a contractor of deliveries of BIM surveys relating to communication, technology and processes, as well as what expectations and experiences the consultants have from the implementation of the Swedish Transport Administration's requirements for the BIM surveys relating to communication, technology and processes.</p><p>Interviews have been conducted with informants both from the Swedish Transport Administration and from different consultancy companies.</p><p>The result shows that the process is still very immature. The technology exists but the knowledge of what BIM is and understanding of the benefit of the process is insufficient. The study concludes by suggesting how the future implementation work should look from a sociotechnical perspective.</p>


corrected abstract:
<p>The Swedish Transport Administration, which is the contractor of this study, has, after the first years of delivering BIM surveys, experienced that their expectations for the delivery of BIM surveys have not been met. The purpose of this study is to investigate what factors may cause the contractor and consultants to fail to reach a consensus on the process and how to synchronise the consultant's process with the contractor so that the quality of the deliveries becomes more satisfactory based on the contractor's requirements.</p><p>The purpose of the study is to analyse, from a sociotechnical perspective, and identify the factors required to obtain an effective BIM survey process.</p><p>The questions are what expectations and experiences the Swedish Transport Administration has as a contractor of deliveries of BIM surveys relating to communication, technology and processes, as well as what expectations and experiences the consultants have from the implementation of the Swedish Transport Administration's requirements for the BIM surveys relating to communication, technology and processes.</p><p>Interviews have been conducted with informants both from the Swedish Transport Administration and from different consultancy companies.</p><p>The result shows that the process is still very immature. The technology exists but the knowledge of what BIM is and understanding of the benefit of the process is insufficient.</p><p>The study concludes by suggesting how the future implementation work should look from a sociotechnical perspective.</p>
----------------------------------------------------------------------
In diva2:1547059 abstract is: <p>This thesis includes investigation of noise exposure and frequency analysis of the noise at the Arkpacken department, Billerud Korsnäs' factory in Frövi. The questions answered in the thesis are:1. What noise levels are the employees exposed to when working at Arkpacken and how do these levels relate to noise exposure limits set by the Swedish work enviroment authority (AFS 2005: 16)?2. What noise levels are there at the places at Arkpacken where employees usually perform tasks and how do these relate to exposure values for disturbing noise (AFS 2005: 16)?3. What is the frequency distribution of the noise when working at Arkpacken and are there frequencies in the noise that should be given special consideration in noise reduction measures?4. According to Buller (AFS 2005: 16), what measures is the employer obliged to take due to the noise levels to which employees are exposed?Noise exposure mapping was done via measurements with noise dosimeter (Brüel &amp; Kjaer 4445/4448) for a total of 6 shifts (one person works at Arkpacken per shift). For mapping of disturbing noise and frequency analysis of the noise, measurements were made at 5 stationary measuring points at and in the area around Arkpacken. These measurements were made with sound level meters (Brüel &amp; Kjaer 2270) with frequency analysis in third-octaves.Based on the values measured with the noise dosimeters, the daily noise exposure level including expanded measurement uncertainty was calculated to 87 dB(A), which exceeds the noise exposure limit set in AFS 2005:16 (85 dB(A)). The measurements also showed that both maximum noise levels and impulse noise exceeded noise exposure limits. For disturbing noise, sound levels between 81 dB(A) and 88 dB(A) were measured in the stationary measuring points in direct connection to Arkpacken. The measurements showed that the main source of noise was the nearby machine (AM3). The pulsating nature of the sound in combination with the high noise levels are probably the factors that contribute most to the degree of disturbance.Based on measured values for both hearing-damaging and disturbing noise, the employer must, in accordance with Buller (AFS 2005: 16), take measures to reduce noise exposure. In this case, measures to reduce the distribution of noise as well as measures to reduce the effect of noise at the receivers are recommended. Examples of the latter are the wearing of hearing-protective device and construction of a soundproof control cabin in connection to Arkpacken.</p>

corrected abstract:
<p>This thesis includes investigation of noise exposure and frequency analysis of the noise at the Arkpacken department, Billerud Korsnäs' factory in Frövi. The questions answered in the thesis are:<ol><li>What noise levels are the employees exposed to when working at Arkpacken and how do these levels relate to noise exposure limits set by the Swedish work enviroment authority (AFS 2005: 16)?</li><li>What noise levels are there at the places at Arkpacken where employees usually perform tasks and how do these relate to exposure values for disturbing noise (AFS 2005: 16)?</li><li>What is the frequency distribution of the noise when working at Arkpacken and are there frequencies in the noise that should be given special consideration in noise reduction measures?</li><li>According to Buller (AFS 2005: 16), what measures is the employer obliged to take due to the noise levels to which employees are exposed?</li></ol></p><p>Noise exposure mapping was done via measurements with noise dosimeter (Brüel &amp; Kjaer 4445/4448) for a total of 6 shifts (one person works at Arkpacken per shift). For mapping of disturbing noise and frequency analysis of the noise, measurements were made at 5 stationary measuring points at and in the area around Arkpacken. These measurements were made with sound level meters (Brüel &amp; Kjaer 2270) with frequency analysis in third-octaves.</p><p>Based on the values measured with the noise dosimeters, the daily noise exposure level including expanded measurement uncertainty was calculated to 87 dB(A), which exceeds the noise exposure limit set in AFS 2005:16 (85 dB(A)). The measurements also showed that both maximum noise levels and impulse noise exceeded noise exposure limits. For disturbing noise, sound levels between 81 dB(A) and 88 dB(A) were measured in the stationary measuring points in direct connection to Arkpacken. The measurements showed that the main source of noise was the nearby machine (AM3). The pulsating nature of the sound in combination with the high noise levels are probably the factors that contribute most to the degree of disturbance.</p><p>Based on measured values for both hearing-damaging and disturbing noise, the employer must, in accordance with Buller (AFS 2005: 16), take measures to reduce noise exposure. In this case, measures to reduce the distribution of noise as well as measures to reduce the effect of noise at the receivers are recommended. Examples of the latter are the wearing of hearing-protective device and construction of a soundproof control cabin in connection to Arkpacken.</p>
----------------------------------------------------------------------
title: "Syntes av polyamider med candida antarctica lipase B"
==>    "Syntes av polyamider med <em>candida antarctica</em> lipase B"

In diva2:802058 abstract is: <p>Polyamides are an important and versatile family of plastics commonly used both as construction plastics and fibers. The global consumption of polyamides has grown quickly, from 800 000 tons per year in 1990 to 2 500 000 tons per year in 2011. Polyamides are highly crystalline and consequently have high melting points. This makes the synthesis of polyamides a highly energy consuming process.The objective of this thesis is to find a synthesis route for polyamides using an enzymatic catalyst. To be able to use enzymes the melting point of the synthesized polyamide needs to be lowered, otherwise the oligomers will solidify before growing to long chains. Since the high melting point is a consequence of the crystallinity the project has applied two routes to lower the crystallinity. The first route, route 1, by using long, branched monomers, and the second route, route 2, by using monomers with an unsaturation in the repeating unit. There are, to my knowledge, only a limited number of studies on polymerization of polyamides using enzymes.Experiments in this report were done at a small laboratory scale. The enzyme used was Candida antarctica lipase B (CALB). The monomers used in route 1, were produced by addition of thiols with different end groups to the internal double bond of methyl oleate. The reactions worked yielding functional monomers used for polyamide synthesis. When cysteamine, coupled to methyl oleate was polymerized the reaction was fast and the limiting factor was viscosity. The degree of polymerization, DP, for the polymerization was 8. Monomers from methyl-3-mercapto propionate and methyl oleate were co-polymerized with diamines of varying lengths. When 1,6-diaminohexane was used solubility problems arose. But the use of the longer diamine, 1,12-diaminedodecane, generated a homogenous system with a conversion of 80% of the monomer from methyl-3-mercapto propionate and methyl oleate and oligomers with a DP of 3. Methanol and chloroform were used as solvents to solvate the shorter diamine, but resulted in a low DP.Route 2 used trans β-Hydromuconic acid and its corresponding dietser. The reactions did not reach the high conversions of monomers needed for a high DP. The main problem was, again, the solubility of 1,6-diaminohexane. As a proof of concept a polyester synthesis was performed in route 2, since there has been more research within this area. The polyester contained an unsaturation in the backbone. The synthesis reached a high conversion and a DP of 11.This thesis is a study of which the important factors are in order to take synthesis of polyamides using enzymatic catalysts further. Complementary work has to be done, but the project has shown that it is possible to synthesis oligoamides with lower crystallinity using CALB as catalyst.</p>

w='dietser' val={'c': 'diester', 's': 'diva2:802058', 'n': 'error in te original'}

corrected abstract:
<p>Polyamides are an important and versatile family of plastics commonly used both as construction plastics and fibers. The global consumption of polyamides has grown quickly, from 800 000 tons per year in 1990 to 2 500 000 tons per year in 2011. Polyamides are highly crystalline and consequently have high melting points. This makes the synthesis of polyamides a highly energy consuming process.</p><p>The objective of this thesis is to find a synthesis route for polyamides using an enzymatic catalyst. To be able to use enzymes the melting point of the synthesized polyamide needs to be lowered, otherwise the oligomers will solidify before growing to long chains. Since the high melting point is a consequence of the crystallinity the project has applied two routes to lower the crystallinity. The first route, route 1, by using long, branched monomers, and the second route, route 2, by using monomers with an unsaturation in the repeating unit. There are, to my knowledge, only a limited number of studies on polymerization of polyamides using enzymes.</p><p>Experiments in this report were done at a small laboratory scale. The enzyme used was <em>Candida antarctica</em> lipase B (CALB). The monomers used in route 1, were produced by addition of thiols with different end groups to the internal double bond of methyl oleate. The reactions worked yielding functional monomers used for polyamide synthesis. When cysteamine, coupled to methyl oleate was polymerized the reaction was fast and the limiting factor was viscosity. The degree of polymerization, DP, for the polymerization was 8. Monomers from methyl-3-mercapto propionate and methyl oleate were co-polymerized with diamines of varying lengths. When 1,6-diaminohexane was used solubility problems arose. But the use of the longer diamine, 1,12-diaminedodecane, generated a homogenous system with a conversion of 80% of the monomer from methyl-3-mercapto propionate and methyl oleate and oligomers with a DP of 3. Methanol and chloroform were used as solvents to solvate the shorter diamine, but resulted in a low DP.</p><p>Route 2 used trans β-Hydromuconic acid and its corresponding dietser. The reactions did not reach the high conversions of monomers needed for a high DP. The main problem was, again, the solubility of 1,6-diaminohexane. As a proof of concept a polyester synthesis was performed in route 2, since there has been more research within this area. The polyester contained an unsaturation in the backbone. The synthesis reached a high conversion and a DP of 11.</p><p>This thesis is a study of which the important factors are in order to take synthesis of polyamides using enzymatic catalysts further. Complementary work has to be done, but the project has shown that it is possible to synthesis oligoamides with lower crystallinity using CALB as catalyst.</p>
----------------------------------------------------------------------
In diva2:1460320 abstract is: <p>The building and construction sector accounts for a significant proportion of carbon dioxide emissions in Sweden, a total of 12.2 million tons of carbon dioxide equivalents, which corresponds to approximately 19 % of the total emissions of greenhouse gas in Sweden. As a response to the fact that construction has a major impact, different environmental certification systems have emerged for buildings. One certification system is the Swedish certification system “Miljöbyggnad”, which includes sustainability issues through the whole construction process. The building can achieve three grades in Miljöbyggnad: bronze; which corresponds to the level in legislations, silver; which is the most common certification grade, and gold; that corresponds to the highest grade, thus requiring the most effort. The grade is given according to how the building performs in different assessment areas, called indicators. For new construction, the system consists of 15 different indicators within three areas: energy, indoor environment and materials. The indicators are connected to both ecological and social sustainability. The indicators connected to ecological sustainability are considered to either minimize hazardous substances, increase the amount of renewable energy or decrease the use of energy, thus a reduction in climate impact. Social sustainability is connected to how the tenant’s wellbeing is impacted by the building. The meaning of wellbeing is, in this case, health and comfort. In this master thesis the theoretical possibility of raising the Miljöbyggnad grade from silver to gold has been studied for the construction of new student housing. Stockholms studentbostäder´s new construction of four houses at Lappkärrsberget has been the study case. Stockholm’s student housing, “Stockholms studentbostäder”, is the largest actor of student housing in Sweden, with approximately 8000 housings. Stockholms studentbostäder is currently in the middle of the process that within the next ten years build 2000 new student housings. A major expansion with 800 housings will be built in their largest housing area, Lappkärrsberget. In the first building phase, the new construction of four houses is carried out, with the buildings having 10 to 13 floors which include 297 housings, mainly one room apartments and some two room apartments. The houses are built for the Miljöbyggnad grade silver according to the manual in version 2.2. After version 2.2, later versions, as version 3.0 have been launched. The main focus has been to identify what theoretical actions are required to raise the grade to gold, and to what cost. Four main questions have been studied:Is it possible to reach the grade gold for the four new buildings in Lappkärrsberget?What would it mean for the buildings, in terms of costs and actions, to raise the grade from silver to gold in version 2.2?How high grade is possible to reach for the indicator energy types, if the tenants have their own electricity agreements in the current version of Miljöbyggnad 2.2, and the later version 3.0? What general differences would it make to build in a later version 3.0, instead of 2.2? The method used has been to study the current status and then together with the manual for Miljöbyggnad make a comparison to achieve the grade gold in a specific gold scenario formulated together with Stockholms studentbostäder. In the next step, actors from the building industry both internal from Stockholms studentbostäder, external at other companies together with suppliers, were asked about what actions were needed to achieve gold and to what cost.For the first question, if it is theoretically possible to build with Miljöbyggnad gold for the new housing in Lappkärrsberget, it was concluded that it would not be possible to achieve gold due to the limited opportunities to optimize the properties of the windows with the current building performance. Therefore, it is recommended to decide what grades off the indicators to aim for and in an early stage develop the construction design and make the construction planning after the indicator grades. To achieve high grades, it is especially important to optimize the windows, in line with the requirements for the indicators. Another uncertain factor concerns how the students would respond in a survey which requires an approved result to reach a gold grade. Although it is not possible to reach gold as a building grade, it was noted that several of the indicators were close to gold and it would not be hard to raise these grades to gold to improve the overall performance of the buildings. In the second question, the cost was calculated when raising the grade from silver to gold. For this it was assumed that the windows and surveys did reach the desirable grade, and in that case the cost would increase between 0,4 % and 1,3 % of the contract budget of the project. The probable value would be somewhere in between, estimated to 0,6 %, which is a low increase in cost. In the third question, it was investigated what grade would be possible to achieve for the indicator energy types if the tenants are required to manage their own electricity agreement, in both version 2.2 and version 3.0. It was concluded that the district heating was required to change, to make an impact on the grade. Stockholm Exergi is the supplier of the district heating for the houses in Lappkärrsberget. Thus, Stockholm Exergi´s three alternatives have been studied, which differs in district heating composition and impact on the environment. The alternatives were studied for different years, due to the fact that district heating changes composition for each year. The preliminary certification of the grade in Lappkärrsberget was made with district heating data from 2016 and no matter what district heating alternative was used from 2016, the highest reachable grade is bronze in version 2.2. In version 3.0, the household electricity, meaning the tenant´s apartment electricity, is optional to use for the calculation of the grade. When calculating the grade without household electricity, with the same data as the preliminary certification, from 2016, the grade was raised to silver. With the alternatives of the district heating that were available year 2020 from Stockholm Exergi it would be possible to reach grade gold with the most environmentally friendly alternative, in version 2.2 and silver would be the highest grade in version 3.0. In the last question, version 2.2 and 3.0 were compared and some differences were noted. For example, the indicator nitrogen dioxide was removed in version 3.0 and the indicator climate impact of the foundation of the building and the building frame was added. The requirements have been updated in version 3.0. For example, the indicator percentage of energy types has a requirement that at least 5 % of the energy needs to be locally produced and renewable to reach gold level, for example with solar panels. The requirements for surveys have also been reduced in version 3.0, to fewer indicators in need of surveys and there is also an alternative to verify the grades by using measurements instead of surveys.</p>

corrected abstract:
<p>The building and construction sector accounts for a significant proportion of carbon dioxide emissions in Sweden, a total of 12.2 million tons of carbon dioxide equivalents, which corresponds to approximately 19 % of the total emissions of greenhouse gas in Sweden. As a response to the fact that construction has a major impact, different environmental certification systems have emerged for buildings. One certification system is the Swedish certification system “Miljöbyggnad”, which includes sustainability issues through the whole construction process. The building can achieve three grades in Miljöbyggnad: bronze; which corresponds to the level in legislations, silver; which is the most common certification grade, and gold; that corresponds to the highest grade, thus requiring the most effort. The grade is given according to how the building performs in different assessment areas, called indicators. For new construction, the system consists of 15 different indicators within three areas: energy, indoor environment and materials. The indicators are connected to both ecological and social sustainability. The indicators connected to ecological sustainability are considered to either minimize hazardous substances, increase the amount of renewable energy or decrease the use of energy, thus a reduction in climate impact. Social sustainability is connected to how the tenant’s wellbeing is impacted by the building. The meaning of wellbeing is, in this case, health and comfort.</p><p>In this master thesis the theoretical possibility of raising the Miljöbyggnad grade from silver to gold has been studied for the construction of new student housing. Stockholms studentbostäder´s new construction of four houses at Lappkärrsberget has been the study case. Stockholm’s student housing, “Stockholms studentbostäder”, is the largest actor of student housing in Sweden, with approximately 8000 housings. Stockholms studentbostäder is currently in the middle of the process that within the next ten years build 2000 new student housings. A major expansion with 800 housings will be built in their largest housing area, Lappkärrsberget. In the first building phase, the new construction of four houses is carried out, with the buildings having 10 to 13 floors which include 297 housings, mainly one room apartments and some two room apartments. The houses are built for the Miljöbyggnad grade silver according to the manual in version 2.2. After version 2.2, later versions, as version 3.0 have been launched.</p><p>The main focus has been to identify what theoretical actions are required to raise the grade to gold, and to what cost. Four main questions have been studied:<ol><li>Is it possible to reach the grade gold for the four new buildings in Lappkärrsberget?</li><li>What would it mean for the buildings, in terms of costs and actions, to raise the grade from silver to gold in version 2.2?</li><li>How high grade is possible to reach for the indicator energy types, if the tenants have their own electricity agreements in the current version of Miljöbyggnad 2.2, and the later version 3.0?</li><li>What general differences would it make to build in a later version 3.0, instead of 2.2?</li></ol></p><p>The method used has been to study the current status and then together with the manual for Miljöbyggnad make a comparison to achieve the grade gold in a specific gold scenario formulated together with Stockholms studentbostäder. In the next step, actors from the building industry both internal from Stockholms studentbostäder, external at other companies together with suppliers, were asked about what actions were needed to achieve gold and to what cost.</p><p>For the first question, if it is theoretically possible to build with Miljöbyggnad gold for the new housing in Lappkärrsberget, it was concluded that it would not be possible to achieve gold due to the limited opportunities to optimize the properties of the windows with the current building performance. Therefore, it is recommended to decide what grades off the indicators to aim for and in an early stage develop the construction design and make the construction planning after the indicator grades. To achieve high grades, it is especially important to optimize the windows, in line with the requirements for the indicators. Another uncertain factor concerns how the students would respond in a survey which requires an approved result to reach a gold grade. Although it is not possible to reach gold as a building grade, it was noted that several of the indicators were close to gold and it would not be hard to raise these grades to gold to improve the overall performance of the buildings.</p><p>In the second question, the cost was calculated when raising the grade from silver to gold. For this it was assumed that the windows and surveys did reach the desirable grade, and in that case the cost would increase between 0,4 % and 1,3 % of the contract budget of the project. The probable value would be somewhere in between, estimated to 0,6 %, which is a low increase in cost.</p><p>In the third question, it was investigated what grade would be possible to achieve for the indicator energy types if the tenants are required to manage their own electricity agreement, in both version 2.2 and version 3.0. It was concluded that the district heating was required to change, to make an impact on the grade. Stockholm Exergi is the supplier of the district heating for the houses in Lappkärrsberget. Thus, Stockholm Exergi´s three alternatives have been studied, which differs in district heating composition and impact on the environment. The alternatives were studied for different years, due to the fact that district heating changes composition for each year. The preliminary certification of the grade in Lappkärrsberget was made with district heating data from 2016 and no matter what district heating alternative was used from 2016, the highest reachable grade is bronze in version 2.2. In version 3.0, the household electricity, meaning the tenant´s apartment electricity, is optional to use for the calculation of the grade. When calculating the grade without household electricity, with the same data as the preliminary certification, from 2016, the grade was raised to silver. With the alternatives of the district heating that were available year 2020 from Stockholm Exergi it would be possible to reach grade gold with the most environmentally friendly alternative, in version 2.2 and silver would be the highest grade in version 3.0.</p><p>In the last question, version 2.2 and 3.0 were compared and some differences were noted. For example, the indicator nitrogen dioxide was removed in version 3.0 and the indicator climate impact of the foundation of the building and the building frame was added. The requirements have been updated in version 3.0. For example, the indicator percentage of energy types has a requirement that at least 5 % of the energy needs to be locally produced and renewable to reach gold level, for example with solar panels. The requirements for surveys have also been reduced in version 3.0, to fewer indicators in need of surveys and there is also an alternative to verify the grades by using measurements instead of surveys.</p>
----------------------------------------------------------------------
In diva2:1340523 abstract is: <p>The ability to maintain an independent living at higher age is closely linked to the physical function of an individual. The evaluation of the upper limbs, including the muscular strength of the shoulder, elbow or wrist has received little attention, yet arms are used in most daily tasks. The upper extremity physical performancecan be measured in several ways but none of them has the recognition to be theprimary method. The aim of this experimental study is to test the new devicedeveloped by Erik Almgren and assess whether this can be a valid tool to reliablyassess abduction shoulder strength.</p><p>Fourteen healthy young subjects participated in this study. After placing in the left wrist the new device, they performed three types of tests: Full Range of Motion, Static measurements and Dynamic measurements. Three trials were done in each test with a recovery of 30 seconds.</p><p>The results of the study showed that the Static measurements were similar to the ones obtained in previous studies in the past for both angles and force. For the first time, the study includes dynamic measurements of the moment of the shoulderjoint. Peak dynamic moments were found to be on average 6.43 N·m ± 1.00 N·mfor the females and 7.90 N·m ±2.33 N·m. This value is 6% lower than the test population’s average maximum static moments- result which in accordance withthe inverse relationship between force and velocity.</p><p>The feasibility of this new device to be used outside a biomechanics lab has been proved, however, further research needs to be done in order to validate angles and measurements of the subjects.</p>

corrected abstract:
<p>The ability to maintain an independent living at higher age is closely linked to the physical function of an individual. The evaluation of the upper limbs, including the muscular strength of the shoulder, elbow or wrist has received little attention, yet arms are used in most daily tasks. The upper extremity physical performance can be measured in several ways but none of them has the recognition to be the primary method. The aim of this experimental study is to test the new device developed by Erik Almgren and assess whether this can be a valid tool to reliably assess abduction shoulder strength.</p><p>Fourteen healthy young subjects participated in this study. After placing in the left wrist the new device, they performed three types of tests: Full Range of Motion, Static measurements and Dynamic measurements. Three trials were done in each test with a recovery of 30 seconds.</p><p>The results of the study showed that the Static measurements were similar to the ones obtained in previous studies in the past for both angles and force. For the first time, the study includes dynamic measurements of the moment of the shoulder joint. Peak dynamic moments were found to be on average 6.43 N·m ± 1.00 N·m for the females and 7.90 N·m ±2.33 N·m. This value is 6% lower than the test population’s average maximum static moments- result which in accordance with the inverse relationship between force and velocity.</p><p>The feasibility of this new device to be used outside a biomechanics lab has been proved, however, further research needs to be done in order to validate angles and measurements of the subjects.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-26
======================================================================
In diva2:1218465 abstract is: <p>Coatings cover most objects in our daily lives. They can have functional properties such as protecting the coated material or esthetic properties to decorate and provide pleasant appearance. Coatings have historically dried by evaporation of solvents, but now, UV curing coatings are on the uprising. Acrylate functional groups on polymers, oligomers and monomers react through radical chain polymerization initiated by photoinitiators. Photoinitiators are molecules which forms radicals when irradiated with UV light.Coatings are often applied on multiple layers, each layer providing one function for the overall system. To function properly and have desired life time the layers must adhere to each other. The problem of intercoat adhesion has yet to find good techniques for analysis and explanation of what influences it. Many theories have been suggested and most likely many play a part in the overall adhesion.The master thesis project aim to investigate which properties that influences intercoat adhesion and how it can be studied. To relate properties of coatings to the intercoat adhesion one UV curing primer is set to use for all coating systems, and various topcoats have been produced with slight changes in formulation. The study can be divided into two parts; a pre-study and a main study. The pre-study follows up on a previous master thesis conducted at Sherwin-Williams AB. The pre-study has waterbased and waterbased UV-curable topcoats. The main study has UV curable topcoats. Properties which are studied in the main study are chemical backbone of the binder, functionality of monomers, influence of addition of wetting agents, defoamers, fillers and pigment, viscosity, density, pH, curing degree at depth, surface energy, surface tension, surface polarity and monomer to binder ratio.The pull-off method is the best method of analysis of intercoat adhesion today. A dolly is glued to a surface and then lifted, the force of lifting the dolly is measured. The method has one significant drawback; the break must be completely in the interphase of where the adhesion wishes to be analyzed. Throughout this study most interphase failures were between substrate and primer, resulting in no value for intercoat adhesion.No correlation was found between intercoat adhesion and chemical backbone of the binder, functionality of monomers, addition of wetting agents, defoamers, viscosity, pH, surface energy, surface tension or surface polarity. Lower intercoat adhesion was observed for coatings containing talc, calcium carbonate and titanium dioxide. Coating containing titanium dioxide showed insufficient curing above a coating thickness of 40 μm. The insufficient curing could be observed as wrinkles on the surface and liquid coating remaining in the coating interphase. Curing degree in depth of the topcoat is believed to be the main reason to decreased intercoat adhesion for the coating containing titanium dioxide. The insufficient curing could not be confirmed with infrared spectrometry. The reason why talc and calcium carbonate showed decreased intercoat adhesion is not known. Indications suggest that a lower monomer to binder ratio decrease intercoat adhesion, theories to explain this are the high viscosity and the low number of functional groups per volume. A higher number of functional groups per volume could increase the number of crosslinks formed between topcoat and primer.</p>


corrected abstract:
<p>Coatings cover most objects in our daily lives. They can have functional properties such as protecting the coated material or esthetic properties to decorate and provide pleasant appearance. Coatings have historically dried by evaporation of solvents, but now, UV curing coatings are on the uprising. Acrylate functional groups on polymers, oligomers and monomers react through radical chain polymerization initiated by photoinitiators. Photoinitiators are molecules which forms radicals when irradiated with UV light.</p><p>Coatings are often applied on multiple layers, each layer providing one function for the overall system. To function properly and have desired life time the layers must adhere to each other. The problem of intercoat adhesion has yet to find good techniques for analysis and explanation of what influences it. Many theories have been suggested and most likely many play a part in the overall adhesion.</p><p>The master thesis project aim to investigate which properties that influences intercoat adhesion and how it can be studied. To relate properties of coatings to the intercoat adhesion one UV curing primer is set to use for all coating systems, and various topcoats have been produced with slight changes in formulation. The study can be divided into two parts; a pre-study and a main study. The pre-study follows up on a previous master thesis conducted at Sherwin-Williams AB. The pre-study has waterbased and waterbased UV-curable topcoats. The main study has UV curable topcoats. Properties which are studied in the main study are chemical backbone of the binder, functionality of monomers, influence of addition of wetting agents, defoamers, fillers and pigment, viscosity, density, pH, curing degree at depth, surface energy, surface tension, surface polarity and monomer to binder ratio.</p><p>The pull-off method is the best method of analysis of intercoat adhesion today. A dolly is glued to a surface and then lifted, the force of lifting the dolly is measured. The method has one significant drawback; the break must be completely in the interphase of where the adhesion wishes to be analyzed. Throughout this study most interphase failures were between substrate and primer, resulting in no value for intercoat adhesion.</p><p>No correlation was found between intercoat adhesion and chemical backbone of the binder, functionality of monomers, addition of wetting agents, defoamers, viscosity, pH, surface energy, surface tension or surface polarity. Lower intercoat adhesion was observed for coatings containing talc, calcium carbonate and titanium dioxide. Coating containing titanium dioxide showed insufficient curing above a coating thickness of 40 μm. The insufficient curing could be observed as wrinkles on the surface and liquid coating remaining in the coating interphase. Curing degree in depth of the topcoat is believed to be the main reason to decreased intercoat adhesion for the coating containing titanium dioxide. The insufficient curing could not be confirmed with infrared spectrometry. The reason why talc and calcium carbonate showed decreased intercoat adhesion is not known. Indications suggest that a lower monomer to binder ratio decrease intercoat adhesion, theories to explain this are the high viscosity and the low number of functional groups per volume. A higher number of functional groups per volume could increase the number of crosslinks formed between topcoat and primer.</p>
----------------------------------------------------------------------
In diva2:1259565 abstract is: <p>Microbiota’s influence on human health and disease is a growing research field including neurodegenerative diseases such as Parkinson’s disease (PD). The disease symptoms involve movement disorder, manifesting tremor, rigidity, bradykinesia and instability. At the molecular level, the disease exhibits; aggregated alfa-synuclein trapped inside neurons in the brain, in so called Lewy bodies, and loss of dopaminergic neurons in substantia nigra.The working hypothesis of this project is that human microbiome composition and interactions mediate environment and lifestyle influences on disease expression of PD. To validate this hypothesis, a mouse model (C57BL/J6 mice) was used. Two knock-in mouse lines were used; one carrying the wild type, human Leucine-Rich-Repeat-Kinase 2 (LRRK2) and the second carrying the most common Caucasian LRKK2/G2019S mutant. LRRK2 is a tyrosine kinase known to interact with Nucleotide-binding oligomerization domain-containing protein 2 (NOD2), a cytosolic microbe peptide sensing receptor. To establish the tools and knowledge required for the analyses, the initial part of the project was to analyze the expression levels of LRRK2 and NOD2 in wild-type C57BL/J6 mice in specific pathogen free (SPF), and mice devoid of exposure to living microbes, so called germ-free (GF) mice. Along with this analyse, expression levels of the transgenic LRKK2 proteins in the genetically modified mice was monitored. The focus was on the following tissues: striatum, midbrain, hippocampus, small intestine and large intestine and applied immune-histochemistry (IHC) combined with Western blot analysis.Results; significantly higher expression levels of LRRK2 were observed in microbe exposed mice versus GF mice with the exception of the large intestine which showed the opposite. Moreover, NOD2 showed a trend of lower expression levels in all brain GF areas tested with the exception to striatum. For the transgenic human knock-in LRKK2 proteins, increased expression of hLRKK2 were observed in striatum and large intestine compared to G2019S. Reduced hLRKK2 expression was observed in midbrain. The results suggest a strong correlation between LRRK2 expression and the gut microbiota and a need for continued research to better understand the role our indigenous microbiome may play in onset/progression of PD.</p>


corrected abstract:
<p>Microbiota’s influence on human health and disease is a growing research field including neurodegenerative diseases such as Parkinson’s disease (PD). The disease symptoms involve movement disorder, manifesting tremor, rigidity, bradykinesia and instability. At the molecular level, the disease exhibits; aggregated alfa-synuclein trapped inside neurons in the brain, in so called Lewy bodies, and loss of dopaminergic neurons in substantia nigra.</p><p>The working hypothesis of this project is that human microbiome composition and interactions mediate environment and lifestyle influences on disease expression of PD. To validate this hypothesis, a mouse model (C57BL/J6 mice) was used. Two knock-in mouse lines were used; one carrying the wild type, human Leucine-Rich-Repeat-Kinase 2 (LRRK2) and the second carrying the most common Caucasian LRKK2/G2019S mutant. LRRK2 is a tyrosine kinase known to interact with Nucleotide-binding oligomerization domain-containing protein 2 (NOD2), a cytosolic microbe peptide sensing receptor. To establish the tools and knowledge required for the analyses, the initial part of the project was to analyze the expression levels of LRRK2 and NOD2 in wild-type C57BL/J6 mice in specific pathogen free (SPF), and mice devoid of exposure to living microbes, so called germ-free (GF) mice. Along with this analyse, expression levels of the transgenic LRKK2 proteins in the genetically modified mice was monitored. The focus was on the following tissues: striatum, midbrain, hippocampus, small intestine and large intestine and applied immune-histochemistry (IHC) combined with Western blot analysis.</p><p>Results; significantly higher expression levels of LRRK2 were observed in microbe exposed mice versus GF mice with the exception of the large intestine which showed the opposite. Moreover, NOD2 showed a trend of lower expression levels in all brain GF areas tested with the exception to striatum. For the transgenic human knock-in LRKK2 proteins, increased expression of hLRKK2 were observed in striatum and large intestine compared to G2019S. Reduced hLRKK2 expression was observed in midbrain. The results suggest a strong correlation between LRRK2 expression and the gut microbiota and a need for continued research to better understand the role our indigenous microbiome may play in onset/progression of PD.</p>
----------------------------------------------------------------------
In diva2:778828 abstract is: <p>Water shortage in several places on this planet together with an increasing environmental awareness have recently been inciting a growing need in society for infrastructural investments giving stormwater, originating from rainfall or snowmelt, priority as an important water resource assets. Even though polluted stormwater usually is extensively accounted for on multiple levels in urban planning, this is usually not reflected in the operational standards on many industrial sites. Biomass combined heat and power (bio-CHP) plants, incorporating large biomass storage facilities outdoors, have a potential of discharging pollutants through stormwater run-off. In order to adequately manage this somewhat diffuse source of pollutants, a proper understanding of the parameters governing stormwater formation and composition, together with proper tools to model this on a continuous basis, are needed.</p><p>With the aims of increasing the industry-wide capability to monitor the quantity and quality of stormwater discharged from bio-CHP plants, a series of field experiments were conducted at Idbäcken bio-CHP, a Vattenfall owned facility, in Nyköping (April-June 2014). The experimental work involved measuring of run-off routing on plant in connection with rainfall as well as analysis of different water quality indicators in samples collected in a local drainage ditch. Additionally, a rain simulator was used to investigate quality, magnitude and dynamics of discharge aspects of biomass leaching, under controlled conditions. The results from these experiments were analysed together with local meteorological data resulting in an annual mass balance for the pollutants. An iterative multiple regression analysis was also made in Excel producing a statistically significant linear relationship between run-off volumetric load (m3), 􀜸􀜴􀜱 , precipitation intensity (mm), <em>P</em>, and amount of biomass stored (m3), 􀜸􀜤􀜫􀜱 , (Equation I). 􀜸 􀜴􀜱 = 3.9􀜲 􀵆 0.0010􀜸􀜤􀜫􀜱 (I)</p><p>In an analogous way, a logarithmic relationship was obtained between precipitation intensity (mm), <em>P</em>, biomass rainfall retention capacity (%), <em>R</em>, incorporating a compensation constant (mm), <em>C</em>, the exact value of which will need further experimental work (Equation II). 􀜴 = 􀵆24.4 ln􁈺􀜲 􀵆 􀜥􁈻 + 115.48 (II)</p><p>For the Idbäcken bio-CHP plant situation, these correlations showed the expected average annual precipitation volume to amount to ~ 8,400 m 3 with ~ 11 % being collected by the drainage ditch. The actual run-off volume contacting with the biomass was estimated to &lt; 7 % of the same total. In addition, typical run-off routing pathways were quantified showing that, within 24 h of 15 mm of rain falling on the fuel storage area: ~ 15 % will be collected by the drainage ditch, 0-10 % will be evaporated depending on seasonal variations and the remaining 75-85 % will stay on the storage area.</p><p>Results from the biomass leachate experiments showed on significant differences between filtered respectively non-filtered samples, which, together with run-off volume data, was seen as an indication that leachate from biomass might not be a primary contributor to the pollutants found in the drainage ditch. The existence of alternative pollutant sources was further confirmed by experiments showing concentrations of Pb and N-tot to be significantly higher in water collected from the drainage ditch than in leachate originating directly from the biomass. As conceivable, albeit not confirmed, pollutant-source candidates were suggested: dust particles originating from the biomass together with nitrogen fixating bacteria and/or leakage of ammonium sulfate from a ChlorOut unit installed at the plant. The latter not likely due to run-off routing patterns on plant.</p><p>The average concentration levels of most pollutants were found to be below the guideline values currently used as a standard for stormwater discharged from urban catchment areas.</p><p>Measures were recommended as a way to decrease the discharge of contaminants. As one solution was suggested recycling of the contaminants to the boiler by means of gathering the stormwater and then spraying it onto the stored biomass. With this method, the plant's flue gas and water treatment system catches the contaminants. Earlier apprehensions regarding this solution, as this has been considered to increase the load of heavy metals on the boiler, with increased risk for corrosion and scale formation, are most probably unfounded. This since only 1-2 􀃅 of the total mass fraction of pollutants in biomass have been proved to be leaching out following as much as 30 mm of rainfall, which, due to high biomass turnover ratios, practically means that the only factor to worry about is an increased moisture content of the fuel. Alternative suggested ways of stormwater pollution management included filtration using granulated blast furnace slag, peat and/or bark. Further studies are recommended to include a more detailed analysis of pollutant content with regards to biomass fine particle fraction as a mean to further investigate the importance of dust particles for pollutant propagation on plant.</p>

w='ln\U0010123a\U00100732' val={'c': '****UNKNOWN*** equation', 's': 'diva2:778828', 'n': 'no full text - part of some equation'}
Note: I can't really make sense of the equation and cannot even recognize what might have generated these characters. To correct this one needs access to the actual thesis.

Possibly Equation 1 is:
\begin{equation}
\var{Q} = 3.9 \times 10^{-3} \var{B} 
\end{equation}


Possibly Equation 2 is:
\begin{equation}
\var{R} = 24.4 \ln(\var{P} + \var{C}) + 115.48
\end{equation}

Where
Equation I:
\var{Q} is defined as the run-off volumetric load (m<sup>3</sup>).
\var{B} is defined as the amount of biomass stored (m<sup>3</sup>).
Equation II:
\var{R} is defined as biomass rainfall retention capacity (%).
\var{P} is defined as precipitation intensity (mm).
\var{C} is defined as a compensation constant (mm).

corrected abstract:
<p>Water shortage in several places on this planet together with an increasing environmental awareness have recently been inciting a growing need in society for infrastructural investments giving stormwater, originating from rainfall or snowmelt, priority as an important water resource assets. Even though polluted stormwater usually is extensively accounted for on multiple levels in urban planning, this is usually not reflected in the operational standards on many industrial sites. Biomass combined heat and power (bio-CHP) plants, incorporating large biomass storage facilities outdoors, have a potential of discharging pollutants through stormwater run-off. In order to adequately manage this somewhat diffuse source of pollutants, a proper understanding of the parameters governing stormwater formation and composition, together with proper tools to model this on a continuous basis, are needed.</p><p>With the aims of increasing the industry-wide capability to monitor the quantity and quality of stormwater discharged from bio-CHP plants, a series of field experiments were conducted at Idbäcken bio-CHP, a Vattenfall owned facility, in Nyköping (April-June 2014). The experimental work involved measuring of run-off routing on plant in connection with rainfall as well as analysis of different water quality indicators in samples collected in a local drainage ditch. Additionally, a rain simulator was used to investigate quality, magnitude and dynamics of discharge aspects of biomass leaching, under controlled conditions. The results from these experiments were analysed to gether with local meteorological data resulting in an annual mass balance for the pollutants. An iterative multiple regression analysis was also made in Excel producing a statistically significant linear relationship between run-off volumetric load (m<sup>3</sup>), 􀜸􀜴􀜱 , precipitation intensity (mm), <em>P</em>, and amount of biomass stored (m<sup>3</sup>), 􀜸􀜤􀜫􀜱 , (Equation I). 􀜸 􀜴􀜱 = 3.9􀜲 􀵆 0.0010􀜸􀜤􀜫􀜱 (I)</p><p>In an analogous way, a logarithmic relationship was obtained between precipitation intensity (mm), <em>P</em>, biomass rainfall retention capacity (%), <em>R</em>, incorporating a compensation constant (mm), <em>C</em>, the exact value of which will need further experimental work (Equation II). 􀜴 = 􀵆24.4 ln\U0010123a\U00100732 equation 􀵆 􀜥􁈻 + 115.48 (II)</p><p>For the Idbäcken bio-CHP plant situation, these correlations showed the expected average annual precipitation volume to amount to ~ 8,400 m<sup>3</sup> with ~ 11 % being collected by the drainage ditch. The actual run-off volume contacting with the biomass was estimated to &lt; 7 % of the same total. In addition, typical run-off routing pathways were quantified showing that, within 24 h of 15 mm of rain falling on the fuel storage area: ~15 % will be collected by the drainage ditch, 0-10 % will be evaporated depending on seasonal variations and the remaining 75-85 % will stay on the storage area.</p><p>Results from the biomass leachate experiments showed on significant differences between filtered respectively non-filtered samples, which, together with run-off volume data, was seen as an indication that leachate from biomass might not be a primary contributor to the pollutants found in the drainage ditch. The existence of alternative pollutant sources was further confirmed by experiments showing concentrations of Pb and N-tot to be significantly higher in water collected from the drainage ditch than in leachate originating directly from the biomass. As conceivable, albeit not confirmed, pollutant-source candidates were suggested: dust particles originating from the biomass together with nitrogen fixating bacteria and/or leakage of ammonium sulfate from a ChlorOut unit installed at the plant. The latter not likely due to run-off routing patterns on plant.</p><p>The average concentration levels of most pollutants were found to be below the guideline values currently used as a standard for stormwater discharged from urban catchment areas.</p><p>Measures were recommended as a way to decrease the discharge of contaminants. As one solution was suggested recycling of the contaminants to the boiler by means of gathering the stormwater and then spraying it onto the stored biomass. With this method, the plant's flue gas and water treatment system catches the contaminants. Earlier apprehensions regarding this solution, as this has been considered to increase the load of heavy metals on the boiler, with increased risk for corrosion and scale formation, are most probably unfounded. This since only 1-2 􀃅 of the total mass fraction of pollutants in biomass have been proved to be leaching out following as much as 30 mm of rainfall, which, due to high biomass turnover ratios, practically means that the only factor to worry about is an increased moisture content of the fuel. Alternative suggested ways of stormwater pollution management included filtration using granulated blast furnace slag, peat and/or bark. Further studies are recommended to include a more detailed analysis of pollutant content with regards to biomass fine particle fraction as a mean to further investigate the importance of dust particles for pollutant propagation on plant.</p>
----------------------------------------------------------------------
In diva2:745018 abstract is: <p>Marine ecosystems can be a promising reservoir of various kinds of chemical components, applicable as pharmaceutical materials, food, cosmetics, nutraceuticals, and others for different industry. As an example, Tunicates, a group of marine animals, have been attracted a lot of attention in medical application, food market, water pollution issues, and Cellulose nanomaterial production due to their consisting of chemical compounds such as cellulose, amino-sugars, and proteins or protein-polysaccharide complexes e.g. collagen, glycosaminoglycan, chitin, scleroprotein, iodine-binding proteins, and elastin. In this project,  two dominant species of Scandinavian Tunicates, i.e. <em>Ciona intestinalis</em> and <em>Clavelina lepadiformis,</em> harvested from Norwegian ocean have been classified according to body sizes, depths from the ocean surface, ages and species, and separated physically into outer layer and internal organs, followed by measurements of sugar composition, oil content, and  protein content. Application potentials have been investigated by trials for production of pure crystalline cellulose, bioethanol, and biodiesel, and by analysis of amino acid composition of the samples.</p><p>The cellulose percentage and cellulose yield for the chemically pure cellulose obtained, is around 96% and 54% respectively, and the protein content is decreased step by step by the acid, alkali, and bleaching process applied. Bioethanol can be obtained by fermentation of tunicate hydrolysate with strains A and C which are derived from <em>Saccharomyces cerevisiae</em>. The biodiesel yield of tunicate samples is around 4-6% as an average. The amino acid compositions in our tunicate samples are similar to egg albumin, implying tunicate being an alternative material for animal feed production.</p><p>Several processing treatments have been conducted with the aims to fractionate tunicate biomass components or enhance the cellulose accessibility and reactivity. After a single processing step, Ba(OH)2 treated samples seemed to be the best in terms of both cellulose preservation (66.5% cellulose) and protein removal (6% protein in the treated residue). Results from the physical separation plus washing reveal that the highest amount of cellulose and protein presents is found in the outer (Tunic) part and internal organs of Tunicate samples respectively. Data obtained from FTIR(Fourier Transform Infrared Spectroscopy) and SEM(Scanning Electron Microscope) indicate that among all processing trials, H3PO4 is the most effective in decreasing the cellulose crystallinity, which renders a higher accessibility for acidic or enzymatic reaction during bioethanol production due to a higher amount of amorphous structure of cellulose.</p><p>From the analysis results of component contents and structures, it could be concluded that increase of deepness results in a decrease of sugar content of the Tunicate samples while there are no differences in protein and carbohydrate content in different tunicate species. The body size has a positive influence on the protein content and the sample age alters the contents of both sugar and protein. In addition, Tunicate oil has high phospholipid content instead of glycerol ester, the latter being the common oil from vegetable origins. Moreover, lots of free fatty acid is present, and the composition profile of Tunicate fatty acids seems to be similar to fish oil, as revealed by NMR (Nuclear Magnetic Resonance Spectroscopy), FTIR, and GC-MS (Gas Chromatography-Mass Spectrometry).</p>

corrected abstract:
<p>Marine ecosystems can be a promising reservoir of various kinds of chemical components, applicable as pharmaceutical materials, food, cosmetics, nutraceuticals, and others for different industry. As an example, Tunicates, a group of marine animals, have been attracted a lot of attention in medical application, food market, water pollution issues, and Cellulose nanomaterial production due to their consisting of chemical compounds such as cellulose, amino-sugars, and proteins or protein-polysaccharide complexes e.g. collagen, glycosaminoglycan, chitin, scleroprotein, iodine-binding proteins, and elastin. In this project, two dominant species of Scandinavian Tunicates, i.e. <em>Ciona intestin alis</em> and <em>Clavelin a lepadiformis,</em> harvested from Norwegian ocean have been classified according to body sizes, depths from the ocean surface, ages and species, and separated physically into outer layer and internal organs, followed by measurements of sugar composition, oil content, and protein content. Application potentials have been investigated by trials for production of pure crystalline cellulose, bioethanol, and biodiesel, and by analysis of amino acid composition of the samples.</p><p>The cellulose percentage and cellulose yield for the chemically pure cellulose obtained, is around 96% and 54% respectively, and the protein content is decreased step by step by the acid, alkali, and bleaching process applied. Bioethanol can be obtained by fermentation of tunicate hydrolysate with strains A and C which are derived from <em>Saccharomyces cerevisiae</em>. The biodiesel yield of tunicate samples is around 4-6% as an average. The amino acid compositions in our tunicate samples are similar to egg albumin, implying tunicate being an alternative material for animal feed production.</p><p>Several processing treatments have been conducted with the aims to fractionate tunicate biomass components or enhance the cellulose accessibility and reactivity. After a single processing step, Ba(OH)2 treated samples seemed to be the best in terms of both cellulose preservation (66.5% cellulose) and protein removal (6% protein in the treated residue). Results from the physical separation plus washing reveal that the highest amount of cellulose and protein presents is found in the outer (Tunic) part and internal organs of Tunicate samples respectively. Data obtained from FTIR(Fourier Transform Infrared Spectroscopy) and SEM(Scanning Electron Microscope) indicate that among all processing trials, H3PO4 is the most effective in decreasing the cellulose crystallinity, which renders a higher accessibility for acidic or enzymatic reaction during bioethanol production due to a higher amount of amorphous structure of cellulose.</p><p>From the analysis results of component contents and structures, it could be concluded that increase of deepness results in a decrease of sugar content of the Tunicate samples while there are no differences in protein and carbohydrate content in different tunicate species. The body size has a positive influence on the protein content and the sample age alters the contents of both sugar and protein. In addition, Tunicate oil has high phospholipid content instead of glycerol ester, the latter being the common oil from vegetable origins. Moreover, lots of free fatty acid is present, and the composition profile of Tunicate fatty acids seems to be similar to fish oil, as revealed by NMR (Nuclear Magnetic Resonance Spectroscopy), FTIR, and GC-MS (Gas Chromatography-Mass Spectrometry).</p>
----------------------------------------------------------------------
In diva2:1673989 abstract is: <p>Glycoside hydrolases (GHs) are invaluable tools for Biotechnology as they can hydrolyse the glycosidic bonds of a large variety of polysaccharides, useful for many industrial sectors. The GH that hydrolyses Chitin, the second most abundant biopolymer found in nature with various applications, is called chitinase. In addition to their catalytic domain, many GHs have one or more Carbohydrate Binding Modules (CBMs), which usually interact with the enzyme’s substrate, or a similar polysaccharide, by binding to it. CBMs can thereby benefit the affinity and thus activity of the enzyme, by increasing contact time with the substrate. In some cases there are other structural benefits, such as thermostability. The research behind CBMs affecting thermostability of GHs is gaining popularity and becoming more extensive, but the mechanism of stabilisation is not fully understood. In this thesis, we explored the attributes a CBM from a novel unpublished family gives to a not-previously-studied GH belonging to the GH18 family. After cloning the two fragments of a multi-modular gene encoding recombinant proteins referred to as GH18 and GH18+CBM, and placing them in vector pET21a we transformed E.coli competent cells. Proceeding we cultivated and induced protein production using Isopropyl β-d-1-thiogalactopyranoside (IPTG), harvested and purified the protein utilizing affinity chromatography and the His-tag already placed on the proteins. The assays we performed involved thermostability, polysaccharide binding assay, and enzyme activity assay, utilizing various techniques. The activity assays posed a challenge since the polysaccharides we were working with were highly insoluble, with chitin, the main substrate, being crystalline and quite inaccessible to the enzyme. In addition, we used Bioinformatics tools to create a structure model of the novel CBM, based on its amino acid sequence and similarity to other studied CBM families. From our data, we concluded that the CBM’s ligand and GH’s substrate are not the same polysaccharide, and some of our data suggest a potential increase in thermostability of the GH18+CBM structure when compared to only GH18, although more experiments need to be performed in order to draw a final conclusion and then investigate how this phenomenon occurs. This thesis adds to our body of knowledge regarding the complexity of GH-CBM interactions, and shows that there is not always a simple or obvious connection between the functions of the two domains.</p>

corrected abstract:
<p>Glycoside hydrolases (GHs) are invaluable tools for Biotechnology as they can hydrolyse the glycosidic bonds of a large variety of polysaccharides, useful for many industrial sectors. The GH that hydrolyses Chitin, the second most abundant biopolymer found in nature with various applications, is called chitinase. In addition to their catalytic domain, many GHs have one or more Carbohydrate Binding Modules (CBMs), which usually interact with the enzyme’s substrate, or a similar polysaccharide, by binding to it. CBMs can thereby benefit the affinity and thus activity of the enzyme, by increasing contact time with the substrate. In some cases there are other structural benefits, such as thermostability. The research behind CBMs affecting thermostability of GHs is gaining popularity and becoming more extensive, but the mechanism of stabilisation is not fully understood. In this thesis, we explored the attributes a CBM from a novel unpublished family gives to a not-previously-studied GH belonging to the GH18 family. After cloning the two fragments of a multi-modular gene encoding recombinant proteins referred to as GH18 and GH18+CBM, and placing them in vector pET21a we transformed <em>E. coli</em> competent cells. Proceeding we cultivated and induced protein production using Isopropyl β-d-1-thiogalactopyranoside (IPTG), harvested and purified the protein utilizing affinity chromatography and the His-tag already placed on the proteins. The assays we performed involved thermostability, polysaccharide binding assay, and enzyme activity assay, utilizing various techniques. The activity assays posed a challenge since the polysaccharides we were working with were highly insoluble, with chitin, the main substrate, being crystalline and quite inaccessible to the enzyme. In addition, we used Bioinformatics tools to create a structure model of the novel CBM, based on its amino acid sequence and similarity to other studied CBM families. From our data, we concluded that the CBM’s ligand and GH’s substrate are not the same polysaccharide, and some of our data suggest a potential increase in thermostability of the GH18+CBM structure when compared to only GH18, although more experiments need to be performed in order to draw a final conclusion and then investigate how this phenomenon occurs. This thesis adds to our body of knowledge regarding the complexity of GH-CBM interactions, and shows that there is not always a simple or obvious connection between the functions of the two domains.</p>
----------------------------------------------------------------------
In diva2:1427280 abstract is: <p>Our population is becoming older and with that, the development of chronic diseasesis is also expected to increase. A chronic illness is a long-term illness which lasts throughout a lifetime, or at least for a very long time. A large part of healthcare resources is already devoted to treating chronically ill patients. These patients are often dependent of both care and medication to maintain a meaningful life. To gain a holistic view of these patients health condition by one/two appointments with physicians yearly is not sufficient in order to conclude a certain health-state. The course of disease for these patients changes daily and require follow-up on disease progression continuously to adapt an appropriate treatment plan. Collecting patient-generated health data (PGHD) facilitates in the process of retrieving moreevidence for better assessment of the disease development. While there is obvious importance and benefit of using of PGHD, this data is not commonly used in healthcare.  Further investigation is needed to understand how PGHD can be more useful.</p><p>This pilot study provides knowledge of the success and failure factors of using PGHD for mainly chronically ill patients, but can be applied to other patient-groups as well.</p><p>The aim of this thesis work was to collect information about what suppliers, governmental organizations and healthcare professionals require for using PGHD in healthcare setting in a greater extent in the future. Methods used to gather information were participatory interviews in combination with qualitative interview questions. Pattern recognition has been created through a thematic analysis andcluster mapping. The data collection resulted in four areas of improvement; patient behaviour, healthcare organization, digitized health data and equipment.</p><p>The result shows overall a positive attitude towards the concept of PGHD by all sectors asked in this project. Stakeholders agree on that PGHD can generate positive outcomes for chronically ill patients. The belief of improving workflow in healthcare with PGHD was also positive. The valuable possibilities generated with PGHD are tailored careflows, improved evaluation of disease status and enhanced quality of care and well-being among others. Additionally, several ongoing projects are taking place, which demonstrate great interest in the area. However, before PGHD can be prescribed by healthcare, studies have to be performed including development of national guidelines for reporting PGHD, building a secure infrastructure and introducing new work routines. Future work will be applying AI-analysis of reported PGHD to facilitate the work of caregivers and development of secure storing solutions for instance with block-chain technology.</p>

w='diseasesis' val={'c': 'disease', 's': 'diva2:1427280', 'n': 'correct in the original'}

corrected abstract:
<p>Our population is becoming older and with that, the development of chronic diseases is also expected to increase. A chronic illness is a long-term illness which lasts throughout a lifetime, or at least for a very long time. A large part of healthcare resources is already devoted to treating chronically ill patients. These patients are often dependent of both care and medication to maintain a meaningful life. To gain a holistic view of these patients health condition by one/two appointments with physicians yearly is not sufficient in order to conclude a certain health-state. The course of disease for these patients changes daily and require follow-up on disease progression continuously to adapt an appropriate treatment plan. Collecting patient-generated health data (PGHD) facilitates in the process of retrieving more evidence for better assessment of the disease development. While there is obvious importance and benefit of using of PGHD, this data is not commonly used in healthcare. Further investigation is needed to understand how PGHD can be more useful.</p><p>This pilot study provides knowledge of the success and failure factors of using PGHD for mainly chronically ill patients, but can be applied to other patient-groups as well. The aim of this thesis work was to collect information about what suppliers, governmental organizations and healthcare professionals require for using PGHD in healthcare setting in a greater extent in the future. Methods used to gather information were participatory interviews in combination with qualitative interview questions. Pattern recognition has been created through a thematic analysis and cluster mapping. The data collection resulted in four areas of improvement; patient behaviour, healthcare organization, digitized health data and equipment.</p><p>The result shows overall a positive attitude towards the concept of PGHD by all sectors asked in this project. Stakeholders agree on that PGHD can generate positive outcomes for chronically ill patients. The belief of improving workflow in healthcare with PGHD was also positive. The valuable possibilities generated with PGHD are tailored care flows, improved evaluation of disease status and enhanced quality of care and well-being among others. Additionally, several ongoing projects are taking place, which demonstrate great interest in the area. However, before PGHD can be prescribed by healthcare, studies have to be performed including development of national guidelines for reporting PGHD, building a secure infrastructure and introducing new work routines. Future work will be applying AI-analysis of reported PGHD to facilitate the work of caregivers and development of secure storing solutions for instance with block-chain technology.</p>
----------------------------------------------------------------------
In diva2:1506678 abstract is: <p>Repetitive tasks, awkward hand/wrist postures and forceful exertions are known risk factors for work-related musculoskeletal disorders (WMSDs) of the hand and wrist. WMSD is a major cause of long work absence, productivity loss, loss in wages and individual suffering. Currently available assessment methods of the hand/wrist motion have the limitations of being inaccurate, e.g. when using self-reports or observations, or expensive and resource-demanding for following analyses, e.g. when using the electrogoniometers. Therefore, there is a need for a risk assessment method that is easy-to-use and can be applied by both researchers and practitioners for measuring wrist angular velocity during an 8-hour working day. Wearable Inertial Measurement Units (IMU) in combination with mobile phone applications provide the possibility for such a method. In order to apply the IMU in the field for assessing the wrist velocity of different work tasks, the accuracy of the method need to be examined. Therefore, this laboratory experiment was conducted to compare a new IMU-based method with the traditional goniometer and standard optical motion capture system. The laboratory experiment was performed on twelve participants. Three standard hand movements, including hand/wrist motion of Flexion-extension (FE), Deviation, and Pronationsupination (PS) at 30, 60, 90 beat-per-minute (bpm), and three simulated work tasks were performed. The angular velocity of the three methods at 50th and 90th percentile were calculated and compared. The mean absolute error and correlation coefficient were analysed for comparing the methods. Increase in error was observed with increase in speed/bpm during the standard hand movements. For standard hand movements, comparison between IMUbyaxis and Goniometer had the smallest difference and highest correlation coefficient. For simulated work tasks, the difference between goniometer and optical system was the smallest. However, for simulated work tasks, the differences between the compared methods were in general much larger than the standard hand movements. The IMU-based method is seen to have potential when compared with the traditional measurement methods. Still, it needs further improvement to be used for risk assessment in the field.</p>


corrected abstract:
<p>Repetitive tasks, awkward hand/wrist postures and forceful exertions are known risk factors for work-related musculoskeletal disorders (WMSDs) of the hand and wrist. WMSD is a major cause of long work absence, productivity loss, loss in wages and individual suffering. Currently available assessment methods of the hand/wrist motion have the limitations of being inaccurate, e.g. when using self-reports or observations, or expensive and resource-demanding for following analyses, e.g. when using the electrogoniometers. Therefore, there is a need for a risk assessment method that is easy-to-use and can be applied by both researchers and practitioners for measuring wrist angular velocity during an 8-hour working day. Wearable Inertial Measurement Units (IMU) in combination with mobile phone applications provide the possibility for such a method. In order to apply the IMU in the field for assessing the wrist velocity of different work tasks, the accuracy of the method need to be examined. Therefore, this laboratory experiment was conducted to compare a new IMU-based method with the traditional goniometer and standard optical motion capture system.</p><p>The laboratory experiment was performed on twelve participants. Three standard hand movements, including hand/wrist motion of Flexion-extension (FE), Deviation, and Pronation-supination (PS) at 30, 60, 90 beat-per-minute (bpm), and three simulated work tasks were performed. The angular velocity of the three methods at 50<sup>th</sup> and 90<sup>th</sup> percentile were calculated and compared. The mean absolute error and correlation coefficient were analysed for comparing the methods. Increase in error was observed with increase in speed/bpm during the standard hand movements. For standard hand movements, comparison between IMU by axis and Goniometer had the smallest difference and highest correlation coefficient. For simulated work tasks, the difference between goniometer and optical system was the smallest. However, for simulated work tasks, the differences between the compared methods were in general much larger than the standard hand movements. The IMU-based method is seen to have potential when compared with the traditional measurement methods. Still, it needs further improvement to be used for risk assessment in the field.</p>
----------------------------------------------------------------------
In diva2:1787539 abstract is: <p>The increase of carbon dioxide levels in the atmosphere have resulted in an increasing interest in using solar light for different purposes, one being chemistry. The introduction of light in chemistry can drive new and exciting chemical reactions, finding applications in many fields such as photocatalysis and photovoltaics. Combining the aspects of light in chemistry together with the high selectivity that characterizes proteins make for interesting and powerful machineries, so-called photoactive proteins. However, studying the behaviors of photoactive proteins is a non-trivial task and many challenges arise from both an experimental and a theoretical point of view. This thesis takes a theoretical perspective. The challenges associated with simulating photoactive protein behavior originate from the wide range of time and length scales involved, ranging from ultrafast and localized excitation processes to large-scale structural changes occurring on longer timescales. This, together with the fact that there is no black box that we can use for novel theoretical studies of photoactive proteins motivates a careful approach for theoretical studies of photoactive proteins.</p><p>In this project, we conduct a theoretical investigation of two photosensitizer proteins (PSP2 and PSP3), recently engineered to capture the essence of plant photosynthesis. The photosensitizer proteins differ by only one residue (position 203), which is an aspartate in PSP2 while a tyrosine in PSP3. Although structurally similar, the proteins demonstrate different photoinduced behaviors. This study aims to shed light on the mechanistic details underlying these differences. As a first step to study PSP2 and PSP3 computationally, we develop a computational protocol for protein preparation. The protocol is then used for a theoretical investigation of the photophysics of the two proteins using quantum mechanics/molecular mechanics simulations. Our results provide a first mapping of the electronic-state manifold of the two proteins. We find an intriguing charge-transfer state in PSP3, involving the tyrosine and the protein chromophore, located below bright state responsible for light absorption. This state is absent in PSP2 and could therefore be the reason for the different photoinduced behavior of the two proteins. However, this requires further studies of the two systems.</p>


corrected abstract:
<p>The increase of carbon dioxide levels in the atmosphere have resulted in an increasing interest in using solar light for different purposes, one being chemistry. The introduction of light in chemistry can drive new and exciting chemical reactions, finding applications in many fields such as photocatalysis and photovoltaics. Combining the aspects of light in chemistry together with the high selectivity that characterizes proteins make for interesting and powerful machineries, so-called photoactive proteins. However, studying the behaviors of photoactive proteins is a non-trivial task and many challenges arise from both an experimental and a theoretical point of view. This thesis takes a theoretical perspective. The challenges associated with simulating photoactive protein behavior originate from the wide range of time and length scales involved, ranging from ultrafast and localized excitation processes to large-scale structural changes occurring on longer timescales. This, together with the fact that there is no black box that we can use for novel theoretical studies of photoactive proteins motivates a careful approach for theoretical studies of photoactive proteins.</p><p>In this project, we conduct a theoretical investigation of two photosensitizer proteins (PSP2 and PSP3), recently engineered to capture the essence of plant photosynthesis. The photosensitizer proteins differ by only one residue (position 203), which is an aspartate in PSP2 while a tyrosine in PSP3. Although structurally similar, the proteins demonstrate different photoinduced behaviors. This study aims to shed light on the mechanistic details underlying these differences. As a first step to study PSP2 and PSP3 computationally, we develop a computational protocol for protein preparation. The protocol is then used for a theoretical investigation of the photophysics of the two proteins using quantum mechanics/molecular mechanics simulations. Our results provide a first mapping of the electronic-state manifold of the two proteins. We find an intriguing charge-transfer state in PSP3, involving the tyrosine and the protein chromophore, located below bright state responsible for light absorption. This state is absent in PSP2 and could therefore be the reason for the different photoinduced behavior of the two proteins. However, this requires further studies of the two systems.</p>
----------------------------------------------------------------------
In diva2:1033770 abstract is: <p>Background: Working in the construction, paper and steel industry in Sweden means maintaining an occupation in a naturally hazardous environment. Engaging contractors in health and safety activities to achieve an improved occupational environment, and thereby raising the quality of production, can often be problematic due to a wide range of factors. These factors are being combated by Swedish companies which employees’ contractors Aim: The aim of this study is to examine which factors that could affect the improvement of the safety culture within a company which employees’ contractors. This study will focus on the client’s organization and the contractor’s participation. Method: A qualitative study was conducted including individual interviews with three companies in the construction, steel and paper industry in Sweden. One company from each field was selected, three managerial staff and three safety representatives interviewed from each. A thematic data analysis was conducted of the results. Results: The results indicate that the factors influencing the work of improving the safety culture in a company that employees’ contractors were as follows; management's approach to security, the organization of the security, cultural differences, control and monitoring of workplace conditions, participation, resources, communication and the contractor’s motivation to work safely. Discussion: A potential disadvantage of this study was that the results relate primarily to the client's perspective. Yet focusing on this perspective may also be seen as an advantage as it is the client who creates the safety culture that contractors should be a part of. The focus of the study was top-down analysis. If the study had instead been focused on a bottom-up perspective, the result would have been centered around strengthen the participation of contractors in how safety measures should be implemented, promoting their own desire to participate in a client’s safety culture. Conclusion: The conclusion of this study is that the promotion of a safety culture should be anchored within the management of a company. It is their approach to security and safety issues that is the deciding factor in promoting a safer workplace culture. Safety and security requirements should also be increasingly considered in the procurement of contractors, furthermore contractors should be included in security operations (including job training) and they should also be given the opportunity to report risks directly to the client's reporting system. This could promote both participation and communication within the company. Our final conclusion is that efforts should be to reduce the line between internal personnel and contractors on safety issues.</p>


corrected abstract:
<p><strong>Background:</strong> Working in the construction, paper and steel industry in Sweden means maintaining an occupation in a naturally hazardous environment. Engaging contractors in health and safety activities to achieve an improved occupational environment, and thereby raising the quality of production, can often be problematic due to a wide range of factors. These factors are being combated by Swedish companies which employees’ contractors</p><p><strong>Aim:</strong> The aim of this study is to examine which factors that could affect the improvement of the safety culture within a company which employees’ contractors. This study will focus on the client’s organization and the contractor’s participation.</p><p><strong>Method:</strong> A qualitative study was conducted including individual interviews with three companies in the construction, steel and paper industry in Sweden. One company from each field was selected, three managerial staff and three safety representatives interviewed from each. A thematic data analysis was conducted of the results.</p><p><strong>Results:</strong> The results indicate that the factors influencing the work of improving the safety culture in a company that employees’ contractors were as follows; management's approach to security, the organization of the security, cultural differences, control and monitoring of workplace conditions, participation, resources, communication and the contractor’s motivation to work safely.</p><p><strong>Discussion:</strong> A potential disadvantage of this study was that the results relate primarily to the client's perspective. Yet focusing on this perspective may also be seen as an advantage as it is the client who creates the safety culture that contractors should be a part of. The focus of the study was top-down analysis. If the study had instead been focused on a bottom-up perspective, the result would have been centered around strengthen the participation of contractors in how safety measures should be implemented, promoting their own desire to participate in a client’s safety culture.</p><p><strong>Conclusion:</strong> The conclusion of this study is that the promotion of a safety culture should be anchored within the management of a company. It is their approach to security and safety issues that is the deciding factor in promoting a safer workplace culture. Safety and security requirements should also be increasingly considered in the procurement of contractors, furthermore contractors should be included in security operations (including job training) and they should also be given the opportunity to report risks directly to the client's reporting system. This could promote both participation and communication within the company. Our final conclusion is that efforts should be to reduce the line between internal personnel and contractors on safety issues.</p>
----------------------------------------------------------------------
In diva2:827343 abstract is: <p>Phosphorus is an endless nutrient that lately has been drawing much attention in correlation to over-fertilization and as a scarce commodity. Phosphorus is essential to all life, humans, animals and plants and the workable phosphorus back-ups seems to be running out. It is therefore important in the future to apply sustainable phosphorus recovery methods to take care of the circulation of phosphorus in today’s society. Sewage sludge is a waste that grows with the increase of population. Digested sewage sludge among other wastes such as digested municipal food waste contain beneficial nutrients such as phosphorus, nitrogen and potassium but also unhealthy trace elements, pathogenies, heavy metals and unwanted organic material. Sewage sludge also contain drug residues and micro plastic fibers.</p><p>As of today it is allowed to spread sewage sludge and digested municipal food waste on farm land to return phosphorus together with other nutrients. Future set of regulations will probably be subtilized which will influence the usage of sewage sludge spreading on farm land. The sewage sludge will most likely have to go through some kind of pretreatment before being used as a resource. An alternative is to institute harder demands on hygenization which can be performed by pasteurization, thermophilic digestion or other treatments involving an increase in temperature to get rid of harmful substances. Another alternative that seems more promising is combustion of sewage sludge and digested municipal food waste, which will destroy unwanted substances such as drug residues, pathogenies, and left-over organic material. Depending in which type of pan the combustion will take place, CFB- (Circulated fluidized bed), BFB- (Bubbling fluidized bed) or grate boiler it will leave different amount of bottom ash and fly ash. Phosphorus and larger amount of heavy metals will be concentrated in the ash. Phosphorus is inert to the temperature changes and finds itself contained in the bottom ash, meanwhile heavy metals evaporate and stack up in the fly ash.</p><p>This report evaluate different kinds of phosphorus recovering methods that can be divided in to main categories, thermo chemical and wet chemical recovery methods. The main purpose is to describe the methods, estimate its cost, find out if the method is ready for commercial use and specify the advantages and disadvantages. Thermo chemical treatment of sewage sludge ash in a BFB and the ASH DEC-process is two thermo chemical treatment methods this report brings up. The first mentioned is under development and its main purpose is to decontaminate the ash and use phosphorus rich whitlockite ash as a fertilizer. The system has an evaluated investment cost of 72 MSEK, with a pay-back time of 4,4 years and a capacity of at least 1000 ton ash/year. The ASH DEC-process is also a decontaminating process where the product is reclaimed as sodium calcium phosphates. The method estimated investment cost is around 140-170 MSEK with a pay-back time of 4 years and a capacity of 30 000 ton ash/year. The ASH DEC-process is currently in use in Germany and Austria.</p><p>Among the wet chemical phosphorus recovery methods presented in this report is, CleanMAP-technology, PASH-Process, SEPHOS-Process, SESAL-Phos process and the BioCon-process. In common for these process is the different leaching and dissolution techniques and the origin of a process water that has to be cleaned before released. CleanMAP-Technology from EasyMining Sweden AB is much promising currently under development. The layout design will encapsulate a capacity of 30 000 ton ash/year, which correspond to the apprehending of nearly 30 % of the produced sewage sludge in Sweden today. CleanMAP-technology gives a water soluble product, ammonium phosphate, with nearly 100 % purity and can be used directly on farm lands. The process is energyefficient with low operating costs and a propitious heavy metal reduction. The technique is under development with no demonstration and no public data of investment costs or operating costs available.</p><p>The PASH-process is an acid leaching method and form calcium phosphate as final product. The process gives a reduction of heavy metals and other metals, aluminum in specific. The process is therefore most appropriate to use on sewage sludge from water treatment plants that uses aluminum as a precipitate chemical. The investment cost is estimated to around 46 MSEK with an operating cost of 37 MSEK and a capacity of 30 000 ton ash/year and a phosphorus recovery of 1 700 ton/year.</p><p>The SEPHOS-process uses acid and base for the dissolution of ash. Phosphorus is recovered as aluminum phosphate that ultimately is passed through a more advanced step called “advanced SEHPOS-process” in purpose to obtain calcium phosphates which are more suitable for plants because aluminum can cause damaged to the roots. The SEPHOS-process is currently under development and there are no public released data on investment costs or operating costs.</p><p>The SESAL-Phos process is a multi-stage process with dissolution of ash with acid and base to obtain calcium phosphate as final product. The process is currently under development.</p><p>The BioCon-process is based on ion exchange technology to be able to separate unwanted metals and inorganic elements. The obtain product is mainly phosphorus acid. The process is comprised and has a large variety of chemical demand. The process is used in Denmark and was implemented in Sweden in early 21th century but had to close down due to operating problems.</p><p>Phosphorus recovery methods that seems most promising for Fortum Värme, accordingly to the advantages and disadvantages is CleanMAP-technology, ASH DEC-process and in the future thermo chemical treatment of sewage sludge ash in a BFB-boiler. CleanMAP-technology can implicate high operating costs due to the addition of chemicals, but they will be part of the products and the facility will be smaller than the ASH DEC-process. The ASH DEC-process is a commercial process in use in both Germany and Austria and has knowledge of operating which will be one of the reasons to consider implementation in Sweden. Thermo chemical treatment in a BFB-boiler has potential both practical and economical to be considered for implementation in the future even though it’s far from a commercial use.</p><p>The quality of digested sewage sludge and digested municipal food waste should have a low ash- and moisture content. Bio fuel contain low ash content but high moisture content. Co-combustion of digested sewage sludge and municipal food waste along with bio fuel is to prefer in an economically point of view because no further investment in a mono-combustion plant is needed for the sludge only. One disadvantage is that the concentration of phosphorus in the ash will be lower and the cost of phosphorus recovery will increase. An appropriate moisture content of the bio fuel should be as low as possible to obtain higher phosphorus concentration in the ash and to get a tolerable heat value in the fuel mix during combustion.</p><p>Whether combustion of digested sewage or more severe demands on hygenization of sewage sludge is the wanted procedure for treatment is today an ongoing discussion. Initially, one alternative is toburn small amounts of digested sewage sludge in those cases heavy metal content is too high to be spread on farm lands, especially cadmium.</p><p>In a sustainable perspective it would be appropriate to see combustion followed by phosphorus recovery methods from ash as a good option of treatment. Both the social- and environmental aspect is attended when the volume of waste is decreased, drug residues destroyed, infectious agents and other pathogenic substances destroyed meanwhile heavy metals is concentrated and gathered for safe depositing. In an economical point of view it is confirmed that the costs to recover phosphorus is today too high in comparison to workable resources. Today the cost of recovered phosphorus is around 28-38 SEK/kg, depending on which method used and the selling price is around 3 SEK/kg, which makes it non profitable.</p><p>For the time being the phosphorus recovery methods need further attention to be able to develop and expand upon. The need is also to make them more effective and to learn how to encounter operating failures and minimize the investment- and operating costs. If so, the recovery methods will bring more attention to companies and make them more interested in investment.</p>

w='hygenization' val={'c': 'hygienization', 's': 'diva2:827343', 'n': 'error in the original'}

corrected abstract:
<p>Phosphorus is an endless nutrient that lately has been drawing much attention in correlation to over-fertilization and as a scarce commodity. Phosphorus is essential to all life, humans, animals and plants and the workable phosphorus back-ups seems to be running out. It is therefore important in the future to apply sustainable phosphorus recovery methods to take care of the circulation of phosphorus in today’s society. Sewage sludge is a waste that grows with the increase of population. Digested sewage sludge among other wastes such as digested municipal food waste contain beneficial nutrients such as phosphorus, nitrogen and potassium but also unhealthy trace elements, pathogenies, heavy metals and unwanted organic material. Sewage sludge also contain drug residues and micro plastic fibers.</p><p>As of today it is allowed to spread sewage sludge and digested municipal food waste on farm land to return phosphorus together with other nutrients. Future set of regulations will probably be subtilized which will influence the usage of sewage sludge spreading on farm land. The sewage sludge will most likely have to go through some kind of pretreatment before being used as a resource. An alternative is to institute harder demands on hygenization which can be performed by pasteurization, thermophilic digestion or other treatments involving an increase in temperature to get rid of harmful substances. Another alternative that seems more promising is combustion of sewage sludge and digested municipal food waste, which will destroy unwanted substances such as drug residues, pathogenies, and left-over organic material. Depending in which type of pan the combustion will take place, CFB- (Circulated fluidized bed), BFB- (Bubbling fluidized bed) or grate boiler it will leave different amount of bottom ash and fly ash. Phosphorus and larger amount of heavy metals will be concentrated in the ash. Phosphorus is inert to the temperature changes and finds itself contained in the bottom ash, meanwhile heavy metals evaporate and stack up in the fly ash.</p><p>This report evaluate different kinds of phosphorus recovering methods that can be divided in to main categories, thermo chemical and wet chemical recovery methods. The main purpose is to describe the methods, estimate its cost, find out if the method is ready for commercial use and specify the advantages and disadvantages. Thermo chemical treatment of sewage sludge ash in a BFB and the ASH DEC-process is two thermo chemical treatment methods this report brings up. The first mentioned is under development and its main purpose is to decontaminate the ash and use phosphorus rich whitlockite ash as a fertilizer. The system has an evaluated investment cost of 72 MSEK, with a pay-back time of 4,4 years and a capacity of at least 1000 ton ash/year. The ASH DEC-process is also a decontaminating process where the product is reclaimed as sodium calcium phosphates. The method estimated investment cost is around 140-170 MSEK with a pay-back time of 4 years and a capacity of 30 000 ton ash/year. The ASH DEC-process is currently in use in Germany and Austria.</p><p>Among the wet chemical phosphorus recovery methods presented in this report is, CleanMAP-technology, PASH-Process, SEPHOS-Process, SESAL-Phos process and the BioCon-process. In common for these process is the different leaching and dissolution techniques and the origin of a process water that has to be cleaned before released. CleanMAP-Technology from EasyMining Sweden AB is much promising currently under development. The layout design will encapsulate a capacity of 30 000 ton ash/year, which correspond to the apprehending of nearly 30 % of the produced sewage sludge in Sweden today. CleanMAP-technology gives a water soluble product, ammonium phosphate, with nearly 100 % purity and can be used directly on farm lands. The process is energy efficient with low operating costs and a propitious heavy metal reduction. The technique is under development with no demonstration and no public data of investment costs or operating costs available.</p><p>The PASH-process is an acid leaching method and form calcium phosphate as final product. The process gives a reduction of heavy metals and other metals, aluminum in specific. The process is therefore most appropriate to use on sewage sludge from water treatment plants that uses aluminum as a precipitate chemical. The investment cost is estimated to around 46 MSEK with an operating cost of 37 MSEK and a capacity of 30 000 ton ash/year and a phosphorus recovery of 1 700 ton/year.</p><p>The SEPHOS-process uses acid and base for the dissolution of ash. Phosphorus is recovered as aluminum phosphate that ultimately is passed through a more advanced step called “advanced SEHPOS-process” in purpose to obtain calcium phosphates which are more suitable for plants because aluminum can cause damaged to the roots. The SEPHOS-process is currently under development and there are no public released data on investment costs or operating costs.</p><p>The SESAL-Phos process is a multi-stage process with dissolution of ash with acid and base to obtain calcium phosphate as final product. The process is currently under development.</p><p>The BioCon-process is based on ion exchange technology to be able to separate unwanted metals and inorganic elements. The obtain product is mainly phosphorus acid. The process is comprised and has a large variety of chemical demand. The process is used in Denmark and was implemented in Sweden in early 21th century but had to close down due to operating problems.</p><p>Phosphorus recovery methods that seems most promising for Fortum Värme, accordingly to the advantages and disadvantages is CleanMAP-technology, ASH DEC-process and in the future thermo chemical treatment of sewage sludge ash in a BFB-boiler. CleanMAP-technology can implicate high operating costs due to the addition of chemicals, but they will be part of the products and the facility will be smaller than the ASH DEC-process. The ASH DEC-process is a commercial process in use in both Germany and Austria and has knowledge of operating which will be one of the reasons to consider implementation in Sweden. Thermo chemical treatment in a BFB-boiler has potential both practical and economical to be considered for implementation in the future even though it’s far from a commercial use.</p><p>The quality of digested sewage sludge and digested municipal food waste should have a low ash- and moisture content. Bio fuel contain low ash content but high moisture content. Co-combustion of digested sewage sludge and municipal food waste along with bio fuel is to prefer in an economically point of view because no further investment in a mono-combustion plant is needed for the sludge only. One disadvantage is that the concentration of phosphorus in the ash will be lower and the cost of phosphorus recovery will increase. An appropriate moisture content of the bio fuel should be as low as possible to obtain higher phosphorus concentration in the ash and to get a tolerable heat value in the fuel mix during combustion.</p><p>Whether combustion of digested sewage or more severe demands on hygenization of sewage sludge is the wanted procedure for treatment is today an ongoing discussion. Initially, one alternative is to burn small amounts of digested sewage sludge in those cases heavy metal content is too high to be spread on farm lands, especially cadmium.</p><p>In a sustainable perspective it would be appropriate to see combustion followed by phosphorus recovery methods from ash as a good option of treatment. Both the social- and environmental aspect is attended when the volume of waste is decreased, drug residues destroyed, infectious agents and other pathogenic substances destroyed meanwhile heavy metals is concentrated and gathered for safe depositing. In an economical point of view it is confirmed that the costs to recover phosphorus is today too high in comparison to workable resources. Today the cost of recovered phosphorus is around 28-38 SEK/kg, depending on which method used and the selling price is around 3 SEK/kg, which makes it non profitable.</p><p>For the time being the phosphorus recovery methods need further attention to be able to develop and expand upon. The need is also to make them more effective and to learn how to encounter operating failures and minimize the investment- and operating costs. If so, the recovery methods will bring more attention to companies and make them more interested in investment.</p>
----------------------------------------------------------------------
In diva2:628959 abstract is: <p>The goal of the thesis is to build a quadcopter that can fly and be controlled via a radio transmitter in all directions, stabilizing itself in the air, to land autonomously and detect collisions in the forward direction.In order to achieve the basic goal of being able to fly, a PID controller was implemented which is used to stabilize the Quadcopter in the air by controlling the motors with help from sensor orientation. This sensor orientation is obtained from a complementary filter that merges angle data from an accelerometer and a gyroscope. Both an accelerometer and a gyroscope are required to automatically stabilize the Quadcopter in the air.To achieve the goal of autonomous landing an ultrasonic sensor was used. An algorithm was developed to read the distance from the ground which was a basis for creating our own algorithm for autonomous landing.The Quadcopter has the ability to stabilize itself in the air, be controlled via a radio transmitter and land autonomously. A safety feature that enables autonomous landing if the Quadcopter travels outside the radio coverage is implemented and also a switch that turns off the engines. The only thing that wasn’t implemented was the crash avoidance in the forward direction because the ultrasonic sensor was not suited for this application.</p>

corrected abstract:
<p>The goal of the thesis is to build a quadcopter that can fly and be controlled via a radio transmitter in all directions, stabilizing itself in the air, to land autonomously and detect collisions in the forward direction.</p><p>In order to achieve the basic goal of being able to fly, a PID controller was implemented which is used to stabilize the Quadcopter in the air by controlling the motors with help from sensor orientation. This sensor orientation is obtained from a complementary filter that merges angle data from an accelerometer and a gyroscope. Both an accelerometer and a gyroscope are required to automatically stabilize the Quadcopter in the air. To achieve the goal of autonomous landing an ultrasonic sensor was used. An algorithm was developed to read the distance from the ground which was a basis for creating our own algorithm for autonomous landing.</p><p>The Quadcopter has the ability to stabilize itself in the air, be controlled via a radio transmitter and land autonomously. A safety feature that enables autonomous landing if the Quadcopter travels outside the radio coverage is implemented and also a switch that turns off the engines. The only thing that wasn’t implemented was the crash avoidance in the forward direction because the ultrasonic sensor was not suited for this application.</p>
----------------------------------------------------------------------
In diva2:949994 abstract is: <p>Segmenting brain MR scans could be highly benecial for diagnosing, treating and evaluating the progress of specic diseases. Up to this point, manual segmentation,performed by experts, is the conventional method in hospitals and clinical environments. Although manual segmentation is accurate, it is time consuming, expensive and might not be reliable. Many non-automatic and semi automatic methods have been proposed in the literature in order to segment MR brain images, but the levelof accuracy is not comparable with manual segmentation.</p><p>The aim of this project is to implement and make a preliminary evaluation of a method based on machine learning technique for segmenting gray matter (GM),white matter (WM) and cerebrospinal uid (CSF) of brain MR scans using images available within the open MICCAI grand challenge (MRBrainS13).The proposed method employs supervised articial neural network based autocontext algorithm, exploiting intensity-based, spatial-based and shape model-basedlevel set segmentation results as features of the network. The obtained average results based on Dice similarity index were 97.73%, 95.37%, 82.76%, 88.47% and 84.78% for intracranial volume, brain (WM + GM), CSF, WM and GM respectively. This method achieved competitive results with considerably shorter required training time in MRBrainsS13 challenge.</p>


w='articial' val={'c': 'artificial', 's': 'diva2:949994'}
w='benecial' val={'c': 'beneficial', 's': ['diva2:1261155', 'diva2:949994'], 'n': 'missing ligature'}

corrected abstract:
<p>Segmenting brain MR scans could be highly beneficial for diagnosing, treating and evaluating the progress of specific diseases. Up to this point, manual segmentation, performed by experts, is the conventional method in hospitals and clinical environments. Although manual segmentation is accurate, it is time consuming, expensive and might not be reliable. Many non-automatic and semi automatic methods have been proposed in the literature in order to segment MR brain images, but the level of accuracy is not comparable with manual segmentation.</p><p>The aim of this project is to implement and make a preliminary evaluation of a method based on machine learning technique for segmenting gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) of brain MR scans using images available within the open MICCAI grand challenge (MRBrainS13).</p><p>The proposed method employs supervised artificial neural network based autocontext algorithm, exploiting intensity-based, spatial-based and shape model-based level set segmentation results as features of the network. The obtained average results based on Dice similarity index were 97.73%, 95.37%, 82.76%, 88.47% and 84.78% for intracranial volume, brain (WM + GM), CSF, WM and GM respectively. This method achieved competitive results with considerably shorter required training time in MRBrainsS13 challenge.</p>
----------------------------------------------------------------------
In diva2:1242460 abstract is: <p>In order to treat the ammonium rich leachate formed in two of the older landfills for non-hazardous waste at Löt avfallsanläggning SÖRAB built a continuous biological treatment plant (KBR). The nitrification has been total except for shorter periods during the startup of the process. However the denitrification was not complete except during periods when phosphoric acid was added to the treatment step. When adding phosphoric acid the phosphorus content in the water became too high and risked to exceed the standards for emitting treated leachate from the facility. To evaluate the need for addition of phosphoric acid in the treatment process, analysis of nutrients, in vitro cultivation of microorganisms, microscopic observations and FISH analysis was conducted from March to May 2017. The factors considered in this report were the temperature, some toxic substances such as heavy metals, nitrates, competition between organisms and phosphate depletion. Extra focus was put on the factors that can be controlled and were thought to be most critical.The composition of the leachate that comes from Löt avfallsanläggning’s old landfills is similar to many other leachate. The time it takes to start the biological processes is long but not unusual for similar treatment plants. By 2017, the inlet flow to the KBR was less than what the KBR was designed for, and consequently the residence time for the leakage water in the KBR was approximately 95 days. When analyzing data from previous years it was noted that the treatment worked down to 10 °C but would not start properly until the temperature reached about 16 °C after a stop in operation during the cold months. It was clear that bioavailable phosphorus was limiting the denitrification in 2017 and that the same level of phosphate taken up in the denitrification always reoccurred in later steps. FISH analysis yielded unclear results with few organisms and many particles. This does not rule out that denitrifying bacteria or phosphor accumulating bacteria exist in the plant but gave no definitive answer. Microscopic studies and in vitro cultivation on nutrient agar plates indicated an increase in the number of organisms over the time of the project (March to May 2017) and presence of many protozoa with flagella suggested that the treatment plant was in a startup stage. A number of filamentous bacteria was also observed, which could contribute to foaming together with storage of polysaccharides.The increase in phosphate concentration in the final treatment steps may be due to lysis of cells and biomass degradation, but the influence of phosphorus accumulating bacteria cannot be ruled out. In the future, to overcome the problem of excess phosphorus in the effluent, the required phosphorus additive can be determined based on the sludge volume (VSS, Volatile Suspended Solids). The leachate can be recirculated back to the denitrification step, alternately; the inlet leachate can be redirected to enter the denitrification step first.</p>


corrected abstract:
<p>In order to treat the ammonium rich leachate formed in two of the older landfills for non-hazardous waste at Löt avfallsanläggning SÖRAB built a continuous biological treatment plant (KBR). The nitrification has been total except for shorter periods during the startup of the process. However the denitrification was not complete except during periods when phosphoric acid was added to the treatment step. When adding phosphoric acid the phosphorus content in the water became too high and risked to exceed the standards for emitting treated leachate from the facility. To evaluate the need for addition of phosphoric acid in the treatment process, analysis of nutrients, <em>in vitro</em> cultivation of microorganisms, microscopic observations and FISH analysis was conducted from March to May 2017. The factors considered in this report were the temperature, some toxic substances such as heavy metals, nitrates, competition between organisms and phosphate depletion. Extra focus was put on the factors that can be controlled and were thought to be most critical.</p><p>The composition of the leachate that comes from Löt avfallsanläggning’s old landfills is similar to many other leachate. The time it takes to start the biological processes is long but not unusual for similar treatment plants. By 2017, the inlet flow to the KBR was less than what the KBR was designed for, and consequently the residence time for the leakage water in the KBR was approximately 95 days. When analyzing data from previous years it was noted that the treatment worked down to 10 °C but would not start properly until the temperature reached about 16 °C after a stop in operation during the cold months. It was clear that bioavailable phosphorus was limiting the denitrification in 2017 and that the same level of phosphate taken up in the denitrification always reoccurred in later steps. FISH analysis yielded unclear results with few organisms and many particles. This does not rule out that denitrifying bacteria or phosphor accumulating bacteria exist in the plant but gave no definitive answer. Microscopic studies and <em>in vitro</em> cultivation on nutrient agar plates indicated an increase in the number of organisms over the time of the project (March to May 2017) and presence of many protozoa with flagella suggested that the treatment plant was in a startup stage. A number of filamentous bacteria was also observed, which could contribute to foaming together with storage of polysaccharides.</p><p>The increase in phosphate concentration in the final treatment steps may be due to lysis of cells and biomass degradation, but the influence of phosphorus accumulating bacteria cannot be ruled out. In the future, to overcome the problem of excess phosphorus in the effluent, the required phosphorus additive can be determined based on the sludge volume (VSS, Volatile Suspended Solids). The leachate can be recirculated back to the denitrification step, alternately; the inlet leachate can be redirected to enter the denitrification step first.</p>
----------------------------------------------------------------------
In diva2:1571396 abstract is: <p>Single-cell RNA-sequencing makes possible to study the gene expression at the level of individual cells. However, one of the main challenges of the single-cell RNA-sequencing analysis today, is the identification and annotation of cell types. The current method consists in manually checking the expression of genes using top differentially expressed genes and comparing them with related cell-type markers available in scientific publications. It is therefore time-consuming and labour intensive. Nevertheless, in the last two years,numerous automatic cell-type identification and annotation tools which use different strategies have been created. But, the lack of specific comparisons of those tools in the literature and especially for immuno-oncologic and oncologic purposes makes difficult for laboratories and companies to know objectively what are the best tools for annotating cell types. In this project, a review of the current tools and an evaluation of R tools were carried out.The annotation performance, the computation time and the ease of use were assessed. After this preliminary results, the best selected R tools seem to be ClustifyR (fast and rather precise) and SingleR (precise) for the correlation-based tools, and SingleCellNet (precise and rather fast) and scPred (precise but a lot of cell types remains unassigned) for the supervised classificationtools. Finally, for the marker-based tools, MAESTRO and SCINA are rather robust if they are provided with high quality markers. </p>


corrected abstract:
<p>Single-cell RNA-sequencing makes possible to study the gene expression at the level of individual cells. However, one of the main challenges of the single-cell RNA-sequencing analysis today, is the identification and annotation of cell types. The current method consists in manually checking the expression of genes using top differentially expressed genes and comparing them with related cell-type markers available in scientific publications. It is therefore time-consuming and labour intensive. Nevertheless, in the last two years, numerous automatic cell-type identification and annotation tools which use different strategies have been created. But, the lack of specific comparisons of those tools in the literature and especially for immuno-oncologic and oncologic purposes makes difficult for laboratories and companies to know objectively what are the best tools for annotating cell types. In this project, a review of the current tools and an evaluation of R tools were carried out. The annotation performance, the computation time and the ease of use were assessed. After this preliminary results, the best selected R tools seem to be <em>ClustifyR</em> (fast and rather precise) and <em>SingleR</em> (precise) for the correlation-based tools, and <em>SingleCellNet</em> (precise and rather fast) and <em>scPred</em> (precise but a lot of cell types remains unassigned) for the supervised classification tools. Finally, for the marker-based tools, <em>MAESTRO</em> and <em>SCINA</em> are rather robust if they are provided with high quality markers. </p>
----------------------------------------------------------------------
In diva2:953499 abstract is: <p>Käppala wastewater treatment plant (WWTP) is located in Lidingö and currently treats water from over half a million people. Every year the Käppala plant produces about 30 000 t sludge. The nutritious sludge is then used as fertilizer on farmland. From 2007 until 2012 the copper content has increased in the produced sludge. Eventually the amount of copper has exceeded the allowed limit for spreading sludge on farmland (300 g copper/ha and year). So only a limited amount of sludge can be spread on the farmland. The limit is set to protect the soil and aquatic organisms since the divalent copper ion has toxic effects on the organisms. Even Bromma and Henriksdal WWTP have had the same increase in copper content in the sludge, but not as highas the Käppala plant.</p><p>The largest part of the copper that ends up in the sludge is predicted to originates from copper pipes in households. That is the reason why a number of different parameters has been investigated in order to see how much each parameter affects the amount of copper that is dissolved in the drinking water. The parameters that affect the most are; pH, alkalinity andnatural organic matter (NOM). When pH is decreasing, more copper will dissolve to the drinking water, but when the alkalinity and the NOM content is increasing, more copper will dissolve to the drinking water. Even the temperature and the stagnation time is important for how fast corrosion of copper pipes will occurs. The Görväln plant purifies and supplies drinking water to the majority of the member municipalities of the Käppala plant. Lovö’s and Norsborg’swaterworks supply drinking water to the other municipalities within the catchment area of Käppala. The alkalinity has slightly increased in the Görväln plant from 2000 to present. This increase should only affect in a very small degree on the increase in copper content in the sludge of Käppala plant. There is no clear trend on the other parameters that is considered to have an influence on the copper content in the sludge.</p><p>The sharp increase in copper content has probably other causes. In February 2009, the Käppala plant, together with Bromma and Henriksdal WWTP, changed analysis laboratory. The change of laboratory seemed to affect the results of the analysis giving higher values of copper contentin the sludge. The standard deviation is also greater in the analysis results of the samples after the change of laboratory compared to with the analysis results before the change of laboratory. However, the uncertainty of the analysis has varied over the years when the rising in the copper content in the sludge occurred. At Stockholm Vatten’s laboratory where samples were sent before the laboratory change was the uncertainty of the analysis very unspecific given. According to the analysis report, the result of the sample had an uncertainty of the analysis between 15-40 %. The samples were after the laboratory change sent to Eurofins laboratory in Lidköping. Here the uncertainty of the analysis has been diminished with the time. After the first years after the change the uncertainty of the analysis was 30 %. But after that the uncertainty of the analysis has declined to 15 %.</p><p>The reason why the Käppala plant had a stronger increase of copper content in the sludge than in Bromma and Henriksdal WWTP is unknown. One explanation could be that the sludge conditioning process Kemicond affected the analysis result since a certain change in concentration could have occurred when the process was in operation. However, the copper content has not decreased as much as expected after Kemicond was liquidated. But the total amount of copper in to the Käppala plant has increased during the period when the copper content in the sludge was higher. This indicates that there are other underlying reasons for the increase of copper in the sludge.</p>


corrected abstract:
<p>Käppala wastewater treatment plant (WWTP) is located in Lidingö and currently treats water from over half a million people. Every year the Käppala plant produces about 30 000 t sludge. The nutritious sludge is then used as fertilizer on farmland. From 2007 until 2012 the copper content has increased in the produced sludge. Eventually the amount of copper has exceeded the allowed limit for spreading sludge on farmland (300 g copper/ha and year). So only a limited amount of sludge can be spread on the farmland. The limit is set to protect the soil and aquatic organisms since the divalent copper ion has toxic effects on the organisms. Even Bromma and Henriksdal WWTP have had the same increase in copper content in the sludge, but not as high as the Käppala plant.</p><p>The largest part of the copper that ends up in the sludge is predicted to originates from copper pipes in households. That is the reason why a number of different parameters has been investigated in order to see how much each parameter affects the amount of copper that is dissolved in the drinking water. The parameters that affect the most are; pH, alkalinity and natural organic matter (NOM). When pH is decreasing, more copper will dissolve to the drinking water, but when the alkalinity and the NOM content is increasing, more copper will dissolve to the drinking water. Even the temperature and the stagnation time is important for how fast corrosion of copper pipes will occurs. The Görväln plant purifies and supplies drinking water to the majority of the member municipalities of the Käppala plant. Lovö’s and Norsborg’s waterworks supply drinking water to the other municipalities within the catchment area of Käppala. The alkalinity has slightly increased in the Görväln plant from 2000 to present. This increase should only affect in a very small degree on the increase in copper content in the sludge of Käppala plant. There is no clear trend on the other parameters that is considered to have an influence on the copper content in the sludge.</p><p>The sharp increase in copper content has probably other causes. In February 2009, the Käppala plant, together with Bromma and Henriksdal WWTP, changed analysis laboratory. The change of laboratory seemed to affect the results of the analysis giving higher values of copper content in the sludge. The standard deviation is also greater in the analysis results of the samples after the change of laboratory compared to with the analysis results before the change of laboratory. However, the uncertainty of the analysis has varied over the years when the rising in the copper content in the sludge occurred. At Stockholm Vatten’s laboratory where samples were sent before the laboratory change was the uncertainty of the analysis very unspecific given. According to the analysis report, the result of the sample had an uncertainty of the analysis between 15-40 %. The samples were after the laboratory change sent to Eurofins laboratory in Lidköping. Here the uncertainty of the analysis has been diminished with the time. After the first years after the change the uncertainty of the analysis was 30 %. But after that the uncertainty of the analysis has declined to 15 %.</p><p>The reason why the Käppala plant had a stronger increase of copper content in the sludge than in Bromma and Henriksdal WWTP is unknown. One explanation could be that the sludge conditioning process Kemicond affected the analysis result since a certain change in concentration could have occurred when the process was in operation. However, the copper content has not decreased as much as expected after Kemicond was liquidated. But the total amount of copper in to the Käppala plant has increased during the period when the copper content in the sludge was higher. This indicates that there are other underlying reasons for the increase of copper in the sludge.</p>
----------------------------------------------------------------------
In diva2:801778 abstract is: <p>Melanoma is the 6th most common cancer type in Sweden and the incidence is increasing. The prognosis for patients with unresectable or metastatic melanoma has been poor since there have been no effective therapies for these patients. In recent years, two new firts line treatments for unresectable or metastatic melanoma have emerged; Ipilimumab (Yervoy®) and Vemurafenib (Zelboraf®). Ipilimumab is a fully human antibody that blocks CTLA-4 and thereby augments T cell response. Around 10% of patiens respond to treatment and the most common side effects associated with Ipilimumab are immune-related adverse events. Vemurafenib is a small molecule drug that inhibits the MAPK pathway. Most patients respond well to treatment with Vemurafenib, and undergo a rapid tumor reduction. However, almost all patients develop a resistance to the drug within a few months of treatment.</p><p>There is a need for better understanding of the effects that these two drugs have on the immune response and to find immune-related biomarkers. In this study we focused on the possible effects of Ipilimumab on the induction and suppressive functions of MDSC-like cells educated in vitro by coculturing monocytes from healthy donors with early passage melanoma cells. For vemurafenib, we improved an assay for immune monitoring of blood samples from patients undergoing treatment and analyzed samples from five patients takenbefore start of treatment, after 8 weeks of treatment and again after 16-20 weeks of treatment.</p><p>the experiments with Ipilimumab and MDSCs did not give any conclusive answers to the effects that blocking of CTLA-4 may have on the induction or functions of these suppressive cells. However, knowledge regarding the coculture method has been gained and the firststeps towards further investigations have been taken. An assay for immune monitoring of patient samples was improved by fine tuning four panels and adding an extra panel. These panels will be used for future immune monitoring in ourresearch group. The immune monitoring of Vemurafenib patients yielded results in form of trends of higher activation of T cells and NK cells after 8 weeks of treatment. The increase in activation could still be seen in the samples taken 16-20 weeks into treatment. Also a trendof decrease in central memory CD4+ and CD8+ T cells was found. These results might in the future be taken into consideration to help in the understanding of the effects that Vemurafenib treatment has on the immune response in different disease stages.</p>

w='firts' val={'c': 'first', 's': 'diva2:801778', 'n': 'no full text'}
w='patiens' val={'c': 'patients', 's': ['diva2:801778', 'diva2:852274'], 'n': 'no full text'}

I assumed the usual italics for Latin words.
corrected abstract:
<p>Melanoma is the 6th most common cancer type in Sweden and the incidence is increasing. The prognosis for patients with unresectable or metastatic melanoma has been poor since there have been no effective therapies for these patients. In recent years, two new first line treatments for unresectable or metastatic melanoma have emerged; Ipilimumab (Yervoy®) and Vemurafenib (Zelboraf®). Ipilimumab is a fully human antibody that blocks CTLA-4 and thereby augments T cell response. Around 10% of patients respond to treatment and the most common side effects associated with Ipilimumab are immune-related adverse events. Vemurafenib is a small molecule drug that inhibits the MAPK pathway. Most patients respond well to treatment with Vemurafenib, and undergo a rapid tumor reduction. However, almost all patients develop a resistance to the drug within a few months of treatment.</p><p>There is a need for better understanding of the effects that these two drugs have on the immune response and to find immune-related biomarkers. In this study we focused on the possible effects of Ipilimumab on the induction and suppressive functions of MDSC-like cells educated <em>in vitro</em> by coculturing monocytes from healthy donors with early passage melanoma cells. For vemurafenib, we improved an assay for immune monitoring of blood samples from patients undergoing treatment and analyzed samples from five patients taken before start of treatment, after 8 weeks of treatment and again after 16-20 weeks of treatment.</p><p>the experiments with Ipilimumab and MDSCs did not give any conclusive answers to the effects that blocking of CTLA-4 may have on the induction or functions of these suppressive cells. However, knowledge regarding the coculture method has been gained and the first steps towards further investigations have been taken. An assay for immune monitoring of patient samples was improved by fine tuning four panels and adding an extra panel. These panels will be used for future immune monitoring in our research group. The immune monitoring of Vemurafenib patients yielded results in form of trends of higher activation of T cells and NK cells after 8 weeks of treatment. The increase in activation could still be seen in the samples taken 16-20 weeks into treatment. Also a trend of decrease in central memory CD4+ and CD8+ T cells was found. These results might in the future be taken into consideration to help in the understanding of the effects that Vemurafenib treatment has on the immune response in different disease stages.</p>
----------------------------------------------------------------------
In diva2:789707 abstract is: <p>Background: The occupational health services (OHS) in Sweden is provided as an impartial expert advice to client companies and shall identify and explain the relationship between work, organization, productivity and health. In general, client companies use the occupational health services particularly for reactive services, when illness has already occured in the company. Construction workers staying constantly in a hazardous environment and are exposed to heavy lifting, monotonous movements and bad working positions. It is therefore important to examine how occupational health can contribute to the preventive work environment for client companies to meet the challenges of the construction work.Aim: The aim of the study was to investigate the opportunities and barriers that exist to the preventive work of occupational health services.Method: A literature review and an interview study was conducted in spring 2013. Scientific papers and other documents were obtained through databases available through the libraries at the Royal Institute of Technology, University of Gothenburg and Halmstad University. Ten individual semi-structural interviews were carried out with a safety engineer and an account manager at a Feelgood unit, as well as with supervisors, safety representatives and with people who had an HR and working position on two of Feelgoods client companies in the construction industry.Results: The analysis revealed eight themes that represented opportunities and barriers to preventive work environment: consultative approach, systematic work environment management, workplaces visits, competence, marketing and communication, management, lack of coordination between professions in occupational health services and customer agreement.Conclusion: The occupational health service needs to take greater responsibility in preventive work environment and challenging client companies to advise them in good working solutions. Occupational health services also needs to be better at marketing their business to client companies to know what they can use OHS for. Feelgood are professional and have high competence in the individual conversations but can improve their professional skills at organizational level. The client companies demand field visits and they saw that OHS has a role in performing technical measurements and help in the process of job rotation. Preventive health services should be written into the agreement in order to be utilized to a greater extent.</p>


w='occured' val={'c': 'occurred', 's': 'diva2:789707', 'n': 'error in original'}

corrected abstract:
<p><em>Background:</em> The occupational health services (OHS) in Sweden is provided as an impartial expert advice to client companies and shall identify and explain the relationship between work, organization, productivity and health. In general, client companies use the occupational health services particularly for reactive services, when illness has already occured in the company. Construction workers staying constantly in a hazardous environment and are exposed to heavy lifting, monotonous movements and bad working positions. It is therefore important to examine how occupational health can contribute to the preventive work environment for client companies to meet the challenges of the construction work.</p><p><em>Aim:</em> The aim of the study was to investigate the opportunities and barriers that exist to the preventive work of occupational health services.</p><p><em>Method:</em> A literature review and an interview study was conducted in spring 2013. Scientific papers and other documents were obtained through databases available through the libraries at the Royal Institute of Technology, University of Gothenburg and Halmstad University. Ten individual semi-structural interviews were carried out with a safety engineer and an account manager at a Feelgood unit, as well as with supervisors, safety representatives and with people who had an HR and working position on two of Feelgoods client companies in the construction industry.</p><p><em>Results:</em> The analysis revealed eight themes that represented opportunities and barriers to preventive work environment: <em>consultative approach</em>, <em>systematic work environment management</em>, <em>workplaces visits</em>, <em>competence</em>, <em>marketing and communication</em>, <em>management</em>, <em>lack of coordination between professions in occupational health services</em> and <em>customer agreement</em>.</p><p><em>Conclusion:</em> The occupational health service needs to take greater responsibility in preventive work environment and challenging client companies to advise them in good working solutions. Occupational health services also needs to be better at marketing their business to client companies to know what they can use OHS for. Feelgood are professional and have high competence in the individual conversations but can improve their professional skills at organizational level. The client companies demand field visits and they saw that OHS has a role in performing technical measurements and help in the process of job rotation. Preventive health services should be written into the agreement in order to be utilized to a greater extent.</p>
----------------------------------------------------------------------
In diva2:1250984 abstract is: <p>Background: Swedish law demands a systematic work environment management system (SWEM) and establishes the employer as responsible for this. In 2016, 44 % of Swedish Work Environment Authorities’ submissions regarded lack of SWEM. The law defines occupational health and safety services (OHSS) as an objective part with expert knowledge within the fields of work environment and rehabilitation.</p><p>Aim of the study: The purpose of this study is to investigate how OHS engineers employed in OHSSs assist customer enterprises in the work of developing and maintaining OHS management systems. The study further aims to identify resources, factors of success, necessary skills and other factors that facilitate this work.</p><p>Method: Six semi-structured interviews were conducted with OHS engineers employed in one of the top five largest OHSS companies in Sweden.</p><p>Result and analyses: Respondents were found to apply similar methods in supporting clients’ OHSM although working in different regional branches and no nationwide training program exists. In working with OHSM support the OHS engineers mainly used self-produced, flexible tools along with external checklists and templates. Dialogue, an active involvement of the client in developing the OHSMS, internal motivation of the company and experience and competence of the OHS engineer was described as the most essential factors of success.</p><p>Conclusions: Our study shows that the OHS engineers interviewed to a large extent work with OHSM support in a way that is consistent with what is found in other studies to be a successful way of collaborating with client companies. Areas of improvement for the OHSS company include deepening relations with clients, contracts better supporting collaboration and procedures for spotting client’s OHSM shortcomings earlier.There is a challenge to find a balance between giving OHS engineers freedom in choosing how they work, providing clients flexible solutions and assuring that certain standards of service are met.We identify the need of a mentorship program for OHS engineers within the OHSS company that would include tutoring both in OHS interventions and the consultant role.Finally, OHS engineers may need to develop their skills in OHS related business economy and how to integrate OHS interventions with the business strategies of the client companies.</p>

corrected abstract:
<p><strong>Background:</strong> Swedish law demands a systematic work environment management system (SWEM) and establishes the employer as responsible for this. In 2016, 44 % of Swedish Work Environment Authorities’ submissions regarded lack of SWEM. The law defines occupational health and safety services (OHSS) as an objective part with expert knowledge within the fields of work environment and rehabilitation.</p><p><strong>Aim of the study:</strong> The purpose of this study is to investigate how OHS engineers employed in OHSSs assist customer enterprises in the work of developing and maintaining OHS management systems. The study further aims to identify resources, factors of success, necessary skills and other factors that facilitate this work.</p><p><strong>Method:</strong> Six semi-structured interviews were conducted with OHS engineers employed in one of the top five largest OHSS companies in Sweden.</p><p><strong>Result and analyses:</strong> Respondents were found to apply similar methods in supporting clients’ OHSM although working in different regional branches and no nationwide training program exists. In working with OHSM support the OHS engineers mainly used self-produced, flexible tools along with external checklists and templates. Dialogue, an active involvement of the client in developing the OHSMS, internal motivation of the company and experience and competence of the OHS engineer was described as the most essential factors of success.</p><p><strong>Conclusions:</strong> Our study shows that the OHS engineers interviewed to a large extent work with OHSM support in a way that is consistent with what is found in other studies to be a successful way of collaborating with client companies. Areas of improvement for the OHSS company include deepening relations with clients, contracts better supporting collaboration and procedures for spotting client’s OHSM shortcomings earlier.</p><p>There is a challenge to find a balance between giving OHS engineers freedom in choosing how they work, providing clients flexible solutions and assuring that certain standards of service are met.</p><p>We identify the need of a mentorship program for OHS engineers within the OHSS company that would include tutoring both in OHS interventions and the consultant role.</p><p>Finally, OHS engineers may need to develop their skills in OHS related business economy and how to integrate OHS interventions with the business strategies of the client companies.</p>
----------------------------------------------------------------------
In diva2:936582 abstract is: <p>3D user interfaces are common today when for example doing industrial design, architecture and 3dmodelling but they are not commonly used in regular interfaces. The hardware requirements for suchinterfaces are not difficult to reach with today’s computers, laptops and mobile devices with only in-tegrated graphics cards.The report found that users of 3D interfaces found them more fun to use and gives a better overviewwhen viewing dense information. Problems exists when using small screens and reading text. Futureimprovements to software tools and APIs as well as improvements in hardware, and new technologiessuch as virtual- and augmented reality will make 3D interfaces easier to develop, use and run.</p>


Note the "3d" at the end of the first line of the abstract, is an error in the original.
corrected abstract:
<p>3D user interfaces are common today when for example doing industrial design, architecture and 3d modelling but they are not commonly used in regular interfaces. The hardware requirements for such interfaces are not difficult to reach with today’s computers, laptops and mobile devices with only integrated graphics cards.</p><p>The report found that users of 3D interfaces found them more fun to use and gives a better overview when viewing dense information. Problems exists when using small screens and reading text. Future improvements to software tools and APIs as well as improvements in hardware, and new technologies such as virtual- and augmented reality will make 3D interfaces easier to develop, use and run.</p>
----------------------------------------------------------------------
In diva2:1038994 abstract is: <p>Affibody molecules are small protein scaffolds that have been engineered to bind to a variety of targets with diversetherapeutic and diagnostic applications. In this study, an array of affibody containing therapeutic constructs,targeting HER2 and HER3, and diagnostic anti-HER3 imaging agents have been purified in preparation for subsequentcancer cell assays and imaging studies in tumour-bearing mice respectively. Herein, the workflow for severalpurification techniques is delineated.</p>

corrected abstract:
<p>Affibody molecules are small protein scaffolds that have been engineered to bind to a variety of targets with diverse therapeutic and diagnostic applications. In this study, an array of affibody containing therapeutic constructs, targeting HER2 and HER3, and diagnostic anti-HER3 imaging agents have been purified in preparation for subsequent cancer cell assays and imaging studies in tumour-bearing mice respectively. Herein, the workflow for several purification techniques is delineated.</p>
----------------------------------------------------------------------
title: "Optimizing a Single Atom Catalyst for theOxygen Evolution Reaction using DensityFunctional Theory"
==>    "Optimizing a Single Atom Catalyst for the Oxygen Evolution Reaction using Density Functional Theory"

In diva2:1353093 abstract is: <p>The growing interest of renewable fuel and energy sources has steadily increased over time due to climate changes. Research is being made around the world to find solutions for the different problems; one possible solution is to produce hydrogen gas to help phase out the usage of fossil fuels. So far, the technology for the hydrogen gas production is expensive for various reasons, one of the challenges is to minimize the energy usage for the production. Hydrogen could be used in fuel cells which can be used to fuel an electric car. In a fuel cell, hydrogen and oxygen gas are mixed to produce electrical energy as the main product, but it also forms thermal energy and water. Hydrogen gas can be produced from the reversed reaction; by electrolysis of water. This reaction requires energy and one way to minimize the energy usage for this is by using acatalyst.</p><p>The goal with this master thesis was to see how the reaction rate of the oxygen evolution reaction can be affected by different single atom catalyst systems. The main structure for this catalyst in this thesis is aporphyrin molecule where different transition metals were tried as the active site. Different modifications on the structure were also made by exchanging some of the structures atoms and by adding different ligands.The purpose of this is to see how these modifications change the activity of the catalyst. The catalysts were optimized and calculated in a computational chemistry program called Gaussian 16. The calculations was made by using the DFT functional PBE0 and the basis sets Def2svp and Def2tzvpp.</p><p>The results show that different modifications do affect the activity of the catalyst. The biggest variations in activity are from placing ligands under the active site while exchanging hydrogens to other substituents on the outer radial position can fine tune the results. The best active sites for this system came by using iridium, rhodium and cobalt which are all elements in group 9 of the periodic table. The lowest overpotential of 0.513 V was given by an iridium based system with four hydrogens exchanged by fluorides.</p>


Note: There were many missing ligatures.
corrected abstract:
<p>The growing interest of renewable fuel and energy sources has steadily increased over time due to climate changes. Research is being made around the world to find solutions for the different problems; one possible solution is to produce hydrogen gas to help phase out the usage of fossil fuels. So far, the technology for the hydrogen gas production is expensive for various reasons, one of the challenges is to minimize the energy usage for the production. Hydrogen could be used in fuel cells which can be used to fuel an electric car. In a fuel cell, hydrogen and oxygen gas are mixed to produce electrical energy as the main product, but it also forms thermal energy and water. Hydrogen gas can be produced from the reversed reaction; by electrolysis of water. This reaction requires energy and one way to minimize the energy usage for this is by using a catalyst.</p><p>The goal with this master thesis was to see how the reaction rate of the oxygen evolution reaction can be affected by different single atom catalyst systems. The main structure for this catalyst in this thesis is a porphyrin molecule where different transition metals were tried as the active site. Different modifications on the structure were also made by exchanging some of the structures atoms and by adding different ligands. The purpose of this is to see how these modifications change the activity of the catalyst. The catalysts were optimized and calculated in a computational chemistry program called Gaussian 16. The calculations was made by using the DFT functional PBE0 and the basis sets Def2svp and Def2tzvpp.</p><p>The results show that different modifications do affect the activity of the catalyst. The biggest variations in activity are from placing ligands under the active site while exchanging hydrogens to other substituents on the outer radial position can fine tune the results. The best active sites for this system came by using iridium, rhodium and cobalt which are all elements in group 9 of the periodic table. The lowest overpotential of 0.513 V was given by an iridium based system with four hydrogens exchanged by fluorides.</p>
----------------------------------------------------------------------
In diva2:628492 abstract is: <p>This thesis investigates the personal account for health information (PHR) that every citizen in Sweden will be offered. The Ministry of Health and Social Affairs has made a decision to conduct a public procurement through Apotekens Service AB. The pro-curement was not finalized at the time of this report.The goal of this thesis is to on behalf of Mawell investigate what business opportunities the launching of a nationally provided service for PHR, and the ecosystem for applica-tions and services, is expected to result in, but also to implement two services to demonstrate the concept of how a service could be used to create benefits for a selected target group.During the thesis work a literature study and interviews with different target groups were performed. The purpose with the interviews was to identify different problems that the target group have in their operations and then analyze and give a proposal to how services developed for PHR could solve the problems.The result of the investigation is proposed services based on the different problems and needs of the targets groups and two service implementations. The report also gives Mawell proposal to how they could continue this work further.</p>

corrected abstract:
<p>This thesis investigates the personal account for health information (PHR) that every citizen in Sweden will be offered. The Ministry of Health and Social Affairs has made a decision to conduct a public procurement through Apotekens Service AB. The procurement was not finalized at the time of this report.</p><p>The goal of this thesis is to on behalf of Mawell investigate what business opportunities the launching of a nationally provided service for PHR, and the ecosystem for applications and services, is expected to result in, but also to implement two services to demonstrate the concept of how a service could be used to create benefits for a selected target group.</p><p>During the thesis work a literature study and interviews with different target groups were performed. The purpose with the interviews was to identify different problems that the target group have in their operations and then analyze and give a proposal to how services developed for PHR could solve the problems.</p><p>The result of the investigation is proposed services based on the different problems and needs of the targets groups and two service implementations. The report also gives Mawell proposal to how they could continue this work further.</p>
----------------------------------------------------------------------
In diva2:1617090 abstract is: <p>Plastic waste is a severe environmental problem in today's society which has been noticed and discussed during the last couple of years. A constant increase of production over the last decades hasled to a large amount of plastic waste ending up in oceans as microplastics. With harder restrictions of plastic use from the European Parliament, alternative plastics that are bio-based and therefore degradable have increased in demand. The aim of this project was therefore to synthesize alignocellulose-based material which contains the minimum amount of latex, the plastic component, while still satisfying the same requirements as a thermoplastic. The original idea was to create the latex with PISA-RAFT technique however, this was not possible since the needed materials could not be delivered due to COVID-19, therefore radical emulsion polymerization was carried out.</p><p>Two latexes were synthesized to create composites with wheat-straw, latex A and latex B. Both latexes consisted of 75% of monomer vinyl acetate (VAc) which was the main component but with different weight percentages of monomers methacrylic acid (MAA) and methyl methacrylate (MMA). Latex A consisted of 20 % MAA and 5% MMA and latex B consisted of 20% MMA and 5% MAA. Latex A and latex B were then mixed with wheat straw to create composites. Due to problems withthe wheat-straw composites one additional composite was created to be able to do all of the analyses. This composite was created by using filter paper as biofiber to mix with the two different latexes. Various characterization analyses including FE-SEM, DLS, DSC, FTIR, NMR, TGA and tensile tests were performed on the composites.</p><p>The NMR and DSC analyses indicated that the actual composition of monomers differs from the theoretical composition and demonstrates that the presence of MAA is hard to detect. This is due to the DSC value for latex A experimental Tg being lower than latex B experimental Tg when latex A consists of more MAA which has a higher detected Tg. During the NMR analysis MAA was also not detected in either latex A nor latex B. The analyses of FTIR contradicts the NMR and DSC analyses hence peaks believed to be from MAA are detected. When comparing the analysis for latex A and B, DLS analysis resulted in latex A having a low PDI and a bigger emulsion sphere size which is preferred when producing composites. The tensile test resulted in latex B achieving the higher values for Young’s modulus and max stress while latex A had a higher value for strain at break. The TGA and DSC analysis however resulted in latex B having a higher Tg and higher thermal stability. The overall analyses indicated that latex B was the most optimal choice for composite production with aslight difference.</p><p>The analysis of the composites indicated by FE-SEM that the interaction between latex and filter paper were higher than for latex and wheat straw. A total of four wheat-straw composites were created with the weight-ratio of wheat-straw:latex, 50:50 and 75:50 for both latex A and B. Due to not being able to grind the wheat straw to the minimum size needed to create composites only FE-SEM and FTIR analyses of the wheat-straw composites could be made. Because of this no conclusion could be made whether the 75:50 or 50:50 weight ratio was the most optimal.</p>

Note: It should be "T<sub>g</sub>>" and not "Tg" - this is an error in the original. See the Swedish abstract for the use of the subscript.

corrected abstract:
<p>Plastic waste is a severe environmental problem in today's society which has been noticed and discussed during the last couple of years. A constant increase of production over the last decades has led to a large amount of plastic waste ending up in oceans as microplastics. With harder restrictions of plastic use from the European Parliament, alternative plastics that are bio-based and therefore degradable have increased in demand. The aim of this project was therefore to synthesize a lignocellulose-based material which contains the minimum amount of latex, the plastic component, while still satisfying the same requirements as a thermoplastic. The original idea was to create the latex with PISA-RAFT technique however, this was not possible since the needed materials could not be delivered due to COVID-19, therefore radical emulsion polymerization was carried out.</p><p>Two latexes were synthesized to create composites with wheat-straw, latex A and latex B. Both latexes consisted of 75% of monomer vinyl acetate (VAc) which was the main component but with different weight percentages of monomers methacrylic acid (MAA) and methyl methacrylate (MMA). Latex A consisted of 20 % MAA and 5% MMA and latex B consisted of 20% MMA and 5% MAA. Latex A and latex B were then mixed with wheat straw to create composites. Due to problems with the wheat-straw composites one additional composite was created to be able to do all of the analyses. This composite was created by using filter paper as biofiber to mix with the two different latexes. Various characterization analyses including FE-SEM, DLS, DSC, FTIR, NMR, TGA and tensile tests were performed on the composites.</p><p>The NMR and DSC analyses indicated that the actual composition of monomers differs from the theoretical composition and demonstrates that the presence of MAA is hard to detect. This is due to the DSC value for latex A experimental Tg being lower than latex B experimental Tg when latex A consists of more MAA which has a higher detected Tg. During the NMR analysis MAA was also not detected in either latex A nor latex B. The analyses of FTIR contradicts the NMR and DSC analyses hence peaks believed to be from MAA are detected. When comparing the analysis for latex A and B, DLS analysis resulted in latex A having a low PDI and a bigger emulsion sphere size which is preferred when producing composites. The tensile test resulted in latex B achieving the higher values for Young’s modulus and max stress while latex A had a higher value for strain at break. The TGA and DSC analysis however resulted in latex B having a higher Tg and higher thermal stability. The overall analyses indicated that latex B was the most optimal choice for composite production with a slight difference.</p><p>The analysis of the composites indicated by FE-SEM that the interaction between latex and filter paper were higher than for latex and wheat straw. A total of four wheat-straw composites were created with the weight-ratio of wheat-straw:latex, 50:50 and 75:50 for both latex A and B. Due to not being able to grind the wheat straw to the minimum size needed to create composites only FE-SEM and FTIR analyses of the wheat-straw composites could be made. Because of this no conclusion could be made whether the 75:50 or 50:50 weight ratio was the most optimal.</p>
----------------------------------------------------------------------
In diva2:1584717 abstract is: <p>Bolivia is a developing country in South America. Many rural communities still lack access to electricity. The extension of the National Grid System to all rural communities is not feasible due to economic and topographic challenges as well as the environmental problems that may arise. To tackle these problems, Off-grid solutions are implemented. Photovoltaic (PV) panels combined with batteries are a viable option for areas located close to the equator and high altitudes such as Bolivia. Almost always a controlled source of energy such as Diesel generators must complement the PV system due to the stochastic nature of solar energy. The use of fossil fuel can be detrimental to the environment and more environmentally friendly solutions are being investigated. The use of wood pellets in Stirling engines is a viable replacement for Diesel generators. </p><p>The purpose of this study is to investigate and compare the environmental impacts caused by two Off-grid hybrid systems. The first one is composed of a Diesel generator, PV panels, and batteries. The second one is composed of a Stirling engine, PV panels, and batteries. The study area chosen for this work is the community El Carmen, Pando, in Bolivia. A Life Cycle Assessment (LCA) model is carried out for the systems according to the 4 phases of the LCA methodology. First, individual LCA models for all midpoint impact categories are generated. Secondly, a comparative LCA between the two systems, both at midpoint and endpoint, is created. Finally, a sensitivity analysis is conducted to determine the robustness of the models. </p><p>The individual midpoint analysis of both systems showed that the controlled part of the electricity production (i.e., the Diesel generator and the Stirling engine) generated the greatest impact in the categories Global warming, Stratospheric ozone depletion, Ionizing radiation, Ozone formation, Fine particulate matter formation, Terrestrial acidification, Human carcinogenic toxicity, Land use, Fossil fuel scarcity, and Water consumption. All the processes related to the PV panels generated a greater impact in all Ecotoxicity categories (terrestrial, marine, and freshwater), Eutrophication (freshwater and marine), and Human non-carcinogenic toxicity. </p><p>The midpoint results of the comparative LCA are inconclusive. Each system received higher scores in certain categories and lower scores in others. No firm conclusion could be drawn regarding the identification of the more environmentally friendly alternative. The Diesel/PV/Batteries system dominated the Global warming, Tropospheric ozone formation, Fine particulate matter formation, Terrestrial acidification, and Fossil resource scarcity categories. The Stirling/PV/Batteries system showed a greater impact on Stratospheric ozone depletion, Ecotoxicity, Eutrophication, Human carcinogenic toxicity, Human non- carcinogenic toxicity, and Mineral resource scarcity. </p><p>The endpoint damage assessment showed that the emissions and midpoint categories described had a greater impact on Human health and Resource scarcity in the case of the Diesel/PV/Batteries system. On the other hand, the Stirling/PV/Batteries system caused greater damage to the Ecosystem category. </p><p>The sensitivity analysis was conducted in two scenarios for each system. In the first scenario, alteration of fuel transport distance, no significant changes were detected in all endpoint categories. In the second scenario, alteration of Diesel/Stirling Contribution, the model showed an increasing trend (~30% for the first system and ~25% for the second one) in all categories when the contribution of the controlled part of the electricity production was increased. </p>


The abstract had unnecessary spaces at the end of each paragraph.
corrected abstract:
<p>Bolivia is a developing country in South America. Many rural communities still lack access to electricity. The extension of the National Grid System to all rural communities is not feasible due to economic and topographic challenges as well as the environmental problems that may arise. To tackle these problems, Off-grid solutions are implemented. Photovoltaic (PV) panels combined with batteries are a viable option for areas located close to the equator and high altitudes such as Bolivia. Almost always a controlled source of energy such as Diesel generators must complement the PV system due to the stochastic nature of solar energy. The use of fossil fuel can be detrimental to the environment and more environmentally friendly solutions are being investigated. The use of wood pellets in Stirling engines is a viable replacement for Diesel generators.</p><p>The purpose of this study is to investigate and compare the environmental impacts caused by two Off-grid hybrid systems. The first one is composed of a Diesel generator, PV panels, and batteries. The second one is composed of a Stirling engine, PV panels, and batteries. The study area chosen for this work is the community El Carmen, Pando, in Bolivia. A Life Cycle Assessment (LCA) model is carried out for the systems according to the 4 phases of the LCA methodology. First, individual LCA models for all midpoint impact categories are generated. Secondly, a comparative LCA between the two systems, both at midpoint and endpoint, is created. Finally, a sensitivity analysis is conducted to determine the robustness of the models.</p><p>The individual midpoint analysis of both systems showed that the controlled part of the electricity production (i.e., the Diesel generator and the Stirling engine) generated the greatest impact in the categories Global warming, Stratospheric ozone depletion, Ionizing radiation, Ozone formation, Fine particulate matter formation, Terrestrial acidification, Human carcinogenic toxicity, Land use, Fossil fuel scarcity, and Water consumption. All the processes related to the PV panels generated a greater impact in all Ecotoxicity categories (terrestrial, marine, and freshwater), Eutrophication (freshwater and marine), and Human non-carcinogenic toxicity.</p><p>The midpoint results of the comparative LCA are inconclusive. Each system received higher scores in certain categories and lower scores in others. No firm conclusion could be drawn regarding the identification of the more environmentally friendly alternative. The Diesel/PV/Batteries system dominated the Global warming, Tropospheric ozone formation, Fine particulate matter formation, Terrestrial acidification, and Fossil resource scarcity categories. The Stirling/PV/Batteries system showed a greater impact on Stratospheric ozone depletion, Ecotoxicity, Eutrophication, Human carcinogenic toxicity, Human non-carcinogenic toxicity, and Mineral resource scarcity.</p><p>The endpoint damage assessment showed that the emissions and midpoint categories described had a greater impact on Human health and Resource scarcity in the case of the Diesel/PV/Batteries system. On the other hand, the Stirling/PV/Batteries system caused greater damage to the Ecosystem category.</p><p>The sensitivity analysis was conducted in two scenarios for each system. In the first scenario, alteration of fuel transport distance, no significant changes were detected in all endpoint categories. In the second scenario, alteration of Diesel/Stirling Contribution, the model showed an increasing trend (~30% for the first system and ~25% for the second one) in all categories when the contribution of the controlled part of the electricity production was increased.</p>
----------------------------------------------------------------------
In diva2:1642418 abstract is: <p>The aim of the project was to identify positions and amino acids that contribute to improved structure and stability of bispecific ADAPT proteins. During the 20 weeks project period, different amino acid substitutions were analysed to evaluate the effect on the three-helical structure and stability of bispecific ADAPTs targeting human serum albumin (HSA) and tumor necrosis factor α (TNFα). Furthermore, the study also included identification of which amino acid substitutions that affect the simultaneous binding ability of the anti-TNFα ADAPT. The amino acids substitutions that demonstrated improved stability was further evaluated in two other bispecific ADAPT proteins targeting epithelial cell adhesion molecule (EpCAM), in terms of structure and stability.</p><p>The TNFα-targeting ADAPT variants was produced in Escherichia coli (E. coli), purified through affinity chromatography using a HSA-coupled matrix and was further analysed and evaluated using SDS-PAGE, circular dichrosim, size-exclusion chromatography and surface plasmon resonance to detect expression levels, yields, thermal stability, secondary structure, and simultaneous binding to TNFα and HSA. Furthermore, the production, purification and evaluation were redone with other bispecific ADAPTs targeting EpCAM, to be able to draw more general conclusions. The outcome showed which amino acids substitutions in the scaffold that improve the structure and stability of the TNFα- and EpCAM-binding ADAPT protein variants, respectively.</p><p>Some of the ADAPT variants targeting TNFα showed improved stability and increased melting temperature. One of the variants with most potential from these mutants was ADAPT_TNFα5_F21K, both able to refold after heat treatment and demonstrated a higher melting temperature in the same order as the original binder. The variant bound HSA but not TNFα, thus consequently was not able to bind TNFα and HSA simultaneously. The variants ADAPT_TNFα5_V17I and ADAPT_TNFα5_M22Q both demonstrated a clear alpha-helix structure, were able to refold after heat treatment and demonstrated simultaneous binding to TNFα and HSA. The melting temperature for ADAPT_TNFα5_V17I was the same as for the original binder (59°C) and ADAPT_TNFα5_M22Q showed a decreased melting temperature (45°C) compared to the original binder. The amino acid substitutions that improve the stability of the original binder was combined and two variants withthese mutations were designed. Unfortunately, these variants could not express in E. coli cells and were not able to be produced. For the EpCAM targeting mutants one variant, ADAPT_EpCAM_02_X11N, showed huge improvements of the stability and structure compared to the original binder ADAPT_EpCAM_02. This variant improved the melting temperature with 24°C compared to the original binder and was able to refold after heat treatment, which the original binder did not have the ability to do. However, ADAPT_EpCAM_02_X11N was not able to simultaneously bind EpCAM and HSA, demonstrating that the mutation also had an effect on the binding ability. In the variant ADAPT_EpCAM_08 the mutation Y5I improved the melting temperature with 14°C compared to the original binder and was able to refold after thermal denaturation. However, the simultaneous binding to EpCAM and HSA was negatively affected.</p><p>The project results have contributed to better understanding of the bispecific ADAPT proteins, which enables further development of the scaffold. The amino acid positions in the scaffold that showed to be important for ADAPT structure and stability will be used in the design of a new ADAPT-library, from which new binders with improved structure and stability hopefully can be selected, which might have the potentially to be used as future therapeutics.</p>

w='dichrosim' val={'c': 'dichroism', 's': 'diva2:1642418', 'n': 'error in original'}

corrected abstract:
<p>The aim of the project was to identify positions and amino acids that contribute to improved structure and stability of bispecific ADAPT proteins. During the 20 weeks project period, different amino acid substitutions were analysed to evaluate the effect on the three-helical structure and stability of bispecific ADAPTs targeting human serum albumin (HSA) and tumor necrosis factor α (TNFα). Furthermore, the study also included identification of which amino acid substitutions that affect the simultaneous binding ability of the anti-TNFα ADAPT. The amino acids substitutions that demonstrated improved stability was further evaluated in two other bispecific ADAPT proteins targeting epithelial cell adhesion molecule (EpCAM), in terms of structure and stability.</p><p>The TNFα-targeting ADAPT variants was produced in <em>Escherichia coli (E. coli)</em>, purified through affinity chromatography using a HSA-coupled matrix and was further analysed and evaluated using SDS-PAGE, circular dichrosim, size-exclusion chromatography and surface plasmon resonance to detect expression levels, yields, thermal stability, secondary structure, and simultaneous binding to TNFα and HSA. Furthermore, the production, purification and evaluation were redone with other bispecific ADAPTs targeting EpCAM, to be able to draw more general conclusions. The outcome showed which amino acids substitutions in the scaffold that improve the structure and stability of the TNFα- and EpCAM-binding ADAPT protein variants, respectively.</p><p>Some of the ADAPT variants targeting TNFα showed improved stability and increased melting temperature. One of the variants with most potential from these mutants was ADAPT_TNFα5_F21K, both able to refold after heat treatment and demonstrated a higher melting temperature in the same order as the original binder. The variant bound HSA but not TNFα, thus consequently was not able to bind TNFα and HSA simultaneously. The variants ADAPT_TNFα5_V17I and ADAPT_TNFα5_M22Q both demonstrated a clear alpha-helix structure, were able to refold after heat treatment and demonstrated simultaneous binding to TNFα and HSA. The melting temperature for ADAPT_TNFα5_V17I was the same as for the original binder (59°C) and ADAPT_TNFα5_M22Q showed a decreased melting temperature (45°C) compared to the original binder. The amino acid substitutions that improve the stability of the original binder was combined and two variants with these mutations were designed. Unfortunately, these variants could not express in <em>E. coli</em> cells and were not able to be produced. For the EpCAM targeting mutants one variant, ADAPT_EpCAM_02_X11N, showed huge improvements of the stability and structure compared to the original binder ADAPT_EpCAM_02. This variant improved the melting temperature with 24°C compared to the original binder and was able to refold after heat treatment, which the original binder did not have the ability to do. However, ADAPT_EpCAM_02_X11N was not able to simultaneously bind EpCAM and HSA, demonstrating that the mutation also had an effect on the binding ability. In the variant ADAPT_EpCAM_08 the mutation Y5I improved the melting temperature with 14°C compared to the original binder and was able to refold after thermal denaturation. However, the simultaneous binding to EpCAM and HSA was negatively affected.</p><p>The project results have contributed to better understanding of the bispecific ADAPT proteins, which enables further development of the scaffold. The amino acid positions in the scaffold that showed to be important for ADAPT structure and stability will be used in the design of a new ADAPT-library, from which new binders with improved structure and stability hopefully can be selected, which might have the potentially to be used as future therapeutics.</p>
----------------------------------------------------------------------
In diva2:802843 abstract is: <p>The industry of agrobiotechnology is a relatively young industry dominated by multinational companies. The regulations surrounding the use of biotechnology to develop genetically modified crops have made it very hard for small or medium sized companies to compete in this industry due to high regulatory costs. The first part thesis describes the regulatory system for commercialization of GMOs in the WU and also presents estimations of the costs experienced by a company from this system. The second part of this thesis describes how biotechnology is used in plant breeding programs, using potato breeding asa specific example. With the help of researchers from MistraBiotech, a new process for developing plant varieties using site-directed mutagenesis has beeneconomically evaluated using a cost/benefit analysis. The results of this case study shows that sitedirected mutagenesis using TALEN has the potential of greatly reducing the time and cost of conventional breeding programs. Benefits arise from the shortening of the breeding program which translates into higher net present values of released varieties and also on the ability of producing new varieties faster. The competitive advantage of adopting new biotechnical methods can be reduced developing cost, a more dynamic and faster developing process and a way of circumventingthe GMO regulations. This could have different impacts on the industry since it could allow smallercompanies to compete with multinational agrochemical companies. It could however also lead to a regained interest from the multinational companies in the European market which would force European companies to compete with much larger companies. </p>

w='WU' val={'c': 'EU', 's': 'diva2:802843', 'n': 'no full text'}

corrected abstract:
<p>The industry of agrobiotechnology is a relatively young industry dominated by multinational companies. The regulations surrounding the use of biotechnology to develop genetically modified crops have made it very hard for small or medium sized companies to compete in this industry due to high regulatory costs. The first part thesis describes the regulatory system for commercialization of GMOs in the EU and also presents estimations of the costs experienced by a company from this system. The second part of this thesis describes how biotechnology is used in plant breeding programs, using potato breeding asa specific example. With the help of researchers from MistraBiotech, a new process for developing plant varieties using site-directed mutagenesis has been economically evaluated using a cost/benefit analysis. The results of this case study shows that site directed mutagenesis using TALEN has the potential of greatly reducing the time and cost of conventional breeding programs. Benefits arise from the shortening of the breeding program which translates into higher net present values of released varieties and also on the ability of producing new varieties faster. The competitive advantage of adopting new biotechnical methods can be reduced developing cost, a more dynamic and faster developing process and a way of circumventing the GMO regulations. This could have different impacts on the industry since it could allow smaller companies to compete with multinational agrochemical companies. It could however also lead to a regained interest from the multinational companies in the European market which would force European companies to compete with much larger companies. </p>
----------------------------------------------------------------------
In diva2:1593450 abstract is: <p>In order to feed a growing population, the crop yield needs to be increased.  One way to do this is to optimise the photosynthetic activity in the plant, which includes improvement of carbon fixation. To succeed with this, knowledge of the regulation of key proteins in the chloroplast is required. The aim of this project is to identify possible regulatory protein-metabolite interactions in chloroplasts from <em>Arabidopsis thaliana</em>. The target proteins are the 11 enzymes of the Calvin-Benson-Bassham cycle. The metabolites of interest are 3PGA, ATP, FBP, GAP, which are intermediates or co-factors of the cycle;2PG, which is a product of a competing reaction in the cycle; and finally G6P, citrate and sucrose, which  are central metabolites in other vital reactions in the cell. Before the experiments with Arabidopsis, spinach was used as a test organism to evaluate the proposed protocols. First, chloroplasts were isolatedfrom leaves. When the integrity of the chloroplasts had been validated, the proteins were extracted. Metabolic interactions with the extracted proteins were analyzed with limited proteolysis-small molecule mapping. This method, which combines limited proteolysis with mass spectrometry, detected severalprotein-metabolite interactions. In Arabidopsis, all enzymes except for FBPase, PPE and TIM had atleast one interaction. In spinach, interactions were seen with FBA, GAPDH, PGK, PRK, RuBisCO,TIM and TK. The results highlight potential regulatory events, which could be used to target bottlenecks in carbon fixation. This could provide a pathway to increase the flux in the Calvin-Benson-Bassham cycle, and thereby improve carbon fixation in plants.</p>

w='atleast' val={'c': 'at least', 's': 'diva2:1593450'}

corrected abstract:
<p>In order to feed a growing population, the crop yield needs to be increased. One way to do this is to optimise the photosynthetic activity in the plant, which includes improvement of carbon fixation. To succeed with this, knowledge of the regulation of key proteins in the chloroplast is required. The aim of this project is to identify possible regulatory protein-metabolite interactions in chloroplasts from <em>Arabidopsis thaliana</em>. The target proteins are the 11 enzymes of the Calvin-Benson-Bassham cycle. The metabolites of interest are 3PGA, ATP, FBP, GAP, which are intermediates or co-factors of the cycle; 2PG, which is a product of a competing reaction in the cycle; and finally G6P, citrate and sucrose, which are central metabolites in other vital reactions in the cell. Before the experiments with <em>Arabidopsis</em>, spinach was used as a test organism to evaluate the proposed protocols. First, chloroplasts were isolated from leaves. When the integrity of the chloroplasts had been validated, the proteins were extracted. Metabolic interactions with the extracted proteins were analyzed with limited proteolysis-small molecule mapping. This method, which combines limited proteolysis with mass spectrometry, detected several protein-metabolite interactions. In <em>Arabidopsis</em>, all enzymes except for FBPase, PPE and TIM had at least one interaction. In spinach, interactions were seen with FBA, GAPDH, PGK, PRK, RuBisCO, TIM and TK. The results highlight potential regulatory events, which could be used to target bottlenecks in carbon fixation. This could provide a pathway to increase the flux in the Calvin-Benson-Bassham cycle, and thereby improve carbon fixation in plants.</p>
----------------------------------------------------------------------
In diva2:1359286 abstract is: <p>Lung cancer has been a major cause of death among types of cancers in the world. In the early stages, lung nodules can be detected by the aid of imaging modalities such as Computed Tomography (CT). In this stage, radiologists look for irregular rounded-shaped nodules in the lung which are normally less than 3 centimeters in diameter. Recent advancements in image analysis have proven that images contain more information than regular parameters such as intensity, histogram and morphological details. Therefore, in this project we have focused on extracting quantitative, hand-crafted features from nearly 1400 lung CT images to train a variety of classifiers based on them. In the first experiment, in total 424 Radiomics features per image has been used to train classifiers such as: Random Forest (RF), Support Vector Machine (SVM), Decision Tree (DT), Naive Bayes (NB), Linear Discriminant Analysis (LDA) and Multi-Layer Perceptron (MLP). In the second experiment, we evaluate each feature category separately with our classifiers. The third experiment includes wrapper feature selection methods (Forward/Backward/Recursive) and filter-based feature selection methods (Fisher score, Gini Index and Mutual information). They have been implemented to find the most relevant feature set in model construction. Performance of each learning method has been evaluated by accuracy score, wherewe achieved the highest accuracy of 78% with Random Forest classifier (74% in 5-fold average) and 0.82 Area Under the Receiver Operating Characteristics (AUROC) curve. After RF, NB and MLP showed the best average accuracy of 71.4% and 71% respectively.</p>


Note the Latin Small Letter I with Diaeresis in "Naïve".
corrected abstract:
<p>Lung cancer has been a major cause of death among types of cancers in the world. In the early stages, lung nodules can be detected by the aid of imaging modalities such as Computed Tomography (CT). In this stage, radiologists look for irregular rounded-shaped nodules in the lung which are normally less than 3 centimeters in diameter. Recent advancements in image analysis have proven that images contain more information than regular parameters such as intensity, histogram and morphological details.</p><p>Therefore, in this project we have focused on extracting quantitative, hand-crafted features from nearly 1400 lung CT images to train a variety of classifiers based on them.</p><p>In the first experiment, in total 424 Radiomics features per image has been used to train classifiers such as: Random Forest (RF), Support Vector Machine (SVM), Decision Tree (DT), Naïve Bayes (NB), Linear Discriminant Analysis (LDA) and Multi-Layer Perceptron (MLP). In the second experiment, we evaluate each feature category separately with our classifiers. The third experiment includes wrapper feature selection methods (Forward/Backward/Recursive) and filter-based feature selection methods (Fisher score, Gini Index and Mutual information). They have been implemented to find the most relevant feature set in model construction.</p><p>Performance of each learning method has been evaluated by accuracy score, where we achieved the highest accuracy of 78% with Random Forest classifier (74% in 5-fold average) and 0.82 Area Under the Receiver Operating Characteristics (AUROC) curve. After RF, NB and MLP showed the best average accuracy of 71.4% and 71% respectively.</p>
----------------------------------------------------------------------
In diva2:1223022 abstract is: <p><strong>Abstract </strong></p><p><strong>Background:</strong> Building work environment co-ordinators Safety Coordinators are responsible for planning, designing projecting and executionperformance at the construction site. This report aims to look into their view on education, experience and competence required to work in these roles.</p><p><strong>Aim: </strong>The first aim is to understand how the building workenvironment co-ordinatorsSafety Coordinators for planning and designing projecting think about resonate around the creation of a work environment plan used as a risk management. tool. The second aim is to examine how building workenvironment co-ordinators Safety Coordinators for execution performing, is handling and interpret the work environment plan as risk management tool. The third aim is to find out in what extent they consider it necessary, as well as what level of education they need. The fourth aim is to evaluate if there is a need for mentoring as a part of education.</p><p><strong>Method: </strong>This one year master hasthesis has been conducted as a qualitative study with individual interviews with four building  work environment co-ordinators construction workers, and one traffic planning co-ordinator. constructor. It has also included observations at different construction works at company 1. A data literature analysis was also conducted.<strong> </strong></p><p><strong>Results: </strong>The study shows that there is a need for different directions in the implementation of training, and that there is a need to repeat the training sessions. The sessions should be a combination of theoretical and practical moments, alternative learning through mentoring. Practical connection has been found to be of great importance for the implementation of good risk management, for building work environment co-ordinators construction environmental coordinators for planning, designing and execution.</p><p><strong>Conclusion:</strong> The study shows that there is a need to develop structure and focus of education for the building work environment co-ordinators construction environment coordinator and traffic planning co-ordinator. Today, only theoretically oriented trainings are offered. It would be of great importance to provide mentoring as part of the education. Repeated training is considered to be important as part of maintaining and further developing risk management and security.</p><p><strong>Key Words:</strong> Construction work coordination, Construction work, roadwork, road worker, workenvironment plan, riskmanagement, BAS P, BAS U.</p><p> </p>

The DiVA abstract has a lot of words that the actual thesis abstract does not have.
Note that "workenvironment" is set as a single word in the actual abstract.

corrected abstract:
<p><strong>Background:</strong> Building workenvironment co-ordinators are responsible for planning, designing and execution at the construction site. This report aims to look into their view on education, experience and competence required to work in these roles.</p><p><strong>Aim:</strong> The first aim is to understand how the building workenvironment co-ordinators for planning and designing think about the creation of a workenvironment plan used as risk management. The second aim is to examine how building workenvironment co-ordinators for execution, is handling and interpret the workenvironment plan as risk management. The third aim is to find out in what extent they consider it necessary, as well as what level of education they need. The fourth aim is to evaluate if there is a need for mentoring as a part of education.</p><p><strong>Method:</strong> This one year master thesis has been conducted as a qualitative study with individual interviews with four building workenvironment co-ordinators, and one traffic planning co-ordinator. It has also included observations at different construction works at company 1. A data literature analysis was also conducted.</p><p><strong>Results:</strong> The study shows that there is a need for different directions in the implementation of training, and that there is a need to repeat the training sessions. The sessions should be a combination of theoretical and practical moments, alternative learning through mentoring. Practical connection has been found to be of great importance for the implementation of good risk management, for building workenvironment co-ordinators for planning, designing and execution.</p><p><strong>Conclusion:</strong> The study shows that there is a need to develop structure and focus of education for the building workenvironment co-ordinators and traffic planning co-ordinator. Today, only theoretically oriented trainings are offered. It would be of great importance to provide mentoring as part of the education. Repeated training is considered to be important as part of maintaining and further developing risk management and security.</p>
----------------------------------------------------------------------
In diva2:1595045 abstract is: <p>Several case studies reported incidences where bacterial or viral infections had serious neuropsychiatric consequences. Some viruses can induce an immune response that result in the production of antibodies (Abs) with affinity for both the pathogens but also for endogenous proteins present in for example the central nervous system (CNS) causing neurological problems. Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) is currently considered to cause several neurological complications and there are several articles reporting neurological symptoms in patients after SARS-CoV-2 infection and some cases of autoimmune diseases that may potentially be associated with the COVID-19 disease. The underlying hypothesis in this project is that SARS-CoV-2, by activating the adaptive immune system, may induce the autoimmunity response which can cause both mild and severe neurological problems. Another hypothesis is that autoantibodies targeting extracellular domains of receptors, channels or transporters are prone to dysregulate normal physiological functions causing neurological symptom. However, these proteins can occur in an active or inactive state and binding of autoantibodies to these proteins might be state specific. The aim of the project was therefore to develop a tissue imaging-based method to detect infection-induced autoimmunity to brain proteins caused by SARS-CoV-2 infections using an immunofluorescence method to identify the normal and the abnormal autoimmunity profiles by comparing serum from controls and post-covid patients. The project focused on developing a method to analyse the different immunoglobulin (Ig) subtypes IgG, IgA and IgM bindings and the effect of the activation receptor state on the Ig binding. An automated staining instrument was also tested to see if this instrument can be a hypothetic candidate for clinical usage and used as a diagnostic tool. The method is an indirect immunohistochemistry (IHC) based on tyramide signal amplification (TSA) using a tissue-based assay and then analysed with microscopy. This project was an important towards development of a functional and effective diagnostic tool. The outcome of the project shows promising results regarding the detection of autoimmunity using IHC and intensity measurements. The Purkinje cells and the dendritic staining observed were also interesting patterns giving some relevant information about the relation between the autoantibodies-induced dysregulation and the neurological or psychiatric symptoms. However, more patient information is needed at the same time as there is a need of larger cohorts and more research to be able to draw a more accurate conclusion. The current study may be at an early-stage but it will contribute to a better understanding of the neurological and psychiatric manifestations in patients with autoantibodies due to infections such as SARS-CoV-2 and may provide clinicians with evidence of autoimmunity. It may also support the clinicians’ treatment decisions for the patients, such as immunotherapy.</p>


corrected abstract:
<p>Several case studies reported incidences where bacterial or viral infections had serious neuropsychiatric consequences. Some viruses can induce an immune response that result in the production of antibodies (Abs) with affinity for both the pathogens but also for endogenous proteins present in for example the central nervous system (CNS) causing neurological problems. Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) is currently considered to cause several neurological complications and there are several articles reporting neurological symptoms in patients after SARS-CoV-2 infection and some cases of autoimmune diseases that may potentially be associated with the COVID-19 disease. The underlying hypothesis in this project is that SARS-CoV-2, by activating the adaptive immune system, may induce the autoimmunity response which can cause both mild and severe neurological problems. Another hypothesis is that autoantibodies targeting extracellular domains of receptors, channels or transporters are prone to dysregulate normal physiological functions causing neurological symptom. However, these proteins can occur in an active or inactive state and binding of autoantibodies to these proteins might be state specific. The aim of the project was therefore to develop a tissue imaging-based method to detect infection-induced autoimmunity to brain proteins caused by SARS-CoV-2 infections using an immunofluorescence method to identify the normal and the abnormal autoimmunity profiles by comparing serum from controls and post-covid patients. The project focused on developing a method to analyse the different immunoglobulin (Ig) subtypes IgG, IgA and IgM bindings and the effect of the activation receptor state on the Ig binding. An automated staining instrument was also tested to see if this instrument can be a hypothetic candidate for clinical usage and used as a diagnostic tool. The method is an indirect immunohis tochemistry (IHC) based on tyramide signal amplification (TSA) using a tissue-based assay and then analysed with microscopy. This project was an important towards development of a functional and effective diagnostic tool. The outcome of the project shows promising results regarding the detection of autoimmunity using IHC and intensity measurements. The Purkinje cells and the dendritic staining observed were also interesting patterns giving some relevant information about the relation between the autoantibodies-induced dysregulation and the neurological or psychiatric symptoms. However, more patient information is needed at the same time as there is a need of larger cohorts and more research to be able to draw a more accurate conclusion. The current study may be at an early-stage but it will contribute to a better understanding of the neurological and psychiatric manifestations in patients with autoantibodies due to infections such as SARS-CoV-2 and may provide clinicians with evidence of autoimmunity. It may also support the clinicians’ treatment decisions for the patients, such as immunotherapy.</p>
----------------------------------------------------------------------
In diva2:1873307 abstract is: <p>The utilisation of biocatalysts, particularly enzymes, in chemical and pharmaceutical industries presents significant advantages over the traditional chemocatalytic methods that historically dominated the industry. A crucial class of enzymes, transaminases, play a central role in the production of chiral amines, fundamental building blocks in these industrial sectors. This study focuses on a specific amine transaminase from <em>Silicibacter pomeroyi</em>. While this enzyme has previously demonstrated the ability to catalyse a variety of reactions for chiral amine production, realising its full potential in industrial applications requires enhanced stability at higher temperatures. In contrast to commonly employed protein engineering methods such as rational design and directed evolution, this study utilises ancestral sequence reconstruction to generate more temperature-resistant variants of this enzyme. Previous applications of this method have shown promising results in generating proteins with increased thermal stability. Through this approach, wherein ancestors of this enzyme are recreated from extant sequences, it is expected that variants capable of maintaining function at higher temperatures will be produced. By exploring this alternative strategy for protein engineering, the study aims to provide more robust biocatalysts for industrial applications. The outcome of this study is that two ancestors exhibited increased thermostability. This was evidenced by the analysis of T<sub>50</sub><sup>15</sup>, which showed an improvement of 3.9 and 6 °C for each respective ancestor. Furthermore, t<sub>1/2 </sub>measurements indicated that they remained active for 2.06 to 3.72 fold longer at 55 °C before becoming inactive. However, they exhibited lower specific activity at room temperature, partially due to only a small fraction of the ancestral protein samples being properly folded. This suggests further improvements and continued investigations into substrate acceptance and stability in different solvents are required. In conclusion, this study demonstrates that ancestral sequence reconstruction is a protein engineering technique effective in enhancing protein thermostability and should be considered a more viable alternative to directed evolution and rational design.</p>


corrected abstract:
<p>The utilisation of biocatalysts, particularly enzymes, in chemical and pharmaceutical industries presents significant advantages over the traditional chemocatalytic methods that historically dominated the industry. A crucial class of enzymes, transaminases, play a central role in the production of chiral amines, fundamental building blocks in these industrial sectors. This study focuses on a specific amine transaminase from <em>Silicibacter pomeroyi</em>. While this enzyme has previously demonstrated the ability to catalyse a variety of reactions for chiral amine production, realising its full potential in industrial applications requires enhanced stability at higher temperatures. In contrast to commonly employed protein engineering methods such as rational design and directed evolution, this study utilises ancestral sequence reconstruction to generate more temperature-resistant variants of this enzyme. Previous applications of this method have shown promising results in generating proteins with increased thermal stability. Through this approach, wherein ancestors of this enzyme are recreated from extant sequences, it is expected that variants capable of maintaining function at higher temperatures will be produced. By exploring this alternative strategy for protein engineering, the study aims to provide more robust biocatalysts for industrial applications. The outcome of this study is that two ancestors exhibited increased thermostability. This was evidenced by the analysis of T<sub>50</sub><sup>15</sup>, which showed an improvement of 3.9 and 6 °C for each respective ancestor. Furthermore, t<sub>1/2</sub> measurements indicated that they remained active for 2.06 to 3.72 fold longer at 55 °C before becoming inactive. However, they exhibited lower specific activity at room temperature, partially due to only a small fraction of the ancestral protein samples being properly folded. This suggests further improvements and continued investigations into substrate acceptance and stability in different solvents are required. In conclusion, this study demonstrates that ancestral sequence reconstruction is a protein engineering technique effective in enhancing protein thermostability and should be considered a more viable alternative to directed evolution and rational design.</p>
----------------------------------------------------------------------
title: "Recombinant expression, purification and crystallization of Pyranose 2-Oxidase from Trametes mutlicolor to investigate the structural determinants of oxygen ractivity"
==>    "Recombinant expression, purification and crystallization of Pyranose 2-Oxidase from Trametes multicolor to investigate the structural determinants of oxygen ractivity"

Tehre seems to be a spelling error in the title.

In diva2:464930 abstract is: <p><strong><em>Background: </em></strong>The molecular basis of enzymatic oxygen reactivity has been investigated for different flavoenzymes but still there are no clearly defined mechanisms for oxygen activation. Pyranose 2-oxidase from <em>Trametes multicolor</em> (P2Ox) and pyranose dehydrogenase from <em>Agaricus meleagris </em>(PDH), share similar catalytic, kinetic and structural features, but still, P2Ox reacts with oxygen whereas PDH does not. <strong><em>Objective:</em></strong> The present study aims to investigate the oxygen reactivity in P2Ox, specifically the importance of Asn593 and surrounding residues. The approach was to target specific active-site residues in P2Ox by directed mutagenesis to disable oxygen reactivity during the oxidative half-reaction without significantly altering the reductive half-reaction. The mutant designs were aided by the structural similarity between P2Ox and PDH. Five mutants were produced in the heterologous host <em>Escherichia coli</em>: N593H, N593Q, N593H/H446E, N593H/H446Q, and N593H/H446N. A preliminary biochemical and kinetic characterization was performed on the mutants, as well as crystallization for crystal-structure determination. <strong><em>Results:</em></strong> Crystals of all variants were obtained at high resolution, except for variant N593H/H446N, which was destabilized and degraded proteolytically. For all mutant structures obtained, the active-site loop is in the closed conformation, implying that the mutants were captured in a conformational state relevant to the oxidative half-reaction. Moreover, N593H shows impaired glucose binding and turnover, and may have impaired C(4a)-adduct formation. N593H/H446E on the other hand, had almost similar glucose affinity as the wild type, but 80-fold lower turnover rate. In addition, the flavin cofactor shows distortion close to the site of mutation. <strong><em>Conclusions:</em></strong> The results pinpoint two mutations, N593H and N593H/H446E that appear particularly interesting for future characterization by stopped-flow kinetics to analyze in detail the effects of the specific mutations on the individual half-reactions.</p>


corrected abstract:
<p><strong><em>Background: </em></strong>The molecular basis of enzymatic oxygen reactivity has been investigated for different flavoenzymes but still there are no clearly defined mechanisms for oxygen activation. Pyranose 2-oxidase from <em>Trametes multicolor</em> (P2Ox) and pyranose dehydrogenase from <em>Agaricus meleagris</em> (PDH), share similar catalytic, kinetic and structural features, but still, P2Ox reacts with oxygen whereas PDH does not. <strong><em>Objective:</em></strong> The present study aims to investigate the oxygen reactivity in P2Ox, specifically the importance of Asn593 and surrounding residues. The approach was to target specific active-site residues in P2Ox by directed mutagenesis to disable oxygen reactivity during the oxidative half-reaction without significantly altering the reductive half-reaction. The mutant designs were aided by the structural similarity between P2Ox and PDH. Five mutants were produced in the heterologous host <em>Escherichia coli</em>: N593H, N593Q, N593H/H446E, N593H/H446Q, and N593H/H446N. A preliminary biochemical and kinetic characterization was performed on the mutants, as well as crystallization for crystal-structure determination. <strong><em>Results:</em></strong> Crystals of all variants were obtained at high resolution, except for variant N593H/H446N, which was destabilized and degraded proteolytically. For all mutant structures obtained, the active-site loop is in the closed conformation, implying that the mutants were captured in a conformational state relevant to the oxidative half-reaction. Moreover, N593H shows impaired glucose binding and turnover, and may have impaired C(4a)-adduct formation. N593H/H446E on the other hand, had almost similar glucose affinity as the wild type, but 80-fold lower turnover rate. In addition, the flavin cofactor shows distortion close to the site of mutation. <strong><em>Conclusions:</em></strong> The results pinpoint two mutations, N593H and N593H/H446E that appear particularly interesting for future characterization by stopped-flow kinetics to analyze in detail the effects of the specific mutations on the individual half-reactions.</p>
----------------------------------------------------------------------
In diva2:1788591 abstract is: <p>γ-Aminobutyric acid type-A receptors (GABAARs) are pentameric ligand-gated chloride channels which exhibit neuro inhibitory effects. Hence, they are the primary drug-targets of multiple anxiolytic and sedative drugs used to inhibit the firing rate of neurons. Despite the importance of these receptors, the open structure of GABAAR has not been resolved, owing to their rapid desensitization kinetics. Diazepam binding inhibitor (DBI) is a neuropeptide previously reported to positively modulate the α5β3 GABAARs. In this study, DBI was recombinantly expressed, and this positive modulation was further investigated and characterized by using two-electrode voltage clamp of Xenopus oocytes. For the purpose of studying DBI modulation, GABA dose-response curve was generated, and its characteristics were assessed. Based on the results, the positive modulation of DBI appears to be concentration dependent. Furthermore, the modulation causes a 2.16-fold increase in GABA-elicited current at its maximum modulatory concentration. Although the current traces present some degree of variability, the results are supported by being consistent with previously reported findings investigating DBI modulation and the dose-response curve for α5β3 GABAARs, respectively. These findings can be used to support future structural studies of GABAARs by utilizing this knowledge of DBI to potentially stabilize the open structure of the receptor, as well as in understanding the mechanism of interaction between DBI and GABAARs.</p>


w='GABAAR' val={'c': 'GABA<sub>A</sub>R', 's': 'diva2:1788591'}

corrected abstract:
<p>γ-Aminobutyric acid type-A receptors (GABA<sub>A</sub>Rs) are pentameric ligand-gated chloride channels which exhibit neuro inhibitory effects. Hence, they are the primary drug-targets of multiple anxiolytic and sedative drugs used to inhibit the firing rate of neurons. Despite the importance of these receptors, the open structure of GABA<sub>A</sub>R has not been resolved, owing to their rapid desensitization kinetics. Diazepam binding inhibitor (DBI) is a neuropeptide previously reported to positively modulate the α5β3 GABA<sub>A</sub>Rs. In this study, DBI was recombinantly expressed, and this positive modulation was further investigated and characterized by using two-electrode voltage clamp of <em>Xenopus oocytes</em>. For the purpose of studying DBI modulation, GABA dose-response curve was generated, and its characteristics were assessed. Based on the results, the positive modulation of DBI appears to be concentration dependent. Furthermore, the modulation causes a 2.16-fold increase in GABA-elicited current at its maximum modulatory concentration. Although the current traces present some degree of variability, the results are supported by being consistent with previously reported findings investigating DBI modulation and the dose-response curve for α5β3 GABA<sub>A</sub>Rs, respectively. These findings can be used to support future structural studies of GABA<sub>A</sub>Rs by utilizing this knowledge of DBI to potentially stabilize the open structure of the receptor, as well as in understanding the mechanism of interaction between DBI and GABA<sub>A</sub>Rs.</p>
----------------------------------------------------------------------
In diva2:1642802 abstract is: <p>A new kind of carbohydrate-binding protein, found in a soil-derived Bacteroidetes species, <em>Chitinophaga pinensis</em>, displays a high binding specificity for branched polysaccharides.</p><p>The biological function of the protein, provisionally named “F-protein”, is still unknown; however, two examples from this new protein family have already been partly characterised. The protein family seems to have the ability to specifically bind branched polysaccharide chains, and each protein can bind multiple polysaccharide chains as it has multiple binding sites. This enables cross-linking of polysaccharides to form hydrogel materials. In this project we have investigated the carbohydrate binding of two F-domains from the Cpin 2580 gene that also includes a glycoside hydrolase family 18 (GH18) chitinase domain and found them to primarily bind to 1-6 linked β-glucans. We have found the GH18 domain’s chitinase activity to be negatively impacted in its naturally occurring F-domain “sandwich” configuration, hinting at a biological non-catalytic role for this domain. We have discovered unexpected chitin binding for the full complex of two Fdomains together with GH18. We have successfully formed hydrogels from scleroglucan at low concentration, using four recombinant proteins containing either of the F-domains, and shown that gels form both with and without an appended GH18 domain. We have investigated the evolutionary context of the F-domain through PSI-BLAST of the full Cpin 2580 protein and the Fdomain sequence alone. We have found the F-domain to occur predominantly appended to a glycoside hydrolase catalytic subunit with and without a secretion domain targeting the protein to the type IX secretion system (T9SS). We have also found it to occur with an alginate lyase subunit, in this case without a T9SS tail. We have theorised a biological role for the Cpin 2580 gene as an adhesion lectin targeting potentially a fungal cell wall, and a biological micro environment dominated by decomposing microbial biomass as carbohydrate source. The sequence analyses presented here will hopefully guiding future efforts to explore for additional F-domain containing genes in nature.</p>


corrected abstract:
<p>A new kind of carbohydrate-binding protein, found in a soil-derived Bacteroidetes species, <em>Chitinophaga pinensis</em>, displays a high binding specificity for branched polysaccharides.</p><p>The biological function of the protein, provisionally named “F-protein”, is still unknown; however, two examples from this new protein family have already been partly characterised. The protein family seems to have the ability to specifically bind branched polysaccharide chains, and each protein can bind multiple polysaccharide chains as it has multiple binding sites. This enables cross-linking of polysaccharides to form hydrogel materials. In this project we have investigated the carbohydrate binding of two F-domains from the Cpin 2580 gene that also includes a glycoside hydrolase family 18 (GH18) chitinase domain and found them to primarily bind to 1-6 linked β-glucans. We have found the GH18 domain’s chitinase activity to be negatively impacted in its naturally occurring F-domain “sandwich” configuration, hinting at a biological non-catalytic role for this domain. We have discovered unexpected chitin binding for the full complex of two F-domains together with GH18. We have successfully formed hydrogels from scleroglucan at low concentration, using four recombinant proteins containing either of the F-domains, and shown that gels form both with and without an appended GH18 domain. We have investigated the evolutionary context of the F-domain through PSI-BLAST of the full Cpin 2580 protein and the F-domain sequence alone. We have found the F-domain to occur predominantly appended to a glycoside hydrolase catalytic subunit with and without a secretion domain targeting the protein to the type IX secretion system (T9SS). We have also found it to occur with an alginate lyase subunit, in this case without a T9SS tail. We have theorised a biological role for the Cpin 2580 gene as an adhesion lectin targeting potentially a fungal cell wall, and a biological micro environment dominated by decomposing microbial biomass as carbohydrate source. The sequence analyses presented here will hopefully guiding future efforts to explore for additional F-domain containing genes in nature.</p>
----------------------------------------------------------------------
In diva2:1474350 abstract is: <p>Skin is an organ with a complex structure which plays a crucial role in thebody’s defence against external threats and in maintaining major homeostatic functions. The need for <em>in vitro</em> models that mimic the <em>in vivo</em> milieu is therefore high and relevant with various applications including, among others, penetration, absorption, and toxicity studies. In this context, the choice of the biomaterial that will provide a 3D scaffold to the cultured cells is defining the model’s success. The FN-4RepCT silk is here suggested as a potent biomaterial for skin tissue engineering applications.</p><p>This recombinantly produced spider silk protein (FN-4RepCT), which can self-assemble into fibrils, creates a robust and elastic matrice with high bioactivity, due to its functionalization with the fibronectin derived RGD-containing peptide. Hence it overcomes the drawbacks of other available biomaterials either synthetic or based on animal derived proteins. Additionally, the FN-4RepCT silk protein can be cast in various 3D formats, two of which are utilized within this project.</p><p>We herein present a bilayered skin tissue equivalent supported by the FN-4RepCT silk. This is constructed by the combination of a foam format, integrated with dermal fibroblasts and endothelial cells, and a membrane format supporting epidermal keratinocytes. As a result, a vascularized dermal layer that contains ECM components (Collagen I, Collagen III, and Elastin) is constructed and attached to an epidermal layer of differentiated keratinocytes.The protocol presented in this project offers a successful method of evenly integrating cells in the FN-4RepCT silk scaffold, while preserving their ability to resume some of their major <em>in vivo</em> functions like proliferation, ECM secretion, construction of vascular networks, and differentiation. The obtained results were evaluated with immunofluorescence stainings of various markers of interest and further analysed, when necessary, with image processing tools. The results that ensued from the herein presented protocol strongly suggest that the FN-4RepCT silk is a promising biomaterial for skin tissue engineering applications.</p>

corrected abstract:
<p>Skin is an organ with a complex structure which plays a crucial role in the body’s defence against external threats and in maintaining major homeostatic functions. The need for <em>in vitro</em> models that mimic the <em>in vivo</em> milieu is therefore high and relevant with various applications including, among others, penetration, absorption, and toxicity studies. In this context, the choice of the biomaterial that will provide a 3D scaffold to the cultured cells is defining the model’s success. The FN-4RepCT silk is here suggested as a potent biomaterial for skin tissue engineering applications.</p><p>This recombinantly produced spider silk protein (FN-4RepCT), which can self-assemble into fibrils, creates a robust and elastic matrice with high bioactivity, due to its functionalization with the fibronectin derived RGD-containing peptide. Hence it overcomes the drawbacks of other available biomaterials either synthetic or based on animal derived proteins. Additionally, the FN-4RepCT silk protein can be cast in various 3D formats, two of which are utilized within this project.</p><p>We herein present a bilayered skin tissue equivalent supported by the FN-4RepCT silk. This is constructed by the combination of a foam format, integrated with dermal fibroblasts and endothelial cells, and a membrane format supporting epidermal keratinocytes. As a result, a vascularized dermal layer that contains ECM components (Collagen I, Collagen III, and Elastin) is constructed and attached to an epidermal layer of differentiated keratinocytes. The protocol presented in this project offers a successful method of evenly integrating cells in the FN-4RepCT silk scaffold, while preserving their ability to resume some of their major <em>in vivo</em> functions like proliferation, ECM secretion, construction of vascular networks, and differentiation. The obtained results were evaluated with immunofluorescence stainings of various markers of interest and further analysed, when necessary, with image processing tools. The results that ensued from the herein presented protocol strongly suggest that the FN-4RepCT silk is a promising biomaterial for skin tissue engineering applications.</p>
----------------------------------------------------------------------
In diva2:1886091 abstract is: <p>Cardiovascular disease is the most common cause of death globally. By measuring biomarkers associated with increased risk for these diseases, (such as Apolipoprotein A1, Apolipoprotein B and Lipoprotein B), many lives could be saved through lifestyle interventions or medication. Screening a large population does however require robust analytical methods and an efficient way to acquire and handle a large amount of samples. Quantitative dried blood spots is a minimally invasive sample type that can be taken at home without the need of a medical professional.They can be sent via conventional post-service which facilitates transport logistics. Unlike a medical transport, the post-service alternative does not offer a temperatureand humidity controlled environment during shipping, which can perturb the protein stability. This project investigated the impact of extreme weather conditions on dried blood spots during shipping, using state-of-the-art mass spectrometry-based targeted proteomics. Absolute quantification of proteins was accomplished by integrating recombinantly produced protein standards; SIS-PrESTs. The study investigated several parameters of potential importance for the stability of measurable protein content during shipping. This included shipping temperature &amp; humidity and packaging. SIS-PrESTs were added at different steps of the workflow in an attempt to track the protein degradation caused by the shipping simulation conditions. In conclusion, the measurable protein content decreased in dried blood spots treated in high temperature &amp; humidity, despite storing samples in zip-lock bags with desiccants. Lastly, protein was detected and quantified samples that were pre-spiked with internal standards, but were measured with less precision compared with samples spiked later in the workflow.</p>


Note that the thesis has "for for" in it.

corrected abstract:
<p>Cardiovascular disease is the most common cause of death globally. By measuring biomarkers associated with increased risk for for these diseases, (such as Apolipoprotein A1, Apolipoprotein B and Lipoprotein B), many lives could be saved through lifestyle interventions or medication. Screening a large population does however require robust analytical methods and an efficient way to acquire and handle a large amount of samples. Quantitative dried blood spots is a minimally invasive sample type that can be taken at home without the need of a medical professional. They can be sent via conventional post-service which facilitates transport logistics. Unlike a medical transport, the post-service alternative does not offer a temperature- and humidity controlled environment during shipping, which can perturb the protein stability. This project investigated the impact of extreme weather conditions on dried blood spots during shipping, using state-of-the-art mass spectrometry-based targeted proteomics. Absolute quantification of proteins was accomplished by integrating recombinantly produced protein standards; SIS-PrESTs.</p><p>The study investigated several parameters of potential importance for the stability of measurable protein content during shipping. This included shipping temperature &amp; humidity and packaging. SIS-PrESTs were added at different steps of the workflow in an attempt to track the protein degradation caused by the shipping simulation conditions.</p><p>In conclusion, the measurable protein content decreased in dried blood spots treated in high temperature &amp; humidity, despite storing samples in zip-lock bags with desiccants. Lastly, protein was detected and quantified samples that were pre-spiked with internal standards, but were measured with less precision compared with samples spiked later in the workflow.</p>
----------------------------------------------------------------------

----------------------------------------------------------------------
In diva2:1690186 abstract is: <p>Disposal of spent nuclear fuels is of great importance to prevent the environment and humans from being affected by long-lived radionuclides for 100,000 years or more. Even though the deep geological repositories are designed to remain durable for many years, spent nuclear fuel may come in contact with groundwater in case of a multi-barrier failure. The inherent radioactivity of spent nuclear fuel causes water radiolysis producing oxidizing and reducing agents. Among the radiolysis products, hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) is reported as a primary contributor to the oxidative dissolution of the fuel matrix, UO<sub>2</sub>. Although UO<sub>2</sub> has low solubility in water, oxidized UO<sub>2</sub>, UO<sub>2</sub><sup>2+</sup> , has several orders of magnitude higher solubility. This poses the risk of the radionuclides being released into the environment. Bicarbonate (HCO<sub>3</sub><sup>–</sup>) is one of the main components of groundwater and is known to increase the dissolution of UO<sub>2</sub><sup>2+</sup>. Therefore, in this study, the effects of HCO<sub>3</sub><sup>–</sup> concentration on the oxidative dissolution of UO<sub>2</sub> were investigated by keeping the initial amount of H<sub>2</sub>O<sub>2</sub> constant at 0.2 mM and changing the HCO<sub>3</sub><sup>–</sup> concentration (1 mM, 2 mM, 5mM, and 10 mM). Besides, the effect of UO<sub>2</sub><sup>2+</sup> on the speciation was investigated by adding uranyl nitrate (UO<sub>2</sub>(NO<sub>3</sub>)<sub>2</sub> x 6H<sub>2</sub>O) to the systems before exposure to H<sub>2</sub>O<sub>2</sub>. The impact of speciation on the kinetics of oxidative dissolution of UO<sub>2</sub> was analyzed. As a result of experiments, it has been concluded that the amount of dissolved UO<sub>2</sub><sup>2+</sup> is higher in higher HCO<sub>3</sub><sup>–</sup> concentration. Also, the rate of the UO<sub>2</sub><sup>2+</sup> dissolution decreases with addition of UO<sub>2</sub><sup>2+</sup> due to the complexes formed in the systems. It was observed that oxidation of UO<sub>2</sub> is the rate limiting reaction atthe beginning of the exposure; therefore, there is a delay in the UO<sub>2</sub><sup>2+</sup> dissolution. On the other hand, it has been seen that the HCO<sub>3</sub><sup>–</sup> deficiency limits the dissolution capacity of the systems. Free H<sub>2</sub>O<sub>2</sub> is the dominant peroxide species in the systems without initially added UO<sub>2</sub><sup>2+</sup> , while -6 and -2 charged complexes are dominant in the systems with initially added UO<sub>2</sub><sup>2+</sup>. The H<sub>2</sub>O<sub>2</sub> complexes are found more effective on the surface mechanism in the systems having lower HCO<sub>3</sub><sup>–</sup> concentration. There is no observable trend in H<sub>2</sub>O<sub>2</sub> consumption rate with respect to HCO<sub>3</sub><sup>–</sup> concentration. Therefore, it was concluded that the H<sub>2</sub>O<sub>2</sub> consumption rate is independent of dissolution reaction. Finally, the dissolution in the system without initially added UO<sub>2</sub><sup>2+</sup> follows the first-order kinetics with respect to HCO<sub>3</sub><sup>–</sup> concentration.</p>

corrected abstract:
<p>Disposal of spent nuclear fuels is of great importance to prevent the environment and humans from being affected by long-lived radionuclides for 100,000 years or more. Even though the deep geological repositories are designed to remain durable for many years, spent nuclear fuel may come in contact with groundwater in case of a multi-barrier failure. The inherent radioactivity of spent nuclear fuel causes water radiolysis producing oxidizing and reducing agents. Among the radiolysis products, hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) is reported as a primary contributor to the oxidative dissolution of the fuel matrix, UO<sub>2</sub>. Although UO<sub>2</sub> has low solubility in water, oxidized UO<sub>2</sub>, UO<sub>2</sub><sup>2+</sup>, has several orders of magnitude higher solubility. This poses the risk of the radionuclides being released into the environment. Bicarbonate (HCO<sub>3</sub><sup>–</sup>) is one of the main components of groundwater and is known to increase the dissolution of UO<sub>2</sub><sup>2+</sup>. Therefore, in this study, the effects of HCO<sub>3</sub><sup>–</sup> concentration on the oxidative dissolution of UO<sub>2</sub> were investigated by keeping the initial amount of H<sub>2</sub>O<sub>2</sub> constant at 0.2 mM and changing the HCO<sub>3</sub><sup>–</sup> concentration (1 mM, 2 mM, 5 mM, and 10 mM). Besides, the effect of UO<sub>2</sub><sup>2+</sup> on the speciation was investigated by adding uranyl nitrate (UO<sub>2</sub>(NO<sub>3</sub>)<sub>2</sub> x 6H<sub>2</sub>O) to the systems before exposure to H<sub>2</sub>O<sub>2</sub>. The impact of speciation on the kinetics of oxidative dissolution of UO<sub>2</sub> was analyzed. As a result of experiments, it has been concluded that the amount of dissolved UO<sub>2</sub><sup>2+</sup> is higher in higher HCO<sub>3</sub><sup>–</sup> concentration. Also, the rate of the UO<sub>2</sub><sup>2+</sup> dissolution decreases with addition of UO<sub>2</sub><sup>2+</sup> due to the complexes formed in the systems. It was observed that oxidation of UO<sub>2</sub> is the rate limiting reaction at the beginning of the exposure; therefore there is a delay in the UO<sub>2</sub><sup>2+</sup> dissolution. On the other hand, it has been seen that the HCO<sub>3</sub><sup>–</sup> deficiency limits the dissolution capacity of the systems. Free H<sub>2</sub>O<sub>2</sub> is the dominant peroxide species in the systems without initially added UO<sub>2</sub><sup>2+</sup>, while -6 and -2 charged complexes are dominant in the systems with initially added UO<sub>2</sub><sup>2+</sup>. The H<sub>2</sub>O<sub>2</sub> complexes are found more effective on the surface mechanism in the systems having lower HCO<sub>3</sub><sup>–</sup> concentration. There is no observable trend in H<sub>2</sub>O<sub>2</sub> consumption rate with respect to HCO<sub>3</sub><sup>–</sup> Concentration. Therefore, it was concluded that the H<sub>2</sub>O<sub>2</sub> consumption rate is independent of dissolution reaction. Finally, the dissolution in the system without initially added UO<sub>2</sub><sup>2+</sup> follows the first-order kinetics with respect to HCO<sub>3</sub><sup>–</sup> concentration.</p>
----------------------------------------------------------------------
In diva2:1873298 abstract is: <p>Currently, most pre-clinical studies are being performed in vivo using animal models, or <em>in vitro</em> in 2D cell cultures. Both are problematic as they do not accurately mimic the environment found<em> in vivo</em> in humans, leading to results that cannot be accurately extrapolated to clinical trials. 3D cell cultures allow for a more <em>in vivo</em>-like model that enables more accurate predictions of drug-dose response rates, pharmacodynamics, and pharmacokinetics. Specifically, hydrogels are often used as scaffolds for 3D culture due to their similarity to the human body’s natural extracellular matrix (ECM). Previous studies have shown that the recombinant spider silk protein, 4RepCT, can self-assemble into different 3D formats resembling the native ECM. It has since been functionalised with the cell-binding motif RGD from fibronectin, giving rise to FN-silk, which facilitates cell adhesion and proliferation. </p><p>The goal of this study is to examine if FN-silk can be used to improve the cultivation of epithelial and endothelial cells on hydrogels. Three different epithelial cell lines (HaCaT, HEKa, and Caco-2) and one endothelial cell line (HDMEC) were cultivated on both collagen type I and fibrin gels containing FN-silk. When cultured on collagen gels, the endothelial and skin epithelial cell lines showed clear improvement when the FN-silk was present. It was also possible to culture endothelial cells on collagen gels coated with the FN silk post-gelation and co-culture them together with vascular smooth muscle cells on collagen gels with FN-silk support. FN-silk was also shown to have a positive impact on collagen gels mechanical properties, preventing gel contraction otherwise caused by the SMCs. The findings of this study, point towards FN-silk improving hydrogel 3D cultivation of epithelial and endothelial cells by improving adhesion, proliferation, and by mimicking the basement membrane found in vivo, with potential applications for pre-clinical testing, 3D blood vessel modelling, and personalised medicine.</p>


I assumed the usual italics for Latin words.
corrected abstract:
<p>Currently, most pre-clinical studies are being performed <em>in vivo</em> using animal models, or <em>in vitro</em> in 2D cell cultures. Both are problematic as they do not accurately mimic the environment found <em>in vivo</em> in humans, leading to results that cannot be accurately extrapolated to clinical trials. 3D cell cultures allow for a more <em>in vivo</em>-like model that enables more accurate predictions of drug-dose response rates, pharmacodynamics, and pharmacokinetics. Specifically, hydrogels are often used as scaffolds for 3D culture due to their similarity to the human body’s natural extracellular matrix (ECM). Previous studies have shown that the recombinant spider silk protein, 4RepCT, can self-assemble into different 3D formats resembling the native ECM. It has since been functionalised with the cell-binding motif RGD from fibronectin, giving rise to FN-silk, which facilitates cell adhesion and proliferation.</p><p>The goal of this study is to examine if FN-silk can be used to improve the cultivation of epithelial and endothelial cells on hydrogels. Three different epithelial cell lines (HaCaT, HEKa, and Caco-2) and one endothelial cell line (HDMEC) were cultivated on both collagen type I and fibrin gels containing FN-silk. When cultured on collagen gels, the endothelial and skin epithelial cell lines showed clear improvement when the FN-silk was present. It was also possible to culture endothelial cells on collagen gels coated with the FN silk post-gelation and co-culture them together with vascular smooth muscle cells on collagen gels with FN-silk support. FN-silk was also shown to have a positive impact on collagen gels mechanical properties, preventing gel contraction otherwise caused by the SMCs. The findings of this study, point towards FN-silk improving hydrogel 3D cultivation of epithelial and endothelial cells by improving adhesion, proliferation, and by mimicking the basement membrane found <em>in vivo</em>, with potential applications for pre-clinical testing, 3D blood vessel modelling, and personalised medicine.</p>
----------------------------------------------------------------------
In diva2:1788568 abstract is: <p>Colorectal cancer (CRC) is the third most common type of cancer and the second leading cause of cancer-related mortality in the world. Estrogen has been found to have a protective role in the development of colorectal cancer and estrogen receptor beta is the predominant estrogen receptor in normal colonic epithelium. Immune cells influence tumor progression and research on the crosstalk between cancer cells and immune cells could be important in future therapies. This study aims to investigate how colorectal cancer cells influence macrophages and vice versa by conducting co-culture experiments and analyzing gene expression using RT-qPCR. Tumor-associated macrophages (TAMs) were polarized from THP-1 cells and cultured together with SW480 colorectal cancer cells with or without the expression of ERβ. Immunofluorescence analysis was performed on colon tissue samples from a colitis-induced mice model to investigate the percentage of different immune cells in the colon. The analysis was done in QuPath and the calculations between an inexperienced user and an experienced user were compared to investigate how the results differ. We found that the formation of TAMs using SW480 conditioned media changed gene expression toward a pro-inflammatory phenotype. The co-culture experiments showed conflicting results but suggest the gene expression of TAMs is altered by being cultured with SW480 and that the gene expression of SW480 cells was affected by being cultured with THP-1 cells. Further, the ERβ expression in SW480 cells affected the gene expression of the cells during co-culture with macrophage-like or TAM THP-1 cells. In the immunofluorescence analysis of mouse colon, the immune cell type with the highest abundance was dendritic cells and the lowest seem to be cytotoxic T-cells, which was around half of the number of T-helper cells. There was a significant difference between the analysis of experienced and inexperienced annotators for two out of ten markers. The conclusions from this study were that SW480 cells have an impact on the gene expression of TAMs and that the gene expression in SW480 was influenced by being in co-culture with THP-1 cells polarized into macrophage-like cells (by PMA) or TAMs (by conditioned media from SW480 cells). Further, ERβ impacted the expression of ICAM1 and IL-1β in SW480 cells during co-culture with macrophage-like or TAM THP-1 cells. By further studying the correlation between macrophages and CRC cells, the research can be broadened which can lead to new approaches to CRC therapies in the future.</p>


corrected abstract:
<p>Colorectal cancer (CRC) is the third most common type of cancer and the second leading cause of cancer-related mortality in the world. Estrogen has been found to have a protective role in the development of colorectal cancer and estrogen receptor beta is the predominant estrogen receptor in normal colonic epithelium. Immune cells influence tumor progression and research on the crosstalk between cancer cells and immune cells could be important in future therapies. This study aims to investigate how colorectal cancer cells influence macrophages and vice versa by conducting co-culture experiments and analyzing gene expression using RT-qPCR. Tumor-associated macrophages (TAMs) were polarized from THP-1 cells and cultured together with SW480 colorectal cancer cells with or without the expression of ERβ. Immunofluorescence analysis was performed on colon tissue samples from a colitis-induced mice model to investigate the percentage of different immune cells in the colon. The analysis was done in QuPath and the calculations between an inexperienced user and an experienced user were compared to investigate how the results differ. We found that the formation of TAMs using SW480 conditioned media changed gene expression toward a pro-inflammatory phenotype. The co-culture experiments showed conflicting results but suggest the gene expression of TAMs is altered by being cultured with SW480 and that the gene expression of SW480 cells was affected by being cultured with THP-1 cells. Further, the ERβ expression in SW480 cells affected the gene expression of the cells during co-culture with macrophage-like or TAM THP-1 cells. In the immunofluorescence analysis of mouse colon, the immune cell type with the highest abundance was dendritic cells and the lowest seem to be cytotoxic T-cells, which was around half of the number of T-helper cells. There was a significant difference between the analysis of experienced and inexperienced annotators for two out of ten markers. The conclusions from this study were that SW480 cells have an impact on the gene expression of TAMs and that the gene expression in SW480 was influenced by being in co-culture with THP-1 cells polarized into macrophage-like cells (by PMA) or TAMs (by conditioned media from SW480 cells). Further, ERβ impacted the expression of <em>ICAM1</em> and <em>IL-1β</em> in SW480 cells during co-culture with macrophage-like or TAM THP-1 cells. By further studying the correlation between macrophages and CRC cells, the research can be broadened which can lead to new approaches to CRC therapies in the future.</p>
----------------------------------------------------------------------
In diva2:1677479 abstract is: <p>Chemical looping combustion (CLC) involves an inherent separation of carbon dioxide (CO<sub>2</sub>), since oxygen (O<sub>2</sub>) is transferred to the fuel via an oxygen carrier, circulating between the air and fuel reactor. With O<sub>2</sub> being removed from nitrogen (N<sub>2</sub>) in the air reactor, a separate stream containing mostly CO<sub>2</sub> and water (H<sub>2</sub>O) is produced in the fuel reactor, eliminating the need of expensive and energy-demanding gas separation technologies. The use of biomass as fuel in CLC may result in negative CO<sub>2</sub> emissions if CO<sub>2</sub> is captured and stored. The CO<sub>2</sub> product gas must comply to certain purity levels depending on ways of CO<sub>2</sub> transportation and where it will be stored. Besides H<sub>2</sub>O and CO<sub>2</sub>, the generated flue gas stream in CLC will also contain trace amounts of nitrogen oxides (NO<sub>x</sub>), sulfur oxides (SO<sub>x</sub>) and other contaminants, thus requiring a deep removal to ppm levels to comply with the stringent CO<sub>2</sub> purity criteria for storage in saline aquifers in this work. Due to an incomplete combustion of fuel gases in CLC, an oxy-polishing step is required for a full conversion to gas products CO<sub>2</sub> and H<sub>2</sub>O. Therefore, pure O<sub>2</sub> is required for the oxy-polishing step. Some residual O2 will also be expected in the flue gas stream and needs to be reduced to ppm levels. The downstream treatment in CLC involves the best available gas processing technologies practiced commercially today, such as electrostatic precipitators (ESPs), wet flue gas desulfurization (WFGD), selective catalytic reduction (SCR) and selective non-catalytic reduction (SNCR). Two CO<sub>2</sub> processing systems are discussed in this work; the precooled Linde Hampson unit and the Distillation Separation unit. For each CO<sub>2</sub> processing unit (CPU), a flue gas treatment is proposed. Amongst the two proposed scenarios, scenario 2, could with highest certainty, produce a liquid CO<sub>2</sub> stream with a purity of 99.998%, complying to the CO<sub>2</sub> criteria set by the Northern Lights Project in Norway. At the time of writing this thesis, no other literature has been published assessing flue gas treatment and CPU alternatives in in bio-CLC.</p>

corrected abstract:
<p>Chemical looping combustion (CLC) involves an inherent separation of carbon dioxide (CO<sub>2</sub>), since oxygen (O<sub>2</sub>) is transferred to the fuel via an oxygen carrier, circulating between the air and fuel reactor. With O<sub>2</sub> being removed from nitrogen (N<sub>2</sub>) in the air reactor, a separate stream containing mostly CO<sub>2</sub> and water (H<sub>2</sub>O) is produced in the fuel reactor, eliminating the need of expensive and energy-demanding gas separation technologies. The use of biomass as fuel in CLC may result in negative CO<sub>2</sub> emissions if CO<sub>2</sub> is captured and stored. The CO<sub>2</sub> product gas must comply to certain purity levels depending on ways of CO<sub>2</sub> transportation and where it will be stored. Besides H<sub>2</sub>O and CO<sub>2</sub>, the generated flue gas stream in CLC will also contain trace amounts of nitrogen oxides (NO<sub>x</sub>), sulfur oxides (SO<sub>x</sub>) and other contaminants, thus requiring a deep removal to ppm levels to comply with the stringent CO<sub>2</sub> purity criteria for storage in saline aquifers in this work. Due to an incomplete combustion of fuel gases in CLC, an oxy-polishing step is required for a full conversion to gas products CO<sub>2</sub> and H<sub>2</sub>O. Therefore, pure O<sub>2</sub> is required for the oxy-polishing step. Some residual O<sub>2</sub> will also be expected in the flue gas stream and needs to be reduced to ppm levels. The downstream treatment in CLC involves the best available gas processing technologies practiced commercially today, such as electrostatic precipitators (ESPs), wet flue gas desulfurization (WFGD), selective catalytic reduction (SCR) and selective non-catalytic reduction (SNCR). Two CO<sub>2</sub> processing systems are discussed in this work; the precooled Linde Hampson unit and the Distillation Separation unit. For each CO<sub>2</sub> processing unit (CPU), a flue gas treatment is proposed. Amongst the two proposed scenarios, scenario 2, could with highest certainty, produce a liquid CO<sub>2</sub> stream with a purity of 99.998%, complying to the CO<sub>2</sub> criteria set by the Northern Lights Project in Norway. At the time of writing this thesis, no other literature has been published assessing flue gas treatment and CPU alternatives in in bio-CLC.</p>
----------------------------------------------------------------------
In diva2:1770897 abstract is: <p>The use of biofuels has increased to reduce the emissions from fossil fuels. However, the use of biofuels results in deposit formation inside the injector, which leads to issues with the drivability of the vehicle. The most believed hypothesis behind the deposit formation is that engine oil contamination in the fuel system leads to formation of calcium sulphate crystals that soft particles adhere to. Moreover, temperature is claimed to have a significant effect on the deposit formation. To ease the shift towards more renewable fuels, more understanding behind the deposit mechanism is needed. Therefore, this study aimed to investigate the mechanism behind internal injector deposits with a newly designed lab scale rig to get a deeper understanding of the mechanism, and especially the role of calcium sulphate and temperature. The aim was achieved by dividing the experimental work into two parts:a) Running the test-rig with test fuels followed by analyses of the samples with mainly SEM-EDX and FTIR, to understand the role of temperature, and engine oil for the deposit mechanism. b) Solubility studies of calcium sulphate to understand the role of calcium sulphate for the deposit mechanism. The results showed that the newly designed test rig worked well, where the results were consistent with literatureand offered repeatability. Thereby, the rig is recommended for future studies of internal injector deposits. A deeper understanding of the mechanism behind the internal injector deposits was successfully achieved. For the first time, a FTIR and SEM-EDX study was performed over a temperature gradient, which made it possible to see the onset temperature for different reactions and to identify deposits in different temperature ranges. It was showed that the deposit formation was temperature dependent with different types of deposits in different temperature ranges. At temperatures below 100°C, zinc sulphate and unreacted metal carboxylates were the dominant species. Above 100°C, the metal carboxylates changed form and became more concentrated while the concentration of zinc sulphate decreased. Moreover, calcium sulphate showed to be formed to a large extent when the temperature exceeded 100°C. The temperature study and results are unique and unknown in the literature. Ostwald ripening has been proposed as a possible mechanism in the literature but turned out to be unlikely. The dissolution of calcium sulphate by engine oil additives followed by precipitation in the fuel showed to be a more likely mechanism. It was also shown that soap deposits promoted the deposit formation of calcium sulphate crystals, which is in line with the field observations since they normally coexist.</p>


corrected abstract:
<p>The use of biofuels has increased to reduce the emissions from fossil fuels. However, the use of biofuels results in deposit formation inside the injector, which leads to issues with the drivability of the vehicle. The most believed hypothesis behind the deposit formation is that engine oil contamination in the fuel system leads to formation of calcium sulphate crystals that soft particles adhere to. Moreover, temperature is claimed to have a significant effect on the deposit formation. To ease the shift towards more renewable fuels, more understanding behind the deposit mechanism is needed. Therefore, this study aimed to investigate the mechanism behind internal injector deposits with a newly designed lab scale rig to get a deeper understanding of the mechanism, and especially the role of calcium sulphate and temperature.</p><p>The aim was achieved by dividing the experimental work into two parts:<ol type="a"><li>Running the test-rig with test fuels followed by analyses of the samples with mainly SEM-EDX and FTIR, to understand the role of temperature, and engine oil for the deposit mechanism.</li><li>Solubility studies of calcium sulphate to understand the role of calcium sulphate for the deposit mechanism.</li></ol></p><p>The results showed that the newly designed test rig worked well, where the results were consistent with literature and offered repeatability. Thereby, the rig is recommended for future studies of internal injector deposits. A deeper understanding of the mechanism behind the internal injector deposits was successfully achieved. For the first time, a FTIR and SEM-EDX study was performed over a temperature gradient, which made it possible to see the onset temperature for different reactions and to identify deposits in different temperature ranges. It was showed that the deposit formation was temperature dependent with different types of deposits in different temperature ranges. At temperatures below 100°C, zinc sulphate and unreacted metal carboxylates were the dominant species. Above 100°C, the metal carboxylates changed form and became more concentrated while the concentration of zinc sulphate decreased. Moreover, calcium sulphate showed to be formed to a large extent when the temperature exceeded 100°C. The temperature study and results are unique and unknown in the literature. Ostwald ripening has been proposed as a possible mechanism in the literature but turned out to be unlikely. The dissolution of calcium sulphate by engine oil additives followed by precipitation in the fuel showed to be a more likely mechanism. It was also shown that soap deposits promoted the deposit formation of calcium sulphate crystals, which is in line with the field observations since they normally coexist.</p>
----------------------------------------------------------------------
title: "Experimental testing of adsorbents for H2S removal in industrial applications: A comparative study on lifetime and cost effectiveness of different materials"
==>    "Experimental testing of adsorbents for H<sub>2</sub>S removal in industrial applications: A comparative study on lifetime and cost effectiveness of different materials"

In diva2:1357100 abstract is: <p>Harmful emissions are a global issue and cause trouble for human health and for the environment. There is a wide variety of pollutants and one pollutant is hydrogen sulfide, H<sub>2</sub>S, that is a member of the group Volatile Sulfur Compounds. H<sub>2</sub>S is a compound that is known for its smell of rotten eggs and is detectable by the human nose at very low concentrations. At higher concentrations, H<sub>2</sub>S is highly toxic and even deadly for humans. It is also a corrosive gas, and can, therefore, cause problems for materials that are being exposed to it. This can be an issue when H<sub>2</sub>S is present in biogas since it can damage engines or pipes. It can also poison catalysts that are used for methane upgrading.</p><p>There are different methods of removing H<sub>2</sub>S from air and common ones are to use adsorption media or catalytic oxidation for gas-solid reactions. The catalytic oxidation is oxidizing the H<sub>2</sub>S and converts it into elemental sulfur. A problem with these techniques is that they need replacement after some time when they have been saturated.</p><p>The aim and objectives for this project are to find appropriate materials to test in a test rig that was finalized at the beginning of the project, to compare their lifetime. This was done to find the most cost effective material for H<sub>2</sub>S removal. The effect of humidity in the air was also examined.</p><p>Eight different samples were tested. Two of these were activated carbonwithout impregnations and the other six were partial catalytic materials (impregnated carbons or metal oxide based materials). The partial catalytic materials were significantly better than the activated carbons. The lifetimes varied among the partial catalytic materials as well, andare believed to be due to different active compounds on the surfacesand the structure. When running the experiments with 70 % relative humidity, the lifetimes were significantly longer than when the same materials were run for 30 %. A lower concentration of H<sub>2</sub>S in low relative humidity showed lower or the same loading capacity than higher concentrations. Regeneration was tested for one of the metal based materials with a satisfactory result.</p>


corrected abstract:
<p>Harmful emissions are a global issue and cause trouble for human health and for the environment. There is a wide variety of pollutants and one pollutant is hydrogen sulfide, H<sub>2</sub>S, that is a member of the group Volatile Sulfur Compounds. H<sub>2</sub>S is a compound that is known for its smell of rotten eggs and is detectable by the human nose at very low concentrations. At higher concentrations, H<sub>2</sub>S is highly toxic and even deadly for humans. It is also a corrosive gas, and can, therefore, cause problems for materials that are being exposed to it. This can be an issue when H<sub>2</sub>S is present in biogas since it can damage engines or pipes. It can also poison catalysts that are used for methane upgrading.</p><p>There are different methods of removing H<sub>2</sub>S from air and common ones are to use adsorption media or catalytic oxidation for gas-solid reactions. The catalytic oxidation is oxidizing the H<sub>2</sub>S and converts it into elemental sulfur. A problem with these techniques is that they need replacement after some time when they have been saturated.</p><p>The aim and objectives for this project are to find appropriate materials to test in a test rig that was finalized at the beginning of the project, to compare their lifetime. This was done to find the most cost effective material for H<sub>2</sub>S removal. The effect of humidity in the air was also examined.</p><p>Eight different samples were tested. Two of these were activated carbon without impregnations and the other six were partial catalytic materials (impregnated carbons or metal oxide based materials). The partial catalytic materials were significantly better than the activated carbons. The lifetimes varied among the partial catalytic materials as well, and are believed to be due to different active compounds on the surfaces and the structure. When running the experiments with 70 % relative humidity, the lifetimes were significantly longer than when the same materials were run for 30 %. A lower concentration of H<sub>2</sub>S in low relative humidity showed lower or the same loading capacity than higher concentrations. Regeneration was tested for one of the metal based materials with a satisfactory result.</p>
----------------------------------------------------------------------
In diva2:1188778 abstract is: <p>This is a comparative study of open-source IoT middleware platforms with the main focus on scalability and reliability. An initial evaluation of available open-source IoT platforms resulted in Kaa and Node-RED being the focus of this thesis. To further analyse the platforms, they were both subjected to testing with three real-world scenarios. The chosen scenarios were a remote-controlled LED, a chat application and a data transmitting sensor. Prototypes were developed for each scenario using a range of programming languages and devices like Raspberry Pi, Android and ESP8266.According to the tests Node-RED has better performance on a single server. It also scales better with the possibility to communicate with external APIs directly unlike Kaa which would require a gateway. Despite these factors, Kaa proved to have better overall scalability and reliability with its built-insecurity and device discovery, it also supports clustering and should prove better in larger environments.</p>


corrected abstract:
<p>This is a comparative study of open-source IoT middleware platforms with the main focus on scalability and reliability. An initial evaluation of available open-source IoT platforms resulted in Kaa and Node-RED being the focus of this thesis. To further analyse the platforms, they were both subjected to testing with three real-world scenarios. The chosen scenarios were a remote-controlled LED, a chat application and a data transmitting sensor. Prototypes were developed for each scenario using a range of programming languages and devices like Raspberry Pi, Android and ESP8266.</p><p>According to the tests Node-RED has better performance on a single server. It also scales better with the possibility to communicate with external APIs directly unlike Kaa which would require a gateway. Despite these factors, Kaa proved to have better overall scalability and reliability with its built-in security and device discovery, it also supports clustering and should prove better in larger environments.</p>
----------------------------------------------------------------------
In diva2:1686917 abstract is: <p>Breast Cancer is the second most common cancer in Sweden. More treatment options are available with a higher chance of survival if Breast cancer is diagnosed early. WHO has recommended performing Breast Cancer screenings on women even before they develop any symptoms. In Sweden, all women between the age of 40 and 74 years are called for a Breast Cancer Screening examination every two years. However, all of the called out women are not appearing for Breast screening therefore the purpose of this degree project is to analyze the possible benefits of setting up Mobile Mammography Units (MMUs) in Sweden. This thesis focuses on need-analysis, cost-analysis, image quality, and quality of care in MMUs compared to fixed units and if MMUs can be used as a tool for dispersal of knowledge regarding preventive care for cancer.</p><p>Further, long waiting queues is one of the major concern and underperformed area in Sweden. Since the enactment of Cancer Care Pathways in Sweden, long waiting queues for patients having non-cancer diseases have been the most frequently mentioned risk in the regional status reports. This study, therefore, aims to analyze if mammography screening in hospitals is adding up to the long waiting ques for other patients or not.</p><p>This study followed a qualitative setting where several semi-structured interviews were conducted with members of the National workgroup of mammography in Sweden, and an abductive inductive approach was followed for the data collection and analysis.</p><p>The results of the study concluded that Mobile mammography units are only beneficial for areas with demographic issues. Mobile mammography units are resource-demanding therefore it is better to invest in fixed units for areas where there are no traveling issues. The mammography unit in a hospital is a separate department and does not affect any other patient by being in the hospital hence not resulting in the “Crowding out effect”. Regarding the Image quality and quality of care, it is similar in both the fixed and mobile units, apart from the quality of care for disabled women who must visit the fixed unit and hence are deprived of close care.</p><p>A proper cost-analysis, with exact figures, for both mobile and fixed mammography units was not found during this study therefore it could not be concluded if MMUs are cost-effective or not. Also, if Mobile Mammography Units are to be used as a tool for the dispersal of knowledge regarding the prevention of cancer, there will be a need to hire extra staff who can perform this job.</p>

w='ques' val={'c': 'queues', 's': 'diva2:1686917', 'n': 'error in original'}

corrected abstract:
<p>Breast Cancer is the second most common cancer in Sweden. More treatment options are available with a higher chance of survival if Breast cancer is diagnosed early. WHO has recommended performing Breast Cancer screenings on women even before they develop any symptoms. In Sweden, all women between the age of 40 and 74 years are called for a Breast Cancer Screening examination every two years. However, all of the called out women are not appearing for Breast screening therefore the purpose of this degree project is to analyze the possible benefits of setting up Mobile Mammography Units (MMUs) in Sweden. This thesis focuses on need-analysis, cost-analysis, image quality, and quality of care in MMUs compared to fixed units and if MMUs can be used as a tool for dispersal of knowledge regarding preventive care for cancer.</p><p>Further, long waiting queues is one of the major concern and underperformed area in Sweden. Since the enactment of Cancer Care Pathways in Sweden, long waiting queues for patients having non-cancer diseases have been the most frequently mentioned risk in the regional status reports. This study, therefore, aims to analyze if mammography screening in hospitals is adding up to the long waiting ques for other patients or not.</p><p>This study followed a qualitative setting where several semi-structured interviews were conducted with members of the National workgroup of mammography in Sweden, and an abductive inductive approach was followed for the data collection and analysis.</p><p>The results of the study concluded that Mobile mammography units are only beneficial for areas with demographic issues. Mobile mammography units are resource-demanding therefore it is better to invest in fixed units for areas where there are no traveling issues. The mammography unit in a hospital is a separate department and does not affect any other patient by being in the hospital hence not resulting in the “Crowding out effect”. Regarding the Image quality and quality of care, it is similar in both the fixed and mobile units, apart from the quality of care for disabled women who must visit the fixed unit and hence are deprived of close care.</p><p>A proper cost-analysis, with exact figures, for both mobile and fixed mammography units was not found during this study therefore it could not be concluded if MMUs are cost-effective or not. Also, if Mobile Mammography Units are to be used as a tool for the dispersal of knowledge regarding the prevention of cancer, there will be a need to hire extra staff who can perform this job.</p>
----------------------------------------------------------------------
In diva2:1223867 abstract is: <p>Alzheimer’s disease (AD) was discovered 111 years ago by Alois Alzheimer. Today, it is the leading cause of dementia in elderly, and incidence is expected to increase with life expectancy. By 2050, the number of affected individuals is predicted to reach 10 million [1]. There have been numerous attempts to describe AD by its primary hallmarks, including amyloid plaques, amyloid beta (Aβ) oligomers, and tau tangles. However, despite several decades of intense research, the cause of AD remains unknown.Recently, there has been a focus on the inflammatory components of AD. There is an extensive activation of the immune system within the CNS of AD patients, but neither its cause nor its role in AD is known. However, there are strong indications that the inflammation has an autoimmune character. Considering this, there is an imperative need to examine autoimmunity within AD. In the present study, a proteomic approach was used to determine the autoantibody profiles within plasma and cerebrospinal fluid (CSF) within AD patients and healthy controls. Paired plasma and CSF samples from 23 healthy controls and 49 patients were included in the present study. In addition, 2 plasma samples and 18 CSF samples from patients were included (not paired). One 380-plex and one 314-plex targeted suspension bead array (SBA), each consisting of color-coded magnetic microspheres with immobilized antigens, were used to analyze autoantibody profiles in all samples. The resulting data revealed an increased autoantibody response towards anti-gens SLC17A6 (Solute Carrier Family 17 Member 6), MAP1A (Microtubule Associated Protein 1A), and MAP2 (Microtubule Associated Protein 2) in patients compared to healthy controls. However, as these antigens have displayed wide reactivities in previous, unpublished studies, they require further investigation to determine their role in AD.Furthermore, the paired CSF and plasma samples were used to investigate the correlation of autoantibody profiles within patients. The correlation was found to follow a normal distribution, with correlation being higher in antigens displaying stronger autoantibody reactivity. This work represents one of the first large-scale studies on the correlation of autoantibody profiles in plasma and CSF.</p>


w='a˙ected' val={'c': 'affected', 's': 'diva2:1223867'}

corrected abstract:
<p>Alzheimer’s disease (AD) was discovered 111 years ago by Alois Alzheimer. Today, it is the leading cause of dementia in elderly, and incidence is expected to increase with life expectancy. By 2050, the number of affected individuals is predicted to reach 10 million [1]. There have been numerous attempts to describe AD by its primary hallmarks, including amyloid plaques, amyloid beta (Aβ) oligomers, and tau tangles. However, despite several decades of intense research, the cause of AD remains unknown.</p><p>Recently, there has been a focus on the inflammatory components of AD. There is an extensive activation of the immune system within the CNS of AD patients, but neither its cause nor its role in AD is known. However, there are strong indications that the inflammation has an autoimmune character. Considering this, there is an imperative need to examine autoimmunity within AD. In the present study, a proteomic approach was used to determine the autoantibody profiles within plasma and cerebrospinal fluid (CSF) within AD patients and healthy controls.</p><p>Paired plasma and CSF samples from 23 healthy controls and 49 patients were included in the present study. In addition, 2 plasma samples and 18 CSF samples from patients were included (not paired). One 380-plex and one 314-plex targeted suspension bead array (SBA), each consisting of color-coded magnetic microspheres with immobilized antigens, were used to analyze autoantibody profiles in all samples. The resulting data revealed an increased autoantibody response towards antigens SLC17A6 (Solute Carrier Family 17 Member 6), MAP1A (Microtubule Associated Protein 1A), and MAP2 (Microtubule Associated Protein 2) in patients compared to healthy controls. However, as these antigens have displayed wide reactivities in previous, unpublished studies, they require further investigation to determine their role in AD.</p><p>Furthermore, the paired CSF and plasma samples were used to investigate the correlation of autoantibody profiles within patients. The correlation was found to follow a normal distribution, with correlation being higher in antigens displaying stronger autoantibody reactivity. This work represents one of the first large-scale studies on the correlation of autoantibody profiles in plasma and CSF.</p>
----------------------------------------------------------------------
In diva2:1568127 abstract is: <p>Health technology assessment (HTA) is a process that evaluates the value and effects of health technology during its life cycle and provides scientific information to decision makers. However, the chasm between healthcare and innovation has been highlighted in several research areas. HTA as a bridge connecting the research and decision-making demands, achieving more transparent processes and high quality of evidence to enable more effective and safer MedTech innovations to reach and benefit healthcare and patients. This thesis aims to examine the current status of HTA across different countries compared to Sweden to identify the major hurdles and enablers in this process and any opportunities for learning across systems. Germany, the United Kingdom, and Finland are three countries being compared to Sweden in the thesis.</p><p>A literature review combined with interviews were the main methodologies used for this project. Peer-reviewed literature, government documents, and official websites gave an overview of the HTA systems in selected countries and laid a solid foundation for the more in-depth interviews. Eight interviews (nine interviewees) with HTA agencies and companies were performed via Zoom, along with an email communication(one interviewee) with Fimea. The interviews were used as a support tool to gain a better understanding of the whole HTA system. Some personal opinions were also helpful to gain a view of the system from an alternative, more practical perspective.</p><p>The results show the different HTA processes and the evidence generation paths in four countries. Views from MedTech companies are also given. The HTA in Sweden has two main pathways: SBU and TLV. These two agencies have particular traits that work differently with distinct purposes. In terms of the evidence generation and assessment methods, they also share different points of view. It is difficult to simply compare these systems in parallel due to the system's complexity and different healthcare conditions in every country. But all countries, including Sweden, should realize the challenges ofthe existing HTA systems, and try to reduce the evidence generation gap between expectation and reality. This would create more opportunities for small MedTech companies to be involved in the process, and actively participate in international HTA cooperation.</p>


The actual abstract says "learnings".

corrected abstract:
<p>Health technology assessment (HTA) is a process that evaluates the value and effects of health technology during its lifecycle and provides scientific information to decision-makers. However, the chasm between healthcare and innovation has been highlighted in several research areas. HTA as a bridge connecting the research and decision-making demands, achieving more transparent processes and high quality of evidence to enable more effective and safer MedTech innovations to reach and benefit healthcare and patients. This thesis aims to examine the current status of HTA across different countries compared to Sweden to identify the major hurdles and enablers in this process and any opportunities for learnings across systems. Germany, the United Kingdom, and Finland are three countries being compared to Sweden in the thesis.</p><p>A literature review combined with interviews were the main methodologies used for this project. Peer-reviewed literature, government documents, and official websites gave an overview of the HTA systems in selected countries and laid a solid foundation for the more in-depth interviews. Eight interviews (nine interviewees) with HTA agencies and companies were performed via Zoom, along with an email communication (one interviewee) with Fimea. The interviews were used as a support tool to gain a better understanding of the whole HTA system. Some personal opinions were also helpful to gain a view of the system from an alternative, more practical perspective.</p><p>The results show the different HTA processes and the evidence generation paths in four countries. Views from MedTech companies are also given. The HTA in Sweden has two main pathways: SBU and TLV. These two agencies have particular traits that work differently with distinct purposes. In terms of the evidence generation and assessment methods, they also share different points of view. It is difficult to simply compare these systems in parallel due to the system's complexity and different healthcare conditions in every country. But all countries, including Sweden, should realize the challenges of the existing HTA systems, and try to reduce the evidence generation gap between expectation and reality. This would create more opportunities for small MedTech companies to be involved in the process, and actively participate in international HTA cooperation.</p>
----------------------------------------------------------------------
In diva2:845713 abstract is: <p>Epigenetic regulation modifies gene expression during the human life cycle and in response</p><p>To environmental factors, without changing the genetic code. Among epigenetic markers, DNA methylation is readily measurable in larger scale using the Illumina 450k microchip array, with around half a million interrogated sites per sample. Recently an age predictor, named ”theepigenetic clock”, based on CpG sites from this array was published, yielding surprisingly high correlation with age (R=0.95)</p><p>Here, previous Illumina 450k measurements of a longitudinal dataset consisting of Swedishtwins, including 1184 samples from six different measurement waves (time points), in total including 471 individuals, has been pre-processed and used to evaluate the epigenetic clock. Different normalization methods have been tested and compared for the dataset, and a pre-processing pipeline has been developed. Further, cross-sectional and total correlation for the epigenetic clock age predictor have veen determined. Age acceleration, previously associated with different age related phenotypes and defined as chronological age subtracted from presdicted age, has been calculated. Finally, longitudinal age differences for chronological and predicted age have been determined and compared.</p><p>The epigenetic clock has been shown to be strongly associated with age for the dataset (R=0.64), though not as strong as in the initial article. Mean age acceleration has been found to be negative for the whole dataset at 9.3 years (SD 8.7).</p><p>These results could point toward the predictor being badly adjusted for older ages, the dataset consisting of individuals with phenotapes associated with negative age acceleration, lacking sample quality control, or a combination of these. In short, these results further supportthe epigenetic clock a sa relatively reliable predictor of age, but with a lower correlation than previously reported.</p>

w='phenotapes' val={'c': 'phenotypes', 's': 'diva2:845713', 'n': 'no full text'}
w='presdicted' val={'c': 'predicted', 's': 'diva2:845713', 'n': 'no full text'}
w='veen' val={'c': 'been', 's': 'diva2:845713', 'n': 'no full text'}

corrected abstract:
<p>Epigenetic regulation modifies gene expression during the human life cycle and in response</p><p>To environmental factors, without changing the genetic code. Among epigenetic markers, DNA methylation is readily measurable in larger scale using the Illumina 450k microchip array, with around half a million interrogated sites per sample. Recently an age predictor, named ”the epigenetic clock”, based on CpG sites from this array was published, yielding surprisingly high correlation with age (R=0.95)</p><p>Here, previous Illumina 450k measurements of a longitudinal dataset consisting of Swedish twins, including 1184 samples from six different measurement waves (time points), in total including 471 individuals, has been pre-processed and used to evaluate the epigenetic clock. Different normalization methods have been tested and compared for the dataset, and a pre-processing pipeline has been developed. Further, cross-sectional and total correlation for the epigenetic clock age predictor have been determined. Age acceleration, previously associated with different age related phenotypes and defined as chronological age subtracted from predicted age, has been calculated. Finally, longitudinal age differences for chronological and predicted age have been determined and compared.</p><p>The epigenetic clock has been shown to be strongly associated with age for the dataset (R=0.64), though not as strong as in the initial article. Mean age acceleration has been found to be negative for the whole dataset at 9.3 years (SD 8.7).</p><p>These results could point toward the predictor being badly adjusted for older ages, the dataset consisting of individuals with phenotypes associated with negative age acceleration, lacking sample quality control, or a combination of these. In short, these results further support the epigenetic clock as a relatively reliable predictor of age, but with a lower correlation than previously reported.</p>
----------------------------------------------------------------------
In diva2:1876296 abstract is: <p>Although the cerebellum has long been recognized for its crucial role in coordination and motor control, many aspects of its function are yet to be fully understood. Despite significant advances in neuroscience, gaps persist in our knowledge regarding the precise cellular and molecular organisation underlying cerebellar function, particularly relating to cognition andbehaviour. This thesis addresses these gaps through a transcriptomics approach, by integrating the high-resolution spatial transcriptomics method Stereo-seq with single nucleus RNA sequencing (snRNA-seq). In the project, two methods of clustering snRNA-seq data (cell clustering and co-expression gene clustering) were evaluated, and gene clustering of snRNA-seq data was used to make cell-type predictions for the spatial Stereo-seq samples in a novel approach to spatial transcriptomics cell segmentation. This same segmentation was used to identify cell-type specific markers for cerebellar cell-types. Additionally, an approach for identifying dendritic enrichment of mRNA in Purkinje cells as a way to indicate local mRNA translationwas developed and evaluated. The results of this study demonstrated that both methods of snRNA-seq clustering could resolve the cellular composition of the tissue and complemented each other, with markers identified through co-expression clustering even having a somewhat higher agreement with the spatial data. The co-expression cell segmentation method proved to be highly efficient in segmenting certain cell populations, particularly large, sparsely distributed cells, while being less precise in cell segmentation of smaller cell types. For clearly segmented cell populations, this method was also useful in finding specific marker genes. However, my proof of principle approach for dendritic enrichment analysis had limited success, and better statistical tools and benchmarking processes are needed to more precisely evaluate these spatial analysis methods.</p>

corrected abstract:
<p>Although the cerebellum has long been recognized for its crucial role in coordination and motor control, many aspects of its function are yet to be fully understood. Despite significant advances in neuroscience, gaps persist in our knowledge regarding the precise cellular and molecular organisation underlying cerebellar function, particularly relating to cognition and behaviour. This thesis addresses these gaps through a transcriptomics approach, by integrating the high-resolution spatial transcriptomics method Stereo-seq with single nucleus RNA sequencing (snRNA-seq). In the project, two methods of clustering snRNA-seq data (cell clustering and co-expression gene clustering) were evaluated, and gene clustering of snRNA-seq data was used to make cell-type predictions for the spatial Stereo-seq samples in a novel approach to spatial transcriptomics cell segmentation. This same segmentation was used to identify cell-type specific markers for cerebellar cell-types. Additionally, an approach for identifying dendritic enrichment of mRNA in Purkinje cells as a way to indicate local mRNA translation was developed and evaluated. The results of this study demonstrated that both methods of snRNA-seq clustering could resolve the cellular composition of the tissue and complemented each other, with markers identified through co-expression clustering even having a somewhat higher agreement with the spatial data. The co-expression cell segmentation method proved to be highly efficient in segmenting certain cell populations, particularly large, sparsely distributed cells, while being less precise in cell segmentation of smaller cell types. For clearly segmented cell populations, this method was also useful in finding specific marker genes. However, my proof of principle approach for dendritic enrichment analysis had limited success, and better statistical tools and benchmarking processes are needed to more precisely evaluate these spatial analysis methods.</p>
----------------------------------------------------------------------
In diva2:1770784 abstract is: <p>This master thesis focuses on the modernisation of the PowerBox, a performance testing machine intended for Olympic wrestlers to help them enhance their ability to prevent penalty points caused by being pushed out of the wrestling area. As the PowerBox has become outdated and underutilized, this study aims to revive the project by exploring how modern hardware and software technology can be integrated to provide real-time feedback and automate post-analysis processes.</p><p>Recognising that the success of modernisation depends not only on the data delivered but also on the user experience, a user-centred development cycle was designed to accommodate the demanding schedules of elite athletes. Within this cycle, a prototype was developed and tested to assess the hardware's validity, software usability, and overall user experience. Additionally, an automated data analysis system was developed, and a proof-of-concept was demonstrated in this thesis.</p><p>The prototype performed exceptionally well, exhibiting high validity of the hardware components, with force correlations exceeding 0.99 and velocity correlations exceeding 0.9. Usability was also rated positively, achieving a 100% success rate in user challenges and an 88.75 score on the system usability scale. Furthermore, users evaluated the overall user experience as excellent based on the user experience questionnaire.</p><p>Notably, the PowerBox generated significant interest not only among wrestlers but also among other athletes, such as rugby players and handball players. This demonstrates the impact of modernising performance testing machines like the PowerBox.</p>

The actual thesis used a right single quote and not a apostrophe. As part fo the corrections, I have used the right single quote as this is used in the actual abstract.

corrected abstract:
<p>This master thesis focuses on the modernisation of the PowerBox, a performance testing machine intended for Olympic wrestlers to help them enhance their ability to prevent penalty points caused by being pushed out of the wrestling area. As the PowerBox has become outdated and underutilized, this study aims to revive the project by exploring how modern hardware and software technology can be integrated to provide real-time feedback and automate post-analysis processes.</p><p>Recognising that the success of modernisation depends not only on the data delivered but also on the user experience, a user-centred development cycle was designed to accommodate the demanding schedules of elite athletes. Within this cycle, a prototype was developed and tested to assess the hardware’s validity, software usability, and overall user experience. Additionally, an automated data analysis system was developed, and a proof-of-concept was demonstrated in this thesis.</p><p>The prototype performed exceptionally well, exhibiting high validity of the hardware components, with force correlations exceeding 0.99 and velocity correlations exceeding 0.9. Usability was also rated positively, achieving a 100% success rate in user challenges and an 88.75 score on the system usability scale. Furthermore, users evaluated the overall user experience as excellent based on the user experience questionnaire.</p><p>Notably, the PowerBox generated significant interest not only among wrestlers but also among other athletes, such as rugby players and handball players. This demonstrates the impact of modernising performance testing machines like the PowerBox.</p>
----------------------------------------------------------------------
In diva2:1577191 abstract is: <p>Mutagenicity of various compounds is traditionally predicted by conventional <em>in vivo</em> and <em>in vitro</em> methods. However, transitioning to in silico methods would be beneficial both ethically and environmentally. The descriptors that can be used to predict mutagenicity are the lowest unoccupied molecular orbital (LUMO) energy, activation energy, the local minimum / maximum electrostatic potential energy (V<sub>s,min</sub>/V<sub>s,max</sub>), the minimum local electron attachment energy (E<sub>s,min</sub>), etc. The activation energy is a descriptor that can predict mutagenicity accurately provided that the reaction mechanism is known or can be assumed. However, determining the activation energy is computationally costly.</p><p>Mutagen-X (MX) and its analogues (compounds with the same backbone but different functional groups, 29 compounds in total), are compounds of which the mutagenicity had been characterized by traditional means but recently also using an in silico method – molecular modeling. Molecular modeling had been successively employed in the study by Kari Tuppurainen [Source]; the LUMO of MX and its analogues had been computed and, importantly, the obtained values demonstrated a statistically significant correlation with the biological activity determined using Ames test.</p><p>The aim of this thesis was to investigate whether the mutagenicity of MX and its analogues could also be determined by computing E<sub>s,min</sub>, V<sub>s,max</sub> and the activation energy. The studied reaction was a Michael addition reaction between an amine group on the guanine nucleobase and the beta position of MX and its analogues. Therefore, the studied parameters (E<sub>s,min</sub>, V<sub>s,max</sub> and the activation energy) were evaluated at the beta position. The computed E<sub>s,min</sub> values were correlated with the biological activity. Activation energies for MX and its analogues were also computed at the beta position and then correlated with the biological activity and E<sub>s,min</sub> values.  If a highly statistically significant correlation between the activation energy and E<sub>s,min </sub>values at the beta position would have been observed, that would indicate that E<sub>s,min</sub> values  could be used as a substitute for the activation energy. E<sub>s,min</sub> values have comparatively low computational cost. However, no statistically significant correlation between Es,min values and the biological activity was observed. Furthermore, no statistically significant correlation was observed between the activation energy and biological activity and/or E<sub>s,min</sub> values, respectively. Thus, there were several indications that the proposed reaction mechanism was incorrect. After consulting literature, we learned that the one electron reduction mechanism would be a more probable reaction mechanism. This could be an explanation as to why a highly statistically significant correlation could be observed for LUMO vs. the biological activity, whereas no correlation was observed for E<sub>s,min</sub>, V<sub>s,max</sub>, the activation energy versus the biological activity. The absence of a correlation for these parameters is thought to be due to the proposed reaction mechanism being inaccurate. Additional studies would have to be performed to further investigate the predictive abilities for mutagenicity of the studied parameters.</p>

Note that the abstract that is in DiVA has corrected some of the grammar - but this is not what is in the actualö abstract.

corrected abstract:
<p>Mutagenicity of various compounds is traditionally predicted by conventional <em>in vivo</em> and <em>in vitro</em> methods. However, transitioning to <em>in silico</em> methods would be beneficial both ethically and environmentally. The descriptors that can be used to predict mutagenicity are the lowest unoccupied molecular orbital (LUMO) energy, activation energy, the local minimum/maximum electrostatic potential energy (V<sub>s,min</sub>/V<sub>s,max</sub>), the minimum local electron attachment energy (E<sub>s,min</sub>), etc. The activation energy is a descriptor that can predict mutagenicity accurately provided that the reaction mechanism is known or can be assumed. However, determining the activation energy is computationally costly.</p><p>Mutagen-X (MX) and its analogues (compounds with the same backbone but different functional groups, 29 compounds in total), are compounds of which the mutagenicity has been characterized by traditional means but recently also using an <em>in silico</em> method – molecular modeling. Molecular modeling has been successively employed in the study by Kari Tuppurainen [1]; the LUMO of MX and its analogues have been computed and, importantly, the obtained values demonstrated a statistically significant correlation with the biological activity determined using Ames test.</p><p>The aim of this thesis was to investigate whether the mutagenicity of MX and its analogues could also be determined by computing E<sub>s,min</sub>, V<sub>s,max</sub> and the activation energy. The studied reaction was a Michael addition reaction between an amine group on the guanine nucleobase and the beta position of MX and its analogues. Therefore, the studied parameters (E<sub>s,min</sub>, V<sub>s,max</sub> and the activation energy) were evaluated at the beta position. The computed E<sub>s,min</sub> values were correlated with the biological activity. Activation energies for MX and its analogues were also computed at the beta position and then correlated with the biological activity and E<sub>s,min</sub> values. If a highly statistically significant correlation between the activation energy and E<sub>s,min</sub> values at the beta position would have been observed, that would indicate that E<sub>s,min</sub> values could be used as a substitute for the activation energy. E<sub>s,min</sub> values have comparatively low computational cost. However, no statistically significant correlation between E<sub>s,min</sub> values and the biological activity was observed. Furthermore, no statistically significant correlation was observed between the activation energy and biological activity and/or E<sub>s,min</sub> values, respectively. Thus, there were several indications that the proposed reaction mechanism was incorrect. After consulting literature, we learned that the one electron reduction mechanism would be a more probable reaction mechanism. This could be an explanation as to why a highly statistically significant correlation could be observed for LUMO vs. the biological activity, whereas no correlation was observed for E<sub>s,min</sub>, V<sub>s,max</sub>, and the activation energy versus the biological activity. The absence of correlation for these parameters is thought to be due to the proposed reaction mechanism being inaccurate. Additional studies must be performed to further investigate the predictive abilities for mutagenicity of the studied parameters.</p>
----------------------------------------------------------------------
In diva2:1876707 abstract is: <p>The ovary is a female reproductive organ that has an elemental role in reproduction and production of hormones. Granulosa cells of the ovary produce 17β-estradiol (E2) which can bind to the nuclear estrogen receptor beta (ERβ). The presence of ERβ is mostly restricted to granulosa cells, where it has an important role in folliculogenesis and ovulation. Granulosa cell tumors (GCTs) are a rare subtype of ovarian cancer and account for 5 % of cases. The incidence rate of GCTs is estimated to be between 0.47 to 1.6 per 100,000. Most GCTs have high expression of ERβ. This is noteworthy since ERβ has been considered a tumor suppressor and is absent in most tumors. Thus, ERβ has the potential to become a diagnostic marker or act as a therapeutic target which illustrates the importance of more knowledge and information within the field. Similar to ERβ, liver receptor homolog-1 (LRH-1) is a member of the nuclear receptor superfamily. In the ovary, LRH-1 is highly expressed in granulosa cells. There, it has an important role in folliculogenesis, ovulation and steroidogenesis. ERβ and LRH-1 have been shown to share binding sites in ovarian chromatin. One of the main objectives of this project was to investigate if ERβ and LRH-1 are part of the same complex since previous researchshows a potential crosstalk between them in murine in vivo models. This was investigated byperforming co-immunoprecipitation (Co-IP) in in vitro models HEPG2 and COV434. The protocol for co-IP was validated in MCF-7 through shown interaction between ERβ and estrogen receptor alpha (ER𝛼). The results do not propose that ERβ and LRH-1 are part of the same complex when performing the co-IP in HEPG2. However, the results could not be validated since ERβ and ER𝛼 failed to interact in the control experiment, most likely due to unsuccessful transfection of ER𝛼 and LRH-1. The co-IP in COV434 showed evidence of ERβ and LRH-1 being in the same complex. However, the results could not be validated since the control experiment was not performed according to plan. The experiment should be performed again with the right positive and negative controls similar to the validated experiment in MCF-7. Furthermore, the project aimed to investigate if ERβ and LRH-1 canrepress each other’s transcriptional activity by performing a Dual-Luciferase assay using in vitro model COV434. The results did not show that ERβ and LRH-1 share negative crosstalk in a human ovarian context since the transcriptional activity of reporter constructs ERE-TATA-Luc and LRH-1-RE-Luc was not decreased in the presence of both ERβ and LRH-1 compared to only the presence of ERβ or LRH-1 respectively. However, the experiment must be repeated to gain trustworthy results. Lastly, the potential binding of LRH-1 to gene GREB1 was also investigated in COV434 using chromatin immunoprecipitation-quantitative polymerase chain reaction (ChIP-qPCR). As a control experiment, the targeted region of GREB1 after the pull-down of ER𝛼 was successfully enriched. However, no enrichment of the targeted region of GREB1 after the pull-down of LRH-1 could be observed, suggesting that LRH-1 does not bind to the GREB1 site. It is plausible that another region of GREB1 not targeted with the qPCR primers used here is bound by LRH-1. </p>

w='ER𝛼)' val={'c': 'ERα', 's': 'diva2:1876707', 'n': 'no full text'}

More formally, the character 𝛼, i.e., 0x1d6fc, Mathematical Italic Small Alpha is from the Mathematical Alphanumeric Symbols (0x1D400-0x1D7FF). However, since the beta has been written using Greek Small Letter Beta,  i.e., 0x3b2, it would seem natural to write the alpha as Greek Small Letter Alpha, i.e., 0x3b1. That is what I have done in the corrected version below. However, as there is no full text in DiVA - I cannot know what was actually used in the thesis.


corrected abstract:
<p>The ovary is a female reproductive organ that has an elemental role in reproduction and production of hormones. Granulosa cells of the ovary produce 17β-estradiol (E2) which can bind to the nuclear estrogen receptor beta (ERβ). The presence of ERβ is mostly restricted to granulosa cells, where it has an important role in folliculogenesis and ovulation. Granulosa cell tumors (GCTs) are a rare subtype of ovarian cancer and account for 5 % of cases. The incidence rate of GCTs is estimated to be between 0.47 to 1.6 per 100,000. Most GCTs have high expression of ERβ. This is noteworthy since ERβ has been considered a tumor suppressor and is absent in most tumors. Thus, ERβ has the potential to become a diagnostic marker or act as a therapeutic target which illustrates the importance of more knowledge and information within the field. Similar to ERβ, liver receptor homolog-1 (LRH-1) is a member of the nuclear receptor superfamily. In the ovary, LRH-1 is highly expressed in granulosa cells. There, it has an important role in folliculogenesis, ovulation and steroidogenesis. ERβ and LRH-1 have been shown to share binding sites in ovarian chromatin. One of the main objectives of this project was to investigate if ERβ and LRH-1 are part of the same complex since previous research shows a potential crosstalk between them in murine in vivo models. This was investigated by performing co-immunoprecipitation (Co-IP) in in vitro models HEPG2 and COV434. The protocol for co-IP was validated in MCF-7 through shown interaction between ERβ and estrogen receptor alpha (ERα). The results do not propose that ERβ and LRH-1 are part of the same complex when performing the co-IP in HEPG2. However, the results could not be validated since ERβ and ERα failed to interact in the control experiment, most likely due to unsuccessful transfection of ERα and LRH-1. The co-IP in COV434 showed evidence of ERβ and LRH-1 being in the same complex. However, the results could not be validated since the control experiment was not performed according to plan. The experiment should be performed again with the right positive and negative controls similar to the validated experiment in MCF-7. Furthermore, the project aimed to investigate if ERβ and LRH-1 can repress each other’s transcriptional activity by performing a Dual-Luciferase assay using in vitro model COV434. The results did not show that ERβ and LRH-1 share negative crosstalk in a human ovarian context since the transcriptional activity of reporter constructs ERE-TATA-Luc and LRH-1-RE-Luc was not decreased in the presence of both ERβ and LRH-1 compared to only the presence of ERβ or LRH-1 respectively. However, the experiment must be repeated to gain trustworthy results. Lastly, the potential binding of LRH-1 to gene GREB1 was also investigated in COV434 using chromatin immunoprecipitation-quantitative polymerase chain reaction (ChIP-qPCR). As a control experiment, the targeted region of GREB1 after the pull-down of ERα was successfully enriched. However, no enrichment of the targeted region of GREB1 after the pull-down of LRH-1 could be observed, suggesting that LRH-1 does not bind to the GREB1 site. It is plausible that another region of GREB1 not targeted with the qPCR primers used here is bound by LRH-1.</p>
----------------------------------------------------------------------
'diva2:1873210' is correct and needs no changes
----------------------------------------------------------------------
In diva2:1289604 abstract is: <p>Existing method for the antibiotic susceptibility test (AST) requires 2-3 days to produce the result. For patients with urinary tract infection (UTI), this lengthy process is critical because they must bear the pain during this period. A newly proposed idea to shorten the time is by using phenotypic antibiotic susceptibility test (AST) method on single bacteria.The first step to achieve the goal is to design and fabricate a platform to capture single bacteria and remove unwanted particles. An integrated micro and nanomembrane structure fulfill the requirement. The micro membrane structure is used to capture intended particles and block larger particles, while nanomembrane is used to remove unwanted nanoparticles. Micro-membrane structure is designed as an array of the microwell, and each microwell is expected to capture the bacteria. The thickness of microwell must be comparable to the bacteria dimension to make the observation of single bacteria be possible. Otherwise, there will be a stack of bacteria and only the bacteria at the top can be observed.The nanoporous membrane is created using aluminum foil through the anodization process and microwell structure is created using OSTE polymer through photolithography. Both materials are inexpensive and the fabrication process is fairly simple. In the final result, the nanoporous membrane is successfully integrated with microwell array. Each microwell has diameter 200 µm and thickness 100 µm. The nanoporous membrane has pore diameters 70nm, thickness 200nm, and approximate porosity 24.6%.This project successfully produces a complete device by integrating microwell array, nanoporous membrane, and a device substrate. However, the desired thickness for the microwell array is not achieved. The thickness of the microwell is 100 µm while the size of a bacteria is only a few microns. Thin microwell array with 5 µm thickness is easily detached from the substrate during development. To solve this problem, a different mixture of developer solution is needed that will develop the OSTE but will not destroy the structure. Nevertheless, this early design of the platform is a promising start in the early development of rapid antibiotic susceptibility test (AST).</p>

corrected abstract:
<p>Existing method for the antibiotic susceptibility test (AST) requires 2-3 days to produce the result. For patients with urinary tract infection (UTI), this lengthy process is critical because they must bear the pain during this period. A newly proposed idea to shorten the time is by using phenotypic antibiotic susceptibility test (AST) method on single bacteria.</p><p>The first step to achieve the goal is to design and fabricate a platform to capture single bacteria and remove unwanted particles. An integrated micro and nanomembrane structure fulfill the requirement. The micro membrane structure is used to capture intended particles and block larger particles, while nanomembrane is used to remove unwanted nanoparticles. Micro-membrane structure is designed as an array of the microwell, and each microwell is expected to capture the bacteria. The thickness of microwell must be comparable to the bacteria dimension to make the observation of single bacteria be possible. Otherwise, there will be a stack of bacteria and only the bacteria at the top can be observed.</p><p>The nanoporous membrane is created using aluminum foil through the anodization process and microwell structure is created using OSTE polymer through photolithography. Both materials are inexpensive and the fabrication process is fairly simple. In the final result, the nanoporous membrane is successfully integrated with microwell array. Each microwell has diameter 200 µm and thickness 100 µm. The nanoporous membrane has pore diameters 70 nm, thickness 200 nm, and approximate porosity 24.6%.</p><p>This project successfully produces a complete device by integrating microwell array, nanoporous membrane, and a device substrate. However, the desired thickness for the microwell array is not achieved. The thickness of the microwell is 100 µm while the size of a bacteria is only a few microns. Thin microwell array with 5 µm thickness is easily detached from the substrate during development. To solve this problem, a different mixture of developer solution is needed that will develop the OSTE but will not destroy the structure. Nevertheless, this early design of the platform is a promising start in the early development of rapid antibiotic susceptibility test (AST).</p>
----------------------------------------------------------------------
In diva2:1510399 abstract is: <p>Infections in critically ill patients are a problem for the healthcare system and at any one time, 70 % of all intensive care unit (ICU) patients are treated with antibiotics. Antibiotics bind toproteins in the blood, but only unbound drug can diffuse over capillary membranes and bindto the targeted receptor. Standard protein binding percentages for antibiotics have been developed from studies on healthy volunteers and dosing regimens for patients are adapted accordingly. The determination of the total concentration of antibiotics in patients’ bloodsamples is, based on the standard percentages, ordinarily representative for the pharmacological effect of the antibiotic. However, certain conditions that are common incritically ill patients can alter protein binding percentages, resulting in a larger or smaller unbound fraction. This in turn can result in toxicity or therapeutic failure.</p><p>The aim of this project was to develop an analytical method for the determination of the unbound concentration of the Beta-Lactam antibiotics cefotaxime, flucloxacillin, cloxacillin and piperacillin in plasma. A method was successfully developed using ultrafiltration for the extraction of unbound analytes and ultra high performance liquid chromatography tandem mass spectrometry, UHPLC-MS/MS, for their quantification. The method was partly validated according to the European Medicines Agency’s guidelines on bioanalytical method validation.</p>

corrected abstract:
<p>Infections in critically ill patients are a problem for the healthcare system and at any one time, 70 % of all intensive care unit (ICU) patients are treated with antibiotics. Antibiotics bind to proteins in the blood, but only unbound drug can diffuse over capillary membranes and bind to the targeted receptor. Standard protein binding percentages for antibiotics have been developed from studies on healthy volunteers and dosing regimens for patients are adapted accordingly. The determination of the total concentration of antibiotics in patients’ blood samples is, based on the standard percentages, ordinarily representative for the pharmacological effect of the antibiotic. However, certain conditions that are common in critically ill patients can alter protein binding percentages, resulting in a larger or smaller unbound fraction. This in turn can result in toxicity or therapeutic failure.</p><p>The aim of this project was to develop an analytical method for the determination of the unbound concentration of the Beta-Lactam antibiotics cefotaxime, flucloxacillin, cloxacillin and piperacillin in plasma. A method was successfully developed using ultrafiltration for the extraction of unbound analytes and ultra high performance liquid chromatography tandem mass spectrometry, UHPLC-MS/MS, for their quantification. The method was partly validated according to the European Medicines Agency’s guidelines on bioanalytical method validation.</p>
----------------------------------------------------------------------
In diva2:724918 abstract is: <p>The risk analysis tool failure mode and effects analysis (FMEA) that analyzes the com-ponents and signals of a electrical system is design dependent and are therefore per-formed late in the development process of electrical systems. This could lead to that some errors are not analyzed in time and may need to be designed away which can lead to increased system complexity as well as longer and more expensive development proc-esses.The objective of this study is that through a literature review identify if there are any methods or approaches that enables Scania to implement a functional hazard analyzes early in the development process of electrical systems and to analyze these.The results of this thesis shows that it is possible to start the FMEA process early in the development process of the electrical system if the engineers have a functional perspec-tive in mind when performing the risk analysis where they list and rank the functions that is provided by the electrical system and their failure modes, failure effects, failure de-tection, severity, probability and occurrence.By using a function based FMEA, the engineer(s) can identify and promptly handle the safety critical functions early in the development process of a electrical system.A existing functionality at Scania has been broken down into functions and a functional hazard analysis has been performed on these as a demonstration of how a function based FMEA can be carried out and look like.</p>

Note the unnecessary hyphens were removed.

corrected abstract:
<p>The risk analysis tool failure mode and effects analysis (FMEA) that analyzes the components and signals of a electrical system is design dependent and are therefore performed late in the development process of electrical systems. This could lead to that some errors are not analyzed in time and may need to be designed away which can lead to increased system complexity as well as longer and more expensive development processes.</p><p>The objective of this study is that through a literature review identify if there are any methods or approaches that enables Scania to implement a functional hazard analyzes early in the development process of electrical systems and to analyze these. The results of this thesis shows that it is possible to start the FMEA process early in the development process of the electrical system if the engineers have a functional perspective in mind when performing the risk analysis where they list and rank the functions that is provided by the electrical system and their failure modes, failure effects, failure detection, severity, probability and occurrence.</p><p>By using a function based FMEA, the engineer(s) can identify and promptly handle the safety critical functions early in the development process of a electrical system. A existing functionality at Scania has been broken down into functions and a functional hazard analysis has been performed on these as a demonstration of how a function based FMEA can be carried out and look like.</p>
----------------------------------------------------------------------
In diva2:1891176 abstract is: <p>Cerebral Palsy (CP) a neuromotor disorder that leads to abnormal developmentof movement, muscle tone and posture and the chance of CP among the newborn is around 2.08‰. This study focuses on studying the spatial distributionpattern of intramuscular fat of the calf muscle in children withCP. Based onthe Magnetic Resonance Imaging (MRI) scans from patients and the controlgroup, the 3D model of each muscle can be recosntructed, as well as thequantification of the fat distribution of different segments. Besides the averageFat Fraction (FF), the spatial distribution of Proton Density Fat Fraction(PDFF) of the muscles in interest are also evaluated.</p>

w='recosntructed' val={'c': 'reconstructed', 's': 'diva2:1891176', 'n': 'error in original'}

corrected abstract:
<p>Cerebral Palsy (CP) a neuromotor disorder that leads to abnormal development of movement, muscle tone and posture and the chance of CP among the newborn is around 2.08‰. This study focuses on studying the spatial distribution pattern of intramuscular fat of the calf muscle in children with CP. Based onthe Magnetic Resonance Imaging (MRI) scans from patients and the control group, the 3D model of each muscle can be recosntructed, as well as thequantification of the fat distribution of different segments. Besides the average Fat Fraction (FF), the spatial distribution of Proton Density Fat Fraction (PDFF) of the muscles in interest are also evaluated.</p>
----------------------------------------------------------------------
In diva2:1805905 abstract is: <p>This report presents an evaluation of a centrifugal air purifier from the company Airission used in an infectious ward at Karolinska University Hospital. Airission's air purifier removes particles and aerosols using centrifugal technology, a relatively untested technology for air purification. The goal was to investigate and attempt to verify the functionality and performance of the air purifier and compare it to a conventional air purifier that uses traditional two-stage filters for air purification.To conduct the study, a bioaerosol measuring instrument was used to measure real-time particle levels in the room. The tests were performed under different operating conditions and time intervals with the air purifier turned on and off. Data collection and analysis included calculating the mean values, comparing the particle levels between different test cases, and calculating the standard deviation.The results showed that Airission's centrifugal air purifier effectively purifies air from particles and aerosols. The purification efficiency was comparable to a conventional air purifier. It was more effective than a conventional air purifier without the use of highefficiency air filters, commonly known as HEPA filters. A significant reduction in the number of particles in the air was observed while the air purifier was in operation. However, some complications arose during the application of the HEPA filter, which could have had a negative impact on both air purifiers.In summary, the analysis demonstrates that the applied centrifugal technology in Airission's air purifier works well. The comparison with the conventional air purifier shows certain advantages of an air purifier that uses centrifugal technology - in terms of both efficiency and quality.This report contributes to the knowledge of air purification solutions to improve air quality and reduce the spread of airborne diseases, especially in hospital environments. The results can be useful for further research and development of more effective air purifiers, which in turn provide better protection for patients and hospital staff exposed to airborne pathogens.</p>


corrected abstract:
<p>This report presents an evaluation of a centrifugal air purifier from the company Airission used in an infectious ward at Karolinska University Hospital. Airission's air purifier removes particles and aerosols using centrifugal technology, a relatively untested technology for air purification. The goal was to investigate and attempt to verify the functionality and performance of the air purifier and compare it to a conventional air purifier that uses traditional two-stage filters for air purification.</p><p>To conduct the study, a bioaerosol measuring instrument was used to measure real-time particle levels in the room. The tests were performed under different operating conditions and time intervals with the air purifier turned on and off. Data collection and analysis included calculating the mean values, comparing the particle levels between different test cases, and calculating the standard deviation.</p><p>The results showed that Airission's centrifugal air purifier effectively purifies air from particles and aerosols. The purification efficiency was comparable to a conventional air purifier. It was more effective than a conventional air purifier without the use of high-efficiency air filters, commonly known as HEPA filters. A significant reduction in the number of particles in the air was observed while the air purifier was in operation. However, some complications arose during the application of the HEPA filter, which could have had a negative impact on both air purifiers.</p><p>In summary, the analysis demonstrates that the applied centrifugal technology in Airission's air purifier works well. The comparison with the conventional air purifier shows certain advantages of an air purifier that uses centrifugal technology - in terms of both efficiency and quality.</p><p>This report contributes to the knowledge of air purification solutions to improve air quality and reduce the spread of airborne diseases, especially in hospital environments. The results can be useful for further research and development of more effective air purifiers, which in turn provide better protection for patients and hospital staff exposed to airborne pathogens.</p>
----------------------------------------------------------------------
title: "Automatisk detektering av skillnader av Androidenhetersanvändargränssnitt"
==>    "Automatisk detektering av skillnader av Androidenheters användargränssnitt"

In diva2:820265 abstract is: <p>User interfaces can look different on different Android devices and it is difficult to tailor anapplication to all Android devices. The goal with this project was to develop a tool that canuse a correct image of a user-interface and find differences in other images taken from otherAndroid devices. A tool was developed from scratch and it proved accurate in most cases.The tool could both find differences and report how large the difference was.</p>

<p>User interfaces can look different on different Android devices and it is difficult to tailor an application to all Android devices. The goal with this project was to develop a tool that can use a correct image of a user-interface and find differences in other images taken from other Android devices. A tool was developed from scratch and it proved accurate in most cases. The tool could both find differences and report how large the difference was.</p>
----------------------------------------------------------------------
In diva2:854656 abstract is: <p>Applying a biorefinery concept for algal biofule production requires that each component of the biomass, i.e., carbohydrates, lipids, proteins be exploited to products. The aim of this study was to evaluate different biorefinery concepts to produce syngas in a WoodRoll®</p><p>Gasifier using microalgae biomass as feedstock. Moreoever, mass culture of microalgae for commercially-viable yields requires CO<sub>2 </sub>addition above ambient air concentrations. Hence we at first compared growth of chlorophyta <em>Dunaliella tertiolecta</em> during exponential phase with continuous sparging of ambient, 3%, 7% and 12% CO<sub>2 </sub>in pre-moistened air. This series of experiments were conducted in order to deliver better understanding of CO<sub>2 </sub>tolerance and its effects on growth for this alga, ahead of a planned follow-on experiment (with this and other algae specie), which will use raw flue gas (i.e., a mixture of approximately 3% CO<sub>2 </sub>together withother natural gas combustion products such as NOx and Sox).There was no statistically significant difference in growth rate observed for treatments with ambient, 3% and 7% CO<sub>2 </sub>in air. However, a decrease in growth was observed for the 12% CO<sub>2 </sub>treatments. Furthermore, three microalgae, <em>Dunaliella teriolecta, Scenedesmus dimorphus,</em> and <em>Chlamydomonas reinhardtii</em> were evaluated for gasification using thermogravimetric analysis, to determine changes in weight relative to temperature applied. Thermogravimetric analysis enables observations to be made for moisture content, reactivity to different pyrolysis and gasification temperatures, and ash content of the biomass. Algal biomass slurries were prepared by centrifugation and lyophylization to approximately 5% moisture content, and samples for thermogravimetric analysis were weighed-out to 200 mg (±0.02g). Each sample was analyzed twice, first at a gasification temperature of 850° C for 90  minutes, and secondly at a gasification temperature of 1100 °C for 20 minutes. For all runs, three different pyrolysis temperatures were used 360 °C, 380 °C and 400 °C. After gsification at 1100 °C, ash content varied between 3-7% depending upon specie, compared to ash content in the range of 5-17% after gasification at 850 °C. The energy distribution after pyrolysis for<em> S. dimorphus</em>, and <em>C. reinhardtii</em> biomass fit perfectly into the energy and mass balances for a WoodRoll ® system, in that each showed high reactivity during gasification and low final ash content. Therefore we conclude that both specie are highly feasible potential fuels for the WoodRoll ® process. <em>Dunaliella tertiolecta</em> is also a good potential fuel for the WoodRoll ® gasification, especially when char yield is maximized through manipulation of process conditions.</p><p>In order to do a comparison study and to evaluate and rate different concepts, 24 different scenarios were designed in total; three different gasification capacities (0.5 MW, 5 MW, 50 MW), two different scenarios for biomass end-product, two different microalgal cultivation systems (Hyperlight ® and Open Pond Raceways) and two different methods for the total capital investment (Lang factor method &amp; Dispersion method). With the delimitations and assumptions within this thesis, only one feasible scenario (5 MW, Open Pond Cultivation System, Scenario B, Dispensed Method) was achieved, a scenario with an internal rate of return (IRR) of 15% and pay back time of 4.9 years, which exceed the given data (IRR 11%).</p>

w='biofule' val={'c': 'biofule', 's': 'diva2:854656'}
w='gsification' val={'c': 'gasification', 's': 'diva2:854656', 'n': 'no full text'}
w='lyophylization' val={'c': 'lyophilization', 's': 'diva2:854656', 'n': 'no full text'}
w='teriolecta' val={'c': 'tertiolecta', 's': 'diva2:854656'}
w='Sox' val={'c': 'SOx', 's': 'diva2:854656', 'n': 'no full text'}
w='Moreoever' val={'c': 'Moreover', 's': 'diva2:854656'}

corrected abstract:
<p>Applying a biorefinery concept for algal biofule production requires that each component of the biomass, i.e., carbohydrates, lipids, proteins be exploited to products. The aim of this study was to evaluate different biorefinery concepts to produce syngas in a WoodRoll®</p><p>Gasifier using microalgae biomass as feedstock. Moreover, mass culture of microalgae for commercially-viable yields requires CO<sub>2</sub> addition above ambient air concentrations. Hence we at first compared growth of chlorophyta <em>Dunaliella tertiolecta</em> during exponential phase with continuous sparging of ambient, 3%, 7% and 12% CO<sub>2 </sub>in pre-moistened air. This series of experiments were conducted in order to deliver better understanding of CO<sub>2</sub> tolerance and its effects on growth for this alga, ahead of a planned follow-on experiment (with this and other algae specie), which will use raw flue gas (i.e., a mixture of approximately 3% CO<sub>2</sub> together with other natural gas combustion products such as NO<sub>x</sub> and SO<sub>x</sub>). There was no statistically significant difference in growth rate observed for treatments with ambient, 3% and 7% CO<sub>2</sub> in air. However, a decrease in growth was observed for the 12% CO<sub>2 </sub>treatments. Furthermore, three microalgae, <em>Dunaliella tertiolecta</em>, <em>Scenedesmus dimorphus,</em> and <em>Chlamydomonas reinhardtii</em> were evaluated for gasification using thermogravimetric analysis, to determine changes in weight relative to temperature applied. Thermogravimetric analysis enables observations to be made for moisture content, reactivity to different pyrolysis and gasification temperatures, and ash content of the biomass. Algal biomass slurries were prepared by centrifugation and lyophilization to approximately 5% moisture content, and samples for thermogravimetric analysis were weighed-out to 200 mg (±0.02g). Each sample was analyzed twice, first at a gasification temperature of 850° C for 90  minutes, and secondly at a gasification temperature of 1100 °C for 20 minutes. For all runs, three different pyrolysis temperatures were used 360 °C, 380 °C and 400 °C. After gasification at 1100 °C, ash content varied between 3-7% depending upon specie, compared to ash content in the range of 5-17% after gasification at 850 °C. The energy distribution after pyrolysis for<em> S. dimorphus</em>, and <em>C. reinhardtii</em> biomass fit perfectly into the energy and mass balances for a WoodRoll® system, in that each showed high reactivity during gasification and low final ash content. Therefore we conclude that both specie are highly feasible potential fuels for the WoodRoll® process. <em>Dunaliella tertiolecta</em> is also a good potential fuel for the WoodRoll® gasification, especially when char yield is maximized through manipulation of process conditions.</p><p>In order to do a comparison study and to evaluate and rate different concepts, 24 different scenarios were designed in total; three different gasification capacities (0.5 MW, 5 MW, 50 MW), two different scenarios for biomass end-product, two different microalgal cultivation systems (Hyperlight® and Open Pond Raceways) and two different methods for the total capital investment (Lang factor method &amp; Dispersion method). With the delimitations and assumptions within this thesis, only one feasible scenario (5 MW, Open Pond Cultivation System, Scenario B, Dispensed Method) was achieved, a scenario with an internal rate of return (IRR) of 15% and pay back time of 4.9 years, which exceed the given data (IRR 11%).</p>
----------------------------------------------------------------------
In diva2:1321701 abstract is: <p>At the moment there are no guidelines to how external service agreements should be managed and administered at Södersjukhuset. The goal with this report has been to provide Södersjukhuset with supporting arguments to decide if they should centralize the management of service agreements or not. In order to achieve this goal it was essential to: study and get a better understanding of the term centralization, examine the serviceagreements’ to get a greater awareness of their extent and analyze Södersjukhuset to gain a better insight of the organisation as a whole. Service agreements were accumulated throughmeetings with the hospital’s employees and through the software Medusa. Calculations were conducted from the agreements to display Södersjukhuset’s current situation. Themanagement of the hospital’s service agreements was considered decentralized and wasdeemed to convey negative economical factors. A proposed solution for the problem was a centralized structure for managing external service agreements which would help the hospital organize and benefit it economically.</p>

corrected abstract:
<p>At the moment there are no guidelines to how external service agreements should be managed and administered at Södersjukhuset. The goal with this report has been to provide Södersjukhuset with supporting arguments to decide if they should centralize the management of service agreements or not. In order to achieve this goal it was essential to: study and get a better understanding of the term centralization, examine the service agreements’ to get a greater awareness of their extent and analyze Södersjukhuset to gain a better insight of the organisation as a whole. Service agreements were accumulated through meetings with the hospital’s employees and through the software Medusa. Calculations were conducted from the agreements to display Södersjukhuset’s current situation. The management of the hospital’s service agreements was considered decentralized and was deemed to convey negative economical factors. A proposed solution for the problem was a centralized structure for managing external service agreements which would help the hospital organize and benefit it economically.</p>
----------------------------------------------------------------------
In diva2:1229234 abstract is: <p>The harsh demands on emissions of toxic formaldehyde and isocyanate has left the industry seek for adhesive with low or no emission of these substances, particularly for XXX XXX XXX XXX. In this report, two new formaldehyde and isocyanate free adhesive systems has been evaluated. These systems contain a XXX backbone that is XXX with XXX, thereby forming XXX. The XXX react through XXX to form XXX XXX that XXX forms XXX by XXX. The mechanical properties were examined through industry standardized bonding tests, and comparisons were made with commercially used XXX systems. To obtain a better understanding of the properties of the adhesive system, the curing time and reaction mechanism was studied by rheometer and infrared spectroscopy (FTIR) respectively. Additionally, the penetration characteristics of the glue-line were examined through Scanning electron microscopy (SEM). The incorporation of XXX and XXX XXX was also examined to test whether one could increase the glue joint strength.The main difficulty with the XXX used was that they, almost in all tests, had shear strengths in dry conditions that has met with the required values from the industry standards (XXX) However, in wet conditions the adhesive had values XXX below the requirements (XXX). An adhesive system with XXX added as XXX showed the highest shear strength in wet conditions showing that the incorporation of:XXX can help with the shear strength of the adhesive system, but this value is still XXX below the required value. The addition of XXX as XXX to the adhesive system didn't show any significant improvements in the overall shear strength for the systems. When performing tensile tests of pure adhesive films, the strength in wet conditions exceeded required values, showing that the intrinsic properties of the adhesive are not the main problem but rather it is the combination of a XXX glue line ( as observed with SEM) with the XXX between XXX and the XXX. The SEM study did show that the penetration decreased slightly with the incorporation of:XXX, suggesting that XXX may be one of the reasons for the XXX water resistances when not formulated By evaluation of the curing process of:XXX and XXX with XX and XX as XXX XXX, it was seen that it was possible to control the curing process depending on concentration and XXX of the XXX XXX to some extent. Lower concentrations of XXX XXX with long curing times were difficult to control since small deviations in amounts seem to affect the curing to a large extent and the overall curing times were thereby difficult to replicate. The FTIR study of the curing process didn't show any differences in the curing process between XXX and XXX. With FTIR, the XXX could be monitored by evaluating XXX XXX present in the adhesive and the XXX XXX in the process. The XXX of XXX could not be detected using FTIR due to obstruction by other XXX in the same XXX, but it was apparent that XXX XXX XXX after approximately half the reaction time (as determined by rheometer) most likely followed by XXX.</p>

corrected abstract:
<p>The harsh demands on emissions of toxic formaldehyde and isocyanate has left the industry seek for adhesive with low or no emission of these substances, particularly for XXX XXX XXX XXX. In this report, two new formaldehyde and isocyanate free adhesive systems has been evaluated. These systems contain a XXX backbone that is XXX with XXX, thereby forming XXX. The XXX react through XXX to form XXX XXX that XXX forms XXX by XXX. The mechanical properties were examined through industry standardized bonding tests, and comparisons were made with commercially used XXX systems. To obtain a better understanding of the properties of the adhesive system, the curing time and reaction mechanism was studied by rheometer and infrared spectroscopy (FTIR) respectively. Additionally, the penetration characteristics of the glue-line were examined through Scanning electron microscopy (SEM). The incorporation of XXX and XXX XXX was also examined to test whether one could increase the glue joint strength. The main difficulty with the XXX used was that they, almost in all tests, had shear strengths in dry conditions that has met with the required values from the industry standards (XXX) However, in wet conditions the adhesive had values XXX below the requirements (XXX). An adhesive system with XXX added as XXX showed the highest shear strength in wet conditions showing that the incorporation of: XXX can help with the shear strength of the adhesive system, but this value is still XXX below the required value. The addition of XXX as XXX to the adhesive system didn't show any significant improvements in the overall shear strength for the systems. When performing tensile tests of pure adhesive films, the strength in wet conditions exceeded required values, showing that the intrinsic properties of the adhesive are not the main problem but rather it is the combination of a XXX glue line ( as observed with SEM) with the XXX between XXX and the XXX. The SEM study did show that the penetration decreased slightly with the incorporation of: XXX, suggesting that XXX may be one of the reasons for the XXX water resistances when not formulated By evaluation of the curing process of: XXX and XXX with XX and XX as XXX XXX, it was seen that it was possible to control the curing process depending on concentration and XXX of the XXX XXX to some extent. Lower concentrations of XXX XXX with long curing times were difficult to control since small deviations in amounts seem to affect the curing to a large extent and the overall curing times were thereby difficult to replicate. The FTIR study of the curing process didn't show any differences in the curing process between XXX and XXX. With FTIR, the XXX could be monitored by evaluating XXX XXX present in the adhesive and the XXX XXX in the process. The XXX of XXX could not be detected using FTIR due to obstruction by other XXX in the same XXX, but it was apparent that XXX XXX XXX after approximately half the reaction time (as determined by rheometer) most likely followed by XXX.</p>
----------------------------------------------------------------------
In diva2:1698199 abstract is: <p><strong>Background:</strong> The yearly incidence of people with Spinal Cord Injury (SCI) is between250,000 and 500,000, according to the World Health Organization (WHO). The injury often reduces the ability to walk. Various consequences affect the nervous system and, thus, the entire body. Therefore, the patient population with SCI is highly heterogeneous also in their gait patterns. Multiple tools are used to classify and understand the walking impairments caused by the injury.</p><p><strong>Objective:</strong> To underline the added value brought by the integration of 3D gait analysis to more standard methods (GDI, GPS, GVS, spatiotemporal parameters, ASIAgrade, muscle strength, and spasticity) in the evaluation and interpretation of gait patterns of subjects with SCI.</p><p><strong>Methods:</strong> 3D gait analysis with a passive optical motion capture system (Vicon)and four force plates was performed in 7 control subjects and 3 with SCI. The model used for marker placement and pre-processing was CGM 2.3. Matlab was used to analyze and plot the kinematic and kinetic joints’ data and calculate the GDI, GPS, and GVS indexes and spatiotemporal parameters for subjects with SCI and the control group. A specialized physiotherapist conducted the clinical assessment of the patients with SCI in a rehabilitation center. This included: ASIA grade and review, muscle strength, and spasticity with Daniels Whorthingham scale and Modified Ashworth scale, respectively. The evaluation of the result was qualitative.</p><p><strong>Results: </strong>The integration of 3D gait analysis show further understanding in the assessment of walking impairments. The indexes resumed the impairments and classified the subjects but lacked temporal and functional perspective. Gait phases and spatiotemporal parameters suggested difficulties in stability and balance but could not identify the problem’s origin. Lastly, clinical assessment enlightened the singular motor and sensory function impairments. 3D gait analysis contextualized these results identifying gait patterns and functional implications.</p><p><strong>Conclusion:</strong> Integrating 3D gait analysis might give a deeper understanding of subjects with SCI’s gait strategies and impairments. Indeed this complex technique links the other methods’ results, contextualizing them and gaining information.</p>

w='Whorthingham' val={'c': 'Worthingham', 's': 'diva2:1698199', 'n': "error in riginal; in the text it should be Daniels and Worthingham's scale"}

corrected abstract:
<p><strong>Background:</strong> The yearly incidence of people with Spinal Cord Injury (SCI) is between 250,000 and 500,000, according to the World Health Organization (WHO). The injury often reduces the ability to walk. Various consequences affect the nervous system and, thus, the entire body. Therefore, the patient population with SCI is highly heterogeneous also in their gait patterns. Multiple tools are used to classify and understand the walking impairments caused by the injury.</p><p><strong>Objective:</strong> To underline the added value brought by the integration of 3D gait analysis to more standard methods (GDI, GPS, GVS, spatiotemporal parameters, ASIA grade, muscle strength, and spasticity) in the evaluation and interpretation of gait patterns of subjects with SCI.</p><p><strong>Methods:</strong> 3D gait analysis with a passive optical motion capture system (Vicon) and four force plates was performed in 7 control subjects and 3 with SCI. The model used for marker placement and pre-processing was CGM 2.3. Matlab was used to analyze and plot the kinematic and kinetic joints’ data and calculate the GDI, GPS, and GVS indexes and spatiotemporal parameters for subjects with SCI and the control group. A specialized physiotherapist conducted the clinical assessment of the patients with SCI in a rehabilitation center. This included: ASIA grade and review, muscle strength, and spasticity with Daniels Whorthingham scale and Modified Ashworth scale, respectively. The evaluation of the result was qualitative.</p><p><strong>Results:</strong> The integration of 3D gait analysis show further understanding in the assessment of walking impairments. The indexes resumed the impairments and classified the subjects but lacked temporal and functional perspective. Gait phases and spatiotemporal parameters suggested difficulties in stability and balance but could not identify the problem’s origin. Lastly, clinical assessment enlightened the singular motor and sensory function impairments. 3D gait analysis contextualized these results identifying gait patterns and functional implications.</p><p><strong>Conclusion:</strong> Integrating 3D gait analysis might give a deeper understanding of subjects with SCI’s gait strategies and impairments. Indeed this complex technique links the other methods’ results, contextualizing them and gaining information.</p>
----------------------------------------------------------------------
In diva2:1586832 abstract is: <p>Patients with cervical cancer are treated both with external beam radiotherapy(EBRT) and brachytherapy (BT) which involves an applicator. During the treatments, CT-images are taken and to perform dose calculations, deformable image registration (DIR) is performed. The image registration involves many challenges, for example, the organs may have different shapes and volumes in the images and the images have different intensities due to the applicator. Many DIR-methods are available but they fail in aligning the multiple organs correctly and simultaneously. This project aimed for developing a method that created triangle meshes of the rectum and bladder in EBRT- and BT-images.The generation of triangle meshes was driven by projection points on the organ and found pairs of points between the same organ in both images. The performance of using these triangle meshes in DIR was compared with that of using boundary conditions (also triangle meshes) and binary masks. Results show that the triangle meshes and boundary conditions performed similarly concerning the DSC, mean DTA and HD validated on rectum, bladder and bones, while the binary masks performed the worst. For the TRE results of the projection points, the triangle meshes outperformed the boundary conditions which shows potential for this method. One drawback of this method is its sensitivity to the initialization for generating triangle meshes, which can be improved. In its developing state, the method has proven to perform well but it remains to see how well it performs on other organs, such as the vagina, cervix and uterus combined and sigmoid. </p>

Note that in the DiVA abstract there were some grammatical corrections, but they have been removed so that the abstract below matches that of the thesis.

corrected abstract:
<p>Patients with cervical cancer are treated both with external beam radiotherapy (EBRT) and brachytherapy (BT) which involves an applicator. During the treatments, CT-images are taken and to perform dose calculations, deformable image registration (DIR) is performed. The image registration involves many challenges, for example, the organs may different shapes and volume in the images and the images have different intensities due to the applicator. Many DIR-methods are available but they fail in aligning the multiple organs correctly and simultaneously. This project aimed for developing a method that created triangle meshes of the rectum and bladder in EBRT- and BT-images. The generation of triangle meshes was driven by projection points on the organ and found pairs of points between the same organ in both images. The performance of using these triangle meshes in DIR was compared with that of using boundary conditions (also triangle meshes) and binary masks. Results show that the triangle meshes and boundary conditions performed similarly concerning the DSC, mean DTA and HD validated on rectum, bladder and bones, while the binary masks performed the worst. For the TRE results of the projection points, the triangle meshes outperformed the boundary conditions which shows potential for this method. One drawback of this method is its sensitivity to the initialization for generating triangle meshes, which can be improved. In its developing state, the method has proven to perform well but it remains to see how well it performs on other organs, such as the vagina, cervix and uterus combined and sigmoid. </p>
----------------------------------------------------------------------
In diva2:1647109 abstract is: <p>To enable a sustainable development, scientific focus is shifting from synthesizing polymers from fossil-based resources to producing bio-based polymers from for example wood. Of particular interest is lignin, which makes up about a third of plant matter but is nowadays most commonly used as fuel. A great deal of research has explored the possibilities opened up by breaking down lignin and modifying its derived molecules, such as ferulic acid. This study aimed to modify ferulic acid into a styrenic monomer with an acidic substituent, and to investigate the aqueous polymerization of this monomer.The first step of modification was the decarboxylation of ferulic acid. This was attempted through recreations of previous studies where base-catalysed decarboxylation was carried out in aprotic polar solvents. After several trials, a successful procedure was found, where triethylamine gave a high-yield decarboxylation after three hours in dimethylformamide at 100 °C. Isolating the product was also initially challenging, but a three-step extraction with H<sub>2</sub>O and Et<sub>2</sub>O finished by a wash of brine gave sufficient purity.Adding an acetic acid-group to the phenol was achieved using a two-step procedure found in literature, but not before trying a dozen novel methods using chloroacetic acid without success. In the working procedure, an acetate group was added using methyl bromoacetate in refluxed acetone with potassium carbonate as base, before demethylating the acetate using lithium hydroxide in a methanol/water mix at room temperature. The resulting monomer, 4-oxy-acetic acid-3-methoxy-styrene (OAMS), was copolymerized with styrene, both in bulk and emulsion radical polymerization. The bulk copolymer displayed a slightly higher molecular weight and glass transition temperature than homopolystyrene, with a significantly higher dispersity. Emulsion polymerization was challenging, as OAMS is not very soluble in either water or styrene, and ultimately, no conclusive results were produced through this route.The study concluded that while it is possible to synthesize pure OAMS from ferulic acid, the degree of usability of this molecule as a monomer in aqueous polymerization is not certain, as it is hardly soluble without the addition of base, which introduces the risk of hydrolysis. Ultimately, although few solid conclusions could be drawn from this study due to several incomplete results, the core objective of modifying ferulic acid and testing polymerization of the resulting monomer was achieved.</p>

Note that the actual abstract says "trial" and not "investigate".

corrected abstract:
<p>To enable a sustainable development, scientific focus is shifting from synthesizing polymers from fossil-based resources to producing bio-based polymers from for example wood. Of particular interest is lignin, which makes up about a third of plant matter but is nowadays most commonly used as fuel. A great deal of research has explored the possibilities opened up by breaking down lignin and modifying its derived molecules, such as ferulic acid. This study aimed to modify ferulic acid into a styrenic monomer with an acidic substituent, and to trial the aqueous polymerization of this monomer.</p><p>The first step of modification was the decarboxylation of ferulic acid. This was attempted through recreations of previous studies where base-catalysed decarboxylation was carried out in aprotic polar solvents. After several trials, a successful procedure was found, where triethylamine gave a high-yield decarboxylation after three hours in dimethylformamide at 100 °C. Isolating the product was also initially challenging, but a three-step extraction with H<sub>2</sub>O and Et<sub>2</sub>O finished by a wash of brine gave sufficient purity.</p><p>Adding an acetic acid-group to the phenol was achieved using a two-step procedure found in literature, but not before trying a dozen novel methods using chloroacetic acid without success. In the working procedure, an acetate group was added using methyl bromoacetate in refluxed acetone with potassium carbonate as base, before demethylating the acetate using lithium hydroxide in a methanol/water mix at room temperature.</p><p>The resulting monomer, 4-oxy-acetic acid-3-methoxy-styrene (OAMS), was copolymerized with styrene, both in bulk and emulsion radical polymerization. The bulk copolymer displayed a slightly higher molecular weight and glass transition temperature than homopolystyrene, with a significantly higher dispersity. Emulsion polymerization was challenging, as OAMS is not very soluble in either water or styrene, and ultimately, no conclusive results were produced through this route.</p><p>The study concluded that while it is possible to synthesize pure OAMS from ferulic acid, the degree of usability of this molecule as a monomer in aqueous polymerization is not certain, as it is hardly soluble without the addition of base, which introduces the risk of hydrolysis.</p><p>Ultimately, although few solid conclusions could be drawn from this study due to several incomplete results, the core objective of modifying ferulic acid and testing polymerization of the resulting monomer was achieved.</p>
----------------------------------------------------------------------
In diva2:1545448 abstract is: <p>The recycling and collection of PET bottles has a long tradition in Sweden dating back to 1994 and is one of the staple recycling industries.Technology has advanced since then, with new recycling processes to assure food grade certified recycled PET and manufacturing processes such as Solid-State polymerization to enable the bottle-to-bottle mantra. Amidst global warming and climate crisis, the interest in recycling and reducing the use of fossil fuel to manufacture new bottles is ever-growing. As a result, manufacturers and breweries want bottles manufactured with higher fractions of recycled PET, and there are already bottles out on the market made from 100% recycled PET.</p><p>In this thesis, the effect that the fraction of recycled PET may have on the mechanical and chemical properties of the final product was tested. Also, the effect that several recycling cycles may have on the product was tested.A lab-scale version of the recycling process used commercially in Sweden by Veolia PET were carried out. Four cycles of the process were carried out on virgin PET material, resulting in material batches krPET-1 to krPET-4. Dog bone samples from each recycled batch were manufactured via injection moulding with 25, 50 and 100% rPET fractions.</p><p>All samples were characterized with various instruments and methods such as FT-IR, Tensile testing, DSC, and intrinsic viscosity testing.From an environmental standpoint, there are clear advantages to an increase in rPET fraction in PET-bottles. Due to issues with manufacturing and the production of samples, only a small sample size was acquired. All the analyses suffered, as a result, making it hard to draw any definite conclusions regarding potential disadvantages with a higher rPET fraction.</p>

The actual abstract does not have "with 25, 50 and 100% rPET fractions" - it also has "molding" and not "moulding".

corrected abstract:
<p>The recycling and collection of PET bottles has a long tradition in Sweden dating back to 1994 and is one of the staple recycling industries.<br>Technology has advanced since then, with new recycling processes to assure food grade certified recycled PET and manufacturing processes such as Solid-State polymerization to enable the bottle-to-bottle mantra. Amidst global warming and climate crisis, the interest in recycling and reducing the use of fossil fuel to manufacture new bottles is ever-growing. As a result, manufacturers and breweries want bottles manufactured with higher fractions of recycled PET, and there are already bottles out on the market made from 100% recycled PET.</p><p>In this thesis, the effect that the fraction of recycled PET may have on the mechanical and chemical properties of the final product was tested. Also, the effect that several recycling cycles may have on the product was tested.</p><p>A lab-scale version of the recycling process used commercially in Sweden by Veolia PET were carried out. Four cycles of the process were carried out on virgin PET material, resulting in material batches krPET-1 to krPET-4. Dog bone samples from each recycled batch were manufactured via injection molding.</p><p>All samples were characterized with various instruments and methods such as FT-IR, Tensile testing, DSC, and intrinsic viscosity testing.</p><p>From an environmental standpoint, there are clear advantages to an increase in rPET fraction in PET-bottles. Due to issues with manufacturing and the production of samples, only a small sample size was acquired. All the analyses suffered, as a result, making it hard to draw any definite conclusions regarding potential disadvantages with a higher rPET fraction.</p>
----------------------------------------------------------------------
In diva2:1451776 abstract is: <p>AbstractThis thesis has been carried out on behalf of Stanley Security Sverige AB. The company Stanley provides a product to calculate the number of people in a queue. The product detects the number of people in a queue with the help of a surveillance camera and deep machine learning. They find that the existing solution does not perform well enough to be deployed and does not detect with high accuracy. In this work, various known object recognition models have been examined and compared, as well as examination of previous work and tests in the field.The result was a reconfigured model prototype that only detected faces in comparison to known models You Only Look Once V3 (YOLOV3) and Single Shot MultiBox Detector (SSD) that detect the entire human body, in addition to other objects. The proposed model performed significantly better than the known models and had higher accuracy. The results indicate that the proposed model prototype can be deployed in real time and be used by Stanely's customers to efficiently allocate resources within their companies.Keywordsmachine learning, image analysis, Python, Darknet, neural network, deep learning, YOLOV3, Single Shot MultiBox Detector.</p>

corrected abstract:
<p>This thesis has been carried out on behalf of Stanley Security Sverige AB. The company Stanley provides a product to calculate the number of people in a queue. The product detects the number of people in a queue with the help of a surveillance camera and deep machine learning. They find that the existing solution does not perform well enough to be deployed and does not detect with high accuracy. In this work, various known object recognition models have been examined and compared, as well as examination of previous work and tests in the field.</p><p>The result was a reconfigured model prototype that only detected faces in comparison to known models You Only Look Once V3 (YOLOV3) and Single Shot MultiBox Detector (SSD) that detect the entire human body, in addition to other objects. The proposed model performed significantly better than the known models and had higher accuracy. The results indicate that the proposed model prototype can be deployed in real time and be used by Stanely's customers to efficiently allocate resources within their companies.</p>
----------------------------------------------------------------------
In diva2:1353778 abstract is: <p>The presteaming of wood chips is an important step in the chemical pulping industry. It removes the air from within wood chips, allowing the cooking liquor to better impregnate wood chips, which leads to a more uniform cooking process, and lowers the amount of rejects. When steaming at atmospheric pressure, it is important that the temperature of the wood chips reach 100ᴼ C, as otherwise there will be an equilibrium leaving some air left inside. Having poorly steamed chips in a process could cause severe problems when it comes to reaching the targeted kappa number, or having the adequate retention time in the digester. There are a few different ways in which the wood chips are presteamed within the industry, however, there is little experimental data regarding the heating time of wood chips that can be used when designing these systems. Most studies have mainly focused on the air removal, or improvement of the impregnation step, and the few studies that have included the heating of the wood chips were limited to only one type of wood chip, or failed to specify the experimental details.</p><p>Therefore, handmade wood chips pine and birch, two tree species commonly found in Sweden, were steamed in an ATEX designed digester with a steam jacket. The wood chips had thermocouples inside them and the temperature and time was recorded, and the effect of different parameters on the heating could thus be studied.The results revealed that there could be more than a minute in average time difference between wood chips of different thicknesses, both for birch and pine, although the difference in heating time was more linearly correlated to thickness for the birch chips. Pine chips of different thickness were also studied when the pressure inside the digester was allowed to build up, which showed that it is mainly thicker chips that have reduced heating time under such circumstances, as the thinner chips stop heating for a while when the steam condensates on colder surroundings. When comparing heartwood and sapwood chips, it was noted that the difference in heating time could be around 1 minute at most for pine, but only a few seconds for birch. This was most likely due to the pine heartwood and sapwood having distinct moisture contents, 25 % and 58 % respectively, while it was 41% and 42 % in birch heartwood and sapwood. Birch and pine chips wee also steamed together, however, the difference in heating time was only a few seconds on average.</p><p>When comparing these experimental results with simulation data of the steaming of wood chips, it fit rather well when it came to the general heating time. However, the effect of increased moisture content had a much larger impact in the simulations, which predicted that more moist wood chips would need several minutes more steaming time, while the experiments only showed at difference of, at most, around 1 minute. When comparing with old experimental data, that has been the basis for the design of older steaming processes, it gave very distinct results, where the effect of thickness did not have as big of an impact as in the old data. No further comparison could be made, however, as the experimental conditions for the old experimental data were not known. Based on these results, it was noted that a steaming time of at least 5 minutes would be needed to ensure that even the largest and more moist chips could reach 100ᴼ C in this system.</p><p>Finally, the condensate from the handmade birch and pine chips was analyzed. It revealed the presence of low molecular weight compounds like methanol, formic acid and acetic acid. Common metal ions were also present,although the amount of sodium ions clearly surpassed the rest. The pH of the pine condensate was measured and it was very high, which implies that the condensate was contaminated.</p>

Note that the actual abstract has "larges" and largest".

corrected abstract:
<p>The presteaming of wood chips is an important step in the chemical pulping industry. It removes the air from within wood chips, allowing the cooking liquor to better impregnate wood chips, which leads to a more uniform cooking process, and lowers the amount of rejects. When steaming at atmospheric pressure, it is important that the temperature of the wood chips reach 100 ᴼC, as otherwise there will be an equilibrium leaving some air left inside. Having poorly steamed chips in a process could cause severe problems when it comes to reaching the targeted kappa number, or having the adequate retention time in the digester. There are a few different ways in which the wood chips are presteamed within the industry, however, there is little experimental data regarding the heating time of wood chips that can be used when designing these systems. Most studies have mainly focused on the air removal, or improvement of the impregnation step, and the few studies that have included the heating of the wood chips were limited to only one type of wood chip, or failed to specify the experimental details.</p><p>Therefore, handmade wood chips pine and birch, two tree species commonly found in Sweden, were steamed in an ATEX designed digester with a steam jacket. The wood chips had thermocouples inside them and the temperature and time was recorded, and the effect of different parameters on the heating could thus be studied. The results revealed that there could be more than a minute in average time difference between wood chips of different thicknesses, both for birch and pine, although the difference in heating time was more linearly correlated to thickness for the birch chips. Pine chips of different thickness were also studied when the pressure inside the digester was allowed to build up, which showed that it is mainly thicker chips that have reduced heating time under such circumstances, as the thinner chips stop heating for a while when the steam condensates on colder surroundings. When comparing heartwood and sapwood chips, it was noted that the difference in heating time could be around 1 minute at most for pine, but only a few seconds for birch. This was most likely due to the pine heartwood and sapwood having distinct moisture contents, 25 % and 58 % respectively, while it was 41 % and 42 % in birch heartwood and sapwood. Birch and pine chips wee also steamed together, however, the difference in heating time was only a few seconds on average.</p><p>When comparing these experimental results with simulation data of the steaming of wood chips, it fit rather well when it came to the general heating time. However, the effect of increased moisture content had a much larger impact in the simulations, which predicted that more moist wood chips would need several minutes more steaming time, while the experiments only showed at difference of, at most, around 1 minute. When comparing with old experimental data, that has been the basis for the design of older steaming processes, it gave very distinct results, where the effect of thickness did not have as big of an impact as in the old data. No further comparison could be made, however, as the experimental conditions for the old experimental data were not known. Based on these results, it was noted that a steaming time of at least 5 minutes would be needed to ensure that even the larges and more moist chips could reach 100 ᴼC in this system.</p><p>Finally, the condensate from the handmade birch and pine chips was analyzed. It revealed the presence of low molecular weight compounds like methanol, formic acid and acetic acid. Common metal ions were also present, although the amount of sodium ions clearly surpassed the rest. The pH of the pine condensate was measured and it was very high, which implies that the condensate was contaminated.</p>
----------------------------------------------------------------------
In diva2:1866084 abstract is: <p>Colorectal cancer (CRC) is a health challenge worldwide and early detection of the disease is crucial to improve patient prognosis. It is common for the first contact with care to occur in primary care centers where general practitioners often face the challenge of distinguishing CRC from other diseases with similar symptoms. In this master thesis, patient records from primary care were used to create, optimize, and evaluate a machine learning model that classifies patients with CRC for early detection of the disease. The data used in the project included parts of electronic health records (EHRs) from both public (SLSO) and privately run (Capio and Praktikertjänst) primary care centers in the Stockholm region. The available dataset was cleaned and pre- processed, and then tested on four separate models. After selecting and optimizing the most promising model, LightGBM, a detailed evaluation of the model was performed. To simulate realistic clinical conditions, data from the three months prior to diagnosis were excluded from two of the datasets. The results were then compared with a baseline machine learning model that utilized ICD codes extracted from EHRs in primary care for early detection of CRC.The results showed that the final developed model had a generally good performance with an AUROC score of a maximum of 85.8%, which indicates very good ability to distinguish between the classes. The performance dropped when using the datasets with 3 months of data removed, but the ROC curves still showed a better ability than random classification to distinguish between the classes with a AUROC score of maximum 60,8%. The results also showed that the model developed in this master thesis outperforms the baseline model, which was based on ICD codes, from a performance perspective. For future development and before a possible clinical implementation, a larger data set should be used for training and testing.</p>

corrected abstract:
<p>Colorectal cancer (CRC) is a health challenge worldwide and early detection of the disease is crucial to improve patient prognosis. It is common for the first contact with care to occur in primary care centers where general practitioners often face the challenge of distinguishing CRC from other diseases with similar symptoms. In this master thesis, patient records from primary care were used to create, optimize, and evaluate a machine learning model that classifies patients with CRC for early detection of the disease.</p><p>The data used in the project included parts of electronic health records (EHRs) from both public (SLSO) and privately run (Capio and Praktikertjänst) primary care centers in the Stockholm region. The available dataset was cleaned and pre-processed, and then tested on four separate models. After selecting and optimizing the most promising model, LightGBM, a detailed evaluation of the model was performed. To simulate realistic clinical conditions, data from the three months prior to diagnosis were excluded from two of the datasets. The results were then compared with a baseline machine learning model that utilized ICD codes extracted from EHRs in primary care for early detection of CRC.</p><p>The results showed that the final developed model had a generally good performance with an AUROC score of a maximum of 85.8%, which indicates very good ability to distinguish between the classes. The performance dropped when using the datasets with 3 months of data removed, but the ROC curves still showed a better ability than random classification to distinguish between the classes with a AUROC score of maximum 60,8%. The results also showed that the model developed in this master thesis outperforms the baseline model, which was based on ICD codes, from a performance perspective. For future development and before a possible clinical implementation, a larger data set should be used for training and testing.</p>
----------------------------------------------------------------------
In diva2:1141866 abstract is: <h1>Abstract</h1><p>The research study presented in this master thesis was done in a Swedish medium-sized factory that manufactures plaster-based construction products and belongs to an international corporate group. Some work tasks in the factory involve staff carrying out scheduled maintenance and cleaning in areas that are classified as confined spaces with inadequate ventilation that can form a hazardous atmosphere, and/or that it is difficult to get in and out. Lack of oxygen, fire and explosion risks may result in serious, even fatal, accidents in confined spaces. At the start of this research study, the existing factory procedures for working in confined spaces did not fully meet the requirements of the corporate group, nor Swedish legislation. The corporate group has challenged the factory to develop "best practices" for work in confined spaces so that other factories can implement the same kind of counter measures. The aim of this study is therefore to develop a tool to prevent accidents by identifying and managing risks associated with work in confined spaces.The research study was performed as a case study. Data collection was done through literature studies and collection an overview of legislative requirements, review of existing company documents, risk assessment tools, notes during project meetings, interviews of the project team, as well as the acquisition of knowledge through participation in the seminar on "work in the confined spaces". To evaluate the quality of the tool, a workshop was conducted with open questions.Based on these research activities a tool is developed to systematically identify and manage risks in confined spaces at the current factory and has the following cycle: 1) Background, 2) Mapping, 3) Risk List, 4) Stratification, 5) Risk Assessment, 6) Organizational and technical counter measures, 7) Implementation, 8) Audits, 9) Inspections, 10) Corrective actions (continuous improvement)The tool can be implemented in the future for the other confined spaces in the factory. The next step is to test the tool in other gypsum factories in the Group and adjust as needed, and to spread the knowledge further.We recommend that 1) the factory Rescue Team develop specific emergency plans for each confined space as well as a schedule for equipment inspection, and that the 2) Project team develop a "flow" to facilitate the review of routines when work in confined spaces is required.</p>


corrected abstract:
<p>The research study presented in this master thesis was done in a Swedish medium-sized factory that manufactures plaster-based construction products and belongs to an international corporate group. Some work tasks in the factory involve staff carrying out scheduled maintenance and cleaning in areas that are classified as confined spaces with inadequate ventilation that can form a hazardous atmosphere, and/or that it is difficult to get in and out. Lack of oxygen, fire and explosion risks may result in serious, even fatal, accidents in confined spaces. At the start of this research study, the existing factory procedures for working in confined spaces did not fully meet the requirements of the corporate group, nor Swedish legislation. The corporate group has challenged the factory to develop "best practices" for work in confined spaces so that other factories can implement the same kind of counter measures. The aim of this study is therefore to develop a tool to prevent accidents by identifying and managing risks associated with work in confined spaces.</p><p>The research study was performed as a case study. Data collection was done through literature studies, an overview of legislative requirements, review of existing company documents, risk assessment tools, notes during project meetings, interviews of the project team, as well as the acquisition of knowledge through participation in the seminar on "work in the confined spaces". To evaluate the quality of the tool, a focus group was conducted with open questions. Based on these research activities a tool is developed to systematically identify and manage risks in confined spaces at the current factory and has the following cycle: 1) Background, 2) Mapping, 3) Risk List, 4) Stratification, 5) Risk Assessment, 6) Organizational and technical counter measures, 7) Implementation, 8) Audits, 9) Inspections, 10) Corrective actions (continuous improvement). The tool can be implemented in the future for the other confined spaces in the factory. The next step is to test the tool in other gypsum factories in the Group and adjust as needed, and to spread the knowledge further.</p><p>We recommend that 1) the factory Rescue Team develop specific emergency plans for each confined space, as well as a schedule for equipment inspection, and that the 2) project team develop a process to facilitate the review of routines when work in confined spaces is required.</p>
----------------------------------------------------------------------
In diva2:1257721 abstract is: <p>Sugar nucleotides are crucial compounds in various biosynthetic pathways. However, their use in industry and research is limited due to high cost and low availability. Developing a process that synthesize sugar nucleotides at low-cost and large quantities is therefore essential. Enzymatic synthesis was shown to be a promising approach. However, enzyme stability and reusability should be improved to accomplish economically viable processes.In this work, sugar nucleotides were synthesized through one-pot multi-enzyme (OPME) regeneration cascades by the use of immobilized enzymes that were produced through epoxy immobilization. This synthesis intends to produce sugar nucleotides with economical and technical efficiency. Therefore, the objective of enzyme immobilization was to utilize low-cost and commercially available supports. Proteins were immobilized from cell lysates to avoid expensive protein purification. Product analysis indicated that side product formation can be neglected. Moreover, immobilized purified proteins achieved much lower productivity, compared to immobilized proteins from cell lysate.Reaction with enzymes immobilized on amino-epoxy support Relizyme HFA 403 achieved the highest productivity of UDP-Glucose compared to other immobilization supports based on UDP-sugar phosphorylase immobilization. The same support was tested on all enzymes from OPME and achieved effective product yield with respect to GlcNAc. In addition, investigation of various immobilization conditions was completed on UDP-sugar phosphorylase that catalyze reaction directly to the product to increase the product yield. Productivity was indeed improved and the same immobilization conditions were applied to other enzymes. However, the same trend was not observed for OPME cascade productivity.The steadiness of immobilized enzymes was tested through storage stability, reusability and thermal stability. After 30 days of storage immobilized enzymes retained 55 % of initial productivity. Their reusability was tested for eight 24 h reaction cycles. After eighth cycle around 10 % of initial activity remained. Thermal stability of immobilized enzymes attained low productivity at 40 ˚C and none at 50 ˚C. Compared to magnetic beads that were also investigated in this work epoxy immobilization exhibited higher productivity. In addition, cost estimation demonstrated that the price of the product synthesized with immobilized enzymes could be lower than the price of commercially available products.</p>

It is likely that the places there was not space after a period are the ends of paragraphs, but as there is not full text - I cannot know.

corrected abstract:
<p>Sugar nucleotides are crucial compounds in various biosynthetic pathways. However, their use in industry and research is limited due to high cost and low availability. Developing a process that synthesize sugar nucleotides at low-cost and large quantities is therefore essential. Enzymatic synthesis was shown to be a promising approach. However, enzyme stability and reusability should be improved to accomplish economically viable processes. In this work, sugar nucleotides were synthesized through one-pot multi-enzyme (OPME) regeneration cascades by the use of immobilized enzymes that were produced through epoxy immobilization. This synthesis intends to produce sugar nucleotides with economical and technical efficiency. Therefore, the objective of enzyme immobilization was to utilize low-cost and commercially available supports. Proteins were immobilized from cell lysates to avoid expensive protein purification. Product analysis indicated that side product formation can be neglected. Moreover, immobilized purified proteins achieved much lower productivity, compared to immobilized proteins from cell lysate. Reaction with enzymes immobilized on amino-epoxy support Relizyme HFA 403 achieved the highest productivity of UDP-Glucose compared to other immobilization supports based on UDP-sugar phosphorylase immobilization. The same support was tested on all enzymes from OPME and achieved effective product yield with respect to GlcNAc. In addition, investigation of various immobilization conditions was completed on UDP-sugar phosphorylase that catalyze reaction directly to the product to increase the product yield. Productivity was indeed improved and the same immobilization conditions were applied to other enzymes. However, the same trend was not observed for OPME cascade productivity. The steadiness of immobilized enzymes was tested through storage stability, reusability and thermal stability. After 30 days of storage immobilized enzymes retained 55 % of initial productivity. Their reusability was tested for eight 24 h reaction cycles. After eighth cycle around 10 % of initial activity remained. Thermal stability of immobilized enzymes attained low productivity at 40 ˚C and none at 50 ˚C. Compared to magnetic beads that were also investigated in this work epoxy immobilization exhibited higher productivity. In addition, cost estimation demonstrated that the price of the product synthesized with immobilized enzymes could be lower than the price of commercially available products.</p>
----------------------------------------------------------------------
In diva2:845671 abstract is: <p>Engineering microorganisms at the systems level is recognized to be the future of metabolic engineering. Thanks to the development of genome annotation, mcroorganisms can be understood, as never before, and be reconstructed in the form of computational models. Flux balance analysis provides a deep insight intocellular metabolism and can guide metabolic engineering strategies. In particular, algorithms can assess the cellular complexity of the metabolism and hint at genetic interventions to improve product productivity. In this work, Synechosystis PCC6803 metabolism was invesetigated in silico. Genetic interventions could besuggested to couple butanol synthesis to growth as a way to improve currentproductivities. Cofactor recycling and, in particular, buffering mechanisms were shown to be important targets. Creating a cofacor imbalance and removing thesebuffering mechanisms is an important driving force. This forces a carbon flux through butanol synthesis to maintain cofactor balance and sustain growth.</p>

w='cofacor' val={'c': 'cofactor', 's': 'diva2:845671', 'n': 'correct in original'}
w='invesetigated' val={'c': 'investigated', 's': 'diva2:845671', 'n': 'correct in original'}
w='mcroorganisms' val={'c': 'microorganisms', 's': 'diva2:845671', 'n': 'correct in original'}

corrected abstract:
<p>Engineering microorganisms at the systems level is recognized to be the future of metabolic engineering. Thanks to the development of genome annotation, microorganisms can be understood, as never before, and be reconstructed in the form of computational models. Flux balance analysis provides a deep insight into cellular metabolism and can guide metabolic engineering strategies. In particular, algorithms can assess the cellular complexity of the metabolism and hint at genetic interventions to improve product productivity. In this work, Synechocystis PCC 6803 metabolism was investigated in silico. Genetic interventions could be suggested to couple butanol synthesis to growth as a way to improve current productivities. Cofactor recycling and, in particular, buffering mechanisms were shown to be important targets. Creating a cofactor imbalance and removing these buffering mechanisms is an important driving force. This forces a carbon flux through butanol synthesis to maintain cofactor balance and sustain growth.</p>
----------------------------------------------------------------------
In diva2:1242467 abstract is: <p>Ozone treatment of pollutants in air is a relatively young technology with limited literature available. To the authors knowledge no literature discussing simulations of commercial ozone treatment without UV-lamps in gas phase has been published up to this point.The purpose of this project was to identify issues and propose recommendations related to the distribution of ozone in industrial ducting and the injection of ozone into commercial ducting. The injection rate of the ozone mixture is small relative to the flow of the treated air stream. In such cases ozone is easily swept away and confined to a limited section of the ducts, affecting overall efficiency. The injection inlet and ducting were simulated together in both 2D and 3D environments using COMSOL Multi-physics with CFD, CAD and transport of diluted species modules.Improving mixing in industrial ducting was simple in comparison to the commercial ducting where the flow is complex. For the commercial application simulations showed that the mixing efficiency varies greatly between injection positions. Based on CFD data in the commercial ducting two models for injection point analysis were developed.2D and 3D simulations showed different result for injections inside the main duct, the 3D case could properly simulate rotating flows inside the main duct which makes certain injection points in the main duct more effective that predicted in 2D.This master thesis project was done in cooperation between Royal Institute of Technology and Ozonetech in Sweden.</p>

The corrections were simply to add the division into paragraphs-

corrected abstract:
<p>Ozone treatment of pollutants in air is a relatively young technology with limited literature available. To the authors knowledge no literature discussing simulations of commercial ozone treatment without UV-lamps in gas phase has been published up to this point.</p><p>The purpose of this project was to identify issues and propose recommendations related to the distribution of ozone in industrial ducting and the injection of ozone into commercial ducting. The injection rate of the ozone mixture is small relative to the flow of the treated air stream. In such cases ozone is easily swept away and confined to a limited section of the ducts, affecting overall efficiency. The injection inlet and ducting were simulated together in both 2D and 3D environments using COMSOL Multi-physics with CFD, CAD and transport of diluted species modules.</p><p>Improving mixing in industrial ducting was simple in comparison to the commercial ducting where the flow is complex. For the commercial application simulations showed that the mixing efficiency varies greatly between injection positions. Based on CFD data in the commercial ducting two models for injection point analysis were developed.</p><p>2D and 3D simulations showed different result for injections inside the main duct, the 3D case could properly simulate rotating flows inside the main duct which makes certain injection points in the main duct more effective that predicted in 2D.</p><p>This master thesis project was done in cooperation between Royal Institute of Technology and Ozonetech in Sweden.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-30
======================================================================
In diva2:1833684 abstract is: <p>Type 2 Diabetes (T2D) has been shown to cause cardiovascular diseases (CVDs), with atherosclerosis and coronary artery disease (CAD) being commonly associated with T2D. In T2D, red blood cells (RBCs) undergo functional alterations, causing reduced NO bioavailability and increased oxidative stress through the formation of reactive oxygen species (ROS), in turn causing endothelial dysfunction. Further, RBCs can communicate with other cells, such as endothelial cells, via extracellular vesicles (EVs), which contain bioactive molecules. This has caused EVs to gain attention as potential mediators of CVDs in T2D. The endothelial nitric oxide synthase (eNOS) pathway is crucial to produce NO; however, in T2D, eNOS becomes uncoupled and, thereby contribute to the production of ROS. Similarly, nicotinamide adenine dinucleotide phosphate (NADPH) Oxidase (NOX) enzymes generate ROS that induce endothelial dysfunction; nevertheless, the role of NOX isoforms in T2D is complex and not fully understood. This thesis project aimed to investigate and identify the molecular signaling mechanisms by which RBCderived EVs induce endothelial dysfunction in T2D. Human blood samples from well-characterized T2D patients and healthy control cohorts were utilized for RBC isolation and EV collection. The impact of EVs derived from T2D-RBCs (T2D-RBCs EVs) was assessed through immunohistochemistry and wire myograph studies, where aortas from male wild-type mice (C57BL/6) were isolated and used. RNA sequencing was used to identify differentially expressed genes and pathways. Human carotid artery endothelial cells (HCtAEC) were used for in vitro studies, examining cell viability and co-incubation with T2D-RBCs EVs and healthy (H)-RBCs EVs. Immunohistochemical staining using oxidative stress markers (4-HNE, phospho-eNOS, and nitrotyrosine) revealed elevated expression in aortas incubated with T2D-RBCs EVs. qPCR analysis showed a significant increase in NOX4 mRNA levels in HCtAEC co-incubated with T2D-RBCs EVs while NOX1, eNOS, and PTP1B expression remained unchanged. Wire myograph studies demonstrated that the endothelial dysfunction caused by T2D-RBCs EVs was rescued by the administration of oxidative stress inhibitor NAC to the aortas in the organ chambers. RNA-Seq identified 26 differentially expressed genes, with TBC1D25 and SLC35B2 potentially linked to T2D. Validation of the RNA-Seq results showed no differences in expression levels, suggesting that the RNA-Seq results may not be reliable. Inhibition of NOX2/4 using inhibitor GLX481304 was found to be toxic for the aortas. As a conclusion, we could state that T2D-RBCs EVs indirectly induce endothelial dysfunction through increased vascular oxidative stress. </p>

corrected abstract:
<p>Type 2 Diabetes (T2D) has been shown to cause cardiovascular diseases (CVDs), with atherosclerosis and coronary artery disease (CAD) being commonly associated with T2D. In T2D, red blood cells (RBCs) undergo functional alterations, causing reduced NO bioavailability and increased oxidative stress through the formation of reactive oxygen species (ROS), in turn causing endothelial dysfunction. Further, RBCs can communicate with other cells, such as endothelial cells, via extracellular vesicles (EVs), which contain bioactive molecules. This has caused EVs to gain attention as potential mediators of CVDs in T2D. The endothelial nitric oxide synthase (eNOS) pathway is crucial to produce NO; however, in T2D, eNOS becomes uncoupled and, thereby contribute to the production of ROS. Similarly, nicotinamide adenine dinucleotide phosphate (NADPH) Oxidase (NOX) enzymes generate ROS that induce endothelial dysfunction; nevertheless, the role of NOX isoforms in T2D is complex and not fully understood. This thesis project aimed to investigate and identify the molecular signaling mechanisms by which RBC derived EVs induce endothelial dysfunction in T2D. Human blood samples from well-characterized T2D patients and healthy control cohorts were utilized for RBC isolation and EV collection. The impact of EVs derived from T2D-RBCs (T2D-RBCs EVs) was assessed through immunohistochemistry and wire myograph studies, where aortas from male wild-type mice (C57BL/6) were isolated and used. RNA sequencing was used to identify differentially expressed genes and pathways. Human carotid artery endothelial cells (HCtAEC) were used for in vitro studies, examining cell viability and co-incubation with T2D-RBCs EVs and healthy (H)-RBCs EVs. Immunohistochemical staining using oxidative stress markers (4-HNE, phospho-eNOS, and nitrotyrosine) revealed elevated expression in aortas incubated with T2D-RBCs EVs. qPCR analysis showed a significant increase in NOX4 mRNA levels in HCtAEC co-incubated with T2D-RBCs EVs while NOX1, eNOS, and PTP1B expression remained unchanged. Wire myograph studies demonstrated that the endothelial dysfunction caused by T2D-RBCs EVs was rescued by the administration of oxidative stress inhibitor NAC to the aortas in the organ chambers. RNA-Seq identified 26 differentially expressed genes, with TBC1D25 and SLC35B2 potentially linked to T2D. Validation of the RNA-Seq results showed no differences in expression levels, suggesting that the RNA-Seq results may not be reliable. Inhibition of NOX2/4 using inhibitor GLX481304 was found to be toxic for the aortas. As a conclusion, we could state that T2D-RBCs EVs indirectly induce endothelial dysfunction through increased vascular oxidative stress. </p>
----------------------------------------------------------------------
In diva2:1763151 abstract is: <p>Structural health monitoring (SHM) is damage detection strategy for aerospace, civiland mechanical infrastructure. This project tries to show that Lamb waves, that are being generated and sensed with piezoelectric transducers, can be used for damage detection in a SHM system. For these piezoelectric transducers to work, filtering and amplification circuits needs to be connected to them. This report include the design,simulation, assembly and testing of these circuits. Due to lack of time, it was not possible to generate and sense actual Lamb waves. The result of the thesis is thatsimulations and tests show that it is possible to generate and sense Lamb waves for damage detection in a SHM system</p>

corrected abstract:
<p>Structural health monitoring (SHM) is damage detection strategy for aerospace, civil and mechanical infrastructure. This project tries to show that Lamb waves, that are being generated and sensed with piezoelectric transducers, can be used for damage detection in a SHM system. For these piezoelectric transducers to work, filtering and amplification circuits needs to be connected to them. This report include the design, simulation, assembly and testing of these circuits. Due to lack of time, it was not possible to generate and sense actual Lamb waves. The result of the thesis is that simulations and tests show that it is possible to generate and sense Lamb waves for damage detection in a SHM system.</p>
----------------------------------------------------------------------
In diva2:1223743 abstract is: <p>Work-related musculoskeletal disorders constitutes a substantial burden for society, generating individual suffering and financial costs. Quantifying the musculoskeletal stress and establishing exposure-response relationships is an important step in facing this problem.</p><p>Observational methods for assessing exposure in the field of ergonomics have shown poor results, and the technical measurement methods that exists are often complicated to use which limits their scope to scientific purposes.</p><p>This work describes the development of a prototype measurement system aimed to simplify ambulatory measurements of musculoskeletal load, specifically aimed at the wrist and hand. Wearable sensors including Inertial Measurement Units (IMU:s) and Electromyography (EMG) were connected to a smartphone and used for measuring wrist movement and forearm muscle activity. Data sampled in the smartphone was stored online in a cloud database, and a webapplication was developed to visualize work-load exposure.</p><p>Testing under controlled conditions indicated that muscular rest can be measured and classified according to suggested risk thresholds. Accurate angular measurements were difficult to implement because of lacking inter-sensor alignment in the horizontal plane, as well as uncertainties in the Bluetooth protocol.</p><p>Future work should focus on the IMU:s and look to further develop a method of correcting the relative angle error, as well as investigating accurate time synchronization of the two sensors.Alternatively, deriving angular velocities directly from the IMU gyroscopes could be investigated.</p>

w='IMU:s' val={'c': 'IMUs', 's': ['diva2:1223743', 'diva2:1327945'], 'n': 'error in original'}

corrected abstract:
<p>Work-related musculoskeletal disorders constitutes a substantial burden for society, generating individual suffering and financial costs. Quantifying the musculoskeletal stress and establishing exposure-response relationships is an important step in facing this problem.</p><p>Observational methods for assessing exposure in the field of ergonomics have shown poor results, and the technical measurement methods that exists are often complicated to use which limits their scope to scientific purposes.</p><p>This work describes the development of a prototype measurement system aimed to simplify ambulatory measurements of musculoskeletal load, specifically aimed at the wrist and hand. Wearable sensors including Inertial Measurement Units (IMU:s) and Electromyography (EMG) were connected to a smartphone and used for measuring wrist movement and forearm muscle activity. Data sampled in the smartphone was stored online in a cloud database, and a webapplication was developed to visualize work-load exposure.</p><p>Testing under controlled conditions indicated that muscular rest can be measured and classified according to suggested risk thresholds. Accurate angular measurements were difficult to implement because of lacking inter-sensor alignment in the horizontal plane, as well as uncertainties in the Bluetooth protocol.</p><p>Future work should focus on the IMU:s and look to further develop a method of correcting the relative angle error, as well as investigating accurate time synchronization of the two sensors. Alternatively, deriving angular velocities directly from the IMU gyroscopes could be investigated.</p>
----------------------------------------------------------------------
In diva2:1774390 abstract is: <p>Type 1 diabetes is caused by autoimmune destruction of insulin-producing β-cells which results in insulin deficiency. An alternative treatment option to life-long injection of exogenous insulin is the transplantation of pancreatic aggregates generated from pluripotent human embryonic stem cells (hESCs). Such insulin-secreting islet-like clusters can be supported by a 3D scaffold made of recombinant spider silk functionalized with the cell binding motif RGD, called BioSilk. BioSilk can self-assemble into microfibers thus forming BioSilk-foams that promote cell adherence and cluster formation of pancreatic aggregates during maturation.</p><p>The aim of this project is to investigate how conventional cryopreservation methods affect the preservation of pancreatic aggregates for further use in transplantation in diabetic patients. After culturing and differentiation of hESCs into pancreatic aggregates, they will be incorporated into BioSilk-foams for further maturation. The cell count, viability, and functionality of the pancreatic aggregates will be compared before and after cryopreservation, with and without the support of BioSilk.</p><p>The outcomes of this project indicate that the BioSilk is able to maintain the viability, cell count and insulin-expression of pancreatic aggregates after cryopreservation, either equal or surpassing free aggregates. Additional repeats of the experiments would be needed to further validate the results. Nevertheless, BioSilk displays a significant potential in preserving theviability and function of pancreatic aggregates.</p>

corrected abstract:
<p>Type 1 diabetes is caused by autoimmune destruction of insulin-producing β-cells which results in insulin deficiency. An alternative treatment option to life-long injection of exogenous insulin is the transplantation of pancreatic aggregates generated from pluripotent human embryonic stem cells (hESCs). Such insulin-secreting islet-like clusters can be supported by a 3D scaffold made of recombinant spider silk functionalized with the cell binding motif RGD, called BioSilk. BioSilk can self-assemble into microfibers thus forming BioSilk-foams that promote cell adherence and cluster formation of pancreatic aggregates during maturation.</p><p>The aim of this project is to investigate how conventional cryopreservation methods affect the preservation of pancreatic aggregates for further use in transplantation in diabetic patients. After culturing and differentiation of hESCs into pancreatic aggregates, they will be incorporated into BioSilk-foams for further maturation. The cell count, viability, and functionality of the pancreatic aggregates will be compared before and after cryopreservation, with and without the support of BioSilk.</p><p>The outcomes of this project indicate that the BioSilk is able to maintain the viability, cell count and insulin-expression of pancreatic aggregates after cryopreservation, either equal or surpassing free aggregates. Additional repeats of the experiments would be needed to further validate the results. Nevertheless, BioSilk displays a significant potential in preserving the viability and function of pancreatic aggregates.</p>
----------------------------------------------------------------------
In diva2:1518885 abstract is: <p>There are many ways to produce energy, using e.g. gas or hydro turbines. To guarantee a stable power output, it is important to consider components that could control and adjust the output power automatically.</p><p>The intention of this thesis work is to carry out a pre-study and system design of a mobileplatform simulator system that could be used by companies like Siemens and help them to reduce their OPEX (Operational expenditure) and easily evaluate their AVR (AutomaticVoltage Regulator) solutions and test improvements. In this document, Siemens has decided to call the simulator system, MPSS (Mobile Platform Simulator System).</p><p>The pre-study includes the theory behind energy production, synchronous generator, simulator system, AVR, control systems and electrical grid. Furthermore, the pre-study includes selection of the proposed components for the simulator system and design of the complete simulator system that will be built by the Siemens R&amp;D engineers at a later stage.</p><p>The Mobile Platform Simulator System (MPSS) is intended to test the AVR performance, which is a component with its prime purpose being to maintain the output voltage values from the generator at a fixed value, regardless of the current being drawn by the load. It is important that these output values are constantly regulated during the process of producing electricity, so that problems such as overvoltage, overcurrent etc. can be prevented.</p><p>The MPSS will also be able to simulate real working scenarios e.g. from the different components of an energy production system, such as gas and hydro turbine, synchronous generator, AVR, electrical grid and serve for personnel training.</p><p>The MPSS will consist of three main components; Simulator, AVR and control system. Therefore, the report will initially provide the background and general theory behind the synchronous generator, AVR and control system used in power generation systems. General information about the electrical grid is also provided. Furthermore, the report suggests the best possible choice for the necessary components to build a MPSS as well instructions on how to perform event simulation. The necessary documentation, including a circuit diagram to support the building of the MPSS by the R&amp;D engineers at late stage, is also provided.</p><p>Finally, the general analysis of the technical and non-technical aspects related to the choice of components, work process, method and result are discussed in the end of this report.</p>

corrected abstract:
<p>There are many ways to produce energy, using e.g. gas or hydro turbines. To guarantee a stable power output, it is important to consider components that could control and adjust the output power automatically.</p><p>The intention of this thesis work is to carry out a pre-study and system design of a mobile platform simulator system that could be used by companies like Siemens and help them to reduce their OPEX (Operational expenditure) and easily evaluate their AVR (Automatic Voltage Regulator) solutions and test improvements. In this document, Siemens has decided to call the simulator system, MPSS (Mobile Platform Simulator System).</p><p>The pre-study includes the theory behind energy production, synchronous generator, simulator system, AVR, control systems and electrical grid. Furthermore, the pre-study includes selection of the proposed components for the simulator system and design of the complete simulator system that will be built by the Siemens R&amp;D engineers at a later stage.</p><p>The Mobile Platform Simulator System (MPSS) is intended to test the AVR performance, which is a component with its’ prime purpose being to maintain the output voltage values from the generator at a fixed value, regardless of the current being drawn by the load. It is important that these output values are constantly regulated during the process of producing electricity, so that problems such as overvoltage, overcurrent etc. can be prevented.</p><p>The MPSS will also be able to simulate real working scenarios e.g. from the different components of an energy production system, such as gas and hydro turbine, synchronous generator, AVR, electrical grid and serve for personnel training.</p><p>The MPSS will consist of three main components; Simulator, AVR and control system. Therefore, the report will initially provide the background and general theory behind the synchronous generator, AVR and control system used in power generation systems. General information about the electrical grid is also provided. Furthermore, the report suggests the best possible choice for the necessary components to build a MPSS as well instructions on how to perform event simulation. The necessary documentation, including a circuit diagram to support the building of the MPSS by the R&amp;D engineers at late stage, is also provided.</p><p>Finally, the general analysis of the technical and non-technical aspects related to the choice of components, work process, method and result are discussed in the end of this report.</p>
----------------------------------------------------------------------
In diva2:460507 abstract is: <p>A great interest has been revealed for the chemical processes taking place in Vasa wood, and several research disciplines are involved in the examination. Today it is known that the polysaccharides in the wood are severely degraded, but not much effort has been put on the conditions of lignin, which is the focus of this study. By means of thioacidolysis and solid state NMR spectroscopy, the condition of the lignin polymer in Vasa wood was evaluated. Samples were taken from interior and exterior timber parts of Vasa, waterlogged Vasa wood, fresh oak wood and from timbers of two other warships of the same age. The guaiacyl and syringyl lignin monomers from the thioacidolysis reaction were analysed with gas chromatography-mass spectrometry. CP/MAS13C NMR was performed directly on the wood samples. The results revealed that no severe degradation of the lignin polymer has occurred in the Vasa wood. Additionally, a study of the neutral sugar composition of the wood was performed, showing no depolymerisation of either cellulose or hemicelluloses in the Vasa samples compared to fresh oak.</p>


w='13C' val={'c': '<sup>13</sup>C', 's': 'diva2:460507', 'n': 'no full text'}

corrected abstract:
<p>A great interest has been revealed for the chemical processes taking place in Vasa wood, and several research disciplines are involved in the examination. Today it is known that the polysaccharides in the wood are severely degraded, but not much effort has been put on the conditions of lignin, which is the focus of this study. By means of thioacidolysis and solid state NMR spectroscopy, the condition of the lignin polymer in Vasa wood was evaluated. Samples were taken from interior and exterior timber parts of Vasa, water logged Vasa wood, fresh oak wood and from timbers of two other warships of the same age. The guaiacyl and syringyl lignin monomers from the thioacidolysis reaction were analysed with gas chromatography-mass spectrometry. CP/MAS <sup>13</sup>C NMR was performed directly on the wood samples. The results revealed that no severe degradation of the lignin polymer has occurred in the Vasa wood. Additionally, a study of the neutral sugar composition of the wood was performed, showing no depolymeris ation of either cellulose or hemicelluloses in the Vasa samples compared to fresh oak.</p>
----------------------------------------------------------------------
In diva2:1765157 abstract is: <p>A changing climate, changing laws and an increased environmental consciousnesshas forced the transport sector to transition to electric power. Batteries and electric motors have seen a quick and powerful development which means that they are now an alternative even for heavy vehicles. A common problem with electric motors forvehicles is bearing currents. The bearing currents occur as a result of electrical discharges in the motor and can damage the bearings inside the motor. The purpose of motor bearings is to offload and reduce friction for the motor shaft. To prevent the issue and to see improvements or deteriorations from different preventativemeasures it is critical to be able to identify bearing currents from data.</p><p>This thesis analyzes relevant research in the area before introducing a method and an algorithm for detecting bearing currents in cooperation with Scania CV. The algorithm is composed of three different parameters which affects the identification in different ways. The tool was able to identify bearing currents from various data and found differences between the number of bearing currents between different test runs of the motor. However, more development of the tool and the possibility to process different kinds of data like voltages inside the motor is needed to be able to find better patterns in the data.</p>


corrected abstract:
<p>A changing climate, changing laws and an increased environmental consciousness has forced the transport sector to transition to electric power. Batteries and electric motors have seen a quick and powerful development which means that they are now an alternative even for heavy vehicles. A common problem with electric motors for vehicles is bearing currents. The bearing currents occur as a result of electrical discharges in the motor and can damage the bearings inside the motor. The purpose of motor bearings is to offload and reduce friction for the motor shaft. To prevent the issue and to see improvements or deteriorations from different preventative measures it is critical to be able to identify bearing currents from data.</p><p>This thesis analyzes relevant research in the area before introducing a method and an algorithm for detecting bearing currents in cooperation with Scania CV. The algorithm is composed of three different parameters which affects the identification in different ways. The tool was able to identify bearing currents from various data and found differences between the number of bearing currents between different test runs of the motor. However, more development of the tool and the possibility to process different kinds of data like voltages inside the motor is needed to be able to find better patterns in the data.</p>
----------------------------------------------------------------------
In diva2:777901 abstract is: <p>Hydrogen is considered as one of the most important fuels for the near future. Hydrogen is currently being used as a feed gas in fuel cells. It can be produced by a reformate process from fossil sources. The steam reforming is the main technology used.  Hydrogen from reforming may contain traces of hydrocarbons besides other products such as                         ,  and  contaminants. The presence of these impurities may have a negative effect on the catalyst of the fuel cell. As a result of this, the lifetime and the performance of the fuel cell decrease. In this study, the influence of propene on the PEM Fuel Cell performance isinvestigated. The study is focused on the adsorption and deactivation phenomena of low concentrations of contaminant on a Pt/C catalyst. In the experiments, cyclic voltammetry and in-line mass spectrometry to analyze the outlet gas are employed. Different adsorption potentials (0.12, 0.15, 0.20, 0.25, 0.30, 0.35 and 0.40 V), temperatures (  and ) and relative humidities ( and <em>)</em> are tested. Thereby, the influence of adsorption potential, temperature and humidity on the fuel cell is discussed. The results show that the adsorption of propene is highly potential dependent and is also influenced by the temperature and the relative humidity by shifting oxidation peaks and decreasing the charge density of oxidation, respectively.  Propene cannot displace hydrogen already adsorbed. Moreover, at  and 90%RH,  is the main oxidation product. In addition, some partially oxidized products may be formed. During the reduction of propene, propane was the main product released. It is also possible that some methane is released as well. Using a small quantity of hydrogen with argon as the carrier gas for impurities resulted in a decrease of the adsorbed species formed on the catalyst.</p>

Note the likely missing text is "carbon dioxide (CO<sub>2</sub>)". The process can also produce carbon monoxide (CO). Acetone (CH<sub>3</sub>)<sub>2</sub>CO) is the likely missing chemical. There is a lot of material missing from this abstract.

corrected abstract:
<p>Hydrogen is considered as one of the most important fuels for the near future. Hydrogen is currently being used as a feed gas in fuel cells. It can be produced by a reformate process from fossil sources. The steam reforming is the main technology used.  Hydrogen from reforming may contain traces of hydrocarbons besides other products such as [missing text], and contaminants. The presence of these impurities may have a negative effect on the catalyst of the fuel cell. As a result of this, the lifetime and the performance of the fuel cell decrease. In this study, the influence of propene on the PEM Fuel Cell performance is investigated. The study is focused on the adsorption and deactivation phenomena of low concentrations of contaminant on a Pt/C catalyst. In the experiments, cyclic voltammetry and in-line mass spectrometry to analyze the outlet gas are employed. Different adsorption potentials (0.12, 0.15, 0.20, 0.25, 0.30, 0.35 and 0.40 V), temperatures ( [missing temperature] and [missing temperature] ) and relative humidities ( [missing relative humidty] and [missing relative humidty]) are tested. Thereby, the influence of adsorption potential, temperature and humidity on the fuel cell is discussed. The results show that the adsorption of propene is highly potential dependent and is also influenced by the temperature and the relative humidity by shifting oxidation peaks and decreasing the charge density of oxidation, respectively.  Propene cannot displace hydrogen already adsorbed. Moreover, at  [missing relative humidty] and 90%RH, [missing chemical] is the main oxidation product. In addition, some partially oxidized products may be formed. During the reduction of propene, propane was the main product released. It is also possible that some methane is released as well. Using a small quantity of hydrogen with argon as the carrier gas for impurities resulted in a decrease of the adsorbed species formed on the catalyst.</p>
----------------------------------------------------------------------
In diva2:1765359 abstract is: <p>Algorithms have been used in finance since the early 2000s and accounted for 25% of the market around 2005. In this research, algorithms account for approximately 85% of the market. The challenge faced by many investors and fund managers is beating the Swedish market index OMXS30. This research investigates publicly available algorithms and their potential for implementation and modification to outperform the market. There is a lot of research done on the subject and most of the research found was mostly at a high academic level. Although few algorithms were found in the search, some algorithms that managed to beat other markets caught interest. The market data for this research was obtained from Nordnets closed API, specifically the historical price data of various financial securities. The algorithms use the historical price data to generate buy and sell signals which represents a trade. These trades were then used to calculate performance metrics such as the geometric mean and the sharpe ratio. The performance metrics are used to measure and compare performance with the OMXS30 using a quantitative method. On average, the algorithms did not perform well on the chosen securities, although some securities stood out in all cases. Beating the market is considered a difficult task, and this research reflects some of the challenges involved. The chosen method highlights the importance of the stocks the algorithms trade, emphasizing that stocks cannot be chosen randomly. Building a fully automated unsupervised trading system is challenging and requires extensive work. Some strategies tend to require human supervision to maximize returns and limit losses, while others yield low returns for low risk.</p>

w='sharpe' val={'c': 'Sharpe', 's': 'diva2:1765359', 'n': 'error in original'}

corrected abstract:
<p>Algorithms have been used in finance since the early 2000s and accounted for 25% of the market around 2005. In this research, algorithms account for approximately 85% of the market. The challenge faced by many investors and fund managers is beating the Swedish market index OMXS30. This research investigates publicly available algorithms and their potential for implementation and modification to outperform the market. There is a lot of research done on the subject and most of the research found was mostly at a high academic level. Although few algorithms were found in the search, some algorithms that managed to beat other markets caught interest. The market data for this research was obtained from Nordnets closed API, specifically the historical price data of various financial securities. The algorithms use the historical price data to generate buy and sell signals which represents a trade. These trades were then used to calculate performance metrics such as the geometric mean and the sharpe ratio. The performance metrics are used to measure and compare performance with the OMXS30 using a quantitative method. On average, the algorithms did not perform well on the chosen securities, although some securities stood out in all cases. Beating the market is considered a difficult task, and this research reflects some of the challenges involved. The chosen method highlights the importance of the stocks the algorithms trade, emphasizing that stocks cannot be chosen randomly. Building a fully automated unsupervised trading system is challenging and requires extensive work. Some strategies tend to require human supervision to maximize returns and limit losses, while others yield low returns for low risk.</p>
----------------------------------------------------------------------
In diva2:1889371 abstract is: <p>With the global population on the rise, the demand for energy is escalating. Unfortunately, much of this energy has historically been derived from fossil fuels, resulting in significant emissions. These emissions, predominantly nitrogen oxides (NOx) and Greenhouse Gases (GHG) such as CO2, pose serious threats to both human health and the environment. Nitrogen oxides act as primary precursors to tropospheric ozone, impacting respiratory health and reproductive cycles of flora. Meanwhile, GHGs contribute to the greenhouse effect, trapping heat in the atmosphere and driving global temperature rise. Among the sectors contributing substantially to GHG emissions is transportation, prompting increasingly stringent regulations on emissions. To meet these regulatory demands and mitigate environmental impact, the adoption of Zero Emission Vehicles (ZEVs) has emerged as a critical solution.</p><p>ZEVs, characterized by their absence of harmful emissions, present promising alternatives to traditional combustion engine vehicles. Leading contenders include electric vehicles (EVs), fuel cell vehicles (FCVs), and hydrogen internal combustion engine vehicles. While EVs, primarily powered by lithium-ion batteries, have seen significant advancement, concerns linger regarding battery recycling, resource extraction practices, and infrastructure limitations.</p><p>Hydrogen, obtained through electrolysis using renewable energy sources, seems an attractive fuel for ZEVs. Fuel cells, leveraging hydrogen's energy to propel vehicles, produce only water as a byproduct through a membrane-based battery system. Despite their appeal, the manufacturing cost of fuel cells, attributed to catalysts and membranes, presents a barrier to widespread adoption, paralleling challenges encountered with electric batteries.</p><p>An alternative approach involves retrofitting existing combustion engines to run on hydrogen fuel, yielding water as the sole emission. While this strategy capitalizes on the efficiency of traditional engines without modification, it encounters an obstacle during cold starts: hydrogen's higher autoignition temperature results in an ignition delay. To solve this issue, an approach is to blend 5% diesel into the fuel mixture. Nonetheless, this compromise disqualifies the vehicle from ZEV classification. Hence, a viable solution involves integrating a catalyst before the combustion chamber. This catalyst facilitates an exothermic reaction by introducing a small quantity of hydrogen, thereby raising the oxygen temperature to expedite ignition</p><p>However, this approach bring some complexities. Challenges include pressure drop issues at full engine load, the cost of catalyst materials, in this case platinum (Pt), and ensuring sufficient temperature for hydrogen autoignition within the combustion chamber. To address these intricacies, this project aims to develop a simulation model using Computational Fluid Dynamics (CFD) to optimize conditions and assess feasibility, employing the Converge software platform.</p><p>To achieve this goal, the catalyst's single channel is initially modeled using surface reactions. The process commences with several preliminary investigations to optimize outcomes. A mesh study is conducted, along with a diffusion analysis, recognizing that at lower temperatures, diffusion's impact is minimal. The chosen mesh size is 3 mm, balancing simulation efficiency with result accuracy, as smaller sizes do not significantly enhance quality or quantity of outcomes. Diffusion settings are established as Mixed, encompassing Knudsen and Molecular models, ensuring versatility across temperature ranges while enhancing accuracy. Laminar flow is confirmed, with Reynolds numbers below 4000.</p><p>With the model configured, a sensitivity analysis is performed to identify critical parameters. Studied parameters include air: fuel ratio (λ), inlet temperature (K), channel length (mm), and cross sectional area (mm2). Additionally, a factorial design is executed to validate results and facilitate data interpretation. Promising results emerge, showing that cold starts achieve adequate temperatures without significant pressure drop issues that could affect fuel efficiency. The air: fuel ratio raises as the most influential parameter in temperature increase. However, considering hydrogen's cost implications, it's imperative to maintain ratios at reasonable levels. Channel length follows as the second most impactful parameter, directly influencing residence time, it is also seen that at a certain length the needed residence time is reached and temperature increase is not as noticeable, making length a critical design consideration. The cross-sectional area of the channel emerges as another crucial factor, influencing fluid-catalyst contact and reaction efficiency.</p><p>Various cold start temperatures ranging from -40 to 0 ℃ are evaluated, with results indicating thermal stability from -30 ℃ onward, being the differences more pronounced from -40 ℃. At full load operation, pressure drop is observed, needing assessment for potential bypass requirements. In instances of pressure drop, both channel length and cross-sectional area play pivotal roles, directly impacting friction losses. Striking a balance between cold start temperature difference and pressure drop at full load is imperative, a consideration thoroughly explored and discussed within the project.</p><p>The results shown in this project, combined with further studies to contribute to broaden the knowledge and scope of the project, demonstrate that the use of a catalyst improves the engine performance, bringing the use of hydrogen as fuel in internal combustion engines closer to a reality.</p>

corrected abstract:
<p>With the global population on the rise, the demand for energy is escalating. Unfortunately, much of this energy has historically been derived from fossil fuels, resulting in significant emissions. These emissions, predominantly nitrogen oxides (NOx) and Greenhouse Gases (GHG) such as CO2, pose serious threats to both human health and the environment. Nitrogen oxides act as primary precursors to tropospheric ozone, impacting respiratory health and reproductive cycles of flora. Meanwhile, GHGs contribute to the greenhouse effect, trapping heat in the atmosphere and driving global temperature rise. Among the sectors contributing substantially to GHG emissions is transportation, prompting increasingly stringent regulations on emissions. To meet these regulatory demands and mitigate environmental impact, the adoption of Zero Emission Vehicles (ZEVs) has emerged as a critical solution.</p><p>ZEVs, characterized by their absence of harmful emissions, present promising alternatives to traditional combustion engine vehicles. Leading contenders include electric vehicles (EVs), fuel cell vehicles (FCVs), and hydrogen internal combustion engine vehicles. While EVs, primarily powered by lithium-ion batteries, have seen significant advancement, concerns linger regarding battery recycling, resource extraction practices, and infrastructure limitations.</p><p>Hydrogen, obtained through electrolysis using renewable energy sources, seems an attractive fuel for ZEVs. Fuel cells, leveraging hydrogen's energy to propel vehicles, produce only water as a byproduct through a membrane-based battery system. Despite their appeal, the manufacturing cost of fuel cells, attributed to catalysts and membranes, presents a barrier to widespread adoption, paralleling challenges encountered with electric batteries.</p><p>An alternative approach involves retrofitting existing combustion engines to run on hydrogen fuel, yielding water as the sole emission. While this strategy capitalizes on the efficiency of traditional engines without modification, it encounters an obstacle during cold starts: hydrogen's higher autoignition temperature results in an ignition delay. To solve this issue, an approach is to blend 5% diesel into the fuel mixture. Nonetheless, this compromise disqualifies the vehicle from ZEV classification. Hence, a viable solution involves integrating a catalyst before the combustion chamber. This catalyst facilitates an exothermic reaction by introducing a small quantity of hydrogen, thereby raising the oxygen temperature to expedite ignition</p><p>However, this approach bring some complexities. Challenges include pressure drop issues at full engine load, the cost of catalyst materials, in this case platinum (Pt), and ensuring sufficient temperature for hydrogen autoignition within the combustion chamber. To address these intricacies, this project aims to develop a simulation model using Computational Fluid Dynamics (CFD) to optimize conditions and assess feasibility, employing the Converge software platform.</p><p>To achieve this goal, the catalyst's single channel is initially modeled using surface reactions. The process commences with several preliminary investigations to optimize outcomes. A mesh study is conducted, along with a diffusion analysis, recognizing that at lower temperatures, diffusion's impact is minimal. The chosen mesh size is 3 mm, balancing simulation efficiency with result accuracy, as smaller sizes do not significantly enhance quality or quantity of outcomes. Diffusion settings are established as Mixed, encompassing Knudsen and Molecular models, ensuring versatility across temperature ranges while enhancing accuracy. Laminar flow is confirmed, with Reynolds numbers below 4000.</p><p>With the model configured, a sensitivity analysis is performed to identify critical parameters. Studied parameters include air: fuel ratio (λ), inlet temperature (K), channel length (mm), and cross sectional area (mm<sup>2</sup>). Additionally, a factorial design is executed to validate results and facilitate data interpretation. Promising results emerge, showing that cold starts achieve adequate temperatures without significant pressure drop issues that could affect fuel efficiency. The air: fuel ratio raises as the most influential parameter in temperature increase. However, considering hydrogen's cost implications, it's imperative to maintain ratios at reasonable levels. Channel length follows as the second most impactful parameter, directly influencing residence time, it is also seen that at a certain length the needed residence time is reached and temperature increase is not as noticeable, making length a critical design consideration. The cross-sectional area of the channel emerges as another crucial factor, influencing fluid-catalyst contact and reaction efficiency.</p><p>Various cold start temperatures ranging from -40 to 0 ℃ are evaluated, with results indicating thermal stability from -30 ℃ onward, being the differences more pronounced from -40 ℃. At full load operation, pressure drop is observed, needing assessment for potential bypass requirements. In instances of pressure drop, both channel length and cross-sectional area play pivotal roles, directly impacting friction losses. Striking a balance between cold start temperature difference and pressure drop at full load is imperative, a consideration thoroughly explored and discussed within the project.</p><p>The results shown in this project, combined with further studies to contribute to broaden the knowledge and scope of the project, demonstrate that the use of a catalyst improves the engine performance, bringing the use of hydrogen as fuel in internal combustion engines closer to a reality.</p>
----------------------------------------------------------------------
diva2:1571941 no changes, no full text
----------------------------------------------------------------------
In diva2:1837095 abstract is: <p>Mental illnesses are the leading cause of disability in the world today, affecting nearly a billion people including 14% of the world’s adolescents. Mental illnesses include both psychiatric conditions and inconveniences and is a broad term to describe multiple different conditions in varying severity. Mental healthcare has become one of the most central focuses for the Swedish healthcare system and the need to further explore the mental healthcare system and the effectiveness of it is highly important. System dynamics and simulation modeling is a great tool in understanding the system and its behavior. Therefore, the aim of this thesis is development of a working system dynamic simulation model, that will show how the public healthcare system provides mental healthcare for depression and anxiety in Stockholm to adults. The system should be represented with relevant parameters of the real world to accurately simulate the mental healthcare infrastructure.</p><p> </p><p>A literature review was conducted to gain an understanding of the structure of the system, its challenges and behavior. The literature review also included gathering of data for the model. Due to privacy concerns of the patients and regulations some of the data was not obtainable throughout the process. The unobtainable data was estimated through information from the provided literature review. </p><p> </p><p>The system dynamic model was used to run various possible future scenarios. The scenarios were used to test different real-world situations and analyze the system output and behavior. The system was validated with historical data to estimate the accuracy of the model, and if the model output were in agreement with the real-world data. The system dynamic model was created in an iterative process including adding and removing parameters, stocks and flows throughout the process.</p><p> </p><p>However, to further validate the simulation model and increase the accuracy, further research needs to be performed. These include interviews with experts for further validation and data gathering of parameters that is not available in the public domain but will not violate privacy or ethics of patients. The result was a system dynamic model of the mental healthcare system and structure in Stockholm. The system dynamic model showed that it keeps up with the increasing demand for mental healthcare and that primary care was the department that was the most effective in treating its patient when having additional resources. </p>

Removed unnecessary empty paragraphs.
corrected abstract:
<p>Mental illnesses are the leading cause of disability in the world today, affecting nearly a billion people including 14% of the world’s adolescents. Mental illnesses include both psychiatric conditions and inconveniences and is a broad term to describe multiple different conditions in varying severity. Mental healthcare has become one of the most central focuses for the Swedish healthcare system and the need to further explore the mental healthcare system and the effectiveness of it is highly important. System dynamics and simulation modeling is a great tool in understanding the system and its behavior. Therefore, the aim of this thesis is development of a working system dynamic simulation model, that will show how the public healthcare system provides mental healthcare for depression and anxiety in Stockholm to adults. The system should be represented with relevant parameters of the real world to accurately simulate the mental healthcare infrastructure.</p><p>A literature review was conducted to gain an understanding of the structure of the system, its challenges and behavior. The literature review also included gathering of data for the model. Due to privacy concerns of the patients and regulations some of the data was not obtainable throughout the process. The unobtainable data was estimated through information from the provided literature review.</p><p>The system dynamic model was used to run various possible future scenarios. The scenarios were used to test different real-world situations and analyze the system output and behavior. The system was validated with historical data to estimate the accuracy of the model, and if the model output were in agreement with the real-world data. The system dynamic model was created in an iterative process including adding and removing parameters, stocks and flows throughout the process.</p><p>However, to further validate the simulation model and increase the accuracy, further research needs to be performed. These include interviews with experts for further validation and data gathering of parameters that is not available in the public domain but will not violate privacy or ethics of patients. The result was a system dynamic model of the mental healthcare system and structure in Stockholm. The system dynamic model showed that it keeps up with the increasing demand for mental healthcare and that primary care was the department that was the most effective in treating its patient when having additional resources.</p>
----------------------------------------------------------------------
diva2:1837826 no changes
----------------------------------------------------------------------
In diva2:1567932 abstract is: <p>Hybrid propellant rockets has not seen as much use as its liquid and solid propellant counterparts. The main reasons for this can be attributed to hybrid propellant rockets historically having lower regression rate and fuel density. However, the impact of these disadvantages have been diminished over the years as a result of increased research. This together with the safety and economic advantages of hybrid propellant rockets, the hybrid system have become a more competitive system to use. Hexamine basedsolid fuel grains have been formulated and evaluated with emphasis on enhancing pot-life and tensile proprieties. By including plasticisers and altering the NCO/OH ratio, a solid fuel grain was successfully produced, overcoming earlier encountered problems with short pot-life as well as having promising tensile properties.</p>

A single extra space is needed: "basedsolid" to "based solid"

corrected abstract:
<p>Hybrid propellant rockets has not seen as much use as its liquid and solid propellant counterparts. The main reasons for this can be attributed to hybrid propellant rockets historically having lower regression rate and fuel density. However, the impact of these disadvantages have been diminished over the years as a result of increased research. This together with the safety and economic advantages of hybrid propellant rockets, the hybrid system have become a more competitive system to use. Hexamine based solid fuel grains have been formulated and evaluated with emphasis on enhancing pot-life and tensile proprieties. By including plasticisers and altering the NCO/OH ratio, a solid fuel grain was successfully produced, overcoming earlier encountered problems with short pot-life as well as having promising tensile properties.</p>
----------------------------------------------------------------------
In diva2:777893 abstract is: <p>Knowledge on metal release processes from stainless steel powder, which can be potentially inhaled at occupational settings, is essential within the framework of human health and environmental risk assessments. An in-depth knowledge concerning powder history, physical properties of particles (e.g. size, morphology, and active surface area) combined with their chemical properties (such as the chemical composition of the particles and their metal release behavior) is needed for better understanding of the interaction mechanisms between metal powders and humans. So far, limited in vitro and in vivo studies exist that assess the correlation between stainless steel surface properties, protein adsorption effects, and metal release processes. The aim of this study is to add information to fill this knowledge gap through in vitro investigations of protein-induced metal release (iron, nickel, chromium, and manganese) and induced surface changes of five differently sized and/or produced (water-atomized (WA) and gas-atomized (GA)) stainless steel powder particles (three austenitic: AISI 316L, 310B, and 304B; one martensitic: AISI 410L; and one ferritic: AISI 430L) after exposure up to one week into a phosphate buffer saline (PBS) solution of pH 7.2-7.4 containing either lysozyme (LYS) or bovine serum albumin (BSA). The results show that the outmost surface oxide composition of the powders strongly depends on the production method and particle size. Gas-atomized 316L powder particles (with spherical shapes) indicated a high relative manganese content in their surface oxide (more significant in the case of 316L particles sized &lt;4µm), while no manganese compounds were detectable in the surface oxide of water-atomized powders (of irregular particle shapes). Although austenitic stainless steels should present non-magnetic properties, the investigation of magnetic properties indicated that differently sized gas-atomized 316L particles and water-atomized 304B were to some extent ferromagnetic suggesting the presence of ferrite. BSA induced a significant enrichment of chromium in the surface oxide of all investigated powders (especially for ferritic WA430L and austenitic WA316L), except in the case of 316L powders (&lt;4µm) showing no significant change. Metal release studies illustrated that both proteins enhanced the amount of released metal, with a preferential iron release from water-atomized particles and manganese release from gas-atomized powders. BSA-containing medium induced the highest extent of metal release in comparison with other tested biological media (up to 35-fold increase in the case of ferritic 430L particles produced by water atomization). Comparison between the metal release behavior of particulate and massive stainless steel indicated a significantly higher extent of metal released from abraded stainless steel sheets compared with particles, which is most probably an effect of freshly abraded surfaces of the massive metal sheets, not true for the particles with aged surface oxides, along with the presence of higher relative chromium content in the surface oxide.</p>

There was a missing hypen in "water-atomized".

corrected abstract:
<p>Knowledge on metal release processes from stainless steel powder, which can be potentially inhaled at occupational settings, is essential within the framework of human health and environmental risk assessments. An in-depth knowledge concerning powder history, physical properties of particles (e.g. size, morphology, and active surface area) combined with their chemical properties (such as the chemical composition of the particles and their metal release behavior) is needed for better understanding of the interaction mechanisms between metal powders and humans. So far, limited in vitro and in vivo studies exist that assess the correlation between stainless steel surface properties, protein adsorption effects, and metal release processes. The aim of this study is to add information to fill this knowledge gap through in vitro investigations of protein-induced metal release (iron, nickel, chromium, and manganese) and induced surface changes of five differently sized and/or produced (water-atomized (WA) and gas-atomized (GA)) stainless steel powder particles (three austenitic: AISI 316L, 310B, and 304B; one martensitic: AISI 410L; and one ferritic: AISI 430L) after exposure up to one week into a phosphate buffer saline (PBS) solution of pH 7.2-7.4 containing either lysozyme (LYS) or bovine serum albumin (BSA). The results show that the outmost surface oxide composition of the powders strongly depends on the production method and particle size. Gas-atomized 316L powder particles (with spherical shapes) indicated a high relative manganese content in their surface oxide (more significant in the case of 316L particles sized &lt;4µm), while no manganese compounds were detectable in the surface oxide of water-atomized powders (of irregular particle shapes). Although austenitic stainless steels should present non-magnetic properties, the investigation of magnetic properties indicated that differently sized gas-atomized 316L particles and water-atomized 304B were to some extent ferromagnetic suggesting the presence of ferrite. BSA induced a significant enrichment of chromium in the surface oxide of all investigated powders (especially for ferritic WA430L and austenitic WA316L), except in the case of 316L powders (&lt;4µm) showing no significant change. Metal release studies illustrated that both proteins enhanced the amount of released metal, with a preferential iron release from water-atomized particles and manganese release from gas-atomized powders. BSA-containing medium induced the highest extent of metal release in comparison with other tested biological media (up to 35-fold increase in the case of ferritic 430L particles produced by water atomization). Comparison between the metal release behavior of particulate and massive stainless steel indicated a significantly higher extent of metal released from abraded stainless steel sheets compared with particles, which is most probably an effect of freshly abraded surfaces of the massive metal sheets, not true for the particles with aged surface oxides, along with the presence of higher relative chromium content in the surface oxide.</p>
----------------------------------------------------------------------
In diva2:744727 abstract is: <p>This project investigates the interaction between the Death-domain asssociated protein 6 (Daxx) and the chromatin remodeler ATRX. These proteins form an important tumor suppressor somplex that specifically inhibits the homologous recombination-based Alternative Lengthening of Telomeres (ALT) pathway of tumorgenesis. It has been reported that a small, globular, domain of Daxx (the Daxx helical bundle, or DHB) interacts with ATRX, however, the interacting region/ s of ATRXare poorly defined. The aim of thos project was to identify ATRX-derived peptide sequences that bind to the DHB domain of Daxx, as well as prepare for future experiments aiming to repeat this experiment with more comprehensive Daxx xonstructs which include the hostine binding domain, HBDm in complex with a H4/H3.3 heterpdimer. To this end. a Daxx construct corresponding to the DHB domain (Daxx<sup>46-144</sup>) and histones H4 and H3.3 were expressed in <em>Escherichia coli</em> and purified. A randomized phage peptide library and phage display was then employed to generate a consensus sequence of peptides with hifh affinity for the Daxx DHB domain, to be aligned- and compared to the full-lenth sequence of ATRX in an attempt to identify the ATRX region/s housing the Daxx interaction.</p><p>Several high-affinity peptides were obtained with this approach; however, these were too few or disparate to generate a comprehensive consensus sequence. Instead, individual peptides were aligned- and compared to the full-length ATRX, yielding one candidate sequence, D10, which partly corresponds to a conserved motif in the c-terminal helicase domain of ATRX. This sequence may indicate the site of the Daxx interaction; however, more research is required tp investigate the significance of the peptide.</p>

w='asssociated' val={'c': 'associated', 's': 'diva2:744727'}
w='full-lenth' val={'c': 'full-length', 's': 'diva2:744727', 'n': 'no full text'}
w='heterpdimer' val={'c': 'heterdimer', 's': 'diva2:744727', 'n': 'no full text'}
w='hifh' val={'c': 'high', 's': 'diva2:744727', 'n': 'no full text'}
w='somplex' val={'c': 'complex', 's': 'diva2:744727', 'n': 'no full text'}
w='thos' val={'c': 'this', 's': 'diva2:744727', 'n': 'no full text'}
w='xonstructs' val={'c': 'constructs', 's': 'diva2:744727', 'n': 'no full text'}

corrected abstract:
<p>This project investigates the interaction between the Death-domain associated protein 6 (Daxx) and the chromatin remodeler ATRX. These proteins form an important tumor suppressor complex that specifically inhibits the homologous recombination-based Alternative Lengthening of Telomeres (ALT) pathway of tumorgenesis. It has been reported that a small, globular, domain of Daxx (the Daxx helical bundle, or DHB) interacts with ATRX, however, the interacting region/ s of ATRX are poorly defined. The aim of this project was to identify ATRX-derived peptide sequences that bind to the DHB domain of Daxx, as well as prepare for future experiments aiming to repeat this experiment with more comprehensive Daxx constructs which include the hostine binding domain, HBDm in complex with a H4/H3.3 heterdimer. To this end. a Daxx construct corresponding to the DHB domain (Daxx<sup>46-144</sup>) and histones H4 and H3.3 were expressed in <em>Escherichia coli</em> and purified. A randomized phage peptide library and phage display was then employed to generate a consensus sequence of peptides with high affinity for the Daxx DHB domain, to be aligned- and compared to the full-length sequence of ATRX in an attempt to identify the ATRX region/s housing the Daxx interaction.</p><p>Several high-affinity peptides were obtained with this approach; however, these were too few or disparate to generate a comprehensive consensus sequence. Instead, individual peptides were aligned- and compared to the full-length ATRX, yielding one candidate sequence, D10, which partly corresponds to a conserved motif in the c-terminal helicase domain of ATRX. This sequence may indicate the site of the Daxx interaction; however, more research is required tp investigate the significance of the peptide.</p>
----------------------------------------------------------------------
In diva2:1134489 abstract is: <p>Background:</p><p>Chemical exposure due to historical treatments with insecticides is of interest to a Swedish museum in Stockholm. In zoological collections skins have been treated with cancerogenic arsenic compounds, while herbs in botanical collections were treated with mercury dichloride (sublimate). The latter may still evaporate reprotoxic mercury.</p><p>Aim:</p><p>The aim of this project is to survey the risks of handling object at the museum, and investigate if the risk vary depending on the type of objects as well as the preparation date of the objects.</p><p>Methods:</p><p>A handheld XRF instrument was used to test 54 dried herbs and mosses, as well as 40 specimens of vertebrates, for arsenic and mercury. Even 16 insect boxes, and dust from the floors were tested. The objects represented the 18th-21th century. Measures were suggested after considering normal handling of the objects (exposure and duration of the exposure), the risk of suffering and the consequence from being exposed to chemicals.</p><p>Results:</p><p>91% of the 54 tested herbs contained mercury. The herbs contained 700 ppm mercury. On average, the sheets of paper that the herbs are fastened on contained 1/10 of the level in the herbs. The herbs from the 19th-20th century contained most mercury. All tested bird objects contained arsenic in the feathers, even those representing the 21st century, with a peak in treatment in the 19th century. On average, the feathers contained 0.7% of arsenic. The fur of the vertebrates contained 0.5%. The treatment of rodents and other mammals peaked in the 18th century. Tested dust from the collections showed similar results to the content of the objects. Lead was also found, mostly in the zoological collections.</p><p>Discussion:</p><p>Herbs treated with mercury dichloride release reprotoxic mercury vapour, but the theoretical concentrations in the air is assumed to be below 1% of the occupational limit values, and the exposure is assumed to be limited. Pregnant and breastfeeding women are considered to be an extra vulnerable group. Arsenic compounds on feathers, furs and skins have acute toxic and cancerogenic properties. By approximation of exposure (probability) and toxic health consequences of the compounds, it was concluded that precautionary measures need to be taken at the museum. The lead with reprotoxic properties that was found in dust needs an extra risk assessment.</p><p>Conclusion:</p><p>Precautionary technical measures are needed to protect against exposure to cancerogenic and reprotoxic chemicals. Measures should be taken at organisational, as well as on individual level.</p>

Note missing the bold face of headings and unnecessary new paragraphs after headings.

corrected abstract:
<p><strong>Background:</strong> Chemical exposure due to historical treatments with insecticides is of interest to a Swedish museum in Stockholm. In zoological collections skins have been treated with cancerogenic arsenic compounds, while herbs in botanical collections were treated with mercury dichloride (sublimate). The latter may still evaporate reprotoxic mercury.</p><p><strong>Aim:</strong> The aim of this project is to survey the risks of handling object at the museum, and investigate if the risk vary depending on the type of objects as well as the preparation date of the objects.</p><p><strong>Methods:</strong> A handheld XRF instrument was used to test 54 dried herbs and mosses, as well as 40 specimens of vertebrates, for arsenic and mercury. Even 16 insect boxes, and dust from the floors were tested. The objects represented the 18th-21th century. Measures were suggested after considering normal handling of the objects (exposure and duration of the exposure), the risk of suffering and the consequence from being exposed to chemicals.</p><p><strong>Results:</strong> 91% of the 54 tested herbs contained mercury. The herbs contained 700 ppm mercury. On average, the sheets of paper that the herbs are fastened on contained 1/10 of the level in the herbs. The herbs from the 19th-20th century contained most mercury. All tested bird objects contained arsenic in the feathers, even those representing the 21st century, with a peak in treatment in the 19th century. On average, the feathers contained 0.7% of arsenic. The fur of the vertebrates contained 0.5%. The treatment of rodents and other mammals peaked in the 18th century. Tested dust from the collections showed similar results to the content of the objects. Lead was also found, mostly in the zoological collections.</p><p><strong>Discussion:</strong> Herbs treated with mercury dichloride release reprotoxic mercury vapour, but the theoretical concentrations in the air is assumed to be below 1% of the occupational limit values, and the exposure is assumed to be limited. Pregnant and breastfeeding women are considered to be an extra vulnerable group. Arsenic compounds on feathers, furs and skins have acute toxic and cancerogenic properties. By approximation of exposure (probability) and toxic health consequences of the compounds, it was concluded that precautionary measures need to be taken at the museum. The lead with reprotoxic properties that was found in dust needs an extra risk assessment.</p><p><strong>Conclusion:</strong> Precautionary technical measures are needed to protect against exposure to cancerogenic and reprotoxic chemicals. Measures should be taken at organisational, as well as on individual level.</p>
----------------------------------------------------------------------
diva2:1869647 no changes
----------------------------------------------------------------------
diva2:975179 no changes
----------------------------------------------------------------------
diva2:707469 no changes
----------------------------------------------------------------------
In diva2:1458436 abstract is: <p>With the current challenges for the healthcare such as increased demand for care, financial and resource constraints along with rapid changes and complexity there is high believe in digital innovation and digitalisation to efficacy resources and aid in delivering a safer, more accessible and patient centred valuable care.</p><p>There is a digitalisation that is ongoing, being used and implemented over several different areas of healthcare. Since healthcare can be seen as a complex adaptive system, there is a need to understand several agents. The aim is to gather more knowledge about perceptions within the physiotherapy staff and give recommendations and directions for improvements regarding digital innovation.</p><p>Opinions about digital innovation have been gathered with open interviews and a semisystematic literature review with focus on physiotherapy. Too find subjective data the mixed method Q methodology was applied.</p><p>The open interviews resulted in eight categories: digital innovation, digital innovation being used, digital innovation not used, management, obstacles, education, wishful thinking, applications and systems and associated opinions. The semi-systematic literature review showed on a rapid scientifically development, 25 articles was found and thematically analysed. 140 cited viewpoints and facts was merged with the results from the open interviews. Ten physiotherapists performed the q-sort consisting of 25 statements. Three factors were found. Interpreted as digital innovation optimism &amp; patient oriented, digital innovation scepticism &amp; management oriented and digital innovation sceptical optimism. Video-call technique is strongly encouraged by factor one contrary to factor two. Integrity is the major conflicting viewpoint between the factors. The result shows that gender can affect if a physiotherapist is either optimistic or sceptical to digital innovation. Using existing models such as UTAUT could improve acceptance about digital innovation. Education is perceived as important among all factors. Nine participants responded on baseline questions showing low knowledge of the term mHealth and little communication with IT departments.</p>

Note: simply missing the italics of the three groups of words.

corrected abstract:
<p>With the current challenges for the healthcare such as increased demand for care, financial and resource constraints along with rapid changes and complexity there is high believe in digital innovation and digitalisation to efficacy resources and aid in delivering a safer, more accessible and patient centred valuable care.</p><p>There is a digitalisation that is ongoing, being used and implemented over several different areas of healthcare. Since healthcare can be seen as a complex adaptive system, there is a need to understand several agents. The aim is to gather more knowledge about perceptions within the physiotherapy staff and give recommendations and directions for improvements regarding digital innovation.</p><p>Opinions about digital innovation have been gathered with open interviews and a semisystematic literature review with focus on physiotherapy. Too find subjective data the mixed method Q methodology was applied.</p><p>The open interviews resulted in eight categories: digital innovation, digital innovation being used, digital innovation not used, management, obstacles, education, wishful thinking, applications and systems and associated opinions. The semi-systematic literature review showed on a rapid scientifically development, 25 articles was found and thematically analysed. 140 cited viewpoints and facts was merged with the results from the open interviews. Ten physiotherapists performed the q-sort consisting of 25 statements. Three factors were found. Interpreted as digital innovation optimism &amp; patient oriented, digital innovation scepticism &amp; management oriented and digital innovation sceptical optimism. Video-call technique is strongly encouraged by factor one contrary to factor two. Integrity is the major conflicting viewpoint between the factors. The result shows that gender can affect if a physiotherapist is either optimistic or sceptical to digital innovation. Using existing models such as UTAUT could improve acceptance about digital innovation. Education is perceived as important among all factors. Nine participants responded on baseline questions showing low knowledge of the term mHealth and little communication with IT departments.</p>
----------------------------------------------------------------------
In diva2:1869802 abstract is: <p>Human motion analysis (HMA) can play a crucial role in sports and healthcare by providing unique insights on movement mechanics in the form of objective measurements and quantitative data. Traditional, state of the art, marker-based techniques, despite their accuracy, come with financial and logistical barriers, and are restricted to laboratory settings. Markerless systems offer much improved affordability and portability, and can potentially be used outside of laboratories. However, these advantages come with a significant cost in accuracy. This thesis attempts to address the challenge of democratizing HMA by leveraging recent advances in smartphone technology and machine learning.\newline\newlineThis thesis evaluates two modalities of performing markerless HMA: Single smartphone using Apple Arkit, and multiple smartphone setup using OpenCap, and compares both to a state of the art multiple-camera marker-based system from Vicon. Additionally, this thesis presents and evaluates two approaches to improving the single smartphone modality: Employing a Gaussian Process Model (GPR), and a Long-short-term-memory (LSTM) neural network to refine the single smartphone data to align with the marker-based result.</p><p>Specific movements were recorded simultaneously with all three modalities on 13 subjects to build a dataset. From this, GPR and LSTM models were trained and applied to refine the single camera modality data. Lower limb joint angles, and joint centers were evaluated across the different modalities, and analyzed for potential use in real-world applications. While the findings of this thesis are promising, as both the GPR and LSTM models improve the accuracy of Apple Arkit, and OpenCap providing accurate and consistent results. It is important to acknowledge limitations regarding demographic diversity and how real-world environmental factors may influence its application.</p><p>This thesis contributes to the efforts in narrowing the gap between marker-based HMA methods, and more accessible solutions.</p>

There was a "\newline\newlineThis" which should have been "</p><p>This"

corrected abstract:
<p>Human motion analysis (HMA) can play a crucial role in sports and healthcare by providing unique insights on movement mechanics in the form of objective measurements and quantitative data. Traditional, state of the art, marker-based techniques, despite their accuracy, come with financial and logistical barriers, and are restricted to laboratory settings. Markerless systems offer much improved affordability and portability, and can potentially be used outside of laboratories. However, these advantages come with a significant cost in accuracy. This thesis attempts to address the challenge of democratizing HMA by leveraging recent advances in smartphone technology and machine learning.</p><p>This thesis evaluates two modalities of performing markerless HMA: Single smartphone using Apple Arkit, and multiple smartphone setup using OpenCap, and compares both to a state of the art multiple-camera marker-based system from Vicon. Additionally, this thesis presents and evaluates two approaches to improving the single smartphone modality: Employing a Gaussian Process Model (GPR), and a Long-short-term-memory (LSTM) neural network to refine the single smartphone data to align with the marker-based result.</p><p>Specific movements were recorded simultaneously with all three modalities on 13 subjects to build a dataset. From this, GPR and LSTM models were trained and applied to refine the single camera modality data. Lower limb joint angles, and joint centers were evaluated across the different modalities, and analyzed for potential use in real-world applications. While the findings of this thesis are promising, as both the GPR and LSTM models improve the accuracy of Apple Arkit, and OpenCap providing accurate and consistent results. It is important to acknowledge limitations regarding demographic diversity and how real-world environmental factors may influence its application.</p><p>This thesis contributes to the efforts in narrowing the gap between marker-based HMA methods, and more accessible solutions.</p>
----------------------------------------------------------------------
In diva2:1520020 abstract is: <p>In this project, apple pomace from a Swedish cider making factory as a by-product was used as a raw material to extract valuable compounds. The extraction was focused on pectin and phenolic compounds with antioxidant activity. For the extraction procedures environmentally friendly processes were chosen without using any harsh chemicals. Phenolic compounds wereinitially extracted from the pomace using 50% aqueous ethanol and then the composition, total phenolic content and antioxidant activity were studied in these extracts. The main focus was on pectins, which were extracted by subcritical water at three different pH conditions (pH 3, 5 and 7) and two different temperatures (120°C and 140°C) in 5-, 10- and 15 minute sequences. Then the pectins were characterized in terms of extraction yield, sugar composition, molecular weight and antioxidant activity and the results were compared in terms of the effect of pH, temperatur eand extraction time. The gelling properties of the different pectins were also studied as a proof of concept in an empirical experiment, where highly viscous liquids were obtained at 5% pectinand 60% sucrose content. In summary, the extracted phenolic compounds have potential to function as naturally derived antioxidants in cosmetics and the pectin may be used as a rheology modifier in water-based formulations of low pH without any additional chemical modifications.</p>

w='temperatur' val={'c': 'temperature', 's': 'diva2:1520020'}
w='eand' val={'c': 'and', 's': 'diva2:1520020'}

corrected abstract:
<p>In this project, apple pomace from a Swedish cider making factory as a by-product was used as a raw material to extract valuable compounds. The extraction was focused on pectin and phenolic compounds with antioxidant activity. For the extraction procedures environmentally friendly processes were chosen without using any harsh chemicals. Phenolic compounds were initially extracted from the pomace using 50% aqueous ethanol and then the composition, total phenolic content and antioxidant activity were studied in these extracts. The main focus was on pectins, which were extracted by subcritical water at three different pH conditions (pH 3, 5 and 7) and two different temperatures (120°C and 140°C) in 5-, 10- and 15 minute sequences. Then the pectins were characterized in terms of extraction yield, sugar composition, molecular weight and antioxidant activity and the results were compared in terms of the effect of pH, temperature and extraction time. The gelling properties of the different pectins were also studied as a proof of concept in an empirical experiment, where highly viscous liquids were obtained at 5% pectin and 60% sucrose content. In summary, the extracted phenolic compounds have potential to function as naturally derived antioxidants in cosmetics and the pectin may be used as a rheology modifier in water-based formulations of low pH without any additional chemical modifications.</p>
----------------------------------------------------------------------
In diva2:1642415 abstract is: <p>Cancer is still one of the most common causes of death world-wide and in parallel there is a need to update the repertoire of therapies that withstand resistance of recurrent cancers. Since the introduction of antibody therapies as anti-cancer pharmaceuticals, recognized as immunotherapy in health care, it has been an increasing field in cancer therapy, as a more targeted treatment compared to chemotherapy. Despite the great success, immunotherapy rely on parenteral administration, partly due to poor tissue penetration. If the treatment is administered intravenously, specialized personnel is required, in addition to that it can be inconvenient for the patient. Also, pharmaceuticals based on antibodies often require costly production steps which yields a high-priced treatment.</p><p>To approach this problem, researchers have developed small affinity domains with the aim to increase tissue penetration while keeping a high specificity to its target. Albumin Binding Domain Derived Affinity Protein (ADAPT) is an example of a small affinity domain of only 7 kDa, which is based on albumin binding domain (ABD) from the streptococcal protein G. Recently, it was shown that the ADAPTs can be further engineered to bind albumin and another relevant target protein of interest simultaneously, which suggests a tolerable half-life in patient serum, alternative administration routes and lower production costs compared to antibody treatments. Furthermore, less side effects are expected due to higher specificity compared to chemotherapy.</p><p>This work presents the characterization of novel ADAPT proteins that the target the cancer relatedproteins C-C motif ligand 7 (CCL7), vascular endothelial growth factor A (VEGF-A) and carcinoembryonic antigen related cell adhesion molecule 5 (CEACAM5). The new constructs were produced recombinantly in <em>Escherichia coli</em> (<em>E. coli</em>) and purified using affinity chromatography. Moreover, the results demonstrate bispecific binding with high affinity towards serum albumin and CCL7 and CEACAM5 respectively, while the ADAPT variants targeting VEGF-A remain to be further developed. Lastly, the importance of different amino acids for structural and binding properties of one CEACAM5 binder are stated. It reveals that the target binding relies on hydrophobic interactions which also can be connected to its poor structural attributes. Accordingly, this project adds new insights about the ADAPTs which can be useful in research towards future clinical applications aimed to improve cancer treatments.</p>

Note: missing hypen in "cancer-related".

corrected abstract:
<p>Cancer is still one of the most common causes of death world-wide and in parallel there is a need to update the repertoire of therapies that withstand resistance of recurrent cancers. Since the introduction of antibody therapies as anti-cancer pharmaceuticals, recognized as immunotherapy in health care, it has been an increasing field in cancer therapy, as a more targeted treatment compared to chemotherapy. Despite the great success, immunotherapy rely on parenteral administration, partly due to poor tissue penetration. If the treatment is administered intravenously, specialized personnel is required, in addition to that it can be inconvenient for the patient. Also, pharmaceuticals based on antibodies often require costly production steps which yields a high-priced treatment.</p><p>To approach this problem, researchers have developed small affinity domains with the aim to increase tissue penetration while keeping a high specificity to its target. Albumin Binding Domain Derived Affinity Protein (ADAPT) is an example of a small affinity domain of only 7 kDa, which is based on albumin binding domain (ABD) from the streptococcal protein G. Recently, it was shown that the ADAPTs can be further engineered to bind albumin and another relevant target protein of interest simultaneously, which suggests a tolerable half-life in patient serum, alternative administration routes and lower production costs compared to antibody treatments. Furthermore, less side effects are expected due to higher specificity compared to chemotherapy.</p><p>This work presents the characterization of novel ADAPT proteins that the target the cancer-related proteins C-C motif ligand 7 (CCL7), vascular endothelial growth factor A (VEGF-A) and carcinoembryonic antigen related cell adhesion molecule 5 (CEACAM5). The new constructs were produced recombinantly in <em>Escherichia coli</em> (<em>E. coli</em>) and purified using affinity chromatography. Moreover, the results demonstrate bispecific binding with high affinity towards serum albumin and CCL7 and CEACAM5 respectively, while the ADAPT variants targeting VEGF-A remain to be further developed. Lastly, the importance of different amino acids for structural and binding properties of one CEACAM5 binder are stated. It reveals that the target binding relies on hydrophobic interactions which also can be connected to its poor structural attributes. Accordingly, this project adds new insights about the ADAPTs which can be useful in research towards future clinical applications aimed to improve cancer treatments.</p>
----------------------------------------------------------------------
diva2:1586220 no changes
----------------------------------------------------------------------
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
diva2:1869483 no changes
----------------------------------------------------------------------
In diva2:1261155 abstract is: <p>The strive for cutting out of a fossil-fuels dependence of countries' economies has driven the global research into developing more sustainable powersources with the ultimate target to completely liberate from coal-red and nuclear power plants. Shifting energy policies to renewable energy resourcesundoubtedly carries many benecial features, but also poses a range of technical challenges, such as the necessity to overcome large uctuations in the energy output from wind or photo-voltaic farms.One solution to this problem proposed in the 1970's is to integrate a large-scale energy storage device such as redox- ow batteries into the electrical grid. The thesis rst presents a brief overview of ow battery technology and applications. Next, a one-dimensional, steady-state, macrohomogeneous mathematical model of a hydrogen-bromine ow battery is developed, described and solved. The results are in good agreement with existing experimental data. Moreover, a parametric study is performed to examine the impact of selected parameters on the overall performance of a single cell. A complete set of eld variable plots is explicitly presented.</p>

w='benecial' val={'c': 'beneficial', 's': ['diva2:1261155', 'diva2:949994'], 'n': 'missing ligature'}
w='uctuations' val={'c': 'fluctuations', 's': 'diva2:1261155', 'n': 'missing ligature'}
w='redoxow' val={'c': 'redox flow', 's': 'diva2:1261155', 'n': 'missing ligature'}
There were also other missing ligratures and the paragraphs were merged.

corrected abstract:
<p>The strive for cutting out of a fossil-fuels dependence of countries’ economies has driven the global research into developing more sustainable power sources with the ultimate target to completely liberate from coal-fired and nuclear power plants. Shifting energy policies to renewable energy resources undoubtedly carries many beneficial features, but also poses a range of technical challenges, such as the necessity to overcome large fluctuations in the energy output from wind or photo-voltaic farms.</p><p>One solution to this problem proposed in the 1970’s is to integrate a large-scale energy storage device such as redox-flow batteries into the electrical grid. The thesis first presents a brief overview of flow battery technology and applications. Next, a one-dimensional, steady-state, macrohomogeneous mathematical model of a hydrogen-bromine flow battery is developed, described and solved. The results are in good agreement with existing experimental data. Moreover, a parametric study is performed to examine the impact of selected parameters on the overall performance of a single cell. A complete set of field variable plots is explicitly presented.</p>
----------------------------------------------------------------------
In diva2:1348113 abstract is: <p>The partial spider silk protein 4RepCT have been functionalized with a range of functional domains using a genetic fusion method. An important complementary method to genetic fusion is Sortase A coupling. The use of enzymatic coupling enables to add functions to silk that cannot be expressed efficiently as a fusion protein with 4RepCT. To increase the yield and the rate of the Sortase A coupling, previous studies suggest an increase in the number of glycine residues on the N-terminal of the nucleophile in the reaction. The aim of the degree project was to design, produce and analyze a new silk protein, G<sub>5</sub>-4RepCT, for improved Sortase A coupling. The silk protein was cloned, expressed in <em>E. coli </em>cells and purified with Immobilized metal affinity chromatography (IMAC). The IgG binding affinity domain Z was coupled to G<sub>5</sub>-4RepCT coatings and G<sub>5</sub>-4RepCT in solution. The amount of coupled product (Z-G<sub>5</sub>-4RepCT) was estimated on the coatings using an IgG-fluorophore. The amount and rate of product formation were analyzed in solution using SDS-PAGE. The G<sub>5</sub>-4RepCT silk protein was compared to wildtype silk (GP-4RepCT) and a silk protein with an interruptive proline in the N-terminal glycine sequence (GP-G<sub>3</sub>-4RepCT). The results show most coupled product for the G<sub>5</sub>-4RepCT silk coatings, and most product at the fastest rate when coupling to G<sub>5</sub>-4RepCT in solution. The findings correlate with previous studies and implies an improved Sortase A coupling with an increased number of N-terminal glycines on the 4RepCT silk.</p>

w='Z-G' val={'c': 'Z-G<sub>5</sub>-4RepCT', 's': 'diva2:1348113', 'n': 'no full text'}

corrected abstract:
<p>The partial spider silk protein 4RepCT have been functionalized with a range of functional domains using a genetic fusion method. An important complementary method to genetic fusion is Sortase A coupling. The use of enzymatic coupling enables to add functions to silk that cannot be expressed efficiently as a fusion protein with 4RepCT. To increase the yield and the rate of the Sortase A coupling, previous studies suggest an increase in the number of glycine residues on the N-terminal of the nucleophile in the reaction. The aim of the degree project was to design, produce and analyze a new silk protein, G<sub>5</sub>-4RepCT, for improved Sortase A coupling. The silk protein was cloned, expressed in <em>E. coli </em>cells and purified with Immobilized metal affinity chromatography (IMAC). The IgG binding affinity domain Z was coupled to G<sub>5</sub>-4RepCT coatings and G<sub>5</sub>-4RepCT in solution. The amount of coupled product (Z-G<sub>5</sub>-4RepCT<sub>5</sub>-4RepCT) was estimated on the coatings using an IgG-fluorophore. The amount and rate of product formation were analyzed in solution using SDS-PAGE. The G<sub>5</sub>-4RepCT silk protein was compared to wildtype silk (GP-4RepCT) and a silk protein with an interruptive proline in the N-terminal glycine sequence (GP-G<sub>3</sub>-4RepCT). The results show most coupled product for the G<sub>5</sub>-4RepCT silk coatings, and most product at the fastest rate when coupling to G<sub>5</sub>-4RepCT in solution. The findings correlate with previous studies and implies an improved Sortase A coupling with an increased number of N-terminal glycines on the 4RepCT silk.</p>
----------------------------------------------------------------------
In diva2:855926 abstract is: <p>The medical field is slowly evolving from traditional views, such as fixed dose for all patients, to taking in aspects of variance between individuals for both treatments and diagnostics. For further modernisation of the medical field should the way diagnostics is performed also be modernised. With every plasma sample sent for diagnostics several separate tests are performed on small fractions of the sample to quantify proteins with known disease correlation instead of doing few tests analysing a larger number of targets. Multiplexed methods capable of analysing larger numbers of biomarkers in a single run have in recent years been developed. One of these methods is Immuno-SILAC that uses assets from the Human Protein Atlas, such as QprESTs and polyclonal antibodies to target and quantify proteins in plasma by mass spectrometric analysis with reduction of analysis time down to 15 minutes, which makes it a relevant method for clincs. To further increase the relevancy of Immuno-SILAC in clinics, evaluation of protocols essential for the function and therefore also limiting for the use of Immuno-SILAC, e.g. trypsin digestion, os of great importance. Aspects to take into evaluation within trypsin digestion are trypsin alternatives, time dependency and effects of denaturing agents such as urea. Improvements in these aspects could reduce the cost of sample preparation, time needed for digestion and increase the columes possible for analysis to increase the medical relevancy of Immuno-SILAC in clinics and make it a viable alternative method to current diagnostics.</p><p>During this master thesis, 7 known diagnostic targets were chosen and evaluated for the development of a PRM-assay, and absolute quantification in plasma with Immuno-SILAC. Aspects such as the effect of urea on trypsin, time dependency of digestion, and alternative trypsin enzymes were evaluated to increase the clinical applicability of Immuno-SILAC further.</p><p>Trypsin evaluation yielded positive results showing peptides with close-to unchaninging ratios over time. Trypsin showed close-to unchanged activity up tp 2 M urea which will decrease the dilution needed for digestion and enable robotics for automatic sample preparation, and digestion with alternative cheaper digestion showed promise for further use in the Immuno-SILAC protocol.</p>

w='unchaninging' val={'c': 'unchanging', 's': 'diva2:855926', 'n': 'no full text'}
w='clincs' val={'c': 'clinics', 's': 'diva2:855926', 'n': 'no full text'}
w='columes' val={'c': 'columns', 's': 'diva2:855926', 'n': 'no full text'}

corrected abstract:
<p>The medical field is slowly evolving from traditional views, such as fixed dose for all patients, to taking in aspects of variance between individuals for both treatments and diagnostics. For further modernisation of the medical field should the way diagnostics is performed also be modernised. With every plasma sample sent for diagnostics several separate tests are performed on small fractions of the sample to quantify proteins with known disease correlation instead of doing few tests analysing a larger number of targets. Multiplexed methods capable of analysing larger numbers of biomarkers in a single run have in recent years been developed. One of these methods is Immuno-SILAC that uses assets from the Human Protein Atlas, such as QprESTs and polyclonal antibodies to target and quantify proteins in plasma by mass spectrometric analysis with reduction of analysis time down to 15 minutes, which makes it a relevant method for clinics. To further increase the relevancy of Immuno-SILAC in clinics, evaluation of protocols essential for the function and therefore also limiting for the use of Immuno-SILAC, e.g. trypsin digestion, os of great importance. Aspects to take into evaluation within trypsin digestion are trypsin alternatives, time dependency and effects of denaturing agents such as urea. Improvements in these aspects could reduce the cost of sample preparation, time needed for digestion and increase the columns possible for analysis to increase the medical relevancy of Immuno-SILAC in clinics and make it a viable alternative method to current diagnostics.</p><p>During this master thesis, 7 known diagnostic targets were chosen and evaluated for the development of a PRM-assay, and absolute quantification in plasma with Immuno-SILAC. Aspects such as the effect of urea on trypsin, time dependency of digestion, and alternative trypsin enzymes were evaluated to increase the clinical applicability of Immuno-SILAC further.</p><p>Trypsin evaluation yielded positive results showing peptides with close-to unchanging ratios over time. Trypsin showed close-to unchanged activity up tp 2 M urea which will decrease the dilution needed for digestion and enable robotics for automatic sample preparation, and digestion with alternative cheaper digestion showed promise for further use in the Immuno-SILAC protocol.</p>
----------------------------------------------------------------------
In diva2:1325213 abstract is: <p>The use of peruorocarbon lled droplets for use as Phase Changing Contrast Agents (PCCAs) is a promosing eld. These capsules also have potential to be used for mediated drug delivery. The phase change, which has given the capsules their name, is the process when the capsule transforms from a droplet into a bubble. This process is referred to as Acoustic Droplet Vaporization (ADV) and can be induced with the use of ultrasonic waves.</p><p>In this study a new type of Perfluorpentane (PFC5) capsules which are stabilized with Cellulose Nano Fibers (CNF) have been evaluated for its potential as a PCCA. To investigate this potential a setup was designed in which the capsules could be exposed to ultrasound waves. Following the ultrasound exposure the capsules were visualized under a light transmission microscope.</p><p>The experiments were conducted for dierent combinations of ultrasound parameters. For each combination eight volume distributions were created, in which two of them as reference cases were not exposed to ultrasound waves. Six cases with the ultrasound ring with different levels of acoustic power, resulting in peak negative pressures ranging from 0.144 to 0.291 MPa. The results showedfthat ADV could be observed when the frequency of the acoustic wave is 3.5 MHz, the pulse repetition frequency is 500 and the burst width is set to 12 cycles. The Peak Negative Pressure (PNP)-threshold for ADV is about 0.200 MPa. When the burst width is set to 8, ADV is also observed however to a lesser extent then when it is set to 12. These results indicate that the CNF-stabilized PFC5 capsules are promising droplets with a potential future as an alternative to currently used PCCAs.</p>

w='lled' val={'c': 'filled', 's': 'diva2:1325213', 'n': 'missing ligature'}
w='peruorocarbon' val={'c': 'perfluorocarbon', 's': 'diva2:1325213', 'n': 'missing ligature'}
w='showedfthat' val={'c': 'showed that', 's': 'diva2:1325213', 'n': 'correct in original'}
w='promosing' val={'c': 'promising', 's': 'diva2:1325213', 'n': 'error in original'}
w='eld' val={'c': 'field', 's': 'diva2:1325213', 'n': 'missing ligature'}

corrected abstract:
<p>The use of perfluorocarbon filled droplets for use as Phase Changing Contrast Agents (PCCAs) is a promosing field. These capsules also have potential to be used for mediated drug delivery. The phase change, which has given the capsules their name, is the process when the capsule transforms from a droplet into a bubble. This process is referred to as Acoustic Droplet Vaporization (ADV) and can be induced with the use of ultrasonic waves.</p><p>In this study a new type of Perfluorpentane (PFC5) capsules which are stabilized with Cellulose Nano Fibers (CNF) have been evaluated for its potential as a PCCA. To investigate this potential a setup was designed in which the capsules could be exposed to ultrasound waves. Following the ultrasound exposure the capsules were visualized under a light transmission microscope.</p><p>The experiments were conducted for dierent combinations of ultrasound parameters. For each combination eight volume distributions were created, in which two of them as reference cases were not exposed to ultrasound waves. Six cases with the ultrasound ring with different levels of acoustic power, resulting in peak negative pressures ranging from 0.144 to 0.291 MPa. The results showedfthat ADV could be observed when the frequency of the acoustic wave is 3.5 MHz, the pulse repetition frequency is 500 and the burst width is set to 12 cycles. The Peak Negative Pressure (PNP)-threshold for ADV is about 0.200 MPa. When the burst width is set to 8, ADV is also observed however to a lesser extent then when it is set to 12. These results indicate that the CNF-stabilized PFC5 capsules are promising droplets with a potential future as an alternative to currently used PCCAs.</p>
----------------------------------------------------------------------
diva2:1183490 multiple errors in the original thesis abstract

w='faciliate' val={'c': 'facilitate', 's': 'diva2:1183490', 'n': 'error in original'}
w='seperately' val={'c': 'separately', 's': 'diva2:1183490', 'n': 'error in original'}
w='survery' val={'c': 'survey', 's': 'diva2:1183490', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:557542 abstract is: <p>This report describes the work on automating the testing of nodes at CCS (Common Control System) in Ericsson. The goal was to every three hours configure nodes with the latest build and run the tests. This process is to be fully automatic without user in-put. The existing configuration tool CICC (Core Integration node Control Center) is to be used for configuration. Before work started fault reports were analyzed and creating a usecase for testing restarts should reduce some faults.The first step was to make the configuration tool CICC automated. To schedule the test-ing the continuous integration tool Jenkins was used. But Jenkins can’t by itself run CICC nor interpret the result. Therefore a wrapper layer was implemented. When the wrapper is finished it stores the results of the configuration run in a XML (eXtensible Markup Language) file, which Jenkins reads. Results can then be seen in Jenkins through web interface. If there were any failures during configuration or testing the failed step will have an error message.The project shows that automation is possible. Automating the testing reduce the time for correcting errors because they are more likely to be found early in the process. Be-fore implementing this project in production some improvements should be made. The most significant improvement is making the configuration and testing of each node par-allel with each other, in order to make the time limit for configuration and testing less of an issue.</p>

w='test-ing' val={'c': 'testing', 's': 'diva2:557542', 'n': 'hyphen at end of line in original'}
Other caes of hyphens at the end of the line.

corrected abstract:
<p>This report describes the work on automating the testing of nodes at CCS (Common Control System) in Ericsson. The goal was to every three hours configure nodes with the latest build and run the tests. This process is to be fully automatic without user input. The existing configuration tool CICC (Core Integration node Control Center) is to be used for configuration. Before work started fault reports were analyzed and creating a usecase for testing restarts should reduce some faults.</p><p>The first step was to make the configuration tool CICC automated. To schedule the testing the continuous integration tool Jenkins was used. But Jenkins can’t by itself run CICC nor interpret the result. Therefore a wrapper layer was implemented. When the wrapper is finished it stores the results of the configuration run in a XML (eXtensible Markup Language) file, which Jenkins reads. Results can then be seen in Jenkins through web interface. If there were any failures during configuration or testing the failed step will have an error message.</p><p>The project shows that automation is possible. Automating the testing reduce the time for correcting errors because they are more likely to be found early in the process. Before implementing this project in production some improvements should be made. The most significant improvement is making the configuration and testing of each node parallel with each other, in order to make the time limit for configuration and testing less of an issue.</p>
----------------------------------------------------------------------
title: "AI Verification Application: Development of an Application for ArtificialNeural Network Verification and Investigation of Hyperparameters for Single Shot Detection Models"
==>    "AI Verification Application: Development of an Application for Artificial Neural Network Verification and Investigation of Hyperparameters for Single Shot Detection Models"

In diva2:1451753 abstract is: <p>Abstract Hyperparameter tuning for Artificial Neural Network models is an important part in the process of producing the best model for a given task. The process heavily relies on the availability of time and computing resources. For Artifi-cial Neural Network models intended to run on embedded systems, models are tuned and trained on powerful computers with far more computing re-sources, prior to deployment on the embedded system itself. Consequently, the performance of these models require validation on the target embedded system. This is required since the performance of the models might be insuf-ficient when executed on an embedded system with limited computing re-sources. The process of validating can be a time-consuming and tedious task. This thesis covers the development of an application prototype which eases the process of validating pre-compiled Artificial Neural Network models on a target embedded system. In addition, the thesis covers an analysis of hyperpa-rameter tuning algorithms, along with an investigation of which parameters are important to tune in Convolutional Neural Networks for Single Shot De-tection. An application prototype was developed, and its functionality was val-idated. An analysis of tuning algorithms was conducted, however, no indefi-nite conclusions could be drawn, since the results were conflicting, and the sample size was too small. Through the investigation of hyperparameters, op-timizer learning rate was determined to be important to tune in Convolutional Neural Networks for Single Shot Detection. Further work, with a larger sample size, might make it possible to identify additional important parameters. KeywordsArtificial Neural Network, Convolutional Neural Network, Hyperparameter tuning, Single Shot Detection, Embedded Machine Learning, Python, Grid search, Random search, Bayesian optimization, automated hyperparameter tuning</p>

w='insuf-ficient' val={'c': 'insufficient', 's': 'diva2:1451753'}

Removed unnecessary hyphens.

corrected abstract:
<p>Hyperparameter tuning for Artificial Neural Network models is an important part in the process of producing the best model for a given task. The process heavily relies on the availability of time and computing resources. For Artificial Neural Network models intended to run on embedded systems, models are tuned and trained on powerful computers with far more computing resources, prior to deployment on the embedded system itself. Consequently, the performance of these models require validation on the target embedded system. This is required since the performance of the models might be insufficient when executed on an embedded system with limited computing resources. The process of validating can be a time-consuming and tedious task. This thesis covers the development of an application prototype which eases the process of validating pre-compiled Artificial Neural Network models on a target embedded system. In addition, the thesis covers an analysis of hyperparameter tuning algorithms, along with an investigation of which parameters are important to tune in Convolutional Neural Networks for Single Shot Detection. An application prototype was developed, and its functionality was validated. An analysis of tuning algorithms was conducted, however, no indefinite conclusions could be drawn, since the results were conflicting, and the sample size was too small. Through the investigation of hyperparameters, optimizer learning rate was determined to be important to tune in Convolutional Neural Networks for Single Shot Detection. Further work, with a larger sample size, might make it possible to identify additional important parameters.</p>
----------------------------------------------------------------------
In diva2:1683604 abstract is: <p>This thesis project has been done during ten weeks on behalf of the companyBitSim NOW. The current method used to measure the length of shot-puts presents a risk of inaccurate results along with the risk of injury for the measuring personnel. With the help of technical aids, a solution with more accurate measurements and a lower risk for injuries could be implemented in the sport of shot-puts.</p><p>This report presents a solution using artificial intelligence to first identify the shotin video films and secondly calculate the length using mathematical formulas. Thesolution is then compared to a method that does not use artificial intelligence, to determine what method is the superior one. The parameters that were compared were the accuracy of the length and the quality of the tracking. The result was analyzed in relation to the aims of the project and then put into a larger context. </p>

w='shotin' val={'c': 'shot in', 's': 'diva2:1683604', 'n': 'correct in original - there words are separated by a newline'}

corrected abstract:
<p>This thesis project has been done during ten weeks on behalf of the company BitSim NOW. The current method used to measure the length of shot-puts presents a risk of inaccurate results along with the risk of injury for the measuring personnel. With the help of technical aids, a solution with more accurate measurements and a lower risk for injuries could be implemented in the sport of shot-puts.</p><p>This report presents a solution using artificial intelligence to first identify the shot in video films and secondly calculate the length using mathematical formulas. The solution is then compared to a method that does not use artificial intelligence, to determine what method is the superior one. The parameters that were compared were the accuracy of the length and the quality of the tracking. The result was analyzed in relation to the aims of the project and then put into a larger context.</p>
----------------------------------------------------------------------
In diva2:1723025 abstract is: <p>The increase in population and growth of industries leads to increasing demand of clean water. Consequently, the amount of water from salt- or brackish water sources that is used will also increase, and the use of desalination methods like RO will become more important. When treating drinking water certain problems may occur due to algae and algae derived products. All types of algae are photosynthetic eukaryotic organisms that lack true roots, stems, and leaves. Although the blue-green algae, cyanobacteria, is prokaryote it is often included as an alga because of their photosynthetic abilities and that they are able to create blooms. </p><p>Algae are very diverse and can vary a lot depending on the source, fresh- salt or brackish water usually contains entirely different taxa. Green algae are common in freshwater whilst red, brown and dinoflagellates are more common in salt- and brackish water. There are even some types of algae that can be found in all types of water. These include cyanobacteria and diatoms. The three main types of problems which can occur because of algae in a desalination plant is contamination of product water, fouling/clogging of pretreatment system and fouling of RO membrane. </p><p>Algal blooms produce large amounts of biomass, which can lead to fouling of both the pretreatment- and RO system. Traditional procedures like direct filtration with granular media, sedimentation, dissolved air flotation (DAF) and coagulation are all effective methods for removal of algal biomass from water. However, in the event of larger algal blooms some of the traditional methods tend to become insufficient. More modern methods can be more effective and robust. These methods include membrane filtration like microfiltration (MF), ultrafiltration (UF) and RO. Problems like pore constriction and loss of effective filtration area are however still possible when using membrane filtration. </p><p>The algae concentration is calculated by measuring the quantity of chlorophyll a, which all algae contain, in a sample. This can be done with spectrophotometry, high performance liquid chromatography (HPLC) or fluorometry. Satellite pictures are often used for measuring chlorophyll a in the ocean. To identify specific species of algae, the use of different microscopic methods is very common where light microscope and fluorescence microscope are the most common. </p><p>Various substances originate from algae, collectively called algae organic matter (AOM). One of the most important AOM is transparent exopolymer particles (TEP). TEP have high surface activity which makes them stickier then most other AOM. When water containing TEP is treated, clogging and biofilm formation may occur. Because of their ability to adsorb suspended particles and nutrients they tend to form excellent conditioning layers for bacterial growth. To analyse TEP they first need to be stained with alcian blue and then measured with a microscope or spectrophotometer. </p><p>Some algae also produce toxins as well as taste and odour compounds (T&amp;O). Dinoflagellates and cyanobacteria are the most toxin-forming algae, and both their toxins can lead to multiple severe poisonings. RO is the most common method to remove algal toxins, but methods like oxidation can inactivate some toxins. Oxidation is more often used for removal of T&amp;Os as they have a lower molecular weight and cannot be fully removed with RO. The analytic methods most suited for analysis of toxins are liquid chromatography coupled to tandem mass spectrometry (LC-MS/MS), enzyme-Linked Immunosorbent Assay (ELISA) and HPLC. Gas chromatography coupled mass spectrometry (GC/MS) is also often used for analysis of T&amp;Os. </p><p>Some of the most common methods of action for reducing the algal derived problems is inlet placement, oxidation, and coagulation. These three methods are used to ease the treatment of drinking water with the use of other processes. The right inlet placement can decrease the number of algae cells that enters the facility. Oxidation, which is usually done with chlorine, inactivates and lyses the cells. Chlorination can however lead to other problems, such as biofouling, and can cause damage to filter membranes. Coagulation forms clots that ease the removal of unwanted substances in later treatment processes. Iron(III)chloride is a very common coagulant, and it is used to lower the risk for particulate fouling. The most widespread modern methods are based on membrane filtration. Other less used processes that are considered are ultrasound and electrochemical methods like electrolysis. Ultrasound can both inactivate algal cells and affects their ability to float, which prevent them from entering the facility. Oxidative substances are generated with electrolysis which inactivate the cells. Electrolysis is better suited for treatment of water with higher salinity since the ion concentration is higher. </p><p>The Swedish law barely mentions algae or AOM in drinkingwater. However, a new EU directive includes limits for concentration of the algal toxin microcystin. In other countries like Finland the law is stricter than in Sweden. The Finnish law gives clear directions at which concentrations specific measures need to be taken and both the algal cell and toxin concentration are regulated. </p><p>Conclusions from this project are correct treatment of drinking water requires an effective and robust analysis and the use of proper process systems. The proper system design does depend on the type of water and algae. Clogging due to TEP occurs more often in waters with higher number of diatoms, as they are the most TEP producing type of algae. When more modern methods like RO are used, substances with lower molecule weight such as toxins are more easily removed. It also makes the removal of T&amp;Os possible. </p>

w='enzyme-Linked' val={'c': 'enzyme-linked', 's': 'diva2:1723025'}
If one is going to capitalics the letters of the acronym, then they should be done consistently.

corrected abstract:
<p>The increase in population and growth of industries leads to increasing demand of clean water. Consequently, the amount of water from salt- or brackish water sources that is used will also increase, and the use of desalination methods like RO will become more important. When treating drinking water certain problems may occur due to algae and algae derived products. All types of algae are photosynthetic eukaryotic organisms that lack true roots, stems, and leaves. Although the blue-green algae, cyanobacteria, is prokaryote it is often included as an alga because of their photosynthetic abilities and that they are able to create blooms. </p><p>Algae are very diverse and can vary a lot depending on the source, fresh- salt or brackish water usually contains entirely different taxa. Green algae are common in freshwater whilst red, brown and dinoflagellates are more common in salt- and brackish water. There are even some types of algae that can be found in all types of water. These include cyanobacteria and diatoms. The three main types of problems which can occur because of algae in a desalination plant is contamination of product water, fouling/clogging of pretreatment system and fouling of RO membrane. </p><p>Algal blooms produce large amounts of biomass, which can lead to fouling of both the pretreatment- and RO system. Traditional procedures like direct filtration with granular media, sedimentation, dissolved air flotation (DAF) and coagulation are all effective methods for removal of algal biomass from water. However, in the event of larger algal blooms some of the traditional methods tend to become insufficient. More modern methods can be more effective and robust. These methods include membrane filtration like microfiltration (MF), ultrafiltration (UF) and RO. Problems like pore constriction and loss of effective filtration area are however still possible when using membrane filtration. </p><p>The algae concentration is calculated by measuring the quantity of chlorophyll a, which all algae contain, in a sample. This can be done with spectrophotometry, high performance liquid chromatography (HPLC) or fluorometry. Satellite pictures are often used for measuring chlorophyll a in the ocean. To identify specific species of algae, the use of different microscopic methods is very common where light microscope and fluorescence microscope are the most common. </p><p>Various substances originate from algae, collectively called algae organic matter (AOM). One of the most important AOM is transparent exopolymer particles (TEP). TEP have high surface activity which makes them stickier then most other AOM. When water containing TEP is treated, clogging and biofilm formation may occur. Because of their ability to adsorb suspended particles and nutrients they tend to form excellent conditioning layers for bacterial growth. To analyse TEP they first need to be stained with alcian blue and then measured with a microscope or spectrophotometer. </p><p>Some algae also produce toxins as well as taste and odour compounds (T&amp;O). Dinoflagellates and cyanobacteria are the most toxin-forming algae, and both their toxins can lead to multiple severe poisonings. RO is the most common method to remove algal toxins, but methods like oxidation can inactivate some toxins. Oxidation is more often used for removal of T&amp;Os as they have a lower molecular weight and cannot be fully removed with RO. The analytic methods most suited for analysis of toxins are liquid chromatography coupled to tandem mass spectrometry (LC-MS/MS), enzyme-linked Immunosorbent Assay (ELISA) and HPLC. Gas chromatography coupled mass spectrometry (GC/MS) is also often used for analysis of T&amp;Os. </p><p>Some of the most common methods of action for reducing the algal derived problems is inlet placement, oxidation, and coagulation. These three methods are used to ease the treatment of drinking water with the use of other processes. The right inlet placement can decrease the number of algae cells that enters the facility. Oxidation, which is usually done with chlorine, inactivates and lyses the cells. Chlorination can however lead to other problems, such as biofouling, and can cause damage to filter membranes. Coagulation forms clots that ease the removal of unwanted substances in later treatment processes. Iron(III)chloride is a very common coagulant, and it is used to lower the risk for particulate fouling. The most widespread modern methods are based on membrane filtration. Other less used processes that are considered are ultrasound and electrochemical methods like electrolysis. Ultrasound can both inactivate algal cells and affects their ability to float, which prevent them from entering the facility. Oxidative substances are generated with electrolysis which inactivate the cells. Electrolysis is better suited for treatment of water with higher salinity since the ion concentration is higher. </p><p>The Swedish law barely mentions algae or AOM in drinkingwater. However, a new EU directive includes limits for concentration of the algal toxin microcystin. In other countries like Finland the law is stricter than in Sweden. The Finnish law gives clear directions at which concentrations specific measures need to be taken and both the algal cell and toxin concentration are regulated. </p><p>Conclusions from this project are correct treatment of drinking water requires an effective and robust analysis and the use of proper process systems. The proper system design does depend on the type of water and algae. Clogging due to TEP occurs more often in waters with higher number of diatoms, as they are the most TEP producing type of algae. When more modern methods like RO are used, substances with lower molecule weight such as toxins are more easily removed. It also makes the removal of T&amp;Os possible. </p>
----------------------------------------------------------------------
In diva2:819025 abstract is: <p>Sveriges Radio stores their data in large archives which makes it hard to retrieve specific information. The sheer size of the archives makes retrieving information about a specific event difficult and causes a big problem. To solve this problem a more consistent use of metadata is needed. This resulted in an investigation about metadata and keyword genera-tion.The appointed task was to automatically generate keywords from transcribed radio shows. This included an investigation of which systems and algorithms that can be used to generate keywords, based on previous works. An application was also developed which suggests keywords based on a text to a user. This application was tested and compared to other al-ready existing software, as well as different methods/techniques based on both linguistic and statistic algorithms. The resulting analysis displayed that the developed application generated many accurate keywords, but also a large amount of keywords in general. The comparison also showed that the recall for the developed algorithm got better results than the already existing software, which in turn produced a better precision in their keywords.</p>


w='al-ready' val={'c': 'already', 's': ['diva2:819025', 'diva2:818785']}
Also removed an unnecessary hyphen and added the paragraph separation.

corrected abstract:
<p>Sveriges Radio stores their data in large archives which makes it hard to retrieve specific information. The sheer size of the archives makes retrieving information about a specific event difficult and causes a big problem. To solve this problem a more consistent use of metadata is needed. This resulted in an investigation about metadata and keyword generation.</p><p>The appointed task was to automatically generate keywords from transcribed radio shows. This included an investigation of which systems and algorithms that can be used to generate keywords, based on previous works. An application was also developed which suggests keywords based on a text to a user. This application was tested and compared to other already existing software, as well as different methods/techniques based on both linguistic and statistic algorithms. The resulting analysis displayed that the developed application generated many accurate keywords, but also a large amount of keywords in general. The comparison also showed that the recall for the developed algorithm got better results than the already existing software, which in turn produced a better precision in their keywords.</p>
----------------------------------------------------------------------
diva2:818785 seems to be a duplicate of diva2:819025 (just above)
----------------------------------------------------------------------
In diva2:934992 abstract is: <p>Nowadays XLPE high voltage cables are used in transmission and distribution of electrical power and has an important role in the electrical system. Power transmission that occurs over long distances is diverted from one point to another and is done with the help of cable terminations. These cable terminations have some vulnerabilities that affect the entire power system.</p><p>The theoretical part of the report was carried out with the help of FEM software Comsol Multiphysics. Results showed that the highest field concentration occurs at the insulation of the cable and caused malfunction and vulnerabilities. In the linear case for the electrical part resulted a high permittivity that the potential and the electric field was reduced, which showed the same result for low conductivity. For the nonlinear material the conductivity changes with the electric field and time.</p><p>The temperature of the linear case showed that at high temperatures the material became more conductive. In the nonlinear case the conductive material was reduced. This could be controlled with different threshold value (Eb) which cannot in the linear case.</p><p>The practical part was done in E.ON:s laboratory for different type of cable terminations that were tested with 33kV. Some of the tested cable terminations were used for testing purposes and was picked out of operation because of a malfunction in the cable. An attempt to control field concentration was carried out in the laboratory and resulted to a reduced field  istribution in the cable termination. The result of the practical part showed how the field concentration was distributed in the cable termination and that the field distribution led to the collapse of the cable.</p>

w='istribution' val={'c': 'distribution', 's': 'diva2:934992', 'n': 'correct in original'}
The above was the only change and it occurs near the end of the abstract.

corrected abstract:
<p>Nowadays XLPE high voltage cables are used in transmission and distribution of electrical power and has an important role in the electrical system. Power transmission that occurs over long distances is diverted from one point to another and is done with the help of cable terminations. These cable terminations have some vulnerabilities that affect the entire power system.</p><p>The theoretical part of the report was carried out with the help of FEM software Comsol Multiphysics. Results showed that the highest field concentration occurs at the insulation of the cable and caused malfunction and vulnerabilities. In the linear case for the electrical part resulted a high permittivity that the potential and the electric field was reduced, which showed the same result for low conductivity. For the nonlinear material the conductivity changes with the electric field and time.</p><p>The temperature of the linear case showed that at high temperatures the material became more conductive. In the nonlinear case the conductive material was reduced. This could be controlled with different threshold value (Eb) which cannot in the linear case.</p><p>The practical part was done in E.ON:s laboratory for different type of cable terminations that were tested with 33kV. Some of the tested cable terminations were used for testing purposes and was picked out of operation because of a malfunction in the cable. An attempt to control field concentration was carried out in the laboratory and resulted to a reduced field distribution in the cable termination. The result of the practical part showed how the field concentration was distributed in the cable termination and that the field distribution led to the collapse of the cable.</p>
----------------------------------------------------------------------
In diva2:801903 abstract is: <p>The pulp and paper industry have in the past been a stable and safe industry. A change has been seen during the last decades and the annual increase and demand for paperderived products has stagnated. Due to this, the industry is looking into new products to produce while utilising the great potential a pulp mill possess with regard to equipment and chemical regeneration. A biorefinery concepts have been suggested were pulp, lignin and lactic acid (100.000 tonne per year) could be produced simultaneously.</p><p>Lactic acid has during the two last decades seen a dramatic increase in demand and interst due to the possibility of polymerisation to produce polyactic acid, a biodegradable biopolymer. Fermentative and synthetic production of lactic acid could be seen in the industry with a larger interest in fermentative production, utilising microorganisms.</p><p>Simulations were conducted to evaluate different process designs for a pulp mill biorefinery concept. Three different process designs were simulated before selection of most suitable process design for integration to a pulp mill model. An alkaline pretreatment followed by simultaneous saccharification and fermentation for production and lastly precipitation and esterification for recovery of lactic acid was selected for integration study. Results indicate that with an investment of 140 MEUR, lactic acid could be produced at a cost of 0.42-0.44 EUR/kg, approximately 55% lower that the market price.</p>

w='interst' val={'c': 'interest', 's': 'diva2:801903'}
w='intresting' val={'c': 'interesting', 's': 'diva2:801903', 'n': 'no full text'}
w='polyactic' val={'c': 'polylactic', 's': 'diva2:801903', 'n': 'no full text'}

corrected abstract:
<p>The pulp and paper industry have in the past been a stable and safe industry. A change has been seen during the last decades and the annual increase and demand for paper derived products has stagnated. Due to this, the industry is looking into new products to produce while utilising the great potential a pulp mill possess with regard to equipment and chemical regeneration. A biorefinery concepts have been suggested were pulp, lignin and lactic acid (100.000 tonne per year) could be produced simultaneously.</p><p>Lactic acid has during the two last decades seen a dramatic increase in demand and interest due to the possibility of polymerisation to produce polylactic acid, a biodegradable biopolymer. Fermentative and synthetic production of lactic acid could be seen in the industry with a larger interest in fermentative production, utilising microorganisms.</p><p>Simulations were conducted to evaluate different process designs for a pulp mill biorefinery concept. Three different process designs were simulated before selection of most suitable process design for integration to a pulp mill model. An alkaline pretreatment followed by simultaneous saccharification and fermentation for production and lastly precipitation and esterification for recovery of lactic acid was selected for integration study. Results indicate that with an investment of 140 MEUR, lactic acid could be produced at a cost of 0.42-0.44 EUR/kg, approximately 55% lower that the market price.</p>
----------------------------------------------------------------------
In diva2:1119292 abstract is: <p>The aim of this project is to investigate the differences in muscle activity of the lower leg between three types of ankle foot orthoses. The reason behind the project is that there is little knowledge about differences between different types of orthoses. The method used to tackle this task was to write a Matlab script that manipulated and anlyzed all the data required to yield a result for each individual orthoses that then could be used to compare them to each other. This was done in four major steps, the first of which was to manipulate the EMG-data of the chosen file so that it became easier to analyze, the second step was to use the data from a pressure sole to determine when steps started and ended. Thirdly, the reference data was manipulated in the same way as in step one. The fourth and final step was to express the chosen file in percent of the reference data. Finally, Excel was used to compare the different orthoses by calculating mean values from the test subjects data. The results showed that the muscel actvity in "Triceps Surae" seems to decrease with increased plantarflexion.The conclussion had to be divided into one conclussion per muscle, the different conlussions differed for every muscle.</p>

w='conclussion' val={'c': 'conclusion', 's': 'diva2:1119292'}
w='conlussions' val={'c': 'conclusions', 's': 'diva2:1119292', 'n': 'correct in original'}
w='muscel' val={'c': 'muscle', 's': 'diva2:1119292', 'n': 'error in original'}
w='actvity' val={'c': 'activity', 's': 'diva2:1119292'}
w='anlyzed' val={'c': 'analyzed', 's': 'diva2:1119292', 'n': 'error in original'}

corrected abstract:
<p>The aim of this project is to investigate the differences in muscle activity of the lower leg between three types of ankle foot orthoses. The reason behind the project is that there is little knowledge about differences between different types of orthoses. The method used to tackle this task was to write a Matlab script that manipulated and anlyzed all the data required to yield a result for each individual orthoses that then could be used to compare them to each other.</p><p>This was done in four major steps, the first of which was to manipulate the EMG-data of the chosen file so that it became easier to analyze, the second step was to use the data from a pressure sole to determine when steps started and ended. Thirdly, the reference data was manipulated in the same way as in step one. The fourth and final step was to express the chosen file in percent of the reference data. Finally, Excel was used to compare the different orthoses by calculating mean values from the test subjects data.</p><p>The results showed that the muscel activity in "Triceps Surae" seems to decrease with increased plantarflexion. The conclusion had to be divided into one conclusion per muscle, the different conclusions differed for every muscle.</p>
----------------------------------------------------------------------
In diva2:787218 abstract is: <p>The project in question relates to reproductive proteins from Pieris Napi, a common Swedish butterfly species. The analysis was performed by capillary electrophoresis and mass spectroscopy, MALDI-TOF.</p><p>To ensure good results from the analysis of the fresh butterflies, a method development was performed on old butterfly samples. Several solvents were used to determine which extraction medium was best suited for protein extraction. MilliQ-water, acetonitrile, hexafluoroisopropanol and methanol were included in the examination. All solvents were well suited as extraction medium, except acetonitrile which lacked clear signals in the electropherograms.</p><p>When the method development was finished different types of proteins could be studied by the aid of the chosen extraction media. After electrophoresis on the fresh samples, it could be presured that both hydrophobic and hydrophilic proteins were present in the spermatophores. Multiple electropherograms with characteristic profiles for each solvent were obtained which indicated that different types of proteins were detected. Some similarities could be observed between the extraction from the MilliQ-water and methanol.</p><p>Protein stability during a three week period was assured by regular controls with electrophoresis. Stability could be shown through similar profiles in the electropherograms from the various weeks. Since the proteins shown such stability, MALDI-TOF analysis was done on the samples after all the electrophoresis was performed. Proteins were examined in both high and low molecular fields. This ensured results in the form of spectra. The spectrum obtained from HFIP-extraction differed from the spectrum from methanol- and water extractions, indicating that hydrophobic proteins are present in the spermatophores.</p>

w='presured' val={'c': 'pressured', 's': ['diva2:787218', 'diva2:787188']}

corrected abstract:
<p>The project in question relates to reproductive proteins from Pieris Napi, a common Swedish butterfly species. The analysis was performed by capillary electrophoresis and mass spectroscopy, MALDI-TOF.</p><p>To ensure good results from the analysis of the fresh butterflies, a method development was performed on old butterfly samples. Several solvents were used to determine which extraction medium was best suited for protein extraction. MilliQ-water, acetonitrile, hexafluoroisopropanol and methanol were included in the examination. All solvents were well suited as extraction medium, except acetonitrile which lacked clear signals in the electropherograms.</p><p>When the method development was finished different types of proteins could be studied by the aid of the chosen extraction media. After electrophoresis on the fresh samples, it could be pressured that both hydrophobic and hydrophilic proteins were present in the spermatophores. Multiple electropherograms with characteristic profiles for each solvent were obtained which indicated that different types of proteins were detected. Some similarities could be observed between the extraction from the MilliQ-water and methanol.</p><p>Protein stability during a three week period was assured by regular controls with electrophoresis. Stability could be shown through similar profiles in the electropherograms from the various weeks. Since the proteins shown such stability, MALDI-TOF analysis was done on the samples after all the electrophoresis was performed. Proteins were examined in both high and low molecular fields. This ensured results in the form of spectra. The spectrum obtained from HFIP-extraction differed from the spectrum from methanol- and water extractions, indicating that hydrophobic proteins are present in the spermatophores.</p>
----------------------------------------------------------------------
In diva2:853640 abstract is: <p>Glycoproteins are proteins with glycans located at the protein surface. These glycans are composed of monosaccharides covalently bound to each other in branced or linear structures. Sialic acids are monosaccharides that are located at terminal positions on the glycans and have been suggested to be of great importance for the proteins function and communication with surrounding cells and other proteins. Sialic acids have been show nto affect the glycoproteins half-life and activity. Furthermore, small modifications of sialic acids can affect the efficacy ad the interaction capacity of glycoproteins. Reseach has shown that it is possible to study sialic acids and their effects on glycoproteins. Furthermore, almost half of all the plasma proteins are glycoproteins, and most likely have sialic acids. It is therefore opportune to look into glycoproteins as drug candidates. Regulators as FDA and EMA already have demands for characterization of sialic acids in drug development. Moreover, the sialic acid content can be used as control for product consistency between batches. A pharmaceutical company needs to lie ahead and in accordance with the regulations, and to do so today it is essential to put resources into developing reliable methods for sialic acid analysis.</p><p>This report suggest an analytical proceduret o analyze the sialic acid content of glycoproteins by using two model glycoproteins, bovine fetuin and apo-transferrin bovine. Different hydrolysis methods for sialic acids release from glycoproteins, bovine fetuin and apo-transferrin bovine. Different hydrolysis methods for sialic acids release from glycoproteins are optimized and compared to find the superior hydrolysis method. The released sialic acid content is identified and quantified usign HPAE-PAD, which is a liquid chromatographic technique at high pH with electrochemical detection.</p><p>Acetic acid hydrolysis is show nto be superior to hydrochloric acid hydrolysis in releasing sialic acid from the model glycoproteins due to less genereation of hydrolysis byproducts. The analytical procedure described for sialic acids released by acetic acid hydrolysis from fetuin is also qualified considering specificity, linearity, accuracy and precision and was found scientificlly sound.</p>

w='branced' val={'c': 'branched', 's': 'diva2:853640'}
w='genereation' val={'c': 'generation', 's': 'diva2:853640', 'n': 'no full text'}
w='nto' val={'c': 'not to', 's': 'diva2:853640', 'n': 'no full text'}
w='scientificlly' val={'c': 'scientifically', 's': 'diva2:853640', 'n': 'no full text'}
w='usign' val={'c': 'using', 's': 'diva2:853640', 'n': 'no full text'}
w='proceduret' val={'c': 'procedure', 's': 'diva2:853640', 'n': 'no full text'}
w='Reseach' val={'c': 'Research', 's': 'diva2:853640', 'n': 'no full text'}

corrected abstract:
<p>Glycoproteins are proteins with glycans located at the protein surface. These glycans are composed of monosaccharides covalently bound to each other in branched or linear structures. Sialic acids are monosaccharides that are located at terminal positions on the glycans and have been suggested to be of great importance for the proteins function and communication with surrounding cells and other proteins. Sialic acids have been shown not to affect the glycoproteins half-life and activity. Furthermore, small modifications of sialic acids can affect the efficacy ad the interaction capacity of glycoproteins. Research has shown that it is possible to study sialic acids and their effects on glycoproteins. Furthermore, almost half of all the plasma proteins are glycoproteins, and most likely have sialic acids. It is therefore opportune to look into glycoproteins as drug candidates. Regulators as FDA and EMA already have demands for characterization of sialic acids in drug development. Moreover, the sialic acid content can be used as control for product consistency between batches. A pharmaceutical company needs to lie ahead and in accordance with the regulations, and to do so today it is essential to put resources into developing reliable methods for sialic acid analysis.</p><p>This report suggest an analytical procedure to analyze the sialic acid content of glycoproteins by using two model glycoproteins, bovine fetuin and apo-transferrin bovine. Different hydrolysis methods for sialic acids release from glycoproteins, bovine fetuin and apo-transferrin bovine. Different hydrolysis methods for sialic acids release from glycoproteins are optimized and compared to find the superior hydrolysis method. The released sialic acid content is identified and quantified using HPAE-PAD, which is a liquid chromatographic technique at high pH with electrochemical detection.</p><p>Acetic acid hydrolysis is shown not to be superior to hydrochloric acid hydrolysis in releasing sialic acid from the model glycoproteins due to less generation of hydrolysis byproducts. The analytical procedure described for sialic acids released by acetic acid hydrolysis from fetuin is also qualified considering specificity, linearity, accuracy and precision and was found scientifically sound.</p>
----------------------------------------------------------------------
In diva2:895301 abstract is: <p>Lithium orthosilicate (Li4SiO4) is known to be a high temperature CO2 capture material.</p><p>This work was focused on comparing Li4SiO4 to the extensively studied CaO as an adsorbent</p><p>in sorption enhanced catalysis. Thermogravimetric analysis was used to study the effects of</p><p>sorption temperature and compaction on Li4SiO4 using 15vol% CO2 in N2. After 2 hours of</p><p>CO2 adsorption at 550°C the powder reached 35wt% uptake of CO2, corresponding to 93.6%</p><p>of maximum efficiency and complete regeneration was possible at 700°C. Pressing Li4SiO4 to</p><p>granular forms greatly decreased CO2 adsorption rates. Efforts to impregnate -Al2O3 with</p><p>the suspended SiO2 solution from aqueous based sol-gel synthesis to produce nanodispersed</p><p>Li4SiO4 failed due to the inability to form the targeted Li4SiO4 complex. X-ray diffraction</p><p>analysis indicated the formation of the gel is crucial for the formation of the crystalline</p><p>Li4SiO4 phase.</p><p>A microreactor was used to study the steam reforming of ethanol over a series of 1% Pt</p><p>on -Al2O3 catalyst composite impregnated over a range of nanodispersed CaO loading at</p><p>S/C=1.5 in dilution. At 400°C enhancement could be observed with the presence of CaO</p><p>sorbent compared to only 1% Pt/Al2O3. However, production quickly diminished due to</p><p>high carbon deposition. For 1% Pt/Al2O3 tested at 400°C, ethylene production was 5 times</p><p>higher than for hydrogen. Above 550°C the ethylene production was reduced to 0.18vol%</p><p>and gas production stability was greatly improved for 1% Pt/Al2O3 and even more so with</p><p>the addition of impregnated CaO sorbent. Hydrogen yields from homogeneous mixtures</p><p>of 1%Pt/Al2O3 with Li4SiO4 powder in a microreactor were about 20% higher than those</p><p>achievable of the same mixture with CaO powder. However, the composite 1%Pt/Al2O3</p><p>with 7.02wt% dispersed CaO gave 100% higher hydrogen production under similar conditions</p><p>despite Li4SiO4 being a superior carbon capture material.</p>

w='Pt/Al2O3' val={'c': 'Pt/Al<sub>2</sub>O<sub>3</sub>', 's': ['diva2:895301', 'diva2:778984'], 'n': 'correct in original'}
w='Li4SiO4' val={'c': 'Li<sub>4</sub>SiO<sub>4</sub>', 's': 'diva2:895301', 'n': 'correct in original'}

Also removed all of the unnecessary paragraph breaks.

corrected abstract:
<p>Lithium orthosilicate (Li<sub>4</sub>SiO<sub>4</sub>) is known to be a high temperature CO2 capture material. This work was focused on comparing Li<sub>4</sub>SiO<sub>4</sub> to the extensively studied CaO as an adsorbent in sorption enhanced catalysis. Thermogravimetric analysis was used to study the effects of sorption temperature and compaction on Li<sub>4</sub>SiO<sub>4</sub> using 15vol% CO2 in N2. After 2 hours of CO2 adsorption at 550°C the powder reached 35wt% uptake of CO<sub>2</sub>, corresponding to 93.6% of maximum efficiency and complete regeneration was possible at 700°C. Pressing Li<sub>4</sub>SiO<sub>4</sub> to granular forms greatly decreased CO2 adsorption rates. Efforts to impregnate γ-Al2O3 with the suspended SiO2 solution from aqueous based sol-gel synthesis to produce nanodispersed Li<sub>4</sub>SiO<sub>4</sub> failed due to the inability to form the targeted Li<sub>4</sub>SiO<sub>4</sub> complex. X-ray diffraction analysis indicated the formation of the gel is crucial for the formation of the crystalline Li<sub>4</sub>SiO<sub>4</sub> phase.</p><p>A microreactor was used to study the steam reforming of ethanol over a series of 1% Pt on γ-Al<sub>2</sub>O</sub>3</sub> catalyst composite impregnated over a range of nanodispersed CaO loading at S/C=1.5 in dilution. At 400°C enhancement could be observed with the presence of CaO sorbent compared to only 1% Pt/Al<sub>2</sub>O<sub>3</sub>. However, production quickly diminished due to high carbon deposition. For 1% Pt/Al<sub>2</sub>O<sub>3</sub> tested at 400°C, ethylene production was 5 times higher than for hydrogen. Above 550°C the ethylene production was reduced to 0.18vol% and gas production stability was greatly improved for 1% Pt/Al<sub>2</sub>O<sub>3</sub> and even more so with the addition of impregnated CaO sorbent. Hydrogen yields from homogeneous mixtures of 1%Pt/Al<sub>2</sub>O<sub>3</sub> with Li<sub>4</sub>SiO<sub>4</sub> powder in a microreactor were about 20% higher than those achievable of the same mixture with CaO powder. However, the composite 1%Pt/Al<sub>2</sub>O<sub>3</sub> with 7.02wt% dispersed CaO gave 100% higher hydrogen production under similar conditions despite Li<sub>4</sub>SiO<sub>4</sub> being a superior carbon capture material.</p>
----------------------------------------------------------------------
In diva2:853824 abstract is: <p>The Human Leukocyte Antingen (HLA) genes encode for the Major Histocompatibillity Complex (MHC) and are know to be highly polymorphic. The HLA genes consist of more than 220 genes and are well studied; due to their high importance in for example organ and stem cell transplantations. HLA polymorphism between a donor and a recipient to achieve a successful transplantation. Sequence-specific oligonucleotie (SSO), Sequence-specific priming (SSP) and Sequenced-based typing (SBT) are PCR-based HLA typing methods that have been used for several years. However, these methods have limitations in terms of resolution and ambiguities. Next Generation Sequencing (NGS) technologies have the ability to overcome these limitations and the demand of an NGS-based HLA typing approach is increasing. This project aims to assess the feasibility and to implement NGS-based HLA typing for diagnostic purposes. A market analysis is performed to understand the market demand and the limitations with today's technologies. hree NGS platforms (Illumina MiSeq system, PacBio RS II and Ion Torrent PGM 314) are compared in two different benchmarks. Next, we practically implement an HLA typing approach by the Illumina MiSeq sequencing technology. In conclusion, the Illumina MiSeq system is today the most suitable platform for HLA typing and provides reagent kits for low and high throughput HLA typing. </p>

w='hree' val={'c': 'Three', 's': 'diva2:853824', 'n': 'no full text'}
w='oligonucleotie' val={'c': 'oligonucleotide', 's': 'diva2:853824', 'n': 'no full text'}

corrected abstract:
<p>The Human Leukocyte Antingen (HLA) genes encode for the Major Histocompatibillity Complex (MHC) and are know to be highly polymorphic. The HLA genes consist of more than 220 genes and are well studied; due to their high importance in for example organ and stem cell transplantations. HLA polymorphism between a donor and a recipient to achieve a successful transplantation. Sequence-specific oligonucleotide (SSO), Sequence-specific priming (SSP) and Sequenced-based typing (SBT) are PCR-based HLA typing methods that have been used for several years. However, these methods have limitations in terms of resolution and ambiguities. Next Generation Sequencing (NGS) technologies have the ability to overcome these limitations and the demand of an NGS-based HLA typing approach is increasing. This project aims to assess the feasibility and to implement NGS-based HLA typing for diagnostic purposes. A market analysis is performed to understand the market demand and the limitations with today's technologies. Three NGS platforms (Illumina MiSeq system, PacBio RS II and Ion Torrent PGM 314) are compared in two different benchmarks. Next, we practically implement an HLA typing approach by the Illumina MiSeq sequencing technology. In conclusion, the Illumina MiSeq system is today the most suitable platform for HLA typing and provides reagent kits for low and high throughput HLA typing.</p>
----------------------------------------------------------------------
In diva2:1540188 abstract is: <p><strong>Background and objectives:</strong> TISSIUM light-activated adhesive was investigated as an alternative to tissue-penetrating products to fix meshes in intraperitoneal laparoscopic ventral hernia repair. The objective of this study was to ensure efficient polymer light activation through commercial meshes and to assess the acute and chronic fixation strength of the light-activated adhesive in a porcine model in comparison to commercial fixation products.</p><p><strong>Methods:</strong> A spectroscopic analysis was conducted on the light-activated adhesive through three different meshes (1, 2, and 3) to quantify the acrylate conversion associated with the level of polymer cross-linking. Two setups were implemented: a static (light source fixed over a drop of polymer) and a dynamic (light source rotated around a pattern of polymer to mimic the surgical procedure). Hernia defects were created in porcine models and repaired either using the light-activated adhesive or a commercial product (A, B, C, and D) to fix a mesh. For each tested condition, the acute and chronic (3 months) fixation strength performances were assessed using burst ball and t-peel mechanical tests.</p><p><strong>Results:</strong> The light activation proved to be effective (more than 90% of the acrylates converted) in static in 7 seconds through the three meshes and in dynamic between 3 min and 5 min 32 sdepending on the considered mesh. In a burst ball test, the light-activated adhesive reached between 42 and 84% of the commercial products’ acute performance with the three meshes (between 75,9 and 95,9 N) and reached 88% of the commercial product A’s chronic performance with mesh 1 (610,1 N). A t-peel test demonstrated similar strength of ingrowth for the repairs using the light-activated adhesive or the commercial product A at the 3-month timepoint with mesh 1 (2,55 and 2,37 N/cm respectively).</p><p><strong>Conclusions: </strong>Data suggest the light-activated adhesive has the potential to be used in intraperitoneal laparoscopic ventral hernia repair. In a reasonable time, the adhesive is efficiently light-activated through commercial meshes. The light-activated adhesive’s performances to fix commercial meshes, both acute and chronic, are similar to commercial products, but with a strong advantage of not being tissue penetrating.</p>

w='sdepending' val={'c': 'depending', 's': 'diva2:1540188', 'n': 'the "s" is from the units of the previoous number - note there is a line break after "32 s" and before "depending"'}
w='t-peel' val={'c': 'T-peel', 's': 'diva2:1540188', 'n': 'error in original'}

corrected abstract:
<p><strong>Background and objectives:</strong> TISSIUM light-activated adhesive was investigated as an alternative to tissue-penetrating products to fix meshes in intraperitoneal laparoscopic ventral hernia repair. The objective of this study was to ensure efficient polymer light activation through commercial meshes and to assess the acute and chronic fixation strength of the light-activated adhesive in a porcine model in comparison to commercial fixation products.</p><p><strong>Methods:</strong> A spectroscopic analysis was conducted on the light-activated adhesive through three different meshes (1, 2, and 3) to quantify the acrylate conversion associated with the level of polymer cross-linking. Two setups were implemented: a static (light source fixed over a drop of polymer) and a dynamic (light source rotated around a pattern of polymer to mimic the surgical procedure). Hernia defects were created in porcine models and repaired either using the light-activated adhesive or a commercial product (A, B, C, and D) to fix a mesh. For each tested condition, the acute and chronic (3 months) fixation strength performances were assessed using burst ball and t-peel mechanical tests.</p><p><strong>Results:</strong> The light activation proved to be effective (more than 90% of the acrylates converted) in static in 7 seconds through the three meshes and in dynamic between 3 min and 5 min 32 s depending on the considered mesh. In a burst ball test, the light-activated adhesive reached between 42 and 84% of the commercial products’ acute performance with the three meshes (between 75,9 and 95,9 N) and reached 88% of the commercial product A’s chronic performance with mesh 1 (610,1 N). A t-peel test demonstrated similar strength of ingrowth for the repairs using the light-activated adhesive or the commercial product A at the 3-month timepoint with mesh 1 (2,55 and 2,37 N/cm respectively).</p><p><strong>Conclusions:</strong> Data suggest the light-activated adhesive has the potential to be used in intraperitoneal laparoscopic ventral hernia repair. In a reasonable time, the adhesive is efficiently light-activated through commercial meshes. The light-activated adhesive’s performances to fix commercial meshes, both acute and chronic, are similar to commercial products, but with a strong advantage of not being tissue penetrating.</p>
----------------------------------------------------------------------
In diva2:922823 abstract is: <p>The waste produced at Swedish nuclear power plants is deposited in an underground repository, SFR. Different substances in the waste or in degradation products may exhibit complexation properties. The complexation of deposited radionuclides with such ligands may affect the mobility of the radionuclides and decrease their sorption on the cement phase, which is used as protective barrier in SFR. In this study, metal ions adsorption to cement in the presence of fiber mass UP2 degradation products under alkaline conditions has been studied. Model metal ions used in this work were nickel (II) and europium (III).</p><p>It has been shown that alkaline degradation of UP2 is dependent on both pH and temperature. Alkaline degradation of UP2 was performed at two different alkalinities. Degradation at pH≈12.5 and constant stirring gave a TOC content of about 900 mg/L, after 6 month at 32 °C. Degradation at higher pH≈13.5, but at the same temperature and stirring conditions, resulted in a TOC concentration of about 5100 mg/L after 6 month degradation. Additionally, UP2 also degraded at elevated temperature. Result from this case gave a TOC concentration of about 252 mg/L after 3 month, when UP2 was conditioned at pH≈12.5 and T=60-65 °C without continuous stirring. Degradation at these forced conditions after 6 month resulted in a decreased pH of the sample (pH=9.3) and a TOC content of about 1100 mg/L.</p><p>Sorption experiment of Ni(II) and Eu(III) onto hydrated cement was made in presence of the UP2 degradation products at different solid to liquid (S:L) ratio (available cement surface area). A precipitate was found on the cement material in the Ni(II) sorption experiments which prevents a proper evaluation of the results. The Kd values for Eu(III) sorption after 33 days of equilibration in the presence of degradation products, is in the range 0.1- 0.28 [m<sup>3</sup>/kg] for cement with a BET specific surface area of about 94.9 m<sup>2</sup>/g, i.e. giving a partition coefficient of (1.0- 3.0)×10-6 m<sup>3</sup>/m<sup>2</sup>.</p>

w='Kd' val={'c': 'K<sub>d</sub>', 's': ['diva2:922823', 'diva2:1447074'], 'n': 'no full text for diva2:922823 but correct in source for diva2:1447074'}

corrected abstract:
<p>The waste produced at Swedish nuclear power plants is deposited in an underground repository, SFR. Different substances in the waste or in degradation products may exhibit complexation properties. The complexation of deposited radionuclides with such ligands may affect the mobility of the radionuclides and decrease their sorption on the cement phase, which is used as protective barrier in SFR. In this study, metal ions adsorption to cement in the presence of fiber mass UP2 degradation products under alkaline conditions has been studied. Model metal ions used in this work were nickel (II) and europium (III).</p><p>It has been shown that alkaline degradation of UP2 is dependent on both pH and temperature. Alkaline degradation of UP2 was performed at two different alkalinities. Degradation at pH≈12.5 and constant stirring gave a TOC content of about 900 mg/L, after 6 month at 32 °C. Degradation at higher pH≈13.5, but at the same temperature and stirring conditions, resulted in a TOC concentration of about 5100 mg/L after 6 month degradation. Additionally, UP2 also degraded at elevated temperature. Result from this case gave a TOC concentration of about 252 mg/L after 3 month, when UP2 was conditioned at pH≈12.5 and T=60-65 °C without continuous stirring. Degradation at these forced conditions after 6 month resulted in a decreased pH of the sample (pH=9.3) and a TOC content of about 1100 mg/L.</p><p>Sorption experiment of Ni(II) and Eu(III) onto hydrated cement was made in presence of the UP2 degradation products at different solid to liquid (S:L) ratio (available cement surface area). A precipitate was found on the cement material in the Ni(II) sorption experiments which prevents a proper evaluation of the results. The K<sub>d</sub> values for Eu(III) sorption after 33 days of equilibration in the presence of degradation products, is in the range 0.1- 0.28 [m<sup>3</sup>/kg] for cement with a BET specific surface area of about 94.9 m<sup>2</sup>/g, i.e. giving a partition coefficient of (1.0- 3.0)×10-6 m<sup>3</sup>/m<sup>2</sup>.</p>
----------------------------------------------------------------------
In diva2:637933 abstract is: <p>Ultrasound is regarded a convenient and safe tool to acquire diagnostic information that we need for clinical use. For a long time ultrasound has been counted as a harmless method, but after all, there is a heating and a me-chanical impact by ultrasound exposure. This influence can reveal both positive (e.g., cell plant growth) and negative (e.g. cell death) effects. Acoustic exposure pattern changed drastically in recent years due to the rapid, technological developments in ultrasound imaging. Ultrasound imaging has become more sophisticated and new techniques are becoming more common, bringing with them not only increased diagnostic capabilities, but also potential threats as far as safety considerations are concerned. The goal of the thesis project is to analyze the ultrasound field characteristics, based on which research would be achievable in the future about how cells are affected by ultrasound exposure with different basic parameters. These parameters include excitation pressure amplitude, number of cycles in a pulse (n), pulse repetition frequency (PRF), acoustic working frequency (f), phase of ultrasound, shape of ultrasound wave (window mode). Some pilot cell experiments are also done in this project.</p><p>Ultrasound-induced bioeffects on cells have been studied by many scientists, and some experiments tell us that ultrasound beams may cause serious mechanical and thermal damage on e.g. cells. Two general indices, the thermal index (TI) the mechanical index (MI) reflect information on the output level of the ultrasound machine and how a change in output would affect the likelihood of inducing a biological effect. Besides these two indices, other six parameters also are valuable to help us understand the potential threat of ultrasound applications. These parameters are peak negative pressure, peak positive pressure, spatial peak temporal peak intensity (Isptp), spatial peak temporal average intensity (Ispta), spatial peak pulse average intensity (Isppa) and output power of transducer (Wo). The above mentioned eight parameters are important in analyzing the acoustic beams.</p><p>During the first phase of the experiment (acquisition of ultrasound field parameters) a hydrophone was put at the focus point of the ultrasound beam to acquire the time domain waveform signal of the ultrasound waves. By setting up f, PRF, n, phase and window mode into the computer controlled pulser (SNAP system, Ritec Inc), dif-ferent beams were sent to the hydrophone. Different combinations of basic parameters lead to 186 sets of acoustic beams. We used the hydrophone and oscilloscope to record the waveform signal respectively. Then by self-designed MATLAB software (Mathematical Computing Software, MATLAB®, Natick, Massachusetts, United States), the desired eight characteristics of acoustic field were calculated.</p><p>Human chronic myelogenous leukemia cell line (K562) were exposed to defined ultrasound waves in the second phase of the experiment. Both trypan blue and resazurin viability assays were used to evaluate effect on the cells immediately after the exposure and 24 hours after the exposure. Resazurin viability assay conducted immediately after the exposure showed reduction of the cell viability up to 46% when the attenuation of amplitude is 0 dB (i.e. the output is the biggest). No cell death was induced. It also showed that after 24 hours the cells viability partially recovered to about 85%. Trypan blue assay showed nearly no cell death was induced.</p>

w='me-chanical' val={'c': 'mechanical', 's': 'diva2:637933', 'n': 'hyphen at end of line in original'}
Also there were missing subscripts.


corrected abstract:
<p>Ultrasound is regarded a convenient and safe tool to acquire diagnostic information that we need for clinical use. For a long time ultrasound has been counted as a harmless method, but after all, there is a heating and a mechanical impact by ultrasound exposure. This influence can reveal both positive (e.g., cell plant growth) and negative (e.g. cell death) effects. Acoustic exposure pattern changed drastically in recent years due to the rapid, technological developments in ultrasound imaging. Ultrasound imaging has become more sophisticated and new techniques are becoming more common, bringing with them not only increased diagnostic capabilities, but also potential threats as far as safety considerations are concerned. The goal of the thesis project is to analyze the ultrasound field characteristics, based on which research would be achievable in the future about how cells are affected by ultrasound exposure with different basic parameters. These parameters include excitation pressure amplitude, number of cycles in a pulse (n), pulse repetition frequency (PRF), acoustic working frequency (f), phase of ultrasound, shape of ultrasound wave (window mode). Some pilot cell experiments are also done in this project.</p><p>Ultrasound-induced bioeffects on cells have been studied by many scientists, and some experiments tell us that ultrasound beams may cause serious mechanical and thermal damage on e.g. cells. Two general indices, the thermal index (TI) the mechanical index (MI) reflect information on the output level of the ultrasound machine and how a change in output would affect the likelihood of inducing a biological effect. Besides these two indices, other six parameters also are valuable to help us understand the potential threat of ultrasound applications. These parameters are peak negative pressure, peak positive pressure, spatial peak temporal peak intensity (I<sub>sptp</sub>), spatial peak temporal average intensity (I<sub>spta</sub>), spatial peak pulse average intensity (I<sub>sppa</sub>) and output power of transducer (Wo). The above mentioned eight parameters are important in analyzing the acoustic beams.</p><p>During the first phase of the experiment (acquisition of ultrasound field parameters) a hydrophone was put at the focus point of the ultrasound beam to acquire the time domain waveform signal of the ultrasound waves. By setting up f, PRF, n, phase and window mode into the computer controlled pulser (SNAP system, Ritec Inc), different beams were sent to the hydrophone. Different combinations of basic parameters lead to 186 sets of acoustic beams. We used the hydrophone and oscilloscope to record the waveform signal respectively. Then by self-designed MATLAB software (Mathematical Computing Software, MATLAB®, Natick, Massachusetts, United States), the desired eight characteristics of acoustic field were calculated.</p><p>Human chronic myelogenous leukemia cell line (K562) were exposed to defined ultrasound waves in the second phase of the experiment. Both trypan blue and resazurin viability assays were used to evaluate effect on the cells immediately after the exposure and 24 hours after the exposure. Resazurin viability assay conducted immediately after the exposure showed reduction of the cell viability up to 46% when the attenuation of amplitude is 0 dB (i.e. the output is the biggest). No cell death was induced. It also showed that after 24 hours the cells viability partially recovered to about 85%. Trypan blue assay showed nearly no cell death was induced.</p>
----------------------------------------------------------------------
In diva2:860338 abstract is: <p>Chemotherapeutic drugs such as carboplatin/gemcitabine administered to non small cell lung cancer (NSCLC) patients frequently induce myelosuppression toxicity potentially leading to reduction or removal of drugs. We set out to identify the genetic variants associated with toxicity induced myekosuppression by whole exome sequencing (WES) 216 NSCLC patients and associating biallelic variants with different quantitative and qualitative measurements of myelosuppression phenotypes.</p><p>WES identified on average 29834 variants in each patient. Biallelic variants from combined patients genotype were associated with each myelosupression phenotype - Thrombocytopenia (TPK), Leukopenia (LPK) and Neutropenia (NPK) using quantitative Log-transforemd (LN) and Empirical normal quantile transformation (ENQT) phenotypes and qualitative high/low toxicity study design in linear and logistic regression methods. Additionally, gene-based SKATO tests were performed for transforemd quantitative phenotypes to investigate enrichment of rare and common variants.</p><p>Due to sample size limitation, none of the variants reached multiple corrected Bonferroni significant or FDR-BH p - values. However, variants with p-value &lt;1.00x 10<sup>-3 </sup><sub>in each studydesign were evaluated for high toxicity. We found five, one and two variants for TPK, LPK and NPK respectively associated in all quantitative and qualitative single variant association study. Furthermore, single variant rs4808 in <em>CAPZA2 </em>and rs 8018462 in <em>SLC7A7 </em>genes were identified by Gene-based SKATO test for TPK and LPK phenotypes. This results could implicated association of <em>CAPZA2</em> and <em>SLC7A7</em> to TPK and LPK myleosupression. However, validation and replication of the variants and genes needs to be further studied in an independent study.</sub></p>

w='myekosuppression' val={'c': 'myelosuppression', 's': 'diva2:860338', 'n': 'no full text'}
w='myelosupression' val={'c': 'myelosuppression', 's': 'diva2:860338', 'n': 'no full text'}
w='myleosupression' val={'c': 'myelosuppression', 's': 'diva2:860338', 'n': 'no full text'}
Also one merged pair of words.

corrected abstract:
<p>Chemotherapeutic drugs such as carboplatin/gemcitabine administered to non small cell lung cancer (NSCLC) patients frequently induce myelosuppression toxicity potentially leading to reduction or removal of drugs. We set out to identify the genetic variants associated with toxicity induced myelosuppression by whole exome sequencing (WES) 216 NSCLC patients and associating biallelic variants with different quantitative and qualitative measurements of myelosuppression phenotypes.</p><p>WES identified on average 29834 variants in each patient. Biallelic variants from combined patients genotype were associated with each myelosuppression phenotype - Thrombocytopenia (TPK), Leukopenia (LPK) and Neutropenia (NPK) using quantitative Log-transforemd (LN) and Empirical normal quantile transformation (ENQT) phenotypes and qualitative high/low toxicity study design in linear and logistic regression methods. Additionally, gene-based SKATO tests were performed for transforemd quantitative phenotypes to investigate enrichment of rare and common variants.</p><p>Due to sample size limitation, none of the variants reached multiple corrected Bonferroni significant or FDR-BH p - values. However, variants with p-value &lt;1.00x 10<sup>-3</sup> in each study design were evaluated for high toxicity. We found five, one and two variants for TPK, LPK and NPK respectively associated in all quantitative and qualitative single variant association study. Furthermore, single variant rs4808 in <em>CAPZA2</em> and rs 8018462 in <em>SLC7A7 </em>genes were identified by Gene-based SKATO test for TPK and LPK phenotypes. This results could implicated association of <em>CAPZA2</em> and <em>SLC7A7</em> to TPK and LPK myelosuppression. However, validation and replication of the variants and genes needs to be further studied in an independent study.</sub></p>
----------------------------------------------------------------------
In diva2:899310 abstract is: <p>The climate change is one of the biggest environmental problems we face today and is due to the increased greenhouse effect caused by mainly carbon dioxide.  SÖRAB is a waste management company in Sweden who has been getting questions from its customers about the amount of carbon dioxide that is emitted when different types of waste is recycled through their process. This thesis answers this question for five different waste types: gypsum, plastic, textile, concrete and tires. In addition to the emissions from the recycling process in SÖRAB the emissions from another alternative process for each waste is estimated. The alternative process will be either incineration or landfill. With the help of both the alternative process and SÖRABs process an environmental benefit is calculated where a positive environmental benefit means that a saving of carbon dioxide emissions has been made by not sending the waste to incineration or landfill.</p><p> </p><p>The carbon emission from the following parts of the process is included:</p><ul><li><p>Transportation of the waste (includes transport by truck and ferry)</p><ul><li><p>Transportation within SÖRABs different facilities</p></li><li><p>Transportation from SÖRABs facilities external recycling companies.</p></li></ul></li><li><p>The energy consumption from ”waste crusher” machine in SÖRAB</p></li><li><p>Fuel consumption from the machines in SÖRAB</p></li><li><p> The carbon dioxide emissions from the external companies where the waste is recycled. This also includes the transportation in the case a part of the waste is further transported to be incinerated and the emission from the incineration itself.</p></li></ul><p> </p><p>The results showed that the carbon dioxide emissions for each waste differed very much. The differences were due to each waste recycling process in SÖRAB, which differed in the transported weights, transport lengths, and emission levels in the external company. For plastics, textiles and tires, it was the transportation to the external companies which was the factor that contributed the most to the total carbon dioxide emission. For gypsum and concrete it was the emissions in the external companies that contributed the most, which is due to transport length being shorter for their recycling processes.</p><p> </p><p>The environmental benefit was the highest for the recycling of tires, 611 kg C0<sub>2</sub>/ton plastic is saved by not sending it to incineration and instead recycle it through SÖRABs process.  The result is mainly due to the alternatives process emission from the ferry and incineration.</p><p> </p><p>The environmental benefits were positive for all the waste types except for plastic</p><p> (-9,85)  kg CO<sub>2</sub>/ton textile) which had a long transportation length in comparison to the alternative process which was to send to incineration. In this report it was nevertheless concluded that this doesn’t mean that it would be a better option to incinerate the plastic since the real environmental benefit of plastic and all the other waste types is in the amount of carbon dioxide that is being saved by not needing to manufacture more of the material that is being regenerated through the recycling process.</p><p> </p>

w='C0' val={'c': 'CO', 's': 'diva2:899310'}
w='C0<sub>2</sub>/ton' val={'c': 'CO<sub>2</sub>/ton', 's': 'diva2:899310'}

Cleaned up the unnecessary HTML.

corrected abstract:
<p>The climate change is one of the biggest environmental problems we face today and is due to the increased greenhouse effect caused by mainly carbon dioxide.  SÖRAB is a waste management company in Sweden who has been getting questions from its customers about the amount of carbon dioxide that is emitted when different types of waste is recycled through their process. This thesis answers this question for five different waste types: gypsum, plastic, textile, concrete and tires. In addition to the emissions from the recycling process in SÖRAB the emissions from another alternative process for each waste is estimated. The alternative process will be either incineration or landfill. With the help of both the alternative process and SÖRABs process an environmental benefit is calculated where a positive environmental benefit means that a saving of carbon dioxide emissions has been made by not sending the waste to incineration or landfill.</p><p>The carbon emission from the following parts of the process is included:<ul><li>Transportation of the waste (includes transport by truck and ferry)<ul><li>Transportation within SÖRABs different facilities</li><li>Transportation from SÖRABs facilities external recycling companies.</li></ul></li><li>The energy consumption from ”waste crusher” machine in SÖRAB</li><li>Fuel consumption from the machines in SÖRAB</li><li>The carbon dioxide emissions from the external companies where the waste is recycled. This also includes the transportation in the case a part of the waste is further transported to be incinerated and the emission from the incineration itself.</li></ul></p><p>The results showed that the carbon dioxide emissions for each waste differed very much. The differences were due to each waste recycling process in SÖRAB, which differed in the transported weights, transport lengths, and emission levels in the external company. For plastics, textiles and tires, it was the transportation to the external companies which was the factor that contributed the most to the total carbon dioxide emission. For gypsum and concrete it was the emissions in the external companies that contributed the most, which is due to transport length being shorter for their recycling processes.</p><p>The environmental benefit was the highest for the recycling of tires, 611 kg CO<sub>2</sub>/ton plastic is saved by not sending it to incineration and instead recycle it through SÖRABs process.  The result is mainly due to the alternatives process emission from the ferry and incineration.</p><p>The environmental benefits were positive for all the waste types except for plastic (-9,85) kg CO<sub>2</sub>/ton textile) which had a long transportation length in comparison to the alternative process which was to send to incineration. In this report it was nevertheless concluded that this doesn’t mean that it would be a better option to incinerate the plastic since the real environmental benefit of plastic and all the other waste types is in the amount of carbon dioxide that is being saved by not needing to manufacture more of the material that is being regenerated through the recycling process.</p>
----------------------------------------------------------------------
In diva2:827347 abstract is: <p>Through laboratory experiments, dewatering and retention of nanocellulose in a DDA (Dynamic Drainage Analyzer) were analysed. By adding retention chemicals in varied amounts, the effects on the dewatering was shown. The Job initiator was Innventia and the laboratory work were made at SP's laboratory where the DDA instrument was available. The DDA instrument is similar to the machine that is used for paper manufacturing in a large scale and therefore, it could be possible to dewater nanocellulose on a similar instrument.</p><p>The goal of the thesis was to develop a nanocellulose film with good barrier properties but also to see how adding carrier fibers effect the properties of the film. The goal was also to see if the retention and dewatering time of nanocellulose are connected and whether it is possible to obtain repeatable results.</p><p>Two different wires were also examined in the laboratory experiments in a DDA, the Albanywire and the Stratexwire. The Albany wire was denser than the Stratex wire and the effect that the density caused on retention and dewatering time was examined.</p><p>Nanocellulose or Microfibrillated cellulose (MFC) is a new and renewable material that is made from wood fibers and is characterized by its gelatinous appearance. [4] Nanocellulose is suited for a variety of products, such as barriers, alone in the form of films or mixed in products. In the manufacture of nanocellulose a homogenizer is used which decomposes cellulose fibers to fibrils fibril aggregate. This was previously a problem while the fibers clogged the homogenizer and the production had a high energy consumption. [4] When it comes to making a nanocellulose film the problems with dewatering remains.</p><p>The nanocellulose was diluted to the desired concentration and before the laboratory experiments it was run through a homogenizer, to disperse the fibrils in the liquid after the dilution. The carrier fibers was prepared in a blender with two liters of tap water before it was added to the homogenized nanocellulose.</p><p>During the experiment in the DDA the amount and proportion of the MFC (microfibrillar cellulose) and carrier fibers (Modorefmassa) was varied. To the MFC and carrier fiber suspension in the DDA two retention chemicals were added in each experiment, C- PAM PL -1520 and EKA NP- 780 in varying amounts. After the dewatering of nanocellulose in the DDA the films were pressed at different pressures and times, thereafter the oxygen permeability was analyzed.</p><p>The film that was considered the most suitable referring to dewatering in the DDA during the attempts was at 0.2 % with 90 % MFC and 10 % carrier fibers. The film gave the highest retention, a good oxygen barrier and was easy to handle. What can be seen from the results of the oxygen barrier measurement is that at 0.2 % with 90 % MFC and 10% carrier fibers obtained the lowest value OTR (oxygen transmission rate), which indicates on a good oxygen barrier. Retention at this concentration was the highest at 87.1 %, while the drainage time was nearly 250 seconds. The dewatering time was high, however during this concentration it’s sometimes difficult to see when the dewatering ended while the time was clocked manually.</p><p>Conclusions from the results are that the carrier fibers doesn’t have a negatively effect on the films, rather they can benefit both the retention and oxygen barrier, however a higher paper weight was obtained and the dewatering time became longer.</p>

w='PL' val={'c': 'PL -1520', 's': 'diva2:827347', 'n': 'correct in original; the space before the - is in the DiVA abstract only'}

corrected abstract:
<p>Through laboratory experiments, dewatering and retention of nanocellulose in a DDA (Dynamic Drainage Analyzer) were analysed. By adding retention chemicals in varied amounts, the effects on the dewatering was shown. The Job initiator was Innventia and the laboratory work were made at SP's laboratory where the DDA instrument was available. The DDA instrument is similar to the machine that is used for paper manufacturing in a large scale and therefore, it could be possible to dewater nanocellulose on a similar instrument.</p><p>The goal of the thesis was to develop a nanocellulose film with good barrier properties but also to see how adding carrier fibers effect the properties of the film. The goal was also to see if the retention and dewatering time of nanocellulose are connected and whether it is possible to obtain repeatable results.</p><p>Two different wires were also examined in the laboratory experiments in a DDA, the Albanywire and the Stratexwire. The Albany wire was denser than the Stratex wire and the effect that the density caused on retention and dewatering time was examined.</p><p>Nanocellulose or Microfibrillated cellulose (MFC) is a new and renewable material that is made from wood fibers and is characterized by its gelatinous appearance. [4] Nanocellulose is suited for a variety of products, such as barriers, alone in the form of films or mixed in products. In the manufacture of nanocellulose a homogenizer is used which decomposes cellulose fibers to fibrils fibril aggregate. This was previously a problem while the fibers clogged the homogenizer and the production had a high energy consumption. [4] When it comes to making a nanocellulose film the problems with dewatering remains.</p><p>The nanocellulose was diluted to the desired concentration and before the laboratory experiments it was run through a homogenizer, to disperse the fibrils in the liquid after the dilution. The carrier fibers was prepared in a blender with two liters of tap water before it was added to the homogenized nanocellulose.</p><p>During the experiment in the DDA the amount and proportion of the MFC (microfibrillar cellulose) and carrier fibers (Modorefmassa) was varied. To the MFC and carrier fiber suspension in the DDA two retention chemicals were added in each experiment, C-PAM PL-1520 and EKA NP-780 in varying amounts. After the dewatering of nanocellulose in the DDA the films were pressed at different pressures and times, thereafter the oxygen permeability was analyzed.</p><p>The film that was considered the most suitable referring to dewatering in the DDA during the attempts was at 0.2 % with 90 % MFC and 10 % carrier fibers. The film gave the highest retention, a good oxygen barrier and was easy to handle. What can be seen from the results of the oxygen barrier measurement is that at 0.2 % with 90 % MFC and 10% carrier fibers obtained the lowest value OTR (oxygen transmission rate), which indicates on a good oxygen barrier. Retention at this concentration was the highest at 87.1 %, while the drainage time was nearly 250 seconds. The dewatering time was high, however during this concentration it’s sometimes difficult to see when the dewatering ended while the time was clocked manually.</p><p>Conclusions from the results are that the carrier fibers doesn’t have a negatively effect on the films, rather they can benefit both the retention and oxygen barrier, however a higher paper weight was obtained and the dewatering time became longer.</p>
----------------------------------------------------------------------
In diva2:1186252 abstract is: <p>The most common way to measure postural stability is to examine the displacement of the center of pressure (CoP). But some scientists claim that the center of mass (CoM) is what really indicates the sway of the whole body, since the body is a multi-joint system. Many previous studies of human balance have targeted groups with diffrent kinds of balance impairments. In a recent study C. Lidbeck investigated factors influencing standing in children with bilateral spastic cerebral palsy (BSCP). The conclusion of that study was that the crouched position, that is common with this kind of disability, was not found to be related to strength and not entirely related to the degree of their motor disorders.</p><p>In this thesis a number methods were chosen to assess the postural stability of children with BSCP, using both the CoP and the CoM. The hypothesis was that the different methods would show different aspects of the children's balance impairment. Also, the influence of visual stimuli on the crouching position was examined. The long term aim is that the results may contribute to a deeper understanding of the balance disturbances that often accompany this group of children.</p><p>16 children with BSCP (GMFCS level I-III) and 20 typically developing (TD) children were included in the study. Data was collected, before the start of this project, using two force plates and an eight-camera 3D motion analysis system with passive markers. The children performed three different standing tasks during 30 seconds each; quiet standing, blindfolded and an attention-task. Five methods were chosen (based on previous literature) and implemented in Matlab to examine the postural stability of the two groups during the three tasks.</p><p>Result shows that all methods used can clearly distinguish between the balance in the BSCP group and the TD group. When comparing the quiet standing task with the blindfolded task in the BSCP group, there were some significant results from the statistical evaluation (P$&lt;$0.05). The result from several of the methods indicated that the children of this group have better postural stability when blindfolded, which is not in agreement with previous literature. In contrast, one method using the total mean velocity indicated that the postural stability decreased. During the attention-task, the methods disagreed with each other, implying a change in balance strategy in the BSCP group that was different from the TD group.</p><p>Four methods are suggested for future studies, two using the CoP and two using the CoM. These four methods highlighted different aspects of the data and in combination they may provide a bigger picture of the postural stability of children with BSCP. Even though there were no significant difference in the vertical displacement of the CoM between the BSCP and the TD group, the CoM was slightly elevated during the attention-task in the BSCP group. In the TD group the CoM was lowered during the same task. This indicates that the children with BSCP in this study straighten up a bit when they can focus on something outside of their own body.</p>

w='diffrent' val={'c': 'different', 's': ['diva2:1186252', 'diva2:935159']}
There were also other missing ligatures.
The $ expression $ was not really necessary, it is just to put a &lt; symbol in.

corrected abstract:
<p>The most common way to measure postural stability is to examine the displacement of the center of pressure (CoP). But some scientists claim that the center of mass (CoM) is what really indicates the sway of the whole body, since the body is a multi-joint system. Many previous studies of human balance have targeted groups with different kinds of balance impairments. In a recent study C. Lidbeck investigated factors influencing standing in children with bilateral spastic cerebral palsy (BSCP). The conclusion of that study was that the crouched position, that is common with this kind of disability, was not found to be related to strength and not entirely related to the degree of their motor disorders.</p><p>In this thesis a number methods were chosen to assess the postural stability of children with BSCP, using both the CoP and the CoM. The hypothesis was that the different methods would show different aspects of the children's balance impairment. Also, the influence of visual stimuli on the crouching position was examined. The long term aim is that the results may contribute to a deeper understanding of the balance disturbances that often accompany this group of children.</p><p>16 children with BSCP (GMFCS level I-III) and 20 typically developing (TD) children were included in the study. Data was collected, before the start of this project, using two force plates and an eight-camera 3D motion analysis system with passive markers. The children performed three different standing tasks during 30 seconds each; quiet standing, blindfolded and an attention-task. Five methods were chosen (based on previous literature) and implemented in Matlab to examine the postural stability of the two groups during the three tasks.</p><p>Result shows that all methods used can clearly distinguish between the balance in the BSCP group and the TD group. When comparing the quiet standing task with the blindfolded task in the BSCP group, there were some significant results from the statistical evaluation (P &lt; 0.05). The result from several of the methods indicated that the children of this group have better postural stability when blindfolded, which is not in agreement with previous literature. In contrast, one method using the total mean velocity indicated that the postural stability decreased. During the attention-task, the methods disagreed with each other, implying a change in balance strategy in the BSCP group that was different from the TD group.</p><p>Four methods are suggested for future studies, two using the CoP and two using the CoM. These four methods highlighted different aspects of the data and in combination they may provide a bigger picture of the postural stability of children with BSCP. Even though there were no significant difference in the vertical displacement of the CoM between the BSCP and the TD group, the CoM was slightly elevated during the attention-task in the BSCP group. In the TD group the CoM was lowered during the same task. This indicates that the children with BSCP in this study straighten up a bit when they can focus on something outside of their own body.</p>
----------------------------------------------------------------------
In diva2:827212 abstract is: <p>The assignment was to produce activated Carbon (AC) using chemical activation and determine how the reaction conditions affect its quality. The parameters were: temperature 400, 450 and 500(°C), the impregnation base KOH and NaCO3 as well as the impregnation ratio of these bases in relation to the biomass 1, 1.5 and 2 *mbio. Mbio is the weight of the biomass and thus the weight relationship was 1:1-, 1:1,5- and 1:2 (mbio : msolution). The concentration of the solution was 20 % by weight.</p><p>In Cuba 2007-2010, the biggest environmental problems were identified by CITMA, an organization working to prevent and neutralize environmental threats. Among these problems were pollution of water, air and land and the shortage of water owing to salt infringement and other contaminations.AC is a porous material used to separate hydrophobic molecules from liquid- or gaseous flows in order to purify these or to reuse the adsorbate.</p><p>AC is used in kitchen fans, in the food industry, for water treatment, medicine and much more. AC’s ability to adsorb is due to the porosity of the material in combination with the hydrophobicity of its surface. Like a sponge, AC has many pores and canals running through the material and the sum of all the walls within the canals has an incredible surface area; 800-1500 m2/g. The pores are divided into three groups based on their diameter. These are, in order from smallest to biggest: micropores, mesopores and macropores. The ratio of these pores can be adjusted to fit the purpose of the AC.</p><p>AC gets its porosity from the activation during which the biomass is reacting with chemicals in high temperatures. The two methods for producing AC are physical/thermal activation and chemical activation of which the former needs higher a temperature and uses gaseous impregnation agents in a two-step mechanism. The chemical activation however uses lower temperatures, liquid impregnation agents (such as an acid or base) and uses only a one-step mechanism.</p><p>During the project chemical activation was used to produce 18 samples, all using different parameters. The parameters adjusted were the temperature, the choice of chemical and the impregnation ration of that chemical. After producing the samples three tests were conducted in terms of adsorption of gaseous ammonia, gaseous gasoline and dissolved iodine to confirm the quality of adsorption of both gases and liquids. The gaseous adsorptions were weighed before and after adsorption to get the weight-% of the AC. Using the equation for the iodine number the capacity of liquid adsorption was assured.</p><p>To obtain AC with good gas adsorption capacity low levels of temperature and impregnation ratio were preferred. The best result was reached with T = 400°C and 1*(mbio) KOH (20 weight-%) and that meant a gas adsorption of 45 weight-% ammonia and 63 weight-% gasoline. The best liquid adsorption was reached with an activated carbon produced with T = 450°C and an impregnation ratio mKOH-solution/ biomass = 1.5. This activated carbon gave an iodine number of 241 mg/g. The distribution was not investigated and thus the result may not be repeatable.</p>

w='mbio' val={'c': 'm<sub>bio</sub>', 's': 'diva2:827212'}
w='Mbio' val={'c': 'M<sub>bio</sub>', 's': 'diva2:827212'}
w='msolution' val={'c': 'm<sub>olution</sub>', 's': 'diva2:827212', 'n': 'correct in original'}

There is also "m<sub>KOH-solution</sub>" and "m<sup>2</sup>".


corrected abstract:
<p>The assignment was to produce activated Carbon (AC) using chemical activation and determine how the reaction conditions affect its quality. The parameters were: temperature 400, 450 and 500(°C), the impregnation base KOH and NaCO3 as well as the impregnation ratio of these bases in relation to the biomass 1, 1.5 and 2 *m<sub>bio</sub>. M<sub>bio</sub> is the weight of the biomass and thus the weight relationship was 1:1-, 1:1,5- and 1:2 (m<sub>bio</sub> : m<sub>solution</sub>). The concentration of the solution was 20 % by weight.</p><p>In Cuba 2007-2010, the biggest environmental problems were identified by CITMA, an organization working to prevent and neutralize environmental threats. Among these problems were pollution of water, air and land and the shortage of water owing to salt infringement and other contaminations.</p><p>AC is a porous material used to separate hydrophobic molecules from liquid- or gaseous flows in order to purify these or to reuse the adsorbate. AC is used in kitchen fans, in the food industry, for water treatment, medicine and much more. AC’s ability to adsorb is due to the porosity of the material in combination with the hydrophobicity of its surface. Like a sponge, AC has many pores and canals running through the material and the sum of all the walls within the canals has an incredible surface area; 800-1500 m<sup>2</sup>/g. The pores are divided into three groups based on their diameter. These are, in order from smallest to biggest: micropores, mesopores and macropores. The ratio of these pores can be adjusted to fit the purpose of the AC.</p><p>AC gets its porosity from the activation during which the biomass is reacting with chemicals in high temperatures. The two methods for producing AC are physical/thermal activation and chemical activation of which the former needs higher a temperature and uses gaseous impregnation agents in a two-step mechanism. The chemical activation however uses lower temperatures, liquid impregnation agents (such as an acid or base) and uses only a one-step mechanism.</p><p>During the project chemical activation was used to produce 18 samples, all using different parameters. The parameters adjusted were the temperature, the choice of chemical and the impregnation ration of that chemical. After producing the samples three tests were conducted in terms of adsorption of gaseous ammonia, gaseous gasoline and dissolved iodine to confirm the quality of adsorption of both gases and liquids. The gaseous adsorptions were weighed before and after adsorption to get the weight-% of the AC. Using the equation for the iodine number the capacity of liquid adsorption was assured.</p><p>To obtain AC with good gas adsorption capacity low levels of temperature and impregnation ratio were preferred. The best result was reached with T = 400°C and 1*(m<sub>bio</sub>) KOH (20 weight-%) and that meant a gas adsorption of 45 weight-% ammonia and 63 weight-% gasoline. The best liquid adsorption was reached with an activated carbon produced with T = 450°C and an impregnation ratio m<sub>KOH-solution</sub>/ biomass = 1.5. This activated carbon gave an iodine number of 241 mg/g. The distribution was not investigated and thus the result may not be repeatable.</p>
----------------------------------------------------------------------
In diva2:744724 abstract is: <p>DNA sequencing is an invaluable tool for studying medical and biological systems. The genetic components of many diseases have been discovered, which holds great promise for new treatment and drug discovery for treat diseases such as cancer. However, the sequencing instruments of today are mainly research instruments and few have been developed for clinical diagnosis at a decentralized clinic. Therefore there is a need of a complementary small-scale sequencing platform that does not require trained personnel and expensive lab equipment. This project aims to contribute to that development by investigating and finding the optimal enzymatic reaction conditions for small-scale single nucleotide sequencing. The first part was to investigate the optimal conditions for on-slie extension and detection. Probes were printed on a CodeLink array followed by hybridisation to a primer-target complex. Extension was done on slide by the polymerase Klenow exo- with specific biotinylated nucleotides. The signals were captured  by streptavidin-coated beads and investigated under a light microscope. The study found that an addition of apyrase to a concentration of 0.25 mU/ul during the extension produced a satisfactory signal. However, there were undesirable extensions that may have occurred as a result of cross hybridisation. Further experiments are needed to assess the extension accuracy. The second part was an investigation of the slippage activity properties of Type IIs restriction enzyme using high throughput sequencing. Synthetic oligonucleotides were cleaved in the conditions recommended by the manufacturers, in general 10 U per 1 µg substrate. The cleaved products were then ligated with enzyme specific adapters and amplified by PCR. Seven enzymes (BbvI, BmpI BsmFI, FauI, MmeI, GsuI and BpuEI) were tsted, and two enzymes were selected for sequencing in the Illumina MiSeq, BbvI and FauI. BbvI have been reported with low percentage of slippage and the sequencing data supported this claim. Moreover, the slippage analysis by sequences logos of the slippage area supports previous results. In contrast, FauI, which was repoted with high percentage slippage, was found to benefit from our conditions and slippage was slightly decreased wit different sequence logo patterns. From this study, it is suggest that the slippage activity and enzyme behaviour depends on reaction condition and may also depend on the sequence around the cognate site.</p>

w='exo- with' val={'c': 'polymerase Klenow exo-', 's': 'diva2:744724', 'n': 'no full text'}
w='on-slie' val={'c': 'on-slide', 's': 'diva2:744724', 'n': 'no full text'}
w='repoted' val={'c': 'reported', 's': 'diva2:744724', 'n': 'no full text'}
w='tsted' val={'c': 'tested', 's': 'diva2:744724', 'n': 'no full text'}

corrected abstract:
<p>DNA sequencing is an invaluable tool for studying medical and biological systems. The genetic components of many diseases have been discovered, which holds great promise for new treatment and drug discovery for treat diseases such as cancer. However, the sequencing instruments of today are mainly research instruments and few have been developed for clinical diagnosis at a decentralized clinic. Therefore there is a need of a complementary small-scale sequencing platform that does not require trained personnel and expensive lab equipment. This project aims to contribute to that development by investigating and finding the optimal enzymatic reaction conditions for small-scale single nucleotide sequencing. The first part was to investigate the optimal conditions for on-slide extension and detection. Probes were printed on a CodeLink array followed by hybridisation to a primer-target complex. Extension was done on slide by the polymerase Klenow exo- with specific biotinylated nucleotides. The signals were captured  by streptavidin-coated beads and investigated under a light microscope. The study found that an addition of apyrase to a concentration of 0.25 mU/ul during the extension produced a satisfactory signal. However, there were undesirable extensions that may have occurred as a result of cross hybridisation. Further experiments are needed to assess the extension accuracy. The second part was an investigation of the slippage activity properties of Type IIs restriction enzyme using high throughput sequencing. Synthetic oligonucleotides were cleaved in the conditions recommended by the manufacturers, in general 10 U per 1 µg substrate. The cleaved products were then ligated with enzyme specific adapters and amplified by PCR. Seven enzymes (BbvI, BmpI BsmFI, FauI, MmeI, GsuI and BpuEI) were tested, and two enzymes were selected for sequencing in the Illumina MiSeq, BbvI and FauI. BbvI have been reported with low percentage of slippage and the sequencing data supported this claim. Moreover, the slippage analysis by sequences logos of the slippage area supports previous results. In contrast, FauI, which was reported with high percentage slippage, was found to benefit from our conditions and slippage was slightly decreased wit different sequence logo patterns. From this study, it is suggest that the slippage activity and enzyme behaviour depends on reaction condition and may also depend on the sequence around the cognate site.</p>
----------------------------------------------------------------------
diva2:1676972 no changes
----------------------------------------------------------------------
In diva2:854709 abstract is: <p>In this thesis, an analysis of vertebrate species with chitin synthase (CHS) genes was made. CHS is the enzyme that produces chitin, a long-chain polymer of N-acetylglucosamine subunits. Chitin is the second most abundant polymer in nature, after cellulose. Together with other polysaccharides and proteins, chitin forms composite materials found in many species. Fungi, arthropods (insects &amp; crustaceans) and molluscs are all examples of organisms that produce chitin.</p><p>Molluscs has for a long time been considered as the most developed group of organisms with CHS and until recently there wasn't any indications that vertebrates produced chitin.</p><p>This analysis shows surprisingly that there are several lower vertebrates, or anamniotes, that have CHS genes, including lamprey <em>(Petromyzon marinus)</em>, several bony fishes <em>(Danio rereio, Oryzias latipes, Gasterosteus aculeatus &amp; Gadus morhua)</em> and western clawed frog <em>(Xenopus tropicalis)</em>.</p><p>A catalytic domain alignment shows a high sequence similarity among the anamniote CHSs, which points to a close relationship to each other. Four out of six motifs important for catalytic function was found in the vertebrate (CHSs, which mean that the enzyme is expressed.</p><p>The phylogenetic analysis suggests that tha lower vertebrate CHSs have diverged from a common ancestral enzyme together with fungi, nematode, mollusc, lancelet, insect and tunicate CHSs. Also the domain architecture of the vertebrate CHSs show great similarity with already established CHS genes from e.g. insect CHS. The consluding analysis of synteny in neighbouring genes showed great synteny in between <em>O. latipes &amp; G. aculeatus</em> CHSs.</p>

w='consluding' val={'c': 'concluding', 's': 'diva2:854709', 'n': 'no full text'}
w='tha' val={'c': 'the', 's': ['diva2:854709', 'diva2:744719']}

corrected abstract:
<p>In this thesis, an analysis of vertebrate species with chitin synthase (CHS) genes was made. CHS is the enzyme that produces chitin, a long-chain polymer of N-acetylglucosamine subunits. Chitin is the second most abundant polymer in nature, after cellulose. Together with other polysaccharides and proteins, chitin forms composite materials found in many species. Fungi, arthropods (insects &amp; crustaceans) and molluscs are all examples of organisms that produce chitin.</p><p>Molluscs has for a long time been considered as the most developed group of organisms with CHS and until recently there wasn't any indications that vertebrates produced chitin.</p><p>This analysis shows surprisingly that there are several lower vertebrates, or anamniotes, that have CHS genes, including lamprey <em>(Petromyzon marinus)</em>, several bony fishes <em>(Danio rereio, Oryzias latipes, Gasterosteus aculeatus &amp; Gadus morhua)</em> and western clawed frog <em>(Xenopus tropicalis)</em>.</p><p>A catalytic domain alignment shows a high sequence similarity among the anamniote CHSs, which points to a close relationship to each other. Four out of six motifs important for catalytic function was found in the vertebrate (CHSs, which mean that the enzyme is expressed.</p><p>The phylogenetic analysis suggests that the lower vertebrate CHSs have diverged from a common ancestral enzyme together with fungi, nematode, mollusc, lancelet, insect and tunicate CHSs. Also the domain architecture of the vertebrate CHSs show great similarity with already established CHS genes from e.g. insect CHS. The concluding analysis of synteny in neighbouring genes showed great synteny in between <em>O. latipes &amp; G. aculeatus</em> CHSs.</p>
----------------------------------------------------------------------
In diva2:839581 abstract is: <p>Indications that existing parameter sets of extended Linear Interaction Energy (LIE) models are transferable between lipases from Rhizomucor Miehei and Thermomyces Lanigunosus in complex with a small set of vinyl esters are demonstrated. By calculat- ing energy terms that represents the cost of forming cavities filled by the ligand and the complex we can add them to a LIE model with en established parameter set. The levels of precision attained will be comparable to those of an optimal fit. It is also demonstrated that the Molecular Mechanics/Poisson Boltzmann Surface Area (MM/PBSA) and Molecular Mechanics/Generalized Born Surface Area (MM/GBSA) methods are in- applicable to the problem of calculating absolute binding energies, even when the largest source of variance has been reduced.</p>

w='Lanigunosus' val={'c': 'Lanuginosus', 's': 'diva2:839581', 'n': 'error in original'}
Added the markup for the italics.

corrected abstract:
<p>Indications that existing parameter sets of extended <em>Linear Interaction Energy</em> (LIE) models are transferable between lipases from <em>Rhizomucor Miehei</em> and <em>Thermomyces Lanigunosus</em> in complex with a small set of vinyl esters are demonstrated. By calculating energy terms that represents the cost of forming cavities filled by the ligand and the complex we can add them to a LIE model with en established parameter set. The levels of precision attained will be comparable to those of an optimal fit. It is also demonstrated that the <em>Molecular Mechanics/Poisson Boltzmann Surface Area</em> (MM/PBSA) and <em>Molecular Mechanics/Generalized Born Surface Area</em> (MM/GBSA) methods are inapplicable to the problem of calculating absolute binding energies, even when the largest source of variance has been reduced.</p>
----------------------------------------------------------------------
In diva2:442651 abstract is: <p><strong>Abstract</strong>Carbon Footprints, as an indicator of climate performance, help identify major GHG emission sources and potential areas of improvement. In the context of greatly expanding sub-national climate efforts, research on Carbon Footprint accounting at municipality level is timely and necessary to facilitate the establishment of local climate strategies. This study aims at exploring the methodologies for Carbon Footprint assessment at municipality level, based on the case study of Haninge municipality in Sweden. In the study, a Greenhouse Gas inventory of Haninge is developed and it is discussed how the municipality can reduce its Carbon Footprint. The Carbon Footprint of Haninge is estimated to be more than 338,225 tonnesCO2eq, and 4.5 tonnes CO2eq per capita. These numbers are twice as large as the production-based emissions, which are estimated to be 169,024 tonnes CO2eq in total, and approximately 2.3 tonnes CO2eq per capita. Among them the most important parts are emissions caused by energy use, and indirect emissions caused by local private consumption. It is worth noting that a large proportion of emissions occur outside Haninge as a result of local consumption. Intensive use of biomass for heat production and electricity from renewable sources and nuclear power have significantly reduced the climate impact of Haninge. The major barrier for Carbon Footprint accounting at municipality level is lack of local statistics. In the case of Sweden, several databases providing emission statistics are used in the research, including KRE, RUS, NIR and Environmental Account.</p>
<p> </p>

partal corrected: diva2:442651: <p><strong>Abstract</strong>Carbon Footprints, as an indicator of climate performance, help identify major GHG emission sources and potential areas of improvement. In the context of greatly expand ing sub-national climate efforts, research on Carbon Footprint accounting at municipality level is timely and necessary to facilitate the establishment of local climate strategies. This study aims at exploring the methodologies for Carbon Footprint assessment at municipality level, based on the case study of Haninge municipality in Sweden. In the study, a Greenhouse Gas inventory of Haninge is developed and it is discussed how the municipality can reduce its Carbon Footprint. The Carbon Footprint of Haninge is estimated to be more than 338,225 tonnesCO2 eq, and 4.5 tonnes CO2 eq per capita. These numbers are twice as large as the production-based emissions, which are estimated to be 169,024 tonnes CO2 eq in total, and approximately 2.3 tonnes CO2 eq per capita. Among them the most important parts are emissions caused by energy use, and indirect emissions caused by local private consumption. It is worth noting that a large proportion of emissions occur outside Haninge as a result of local consumption. Intensive use of biomass for heat production and electricity from renewable sources and nuclear power have significantly reduced the climate impact of Haninge. The major barrier for Carbon Footprint accounting at municipality level is lack of local statistics. In the case of Sweden, several databases providing emission statistics are used in the research, including KRE, RUS, NIR and Environmental Account.</p>
<p> </p>
w='sub-national' val={'c': 'subnational', 's': 'diva2:442651', 'n': 'hyphenated in original'}

corrected abstract:
<p>Carbon Footprints, as an indicator of climate performance, help identify major GHG emission sources and potential areas of improvement. In the context of greatly expanding sub-national climate efforts, research on Carbon Footprint accounting at municipality level is timely and necessary to facilitate the establishment of local climate strategies. This study aims at exploring the methodologies for Carbon Footprint assessment at municipality level, based on the case study of Haninge municipality in Sweden. In the study, a Greenhouse Gas inventory of Haninge is developed and it is discussed how the municipality can reduce its Carbon Footprint. The Carbon Footprint of Haninge is estimated to be more than 338,225 tonnes CO<sub>2</sub> eq, and 4.5 tonnes CO<sub>2</sub> eq per capita. These numbers are twice as large as the production-based emissions, which are estimated to be 169,024 tonnes CO<sub>2</sub> eq in total, and approximately 2.3 tonnes CO<sub>2</sub> eq per capita. Among them the most important parts are emissions caused by energy use, and indirect emissions caused by local private consumption. It is worth noting that a large proportion of emissions occur outside Haninge as a result of local consumption. Intensive use of biomass for heat production and electricity from renewable sources and nuclear power have significantly reduced the climate impact of Haninge. The major barrier for Carbon Footprint accounting at municipality level is lack of local statistics. In the case of Sweden, several databases providing emission statistics are used in the research, including KRE, RUS, NIR and Environmental Account.</p>
----------------------------------------------------------------------
title: "CCS via Electrochemical CO2 Reduction to Ethylene-based Polymeric Construction Materials"
==>    "CCS via Electrochemical CO<sub>2</sub> Reduction to Ethylene-based Polymeric Construction Materials"

In diva2:1627230 abstract is: <p>The IPCC SR15 reported that all future scenarios to limit climate change to 1.5°C are heavily reliant on negative emission technologies, such as geographical CO<sub>2</sub> storage employed by Stockholm Exergi’s Värtaverket. But can stronger climate benefits be achieved through a circular carbon economy? The formation of a carbon circular economy is imperative towards achieving global carbon neutrality, but how do we get there? Electrolysis of CO<sub>2</sub> offers an economically and environmentally attractive route to upgrade CO<sub>2</sub> emissions to valuable fuels and feedstocks, thus reducing the use of fossil resources and CO<sub>2</sub> emissions to the atmosphere, hence closing the cycle. </p><p>This thesis explores the possibility of removing the 720,000 tCO<sub>2</sub> emissions of the case study waste-fired CHP plant, Stockholm Exergi’s Högdalenverket, via the electrochemical reduction of CO<sub>2</sub> (eCO<sub>2</sub>RR) towards ethylene, with the goal of producing polymeric construction materials, to act as a carbon sink. These polymers were evaluated on criteria such as, capacity as a carbon sink, market size and LCA. Ethylene is the prevailing commodity chemical for polymer production and has a significant carbon footprint of 1.73 tonCO<sub>2</sub> per tonne of ethylene produced. Displacement via the eCO<sub>2</sub>RR would prevent substantial CO<sub>2</sub> emissions and bridge the gap between fossil and renewable resources. </p><p>This thesis describes a preliminary process design, complete with technoeconomic model to calculate the economics, mass and energy balances of numerous scenarios. Electrocatalyst data from an in-depth literature review comprising of over 100 catalysts was drawn, with 5 electrocatalyst candidates selected based on strengths in particular figures of merit, to determine performance targets for profitability. The technoeconomic model concluded that at the current price point of 700 SEK/MWh, none of the electrocatalysts could achieve profitability. Lowering the electricity price to the levelized-cost of electricity (LCOE) for wind, 335 SEK/MWh, yielded highly profitable results, including IRR up-to 41.3%. Model parameters were changed to determine the most important variables in an extensive sensitivity analysis. Concluding that performance targets require current densities of 400-600 mA/cm<sup>2</sup> whilst maintaining as low cell voltage as possible (&lt;2.4 V). When specifically targeting ethylene, it is beneficial to increase profitability through targeting more valuable, formic, or acetic acid, which has the advantage of easier liquid-gas separation and to avoid production of methane and ethanol. For stability, 2-4 years minimum is required for the catalyst-coated membrane (CCM), 10 years for the stack and 20 years for the electrolyser systems.   </p><p>In the environmental analysis, capabilities for carbon storage were studied via CO<sub>2</sub> balance. This was achieved by taking the direct emissions removed from Högdalenverket, the indirect emissions prevented by replacing conventional processes and by the carbon intensity of the electricity source. Based on the average energy efficiency and performance of the electrocatalysts, the host country would require a carbon intensity of electricity production below 101 and 153 tCO₂/GWh for NET direct and indirect CO2 removal, respectively. Consequently, higher CO<sub>2</sub> savings were achieved by trading low-carbon Swedish electricity to neighbouring countries with much higher carbon intensities. Overall, the direct carbon reduction was between 487,300 to 575,000 tCO₂ and indirect reduction of between 1,065,000 to 1,219,000 tCO₂, subject to energy efficiency and targeted products. </p><p>It remains that aside from the technical performance aspects of the eCO<sub>2</sub>RR catalysts, the major roadblock towards the commercial success of all eCO<sub>2</sub>RR projects is the required adjustments to regulatory framework, such that electricity for electrolysis projects towards green chemicals exempt from taxes in a similar way to renewable biomass combustion exempt from CO<sub>2</sub> taxes.</p>

w='tonCO' val={'c': 'tonCO<sub>2</sub>', 's': 'diva2:1627230'}
Also fixed cm^2.

corrected abstract:
<p>The IPCC SR15 reported that all future scenarios to limit climate change to 1.5°C are heavily reliant on negative emission technologies, such as geographical CO<sub>2</sub> storage employed by Stockholm Exergi’s Värtaverket. But can stronger climate benefits be achieved through a circular carbon economy? The formation of a carbon circular economy is imperative towards achieving global carbon neutrality, but how do we get there? Electrolysis of CO<sub>2</sub> offers an economically and environmentally attractive route to upgrade CO<sub>2</sub> emissions to valuable fuels and feedstocks, thus reducing the use of fossil resources and CO<sub>2</sub> emissions to the atmosphere, hence closing the cycle.</p><p>This thesis explores the possibility of removing the 720,000 tCO<sub>2</sub> emissions of the case study waste-fired CHP plant, Stockholm Exergi’s Högdalenverket, via the electrochemical reduction of CO<sub>2</sub> (eCO<sub>2</sub>RR) towards ethylene, with the goal of producing polymeric construction materials, to act as a carbon sink. These polymers were evaluated on criteria such as, capacity as a carbon sink, market size and LCA. Ethylene is the prevailing commodity chemical for polymer production and has a significant carbon footprint of 1.73 tonCO<sub>2</sub> per tonne of ethylene produced. Displacement via the eCO<sub>2</sub>RR would prevent substantial CO<sub>2</sub> emissions and bridge the gap between fossil and renewable resources.</p><p>This thesis describes a preliminary process design, complete with technoeconomic model to calculate the economics, mass and energy balances of numerous scenarios. Electrocatalyst data from an in-depth literature review comprising of over 100 catalysts was drawn, with 5 electrocatalyst candidates selected based on strengths in particular figures of merit, to determine performance targets for profitability. The technoeconomic model concluded that at the current price point of 700 SEK/MWh, none of the electrocatalysts could achieve profitability. Lowering the electricity price to the levelized-cost of electricity (LCOE) for wind, 335 SEK/MWh, yielded highly profitable results, including IRR up-to 41.3%. Model parameters were changed to determine the most important variables in an extensive sensitivity analysis. Concluding that performance targets require current densities of 400-600 mA/cm<sup>2</sup> whilst maintaining as low cell voltage as possible (&lt;2.4 V). When specifically targeting ethylene, it is beneficial to increase profitability through targeting more valuable, formic, or acetic acid, which has the advantage of easier liquid-gas separation and to avoid production of methane and ethanol. For stability, 2-4 years minimum is required for the catalyst-coated membrane (CCM), 10 years for the stack and 20 years for the electrolyser systems.</p><p>In the environmental analysis, capabilities for carbon storage were studied via CO<sub>2</sub> balance. This was achieved by taking the direct emissions removed from Högdalenverket, the indirect emissions prevented by replacing conventional processes and by the carbon intensity of the electricity source. Based on the average energy efficiency and performance of the electrocatalysts, the host country would require a carbon intensity of electricity production below 101 and 153 tCO₂/GWh for NET direct and indirect CO<sub>2</sub> removal, respectively. Consequently, higher CO<sub>2</sub> savings were achieved by trading low-carbon Swedish electricity to neighbouring countries with much higher carbon intensities. Overall, the direct carbon reduction was between 487,300 to 575,000 tCO₂ and indirect reduction of between 1,065,000 to 1,219,000 tCO₂, subject to energy efficiency and targeted products.</p><p>It remains that aside from the technical performance aspects of the eCO<sub>2</sub>RR catalysts, the major roadblock towards the commercial success of all eCO<sub>2</sub>RR projects is the required adjustments to regulatory framework, such that electricity for electrolysis projects towards green chemicals exempt from taxes in a similar way to renewable biomass combustion exempt from CO<sub>2</sub> taxes.</p>
----------------------------------------------------------------------
In diva2:744712 abstract is: <p>This thesis gives a brief insight on how protein engineering is made with ω-Transaminases - enzymes that are used to create chiral amines which are included in many pharmaceuticals, fine chemicals and agrochemicals - in order to find ω-Transaminase variants that have potential for scale up in industrial processes.</p><p>Several ways to produce (<em>S</em>)-amines with ω-Transaminases exist today as most characterized ω-Transaminases are (<em>S</em>)-selective. The (<em>R</em>)-selective ω-Transaminases are in the other hand rare and in 2003, only 1 (<em>R</em>)-selective ω-Transaminases was known. In 2012, the group of Svedendahl Humble <em>et al.</em> to change the enantioselectivity for the substrate 2-aminotetralin from E = 3.9 (<em>S</em>) to E = 63 (<em>R</em>) by introducing two poiny mutations (F88A/A231F) in the active site of <em>Chromobacterium violaceum </em>ω-Transaminase<em>.</em></p><p>By using the same variant (F88A/A231F) as a starting template, two new residues in the active site were targeted for site directed mutagenesis that hopefully would give variatns with increased E-caalue for <em>(R</em>)-2-aminotetralin or with changed enantiopreference, frpm <em>(S)</em> to <em>(R)</em>, for other stubstrates.<em></em></p><p>This report covers most of the steps, starting from the rational design of the active site and ends up with screening and kinetics of the possible hits using one template substrate, 1-aminotetralin.</p>

w='variatns' val={'c': 'variants', 's': 'diva2:744712'}
w='poiny' val={'c': 'pointy', 's': 'diva2:744712', 'n': 'no full text'}
w='stubstrates' val={'c': 'substrates', 's': 'diva2:744712', 'n': 'no full text'}

corrected abstract:
<p>This thesis gives a brief insight on how protein engineering is made with ω-Transaminases - enzymes that are used to create chiral amines which are included in many pharmaceuticals, fine chemicals and agrochemicals - in order to find ω-Transaminase variants that have potential for scale up in industrial processes.</p><p>Several ways to produce (<em>S</em>)-amines with ω-Transaminases exist today as most characterized ω-Transaminases are (<em>S</em>)-selective. The (<em>R</em>)-selective ω-Transaminases are in the other hand rare and in 2003, only 1 (<em>R</em>)-selective ω-Transaminases was known. In 2012, the group of Svedendahl Humble <em>et al.</em> to change the enantioselectivity for the substrate 2-aminotetralin from E = 3.9 (<em>S</em>) to E = 63 (<em>R</em>) by introducing two pointy mutations (F88A/A231F) in the active site of <em>Chromobacterium violaceum </em>ω-Transaminase<em>.</em></p><p>By using the same variant (F88A/A231F) as a starting template, two new residues in the active site were targeted for site directed mutagenesis that hopefully would give variants with increased E-caalue for <em>(R</em>)-2-aminotetralin or with changed enantiopreference, frpm <em>(S)</em> to <em>(R)</em>, for other substrates.</p><p>This report covers most of the steps, starting from the rational design of the active site and ends up with screening and kinetics of the possible hits using one template substrate, 1-aminotetralin.</p>
----------------------------------------------------------------------
In diva2:1711888 abstract is: <p>Heat shock protein 90 kDa (HSP90) forms a remarkably complicated network with a variety of cochaperones. The complex of FK506-binding protein 51 kDa (FKBP51) and HSP90 mediates protein folding and function, promoting tau aggregation in Alzheimer's disease and influencing stress-related disorders, obesity, type two diabetes, etc. In collaboration with the molecular chaperone HSP90, FKBP51 has recently been proposed as a promising therapeutic target for Alzheimer's disease (AD). Thus, the knock-in mouse harboring point mutations in the tetratricopeptide repeat (TPR) domain of FKBP51 rendering it unable to interact with HSP90 were created to investigate the potential therapeutic targets for the treatment of these diseases. Glucocorticoid receptor (GR) traditionally served as the starting point for the initial studies of FKBP51 function and mechanism which can be stimulated by the synthetic glucocorticoid, dexamethasone (Dexa). The primary goal of the project is to comprehend the biological significance of FKBP51-HSP90 interactions. It is unclear how FKBP51 mutation affects the protein-protein interaction and glucocorticoid signaling. Here, embryonic fibroblasts (MEFs) isolated from wildtype and FKBP51 mutant mouse were analyzed with respect to protein localization, protein expression, and gene expression. Although no certain difference between wildtype and mutant mice was seen in Dexa-mediated glucocorticoid signaling, the post-translational modifications (PTMs) in exposure to Dexa treatment of FKBP51 occur in wildtype mice to a significantly higher extent than in Fkbp51mute mice. The phosphorylation modification of FKBP51 was initially hypothesized and confirmed by phosphorylation enrichment strategies. However, confirmation has not yet been obtained.</p>

w='Fkbp51mute' val={'c': 'Fkbp51<sup>mute</sup>', 's': 'diva2:1711888'}

corrected abstract:
<p>Heat shock protein 90 kDa (HSP90) forms a remarkably complicated network with a variety of cochaperones. The complex of FK506-binding protein 51 kDa (FKBP51) and HSP90 mediates protein folding and function, promoting tau aggregation in Alzheimer's disease and influencing stress-related disorders, obesity, type two diabetes, etc. In collaboration with the molecular chaperone HSP90, FKBP51 has recently been proposed as a promising therapeutic target for Alzheimer's disease (AD). Thus, the knock-in mouse harboring point mutations in the tetratricopeptide repeat (TPR) domain of FKBP51 rendering it unable to interact with HSP90 were created to investigate the potential therapeutic targets for the treatment of these diseases. Glucocorticoid receptor (GR) traditionally served as the starting point for the initial studies of FKBP51 function and mechanism which can be stimulated by the synthetic glucocorticoid, dexamethasone (Dexa). The primary goal of the project is to comprehend the biological significance of FKBP51-HSP90 interactions. It is unclear how FKBP51 mutation affects the protein-protein interaction and glucocorticoid signaling. Here, embryonic fibroblasts (MEFs) isolated from wildtype and FKBP51 mutant mouse were analyzed with respect to protein localization, protein expression, and gene expression. Although no certain difference between wildtype and mutant mice was seen in Dexa-mediated glucocorticoid signaling, the post-translational modifications (PTMs) in exposure to Dexa treatment of FKBP51 occur in wildtype mice to a significantly higher extent than in Fkbp51<sup>mute</sup> mice. The phosphorylation modification of FKBP51 was initially hypothesized and confirmed by phosphorylation enrichment strategies. However, confirmation has not yet been obtained.</p>
----------------------------------------------------------------------
In diva2:801911 abstract is: <p>During the course of evolution one strand of a pair of autosomes obtained a sex-determining gene and subsequently developed to the Y chromosome. Recombination between the sex chromosomes does not occur except on the tips of the chromosomes between regions calld pseudoautosomal regions (PARs). Thus these regions behave like autosomes and the PARs are therefore not inherited in a sex-linked pattern. The PARs of the Y and X chromosomes crossover at least once during each male meiosis event and this makes the recombination rate much higher for these regions in comparison to the rest of the genome. But the exact borders of recombination between the PARs have not been defined. Thus it is hypothesised that the genes loacted directly below PAR1 may be exposed to variation due to recombination with Y-linked pseudogenes. The genes located immediately below PAR1 encode for proteins involved in metabolic processes and a mutation in these genes may cause inherited metabolic diseases.</p><p>To test this hypothesis a family of then members was chosen where previous whole genome sequencing (WGS) had identified variations in the genes immediately below the PAR1. The exons of the genes centromeric to PAR1 on the X chromosome were sequenced using Sanger sequencing so two questions could be answered. The first question was if the mutations identified with WGS on the X chromosome may have been false positives due to miss-mapping of pseudogenes on the Y chromosome. And the second question to be answered was to determine if the genes centromeric to PAR are exposed to variations due to recombination with the Y-linked pseudogenes. The first question was answered by making a direct comparison between the variations identified with the different sequencing mathods. The second question was answered by following the inheritance patterns of the mutation. If the genes centromeric to PAR are exposed to variations then there would be an irregular pattern of inheritance of mutations.</p><p>The results of this work show that the variations identified with WGS completely matched the results of the Sanger sequencing for four of the five examined genes. It can hence be concluded that for these four genes the variations identified on the X-chromosome with WGS were correctly mapped. The results also show and inheritance pattern of these variations that would be expected if no recombination occurred with the Y-linked pseudogenes and it can thus be concluded that the genes centromeric to PAR1 are located outside of PAR1.</p>

w='calld' val={'c': 'called', 's': 'diva2:801911', 'n': 'no full text'}
w='loacted' val={'c': 'located', 's': 'diva2:801911', 'n': 'no full text'}
w='mathods' val={'c': 'methods', 's': 'diva2:801911', 'n': 'no full text'}

corrected abstract:
<p>During the course of evolution one strand of a pair of autosomes obtained a sex-determining gene and subsequently developed to the Y chromosome. Recombination between the sex chromosomes does not occur except on the tips of the chromosomes between regions called pseudoautosomal regions (PARs). Thus these regions behave like autosomes and the PARs are therefore not inherited in a sex-linked pattern. The PARs of the Y and X chromosomes crossover at least once during each male meiosis event and this makes the recombination rate much higher for these regions in comparison to the rest of the genome. But the exact borders of recombination between the PARs have not been defined. Thus it is hypothesised that the genes located directly below PAR1 may be exposed to variation due to recombination with Y-linked pseudogenes. The genes located immediately below PAR1 encode for proteins involved in metabolic processes and a mutation in these genes may cause inherited metabolic diseases.</p><p>To test this hypothesis a family of then members was chosen where previous whole genome sequencing (WGS) had identified variations in the genes immediately below the PAR1. The exons of the genes centromeric to PAR1 on the X chromosome were sequenced using Sanger sequencing so two questions could be answered. The first question was if the mutations identified with WGS on the X chromosome may have been false positives due to miss-mapping of pseudogenes on the Y chromosome. And the second question to be answered was to determine if the genes centromeric to PAR are exposed to variations due to recombination with the Y-linked pseudogenes. The first question was answered by making a direct comparison between the variations identified with the different sequencing methods. The second question was answered by following the inheritance patterns of the mutation. If the genes centromeric to PAR are exposed to variations then there would be an irregular pattern of inheritance of mutations.</p><p>The results of this work show that the variations identified with WGS completely matched the results of the Sanger sequencing for four of the five examined genes. It can hence be concluded that for these four genes the variations identified on the X-chromosome with WGS were correctly mapped. The results also show and inheritance pattern of these variations that would be expected if no recombination occurred with the Y-linked pseudogenes and it can thus be concluded that the genes centromeric to PAR1 are located outside of PAR1.</p>
----------------------------------------------------------------------
In diva2:744719 abstract is: <p>A protocol for production of his-mumps nucleocapsid virus-like parcticles has together with an external collaborating company, been created by Andres Veide and Johan Norén (KTH)- The purification process has suffered from unexpected yield losses with the hypothesis that tha virus-like particles shred on a final downstream processing filtration step. The aim of this thesis, apart from to verify or discard the hypothesis, has been to characterize the mumps nucleocapsid protein with respect to its size and self-assembly.</p><p>The particles were characterized using dynamic light scattering, optical density, size exclusion chromatography and gel electrophoresis. The results indicates that the virus-like particles clute from the primary purification step, IMAC, as a donut-shpaed structure and due to the following buffer exchange to 50 mM HEPES assembles into a cylinder-shaped structure. It is thus not the filtration in itself that affects the protein; it is rather the buffer exchange. This thesis suggests a way of manipulating the size of the particles by changing buffer composition. Further work must sill be done, and analysis with transmission electron microscopy is of certain interest.</p>

w='donut-shpaed' val={'c': 'donut-shaped', 's': 'diva2:744719', 'n': 'no full text'}
w='parcticles' val={'c': 'particles', 's': 'diva2:744719', 'n': 'no full text'}
w='tha' val={'c': 'the', 's': ['diva2:854709', 'diva2:744719']}

corrected abstract:
<p>A protocol for production of his-mumps nucleocapsid virus-like particles has together with an external collaborating company, been created by Andres Veide and Johan Norén (KTH)- The purification process has suffered from unexpected yield losses with the hypothesis that the virus-like particles shred on a final downstream processing filtration step. The aim of this thesis, apart from to verify or discard the hypothesis, has been to characterize the mumps nucleocapsid protein with respect to its size and self-assembly.</p><p>The particles were characterized using dynamic light scattering, optical density, size exclusion chromatography and gel electrophoresis. The results indicates that the virus-like particles clute from the primary purification step, IMAC, as a donut-shaped structure and due to the following buffer exchange to 50 mM HEPES assembles into a cylinder-shaped structure. It is thus not the filtration in itself that affects the protein; it is rather the buffer exchange. This thesis suggests a way of manipulating the size of the particles by changing buffer composition. Further work must sill be done, and analysis with transmission electron microscopy is of certain interest.</p>
----------------------------------------------------------------------
In diva2:755541 abstract is: <p>The Lateral Habenula (LHb) have been implicated in both reward-seeking behavior and in depressive disorders due to its modulatory effects on dopamine rich areas. Excitatory projections from LHb target GABAergic interneurons of both ventral tegmental area (VTA) and rostromedial tegmental nucleus (RMTg) and consequently provide strong inhibition on VTA‟s dopaminergic neurons. These reward related signals are provided to LHb from distinct neuronal populations in internal Globus Pallidus (GPi). Here by using a dual viral combination of an adeno-associated helper virus (AAV) and a genetically modified rabies virus that displays specific transsynaptic retrograde spread we are providing anatomical evidence for a strong innervations of the LHb by VGLUT2+ glutaminergic and SOM+ GABAergic GPi neurons. Our results provide the first direct evidence for both an excitatory and an inhibitory projection m, from GPi to the LHb. Given the importance of the LHb as a modulatory nucleus of the dopaminergic system, the definition of its connectivity and function will give valuable insights in the understanding of both reward-seeking behavior and depressive disorders.</p>

w='VTA‟s' val={'c': "VTA's", 's': 'diva2:755541', 'n': 'correct in original'}

corrected abstract:
<p>The Lateral Habenula (LHb) have been implicated in both reward-seeking behavior and in depressive disorders due to its modulatory effects on dopamine rich areas. Excitatory projections from LHb target GABAergic interneurons of both ventral tegmental area (VTA) and rostromedial tegmental nucleus (RMTg) and consequently provide strong inhibition on VTA's dopaminergic neurons. These reward related signals are provided to LHb from distinct neuronal populations in internal Globus Pallidus (GPi). Here by using a dual viral combination of an adeno-associated helper virus (AAV) and a genetically modified rabies virus that displays specific transsynaptic retrograde spread we are providing anatomical evidence for a strong innervations of the LHb by VGLUT2+ glutaminergic and SOM+ GABAergic GPi neurons. Our results provide the first direct evidence for both an excitatory and an inhibitory projection from GPi to the LHb. Given the importance of the LHb as a modulatory nucleus of the dopaminergic system, the definition of its connectivity and function will give valuable insights in the understanding of both reward-seeking behavior and depressive disorders.</p>
----------------------------------------------------------------------
In diva2:1583834 abstract is: <p>Uranium oxide (UO<sub>x</sub>) is an energy dense material commonly used in nuclear fuel. UO<sub>x</sub> powder is pressed and sintered to produce uranium dioxide (UO<sub>2</sub>) pellets which are loaded into fuel rods. The rods are then mounted together in a final nuclear fuel assembly. Stability and predictability of the manufacturing processes during UO<sub>2</sub> pellet production is of high importance. To achieve desired properties and quality of the UO<sub>2</sub> pellets, the ability to assess the characteristics of the UO<sub>x</sub> powder is crucial. Sinterability is the most important characteristic which describes the behavior of the UO<sub>x</sub> powder during reduction in high temperatures. Recycled uranium dioxide is oxidized into U<sub>3</sub>O<sub>8</sub> powder which can be used to modify the sinterability due to its pore forming ability.</p><p>This study describes the characterization of uranium oxide powders and pellets regarding physicochemical properties relating to sintering behavior. Statistical analyses of historical data were also performed and showed a complexity of the relation between powder properties and  sinterability. The effect of U<sub>3</sub>O<sub>8</sub> powder in different blends of UO<sub>2</sub> powders of high and low sinterability were analyzed. Varying U<sub>3</sub>O<sub>8</sub> powder batch did not influence the diameter shrinkage after sintering except for one case. UO<sub>2</sub> powder blends showed deviating behavior from their virgin powder constituents.</p><p>Chemical activity of UO<sub>2</sub> was analyzed by oxidation with H<sub>2</sub>O<sub>2</sub>. The consumption rate of H<sub>2</sub>O<sub>2</sub> was shown to be equal for active and incative UO<sub>2</sub> powders under equal specific surface area/solution volume ratio.</p>


w='incative' val={'c': 'inactive', 's': 'diva2:1583834', 'n': 'the abstract in DiVA does not match the abstract in the thesis'}

corrected abstract:
<p>Uranium oxide (UO<sub>x</sub>) is an energy dense material commonly used in nuclear fuel. UO<sub>x</sub> powder is pressed and sintered to produce uranium dioxide (UO<sub>2</sub>) pellets which are loaded into fuel rods. The rods are then mounted together in a final nuclear fuel assembly. Stability and predictability of the manufacturing processes during UO<sub>2</sub> pellet production is of high importance. To achieve desired properties and quality of the UO<sub>2</sub> pellets, the ability to assess the characteristics of the UO<sub>x</sub> powder is crucial. Sinterability is the most important characteristic which describes the behavior of the UO<sub>x</sub> powder during reduction in high temperatures. Recycled uranium dioxide is oxidized into U<sub>3</sub>O<sub>8</sub> powder which can be used to modify the sinterability due to its pore forming ability.</p><p>This study describes the characterization of uranium oxide powders and pellets regarding physicochemical properties relating to sintering behavior. Statistical analyses of historical data were also performed and showed a complexity of the relation between powder properties and sinterability. The effect of U<sub>3</sub>O<sub>8</sub> powder in different blends of UO<sub>2</sub> powders of high and low sinterability were analyzed. Varying U<sub>3</sub>O<sub>8</sub> powder batch did not influence the diameter shrinkage after sintering except for one case. UO<sub>2</sub> powder blends showed deviating behavior from their virgin powder constituents.</p><p>Chemical activity of UO<sub>2</sub> was analyzed by oxidation with H<sub>2</sub>O<sub>2</sub>. The consumption rate of H<sub>2</sub>O<sub>2</sub> was shown to depend on specific surface area, although further studies are planned.</p>
----------------------------------------------------------------------
In diva2:1558656 abstract is: <p>The charging behaviour of the amine moiety at the water-air interface upon changes in the aqueous subphase pH and NaCl concentration has been studied using  the non-linear laser spectroscopy technique, Vibrational Sum Frequency Spectroscopy (VSFS).  The model surface consisted of a Langmuir monolayer of 1-docosaneamine, an insoluble fatty amine that exposes its NH<sub>2</sub>  group to solution. One of the main purposes of the project is to determine the surface, and the apparent pKa of the amine moiety, as well as testing the limits of validity of classical formulations of the electrical double layer theory within the Poisson-Boltzmann formalism. Molecular information of the charging behaviour was obtained from the VSFS spectra by targeting the NH, OH and CH stretching modes. Specifically, spectral features from the neutral amine (i.e. R-NH<sub>2</sub>) and charged (R-NH<sub>3</sub>+) groups could be identified and directly correlated to the surface charge of the monolayer. The intensity of the  OH bands from water molecules in the diffuse double layer, were linked to the surface potential, and finally, the CH modes from the surfactant alkyl chain gave information of the packing density in the monolayer. Additional experiments were also carried out in D<sub>2</sub>O to help confirm the assignment of the NH<sub>3</sub>+ stretching modes that had not been previously reported.</p><p>The results show that as predicted from the Gouy-Chapman electric double layer model, the apparent pKa of the fatty amine monolayer is significantly lower than in the bulk  (∼ 4 compared with 10.5) . However, the data show indication that the intrinsic pKa at the surface is also lower than in the bulk (9.7+/- 0.7, compared to 10.5), an effect that is ascribed to the 2D molecular confinement in the monolayer. A more quantitative comparison with the theoretical predictions was nonetheless hampered by a poor selection of the experimental reference for comparing data collected in different days, and the limitations in the fitting routines due spectral overlap of the relatively weak amine bands with the OH and CH stretching modes. Interestingly, at high pH when the fatty amine is fully uncharged, the sum frequency spectra show evidence that OH- ion preferentially adsorbed to the surface, making it effectively net negatively charged.</p><p>Overall, the studies presented in this master thesis, improve our molecular understanding of the behaviour of the biophysically relevant amine-functionality at interfaces.</p>

w='OH- ion' val={'c': 'OH<sup>-</sup>', 's': 'diva2:1558656', 'n': 'correct in opriginal'}
w='OHion' val={'c': 'OH<sup>-</sup> ion', 's': 'diva2:1558656', 'n': 'correct in original'}

corrected abstract:
<p>The charging behaviour of the amine moiety at the water-air interface upon changes in the aqueous subphase pH and NaCl concentration has been studied using the non-linear laser spectroscopy technique, Vibrational Sum Frequency Spectroscopy (VSFS). The model surface consisted of a Langmuir monolayer of 1-docosaneamine, an insoluble fatty amine that exposes its NH<sub>2</sub> group to solution. One of the main purposes of the project is to determine the surface, and the apparent pK<sub>a</sub> of the amine moiety, as well as testing the limits of validity of classical formulations of the electrical double layer theory within the Poisson-Boltzmann formalism. Molecular information of the charging behaviour was obtained from the VSFS spectra by targeting the NH, OH and CH stretching modes. Specifically, spectral features from the neutral amine (i.e. R-NH<sub>2</sub>) and charged (R-NH<sub>3</sub>+) groups could be identified and directly correlated to the surface charge of the monolayer. The intensity of the OH bands from water molecules in the diffuse double layer, were linked to the surface potential, and finally, the CH modes from the surfactant alkyl chain gave information of the packing density in the monolayer. Additional experiments were also carried out in D<sub>2</sub>O to help confirm the assignment of the NH<sub>3</sub><sup>+</sup> stretching modes that had not been previously reported.</p><p>The results show that as predicted from the Gouy-Chapman electric double layer model, the apparent pK<sub>a</sub> of the fatty amine monolayer is significantly lower than in the bulk (∼ 4 compared with 10.5) . However, the data show indication that the intrinsic pK<sub>a</sub> at the surface is also lower than in the bulk (9.7±0.7, compared to 10.5), an effect that is ascribed to the 2D molecular confinement in the monolayer. A more quantitative comparison with the theoretical predictions was nonetheless hampered by a poor selection of the experimental reference for comparing data collected in different days, and the limitations in the fitting routines due spectral overlap of the relatively weak amine bands with the OH and CH stretching modes. Interestingly, at high pH when the fatty amine is fully uncharged, the sum frequency spectra show evidence that OH<sup>-</sup> ion preferentially adsorbed to the surface, making it effectively net negatively charged.</p><p>Overall, the studies presented in this master thesis, improve our molecular understanding of the behaviour of the biophysically relevant amine-functionality at interfaces.</p>
----------------------------------------------------------------------
In diva2:1708926 abstract is: <p>Hydrotreating processes are of high importance in helping to obtain the desired characteristics of products as well as to comply with the legislation regarding health hazards and environmental pollution. Hydrotreating reactions are catalytic reactions which imply that the understanding and utilization of the most suitable catalysts is crucial. While hydrodesulfurization is a vastly studied branch of hydrotreating, hydrodearomatization (HDA), and hydrodenitrogenation (HDN) processes are less discussed and understood. However, aromatic compounds along with nitrogen-containing inhibitors are naturally present in the hydrotreater feeds. Therefore, the aim of this study was the preparation and evaluation of hydrotreating catalysts with the main focus on HDA and HDN reactions.</p><p>According to the current state of the art, the utilization of chelating agents during preparation has a positive impact on the characteristics and activity of hydrotreating catalysts therefore NiMo catalysts with (Type II) and without (Type I) a chelating agent were prepared and evaluated towards HDA and HDN reactions. The catalysts were prepared and characterized at KTH and then activated (sulfided) and evaluated at Nynas AB. The activity of the sulfided catalysts was evaluated using surrogate mixture models containing phenanthrene (PHE) as an aromatic compound, and carbazole (CBZ) or acridine (ACR). The latter ones were representing two types of nitrogen-containing inhibitors, non-basic and basic.</p><p>The activity testing was carried out in a trickle-bed microreactor during three-step experiments in the presence and absence of the organic nitrogen compounds (mode switches). During the mode switches the activity of the catalysts under varying conditions was investigated. The operating temperature of the reactor varied between 300 and 320°C under constant H2 pressure of 120 barg.</p><p>The catalytic activity was positively correlated with temperature with the catalysts exhibiting lower activities at 300°C than at 320°C. It is noteworthy that the activity of all the catalysts was hindered by the presence of both nitrogen compounds at all temperatures with the basic nitrogen (ACR) being more inhibitory for both catalysts. CBZ inhibition to the HDA reactions showed reversibility, while ACR had a more permanent inhibiting effect in the case of the Type II catalyst. The results indicated that despite the preliminary assumptions, the Type I catalyst outperformed the Type II.</p>

w='barg' val={'c': 'bars', 's': 'diva2:1708926', 'n': 'error in original'}
As the error is in the original, the error is retained. Interestingly, the body of the thesis does not give this pressure information.

corrected abstract:
<p>Hydrotreating processes are of high importance in helping to obtain the desired characteristics of products as well as to comply with the legislation regarding health hazards and environmental pollution. Hydrotreating reactions are catalytic reactions which imply that the understanding and utilization of the most suitable catalysts is crucial. While hydrodesulfurization is a vastly studied branch of hydrotreating, hydrodearomatization (HDA), and hydrodenitrogenation (HDN) processes are less discussed and understood. However, aromatic compounds along with nitrogen-containing inhibitors are naturally present in the hydrotreater feeds. Therefore, the aim of this study was the preparation and evaluation of hydrotreating catalysts with the main focus on HDA and HDN reactions.</p><p>According to the current state of the art, the utilization of chelating agents during preparation has a positive impact on the characteristics and activity of hydrotreating catalysts therefore NiMo catalysts with (Type II) and without (Type I) a chelating agent were prepared and evaluated towards HDA and HDN reactions. The catalysts were prepared and characterized at KTH and then activated (sulfided) and evaluated at Nynas AB. The activity of the sulfided catalysts was evaluated using surrogate mixture models containing phenanthrene (PHE) as an aromatic compound, and carbazole (CBZ) or acridine (ACR). The latter ones were representing two types of nitrogen-containing inhibitors, non-basic and basic.</p><p>The activity testing was carried out in a trickle-bed microreactor during three-step experiments in the presence and absence of the organic nitrogen compounds (mode switches). During the mode switches the activity of the catalysts under varying conditions was investigated. The operating temperature of the reactor varied between 300 and 320°C under constant H<sub>2</sub> pressure of 120 barg.</p><p>The catalytic activity was positively correlated with temperature with the catalysts exhibiting lower activities at 300°C than at 320°C. It is noteworthy that the activity of all the catalysts was hindered by the presence of both nitrogen compounds at all temperatures with the basic nitrogen (ACR) being more inhibitory for both catalysts. CBZ inhibition to the HDA reactions showed reversibility, while ACR had a more permanent inhibiting effect in the case of the Type II catalyst. The results indicated that despite the preliminary assumptions, the Type I catalyst outperformed the Type II.</p>
----------------------------------------------------------------------
In diva2:1433825 abstract is: <p>The sixth most common cause of death in low income countries is malaria. The disease is spread by mosquitoes of the <em>Anopheles</em> genus. When finding a suitable oviposition site <em>i.e.</em> a place to lay their eggs, the mosquitoes determine site suitability partially using the sense of smell. Previous research has shown that the malaria vector <em>Anopheles Gambiae Sensu Latu</em> has been attracted towards cedrol. Similarly, other research has shown that the malaria vector <em>Anopheles Arabiensis</em> has been attracted towards the odours around rice. In this project, the volatile compounds from Kenyan <em>Cyperus Rotundus</em> have been mapped in hope of being able to identify volatile compounds that attract or repel the <em>Anopheles Gambiae</em> malaria vector. As a comparison of the results from these plants, <em>in situ</em> analysis of <em>Cyperus Involukratus</em> have been carried out. Using the same method as for <em>Cyperus Rotundus, Panicum Repens</em> from Kenya were also analysed. For the extraction of <em>Cyperus Involukratus</em>, a crown of the plant was immersed into a bag or round bottom flask, and volatile compounds from the plant were extracted using a solid phase micro extraction fiber. For the extraction from <em>Panicum Repens</em> and <em>Cyperus Rotundus</em> a headspace was created using a round bottom flask with water, sealed with aluminium foil. A solid phase microextraction fiber was then inserted through the aluminium foil and extraction was allowed to take place. Before the final analysis of these plant samples were performed, the extraction method was optimized. After sample preparation the compounds collected onto the fiber were analysed using gas chromatography mass spectrometry. In the final part of the project <em>Cyperus Rotundus</em> and <em>Panicum Repens</em> were washed and split into aerial parts and root systems respectively. Different extraction times were used for the extraction of volatiles from <em>Cyperus Involukratus</em>, but analysis of the fibers showed very few interesting compounds. Limonene, a monoterpene was found in some of the extractions. In some experiments, even with long extraction times and keeping the plants enclosed, no mono or sesquiterpenes were found. These results were expected by the absence of smell from the plant. During optimization it was found that the fiber used for extraction required a higher temperature in the injector port to be able to release the compounds bound to it. An extraction time of four hours was found to be necessary to let the headspace system equilibrate enough for the volatile compounds to collect onto the fiber. During analysis of the <em>Cyperus Rotundus</em> aerial parts, a high content of monoterpenes and sesquiterpenes was found. In the Cyperus Rotundus root systems only a few monoterpenes were found, but more sesquiterpenes and sesquiterpene derivatives were found than in the aerial parts of the same plant. <em>Panicum Repens</em> did not seem to contain but a few mono or sesquiterpenes. Cyperene was found, which is a sesquiterpene, but most compounds were not of interest. Many of the compounds identified seems to be coming from contamination of this plant sample.</p>

w='Latu' val={'c': 'Lato', 's': 'diva2:1433825', 'n': 'no full text - "from Anopheles Gambiae Sensu Lato"'}

corrected abstract:
<p>The sixth most common cause of death in low income countries is malaria. The disease is spread by mosquitoes of the <em>Anopheles</em> genus. When finding a suitable oviposition site <em>i.e.</em> a place to lay their eggs, the mosquitoes determine site suitability partially using the sense of smell. Previous research has shown that the malaria vector <em>Anopheles Gambiae Sensu Lato</em> has been attracted towards cedrol. Similarly, other research has shown that the malaria vector <em>Anopheles Arabiensis</em> has been attracted towards the odours around rice. In this project, the volatile compounds from Kenyan <em>Cyperus Rotundus</em> have been mapped in hope of being able to identify volatile compounds that attract or repel the <em>Anopheles Gambiae</em> malaria vector. As a comparison of the results from these plants, <em>in situ</em> analysis of <em>Cyperus Involukratus</em> have been carried out. Using the same method as for <em>Cyperus Rotundus, Panicum Repens</em> from Kenya were also analysed. For the extraction of <em>Cyperus Involukratus</em>, a crown of the plant was immersed into a bag or round bottom flask, and volatile compounds from the plant were extracted using a solid phase micro extraction fiber. For the extraction from <em>Panicum Repens</em> and <em>Cyperus Rotundus</em> a headspace was created using a round bottom flask with water, sealed with aluminium foil. A solid phase microextraction fiber was then inserted through the aluminium foil and extraction was allowed to take place. Before the final analysis of these plant samples were performed, the extraction method was optimized. After sample preparation the compounds collected onto the fiber were analysed using gas chromatography mass spectrometry. In the final part of the project <em>Cyperus Rotundus</em> and <em>Panicum Repens</em> were washed and split into aerial parts and root systems respectively. Different extraction times were used for the extraction of volatiles from <em>Cyperus Involukratus</em>, but analysis of the fibers showed very few interesting compounds. Limonene, a monoterpene was found in some of the extractions. In some experiments, even with long extraction times and keeping the plants enclosed, no mono or sesquiterpenes were found. These results were expected by the absence of smell from the plant. During optimization it was found that the fiber used for extraction required a higher temperature in the injector port to be able to release the compounds bound to it. An extraction time of four hours was found to be necessary to let the headspace system equilibrate enough for the volatile compounds to collect onto the fiber. During analysis of the <em>Cyperus Rotundus</em> aerial parts, a high content of monoterpenes and sesquiterpenes was found. In the Cyperus Rotundus root systems only a few monoterpenes were found, but more sesquiterpenes and sesquiterpene derivatives were found than in the aerial parts of the same plant. <em>Panicum Repens</em> did not seem to contain but a few mono or sesquiterpenes. Cyperene was found, which is a sesquiterpene, but most compounds were not of interest. Many of the compounds identified seems to be coming from contamination of this plant sample.</p>
----------------------------------------------------------------------
In diva2:951121 abstract is: <p>Fossil resources are the major source from which carbon based materials and chemicals are produced, this is not sustainable. With diminishing supplies and increasing prices and environmental concern, alternative resources must be developed. By using chemical-catalytic methods it is possible to utilize solar energy stored as biomass for production of a wide variety of chemicals and materials. It has previously been established that it is possible to produce 5-(chloromethyl)furfural (CMF) from a wide variety of biomass containing sucrose, glucose or cellulose. CMF is a platform chemical that can be converted into fuels, monomers for consumer plastics and highly valuable pharmaceuticals. When performing the process on lignocellulosic material a residue containing lignin is left as byproduct. Much is still to be investigated around the CMF process, and the present study aims to analyze the lignin from the CMF process (performed on spruce sawdust) in terms of its composition and heating value, to find a simple method to produce the promising monomer (2,5-furandicarboxylic acid, FDCA) from CMF in one catalytic step, and to depolymerize and hydrogenate lignin from the CMF process using HI and H2 in a catalytic cycle.</p><p>Composition analysis of the CMF lignin showed low ash content with a decrease in ash-forming inorganic elements compared with the feedstock. During the process chlorine is introduced in the material to a concentration of 1.7%. The heating value of CMF lignin is ~30% higher than the feedstock. Attempts to oxidize CMF with H2O2 to FDCA revealed a fast and efficient reaction between CMF and H2O2 with 5-(hydroperoxymethyl)furfural (HPMF) as the product. HPMF is a novel structure and may prove interesting in future derivatization of CMF. Experiments were performed to investigate the effect of HI on CMF lignin. Results show that HI depolymerizes lignin to organic soluble fragments to a maximum of 31% dissolution after 6 hours at 125 °C. A higher temperature and reaction time is desirable to achieve higher dissolution. The soluble fraction is a mixture of compounds.</p><p>Clean lignin is highly desirable in many processes. Removal of chlorine would yield a clean lignin suitable for combustion or as a component in materials. The possibility to depolymerize CMF lignin is also promising as a way to produce products from it. HPMF may prove to be interesting as an intermediate when oxidizing CMF.</p>

w='H2O2' val={'c': 'H<sub>2</sub>O<sub>2</sub>', 's': 'diva2:951121', 'n': 'correct in original'}

corrected abstract:
<p>Fossil resources are the major source from which carbon based materials and chemicals are produced, this is not sustainable. With diminishing supplies and increasing prices and environmental concern, alternative resources must be developed. By using chemical-catalytic methods it is possible to utilize solar energy stored as biomass for production of a wide variety of chemicals and materials. It has previously been established that it is possible to produce 5-(chloromethyl)furfural (CMF) from a wide variety of biomass containing sucrose, glucose or cellulose. CMF is a platform chemical that can be converted into fuels, monomers for consumer plastics and highly valuable pharmaceuticals. When performing the process on lignocellulosic material a residue containing lignin is left as byproduct. Much is still to be investigated around the CMF process, and the present study aims to analyze the lignin from the CMF process (performed on spruce sawdust) in terms of its composition and heating value, to find a simple method to produce the promising monomer (2,5-furandicarboxylic acid, FDCA) from CMF in one catalytic step, and to depolymerize and hydrogenate lignin from the CMF process using HI and H<sub>2</sub> in a catalytic cycle.</p><p>Composition analysis of the CMF lignin showed low ash content with a decrease in ash-forming inorganic elements compared with the feedstock. During the process chlorine is introduced in the material to a concentration of 1.7%. The heating value of CMF lignin is ~30% higher than the feedstock. Attempts to oxidize CMF with H<sub>2</sub>O<sub>2</sub> to FDCA revealed a fast and efficient reaction between CMF and H<sub>2</sub>O<sub>2</sub> with 5-(hydroperoxymethyl)furfural (HPMF) as the product. HPMF is a novel structure and may prove interesting in future derivatization of CMF. Experiments were performed to investigate the effect of HI on CMF lignin. Results show that HI depolymerizes lignin to organic soluble fragments to a maximum of 31% dissolution after 6 hours at 125 °C. A higher temperature and reaction time is desirable to achieve higher dissolution. The soluble fraction is a mixture of compounds.</p><p>Clean lignin is highly desirable in many processes. Removal of chlorine would yield a clean lignin suitable for combustion or as a component in materials. The possibility to depolymerize CMF lignin is also promising as a way to produce products from it. HPMF may prove to be interesting as an intermediate when oxidizing CMF.</p>
----------------------------------------------------------------------
In diva2:765778 abstract is: <p>Lewis acids can be used as catalysts in different reactions, but the term Lewis acid catalysts often refers to metal salts. Metal complexes have been widely used for asymmetric catalysis. Asymmetric synthesis can however be performed in a metal-free way by using organocatalysis. New Lewis acid catalysts that are more effective, enantioselective and environmental friendly is of interest. This new type of Lewis acid catalysts could for example be of carbocation based character.</p><p>The aim of this project was to synthesize chiral carbocations with different degree of sterical hindrance and investigate their catalytic ability in Diels-Alder reactions. It was presumed that the Diels-Alder reactions were going to be performed in an asymmetric way since the carbocation catalysts were achiral.</p><p>Two chiral carbocations were synthesized successfully. The first synthesized carbocation, the less sterical hindered compound <strong>8</strong>, was formed as a racemic mixture. The second carbocation, compound <strong>16</strong>, could be formed as an enatiomeric pure compound. Both carbocations showed catalytic ability in Diels-Alder reactions and compound <strong>8 </strong>was comparable with some common Lewis acid catalysts. In general, when using compound <strong>8 </strong>as catalyst, higher catalyst amount gave higher conversions. Higher concentrations also gave higher conversions, but up to a certain level. No trend between polarity of different solvents and conversions could be seen. However, an increased temperature leads to faster reactions. The more rigid and sterical hindered compound <strong>16 </strong>catalyzed the reactions slower than compound <strong>8</strong>. The longer reaction time may indicate that the reaction occurs with higher selectivity, but no method to measure the <em>ee </em>of the product was found.</p><p>An attempt to synthesize a third even more sterical hindered chiral carbocation, compound <strong>19</strong>, resulted in a product contaminated by impurities that showed a catalytic ability lower than compound <strong>8 </strong>and compound <strong>16 </strong>in Diels-Alder reactions.</p><p>The synthesis and the use of carbocations as Lewis acid catalysts in Diels-Alder reactions seem promising as a new type of catalysts even though there are questions that are still unanswered, e.g. counter ions effects, possible side reactions, selectivity etc.</p>

w='enatiomeric' val={'c': 'enantiomeric', 's': 'diva2:765778'}
Note "ee" stands for "Enantiomeric excess"


corrected abstract:
<p>Lewis acids can be used as catalysts in different reactions, but the term Lewis acid catalysts often refers to metal salts. Metal complexes have been widely used for asymmetric catalysis. Asymmetric synthesis can however be performed in a metal-free way by using organocatalysis. New Lewis acid catalysts that are more effective, enantioselective and environmental friendly is of interest. This new type of Lewis acid catalysts could for example be of carbocation based character.</p><p>The aim of this project was to synthesize chiral carbocations with different degree of sterical hindrance and investigate their catalytic ability in Diels-Alder reactions. It was presumed that the Diels-Alder reactions were going to be performed in an asymmetric way since the carbocation catalysts were achiral.</p><p>Two chiral carbocations were synthesized successfully. The first synthesized carbocation, the less sterical hindered compound <strong>8</strong>, was formed as a racemic mixture. The second carbocation, compound <strong>16</strong>, could be formed as an enantiomeric pure compound. Both carbocations showed catalytic ability in Diels-Alder reactions and compound <strong>8</strong> was comparable with some common Lewis acid catalysts. In general, when using compound <strong>8</strong> as catalyst, higher catalyst amount gave higher conversions. Higher concentrations also gave higher conversions, but up to a certain level. No trend between polarity of different solvents and conversions could be seen. However, an increased temperature leads to faster reactions. The more rigid and sterical hindered compound <strong>16</strong> catalyzed the reactions slower than compound <strong>8</strong>. The longer reaction time may indicate that the reaction occurs with higher selectivity, but no method to measure the <em>ee</em> of the product was found.</p><p>An attempt to synthesize a third even more sterical hindered chiral carbocation, compound <strong>19</strong>, resulted in a product contaminated by impurities that showed a catalytic ability lower than compound <strong>8</strong> and compound <strong>16</strong> in Diels-Alder reactions.</p><p>The synthesis and the use of carbocations as Lewis acid catalysts in Diels-Alder reactions seem promising as a new type of catalysts even though there are questions that are still unanswered, e.g. counter ions effects, possible side reactions, selectivity etc.</p>
----------------------------------------------------------------------
In diva2:744725 abstract is: <p>Solid-Phase Peptide Sythesis (SPPS) is the method of choice for efficient peptide synthesis. SPPS is a stepwise method where a growing peptide is synthesised onto a linker molecule attached to a solid suuport (resin). In the final step of the synthesis, the peptide must be cleaved off from the lnker molcule and the protective groups removed concomitantly. This procedure is conventionally carried out with trifluoracetic acid (TFA). This is a corrosive and toxic chemical that is difficult to handle and requires special safety procedures for its disposal. Hence, it is of large interest to find a safer and a more environmentally friendly alternative. Therefore, the purpose of this study was to investigate if subcritical water under mild acidic conditions were achieved with the Biotage®  Initiator<sup>+</sup> Microwave Synthesizer, limited to a maximum pressure of 30 bar. A temerature range of 200-230°C and pressures of 15.5-27.8 bar (where the dielectric constant of water is close to MeOH or DMF) was examined for various reaction times and pH, by addition of HCI, to find suitable conditions for efficient cleavage of amino acid-resin conjugates. The main part of the experiments was carried out on pre-loaded Fmoc-Ohe-Wang resin, where a maximum cleavage of 77% was obtained. Further studies should focus on the influence of subcritical conditions on amide bonds and side reactions that might occur. It should also be investigated if the conditions where cleavage of the Wang linker is obtained are sufficient for concomitant deprotection of the side chain protective groups, as is the case in conventional cleavage and deprotection.</p>

w='lnker' val={'c': 'linker', 's': 'diva2:744725', 'n': 'no full text'}
w='molcule' val={'c': 'molecule', 's': 'diva2:744725', 'n': 'no full text'}
w='suuport' val={'c': 'support', 's': 'diva2:744725', 'n': 'no full text'}
w='temerature' val={'c': 'temperature', 's': 'diva2:744725', 'n': 'no full text'}
w='Sythesis' val={'c': 'Synthesis', 's': 'diva2:744725', 'n': 'no full text'}

corrected abstract:
<p>Solid-Phase Peptide Synthesis (SPPS) is the method of choice for efficient peptide synthesis. SPPS is a stepwise method where a growing peptide is synthesised onto a linker molecule attached to a solid support (resin). In the final step of the synthesis, the peptide must be cleaved off from the linker molecule and the protective groups removed concomitantly. This procedure is conventionally carried out with trifluoracetic acid (TFA). This is a corrosive and toxic chemical that is difficult to handle and requires special safety procedures for its disposal. Hence, it is of large interest to find a safer and a more environmentally friendly alternative. Therefore, the purpose of this study was to investigate if subcritical water under mild acidic conditions were achieved with the Biotage®  Initiator<sup>+</sup> Microwave Synthesizer, limited to a maximum pressure of 30 bar. A temperature range of 200-230°C and pressures of 15.5-27.8 bar (where the dielectric constant of water is close to MeOH or DMF) was examined for various reaction times and pH, by addition of HCI, to find suitable conditions for efficient cleavage of amino acid-resin conjugates. The main part of the experiments was carried out on pre-loaded Fmoc-Ohe-Wang resin, where a maximum cleavage of 77% was obtained. Further studies should focus on the influence of subcritical conditions on amide bonds and side reactions that might occur. It should also be investigated if the conditions where cleavage of the Wang linker is obtained are sufficient for concomitant deprotection of the side chain protective groups, as is the case in conventional cleavage and deprotection.</p>
----------------------------------------------------------------------
In diva2:855720 abstract is: <p>Somatic copy number alterations (SCNAs) are an important type of structural variations that affect cancer pathogenesis. Accirate detection of SCNAs is a crucial task as it can lead to identification of events driving cancer development. The advent of next-generation sequencing technologies has revolutionized the field of genomics and variant analysis. While whole-genome sequencing can give a broader view of the genome, whole-exome sequencing has the advantage of time and cost efficiency. Several algorithms have been developed to detect SCNAs from whole-genome and whole-exome sequencing data. However, their relative performance and efficiency was not well described. In this thesis, we present a comparative analysis of six SCNA detection, algorithms in sequencing data including ControlFreeC, BICseq, HMMcopy, CNAnorm, ExomeCNV and VarScan2. We use simulated data as well as a real dataset of 11 breast cancer samples subjected to whole-genome, whole-exome sequencing and SNP array genotyping. We address the relative strenths and limitations of each algorith, and we explore the relative merits of using whole-genome over whole-exome sequencing data.</p>

w='strenths' val={'c': 'strengths', 's': 'diva2:855720', 'n': 'no full text'}
w='Accirate' val={'c': 'Accurate', 's': 'diva2:855720', 'n': 'no full text'}

corrected abstract:
<p>Somatic copy number alterations (SCNAs) are an important type of structural variations that affect cancer pathogenesis. Accurate detection of SCNAs is a crucial task as it can lead to identification of events driving cancer development. The advent of next-generation sequencing technologies has revolutionized the field of genomics and variant analysis. While whole-genome sequencing can give a broader view of the genome, whole-exome sequencing has the advantage of time and cost efficiency. Several algorithms have been developed to detect SCNAs from whole-genome and whole-exome sequencing data. However, their relative performance and efficiency was not well described. In this thesis, we present a comparative analysis of six SCNA detection, algorithms in sequencing data including ControlFreeC, BICseq, HMMcopy, CNAnorm, ExomeCNV and VarScan2. We use simulated data as well as a real dataset of 11 breast cancer samples subjected to whole-genome, whole-exome sequencing and SNP array genotyping. We address the relative strengths and limitations of each algorith, and we explore the relative merits of using whole-genome over whole-exome sequencing data.</p>
----------------------------------------------------------------------
In diva2:1723014 abstract is: <p>The distinct composition of the 20 naturally occurring amino acids gives rise to the vast spectrum of proteins with unique sequences. In this context, a special region within protein sequence includes the repetition of single amino acid generally referred to as homorepeats. This simple sequence motifs has been reported to exist in proteins involved in different biological processes and associated with several disease states. In this project a large-scale data analysis of protein sequences and structures to identify and characterize homorepeats, has been done. For that purpose, an in-house analysis script in the python programming language has been written, which automates the algorithm of scanning for repetitive amino acids in the primary sequence of proteins. Further, another python script has also been developed, which reads a structure file and characterizes the secondary structure preference of the different single amino acid repeats. The results showed that such sequence motifs are predominantly composed of hydrophilic residues with a few interesting exceptions of non- polar residues such as alanine, proline, and leucine. Preliminary structural analysis suggests that the different amino acids in the homorepeats could have preference for distinct secondary structural states. Collectively, this work provides interesting insights into the biased sequence-structure distribution of amino acids in homorepats and a basis for further exploration. </p>

w='homorepats' val={'c': 'homorepeats', 's': 'diva2:1723014', 'n': 'no full text'}

corrected abstract:
<p>The distinct composition of the 20 naturally occurring amino acids gives rise to the vast spectrum of proteins with unique sequences. In this context, a special region within protein sequence includes the repetition of single amino acid generally referred to as homorepeats. This simple sequence motifs has been reported to exist in proteins involved in different biological processes and associated with several disease states. In this project a large-scale data analysis of protein sequences and structures to identify and characterize homorepeats, has been done. For that purpose, an in-house analysis script in the python programming language has been written, which automates the algorithm of scanning for repetitive amino acids in the primary sequence of proteins. Further, another python script has also been developed, which reads a structure file and characterizes the secondary structure preference of the different single amino acid repeats. The results showed that such sequence motifs are predominantly composed of hydrophilic residues with a few interesting exceptions of non- polar residues such as alanine, proline, and leucine. Preliminary structural analysis suggests that the different amino acids in the homorepeats could have preference for distinct secondary structural states. Collectively, this work provides interesting insights into the biased sequence-structure distribution of amino acids in homorepeats and a basis for further exploration. </p>
----------------------------------------------------------------------
In diva2:1745402 abstract is: <p>Cells utilise various stress responses that are rapidly activated to avoid cell death under adverse conditions. Tracking gene transcription offers insight into the immediate changes in the cell, and one method to study the process of nascent transcription is precision run-on sequencing (PRO-seq). In PRO-seq, biotin-labeled nucleotides are added to halt the transcription as the RNA polymerase cannot continue transcription after incorporating biotin-labeled nucleotide to the nascent RNA. The biotin-labeled nascent RNAs are then isolated from the myriad of RNAs in the cell and sequenced. Simplifying the analysis of sequenced data and making the analysis more available for a larger group of scientist is needed, and therefore the aim of this thesis is to build a computational pipeline to analyse PRO-seq data. The pipeline consists of five shell and three R scripts that create a reference genome index for alignment, load experimental data, align the data to the reference genome, and output .bed and .bigWig files for further analysis. Using the profile of nascent transcription, the pipeline then identifies functional genomic regions and outputs gene expression activity based on engaged polymerase counts. The data used in this study are from heat shock cells from Homo sapiens, Canis lupis familiaris, Mus musculus, and Drosophila melanogaster. This analysis strategy provides a method to visualise gene lengths, map functional genomic regions, count engaged RNA polymerase and identify unannotated genes and enhancers. The analysis showed that the use of bidirectional transcription to study cell stress is more useful in mammals than in insects and that genes encoding chaperone machineries were induced in all organisms upon heat shock. The pipeline developed in this Masters Thesis offers a standardised and user-friendly method to study PRO-seq data and simplifies the analysis for laboratories with less experience in data analysis, additionally it is a tool to handle and automate processing of large amounts of data from distinct organisms. The computational pipeline outputs profiles of engaged RNA polymerases genome-wide, maps functional genomic regions, and counts transcriptional activity of genes and enhancers. </p>

w='lupis' val={'c': 'lupus', 's': 'diva2:1745402', 'n': 'correct in original, part of , Canis lupus familiaris'}

corrected abstract:
<p>Cells utilise various stress responses that are rapidly activated to avoid cell death under adverse conditions. Tracking gene transcription offers insight into the immediate changes in the cell, and one method to study the process of nascent transcription is precision run-on sequencing (PRO-seq). In PRO-seq, biotin-labeled nucleotides are added to halt the transcription as the RNA polymerase cannot continue transcription after incorporating biotin-labeled nucleotide to the nascent RNA. The biotin-labeled nascent RNAs are then isolated from the myriad of RNAs in the cell and sequenced. Simplifying the analysis of sequenced data and making the analysis more available for a larger group of scientist is needed, and therefore the aim of this thesis is to build a computational pipeline to analyse PRO-seq data. The pipeline consists of five shell and three R scripts that create a reference genome index for alignment, load experimental data, align the data to the reference genome, and output .bed and .bigWig files for further analysis. Using the profile of nascent transcription, the pipeline then identifies functional genomic regions and outputs gene expression activity based on engaged polymerase counts. The data used in this study are from heat shock cells from <em>Homo sapiens</em>, <em>Canis lupus familiaris</em>, <em>Mus musculus</em>, and <em>Drosophila melanogaster</em>. This analysis strategy provides a method to visualise gene lengths, map functional genomic regions, count engaged RNA polymerase and identify unannotated genes and enhancers. The analysis showed that the use of bidirectional transcription to study cell stress is more useful in mammals than in insects and that genes encoding chaperone machineries were induced in all organisms upon heat shock. The pipeline developed in this Masters Thesis offers a standardised and user-friendly method to study PRO-seq data and simplifies the analysis for laboratories with less experience in data analysis, additionally it is a tool to handle and automate processing of large amounts of data from distinct organisms. The computational pipeline outputs profiles of engaged RNA polymerases genome-wide, maps functional genomic regions, and counts transcriptional activity of genes and enhancers.</p>
----------------------------------------------------------------------
In diva2:744997 abstract is: <p>The dopamine system innervates diﬀerent regions of the brain via different pathways, and regulates motor function, cognitive behavior and emotions as well as motivation and reward. The binding to dopamine receptors in different brain regions is therefore important for different functions.</p><p>With aid of Positron Emission Tomography (PET) it is possible to investigate receptor binding in different regions for different neurotransmitters in vivo. There are different kinds of dopamine receptors, these subtypes are grouped into two categories, D1-like and D2- like receptors that have different intracellular signalling pathways. The dopamine receptors are distributed over the entire brain, but the receptor densities vary over different regions. The relationship between receptor density for different regions and across subjects are poorly understood. Radioligands used in this study was <sup>11</sup>C-SCH23390 for D1 binding and <sup>11</sup>C-Raclopride for D2 binding. Regions included in this investigation was striatum, amygdala, hippocampus, anterior cingulated cortex, orbitofrontal cortex, lateral temporal cortex and lateral parietal cortex. Manual region of interests were manually delineated for striatal areas, and the Anatomical Automatic Labelling in SPM5 were used for obtaining the BP values for extrastriatal areas. The participants in this study were 30 healthy volunteering male subjects with average age of 24.9 (std 2.7). No linear Pearson correlations were found between D1 and D2 for neither of the included regions. With aid of principal component analysis and the clustering techniques K-means and Fuzzy c-means, the subjects were divided into different clusters depending on their individual dopamine D1 and D2 receptor binding proﬁles. One subgroup had high D1−levels, and low D2−levels, the other group low D1−levels and high D2−levels. These formations were not distinguishable by visual inspection of the group as a single entity, but were identiﬁed by the cluster algorithms.</p>

w='C-SCH23390' val={'c': '<sup>11</sup>C-SCH23390', 's': 'diva2:744997'}
w='D2like' val={'c': 'D2-like', 's': 'diva2:744997', 'n': 'no full text'}
w='D2- like' val={'c': 'D2-like', 's': 'diva2:744997', 'n': 'no full text'}

corrected abstract:
<p>The dopamine system innervates diﬀerent regions of the brain via different pathways, and regulates motor function, cognitive behavior and emotions as well as motivation and reward. The binding to dopamine receptors in different brain regions is therefore important for different functions.</p><p>With aid of Positron Emission Tomography (PET) it is possible to investigate receptor binding in different regions for different neurotransmitters in vivo. There are different kinds of dopamine receptors, these subtypes are grouped into two categories, D1-like and D2-like receptors that have different intracellular signalling pathways. The dopamine receptors are distributed over the entire brain, but the receptor densities vary over different regions. The relationship between receptor density for different regions and across subjects are poorly understood. Radioligands used in this study was <sup>11</sup>C-SCH23390 for D1 binding and <sup>11</sup>C-Raclopride for D2 binding. Regions included in this investigation was striatum, amygdala, hippocampus, anterior cingulated cortex, orbitofrontal cortex, lateral temporal cortex and lateral parietal cortex. Manual region of interests were manually delineated for striatal areas, and the Anatomical Automatic Labelling in SPM5 were used for obtaining the BP values for extrastriatal areas. The participants in this study were 30 healthy volunteering male subjects with average age of 24.9 (std 2.7). No linear Pearson correlations were found between D1 and D2 for neither of the included regions. With aid of principal component analysis and the clustering techniques K-means and Fuzzy c-means, the subjects were divided into different clusters depending on their individual dopamine D1 and D2 receptor binding proﬁles. One subgroup had high D1−levels, and low D2−levels, the other group low D1−levels and high D2−levels. These formations were not distinguishable by visual inspection of the group as a single entity, but were identiﬁed by the cluster algorithms.</p>
----------------------------------------------------------------------
In diva2:777202 abstract is: <p>In a previous project, mobile exposures in road environments have been performed in</p><p>various areas of the world. A number of materials and coatings have been attached under</p><p>trailers which travels long distances in different types of road environments. One of the</p><p>materials, zinc, demonstrated various corrosion rates in different parts of the world. In this</p><p>investigation two various accelerated corrosion tests have been performed at the laboratory</p><p>of Swerea Kimab in order to realize parameter influence on zinc. Even though the corrosion</p><p>rates are known, the relation to formed corrosion products has not been investigated</p><p>earlier. In the present study, corrosion products on zinc have been analyzed using XRD and</p><p>FTIR. The results from analyses of formed corrosion products have been evaluated together</p><p>with measured corrosion rates, both from the mobile exposure and the accelerated tests, in</p><p>order to try to understand under what conditions certain corrosion products are formed and</p><p>how it affects the corrosion rate.</p><p>A cold humid climate was found to be the most corrosive. In these environments, de</p><p>‐icing</p><p>salts are used which promotes formation of Simonkolleite and in a subsequent step;</p><p>Gordaite (if there is a not negligible deposition rate of SO2/SO42‐) which is often observed as the end product in marine environments.</p>

w='SO2' val={'c': 'SO<sub>2/sub>', 's': 'diva2:777202'}
w='SO42-' val={'c': 'SO<sub>4</sub><sup>2-</sup>', 's': 'diva2:777202'}

corrected abstract:
<p>In a previous project, mobile exposures in road environments have been performed in various areas of the world. A number of materials and coatings have been attached under trailers which travels long distances in different types of road environments. One of the materials, zinc, demonstrated various corrosion rates in different parts of the world. In this investigation two various accelerated corrosion tests have been performed at the laboratory of Swerea Kimab in order to realize parameter influence on zinc. Even though the corrosion rates are known, the relation to formed corrosion products has not been investigated earlier. In the present study, corrosion products on zinc have been analyzed using XRD and FTIR. The results from analyses of formed corrosion products have been evaluated together with measured corrosion rates, both from the mobile exposure and the accelerated tests, in order to try to understand under what conditions certain corrosion products are formed and how it affects the corrosion rate.</p><p>A cold humid climate was found to be the most corrosive. In these environments, de‐icing salts are used which promotes formation of simonkolleite and in a subsequent step; gordaite (if there is a not negligible deposition rate of SO<sub>2/sub>/SO<sub>4</sub><sup>2‐</sup>) which is often observed as the end product in marine environments.</p>
----------------------------------------------------------------------
In diva2:1136064 abstract is: <p>Nanocomposites of polypropylene and montmorillonite clay can be used in many applications thanks to t heir high thermal stability and mechanical strength . It is of outmost importance that the degradation of these materials can be contr olled and under stood so that t he material can be used for t he exact right application without breakage. Cracking in these nanocomposites while they undergo thermal aging is a fact. The aim of this study was to investigate which parameters that influence the cracking and what can be done to minimize it. Two main parameters have been investigated; the compounding ofthe materials and the cooling rate in the processing step. Results showed that the use of a twin-s crew extruder (TSE) provided better results as compared to a microcompounder and that the cooling rate should be fast, to minimize the cracking. The results also show that stainless steel used in the compressionmould ing step has a major impact on the cracking.</p>

w='erials' val={'c': 'materials', 's': 'diva2:1136064', 'n': 'no full text'}
w='contr' val={'c': 'controlled', 's': 'diva2:1136064', 'n': 'no full text'}
w='ing' val={'c': 'compression moulding', 's': 'diva2:1136064', 'n': 'no full text'}

corrected abstract:
<p>Nanocomposites of polypropylene and montmorillonite clay can be used in many applications thanks to their high thermal stability and mechanical strength. It is of outmost importance that the degradation of these materials can be controlled and understood so that the material can be used for the exact right application without breakage. Cracking in these nanocomposites while they undergo thermal aging is a fact. The aim of this study was to investigate which parameters that influence the cracking and what can be done to minimize it. Two main parameters have been investigated; the compounding of the materials and the cooling rate in the processing step. Results showed that the use of a twin-s crew extruder (TSE) provided better results as compared to a microcompounder and that the cooling rate should be fast, to minimize the cracking. The results also show that stainless steel used in the compression moulding step has a major impact on the cracking.</p>
----------------------------------------------------------------------
In diva2:1738587 abstract is: <p>Malignant cancer cells undergo uncontrolled proliferation and can metastasize to distant organs. A critical step in metastasis is the ability of cancer cells to invade neighboring tissues before they can disseminate to distant organs. Understanding the mechanisms of invasiveness could therefore lead to the discovery of new druggable targets to prevent metastasis. In this project, the invasiveness properties of human melanoma cells were studied in vitro using a CRISPR knockout screen. For this, three pooled CRISPR libraries were evaluated, and then one of them was selected for the screen. When the gRNA representation of the selected epigentic knockout library had been validated, a lentiviral library was generated for transduction of A375 melanoma cells. The invasiveness of the mutated melanoma cells was then examined with an optimized invasion assay using Matrigel invasion chambers. Transduced melanoma cells were seeded into the upper chamber and allowed to migrate through the Matrigel towards the bottom chamber that had a higher concentration of chemoattractant. Cells in the upper and bottom chamber were then separately collected and their genomic DNA was isolated. The sequencing library was produced by PCR amplification and sequenced with Illumina next-generation sequencing technology. The sequencing data from the CRISPR screen is not included in this report.</p>

w='epigentic' val={'c': 'epigenetic', 's': 'diva2:1738587', 'n': 'error in original'}

corrected abstract:
<p>Malignant cancer cells undergo uncontrolled proliferation and can metastasize to distant organs. A critical step in metastasis is the ability of cancer cells to invade neighboring tissues before they can disseminate to distant organs. Understanding the mechanisms of invasiveness could therefore lead to the discovery of new druggable targets to prevent metastasis. In this project, the invasiveness properties of human melanoma cells were studied <em>in vitro</em> using a CRISPR knockout screen. For this, three pooled CRISPR libraries were evaluated, and then one of them was selected for the screen. When the gRNA representation of the selected epigentic knockout library had been validated, a lentiviral library was generated for transduction of A375 melanoma cells. The invasiveness of the mutated melanoma cells was then examined with an optimized invasion assay using Matrigel invasion chambers. Transduced melanoma cells were seeded into the upper chamber and allowed to migrate through the Matrigel towards the bottom chamber that had a higher concentration of chemoattractant. Cells in the upper and bottom chamber were then separately collected and their genomic DNA was isolated. The sequencing library was produced by PCR amplification and sequenced with Illumina next-generation sequencing technology. The sequencing data from the CRISPR screen is not included in this report.</p>
----------------------------------------------------------------------
In diva2:1109225 abstract is: <p>This paper has investigated the possibility of optimizing the power consumption of pumps through the control system for a wastewater pumping stations. Further-more, the possibilities of developing new control functions has been investigated, with the purpose to simplify the operation maintenance of the station. To investigate energy consumption, a theoretical model of a pumping station has been made where energy tests for three different control methods have been per-formed. Two of the methods studied were on/off control and a P controller which are commonly used in pump stations. The third method was a Fuzzy controller which, with regard to the level and level change, regulates the speed of the pump.The results from the energy tests showed that on/off control consumed most en-ergy and the Fuzzy controller demonstrated best features to optimize energy con-sumption. The Fuzzy controller’s energy consumption was less in most of the tests compared with the P controller. The Fuzzy controller showed the best properties when the inflow was low relative to the pump’s capacity and the bottom area of the sump was smaller.When investigating new control functions, interviews with management executives has been made to get a picture of the problems that may occur in a pump station. The control functions developed had the purpose of reducing the number of clogged pumps as this was the biggest problem.</p>


w='en-ergy' val={'c': 'energy', 's': 'diva2:1109225'}
There were multiple words with unnecessary hyphen and the paragraphs were all merged, but now are separated.


corrected abstract:
<p>This paper has investigated the possibility of optimizing the power consumption of pumps through the control system for a wastewater pumping stations. Furthermore, the possibilities of developing new control functions has been investigated, with the purpose to simplify the operation maintenance of the station.</p><p>To investigate energy consumption, a theoretical model of a pumping station has been made where energy tests for three different control methods have been performed. Two of the methods studied were on/off control and a P controller which are commonly used in pump stations. The third method was a Fuzzy controller which, with regard to the level and level change, regulates the speed of the pump.</p><p>The results from the energy tests showed that on/off control consumed most energy and the Fuzzy controller demonstrated best features to optimize energy consumption. The Fuzzy controller’s energy consumption was less in most of the tests compared with the P controller. The Fuzzy controller showed the best properties when the inflow was low relative to the pump’s capacity and the bottom area of the sump was smaller.</p><p>When investigating new control functions, interviews with management executives has been made to get a picture of the problems that may occur in a pump station. The control functions developed had the purpose of reducing the number of clogged pumps as this was the biggest problem.</p>
----------------------------------------------------------------------
In diva2:1674058 abstract is: <p>A strategy to suppress the growth of dendrites on solid state lithium anodes was investigated. Using density functional theory, four liquid crystal molecules were adsorbed on a solid lithium surface leading to an interfacial stabilization. This stabilization has earlier been used as a descriptor in a phase-field model which investigated dendrite suppression. The replication of this phase-field model was out of the scope of this thesis and left as future work. The LC molecules interacted strongly with the surface, and the calculated adsorption energies had an considerable impact on the interfacial energies of the lithium surface. A liquid crystal phase was also simulated, with a cohesive energy of the same magnitude as liquid water. This energy was lower than the adsorption energies, indicating that there is a driving force for the LC molcules to adsorb to the surface. Furthermore, the redox stability of the molecules in the proximity of the lithium surface was investigated, where two of them had LUMO energies below the Fermi level of lithium. Those two molecules were thus not considered sufficiently stable to not take part in any electrochemical reactions with solid lithium. Finally, the surface diffusion barrier of adsorbed lithium atoms was investigated. The barrier with and without the liquid crystals adsorbed to the surface was compared, which showed that the diffusion barrier was even higher with the molecules adsorbed.</p>


w='molcules' val={'c': 'molecules', 's': 'diva2:1674058'}

corrected abstract:
<p>A strategy to suppress the growth of dendrites on solid state lithium anodes was investigated. Using density functional theory, four liquid crystal molecules were adsorbed on a solid lithium surface leading to an interfacial stabilization. This stabilization has earlier been used as a descriptor in a phase-field model which investigated dendrite suppression. The replication of this phase-field model was out of the scope of this thesis and left as future work. The LC molecules interacted strongly with the surface, and the calculated adsorption energies had an considerable impact on the interfacial energies of the lithium surface. A liquid crystal phase was also simulated, with a cohesive energy of the same magnitude as liquid water. This energy was lower than the adsorption energies, indicating that there is a driving force for the LC molecules to adsorb to the surface. Furthermore, the redox stability of the molecules in the proximity of the lithium surface was investigated, where two of them had LUMO energies below the Fermi level of lithium. Those two molecules were thus not considered sufficiently stable to not take part in any electrochemical reactions with solid lithium. Finally, the surface diffusion barrier of adsorbed lithium atoms was investigated. The barrier with and without the liquid crystals adsorbed to the surface was compared, which showed that the diffusion barrier was even higher with the molecules adsorbed.</p>
----------------------------------------------------------------------
In diva2:765806 abstract is: <p>Novel hyperbranched-linear ABA block copolymers with a linear poly(ethylene glycol) (PEG) B part and a hyperbranched A part based on 2,2-bis(hydroxymethyl) propionic acid (bis-MPA) monomer were successfully synthesized. A pseudo one step condensation esterification was used to produce hyperbranched polymers from generation 2 (G2) to generation 7 (G7) with PEG (Mw = 2000, 6000, 20000 g/mol) core units. The synthesized polymers were characterized to determine degree of branching (DB), molecular weight (Mw,Mn), thermal properties (Tm, ΔH) and solubility. 1HNMR integrals confirmed that all generations for all three core PEG lengths were synthesized successfully. An increased intensity for the ester bonds and no residual carboxylic acid groups from bis-MPA monomer could be seen with IR spectroscopy. The hyperbranched polymers had a decreasing Tm and ΔH with increasing generation and a degree of branching between 40 and 60 % for all PEG lengths. PEG20K-Gx-OH was used to increase the viscosity of water. PEG20K-G2-OH had the highest intrinsic viscosity. Further functionalisation was performed to introduce allyl functionalities on the PEG6K-Gx-OH system which was then thiol-ene coupled with 3-mercapto propionic acid and cross-linked into gels using UV initiated thiol-ene chemistry. Hydrophobic and bulky benzylidene groups were attached to the..</p>


w='PEG20K-G2-OH' val={'c': 'PEG<sub>20K</sub>-G<sub>2</sub>-OH', 's': 'diva2:765806', 'n': 'no full text'}

corrected abstract:
<p>Novel hyperbranched-linear ABA block copolymers with a linear poly(ethylene glycol) (PEG) B part and a hyperbranched A part based on 2,2-bis(hydroxymethyl) propionic acid (bis-MPA) monomer were successfully synthesized. A pseudo one step condensation esterification was used to produce hyperbranched polymers from generation 2 (G2) to generation 7 (G7) with PEG (Mw = 2000, 6000, 20000 g/mol) core units. The synthesized polymers were characterized to determine degree of branching (DB), molecular weight (Mw,Mn), thermal properties (Tm, ΔH) and solubility. 1HNMR integrals confirmed that all generations for all three core PEG lengths were synthesized successfully. An increased intensity for the ester bonds and no residual carboxylic acid groups from bis-MPA monomer could be seen with IR spectroscopy. The hyperbranched polymers had a decreasing Tm and ΔH with increasing generation and a degree of branching between 40 and 60 % for all PEG lengths. PEG20K-Gx-OH was used to increase the viscosity of water. PEG<sub>20K</sub>-G<sub>2</sub>-OH had the highest intrinsic viscosity. Further functionalisation was performed to introduce allyl functionalities on the PEG6K-Gx-OH system which was then thiol-ene coupled with 3-mercapto propionic acid and cross-linked into gels using UV initiated thiol-ene chemistry. Hydrophobic and bulky benzylidene groups were attached to the..</p>
----------------------------------------------------------------------
In diva2:697402 abstract is: <p>A human-centered approach when developing new support systems in vehicles has the potential to enable the driver to make safe decisions in the transition between manual and automatic control. However, careful considerations have to be taken. Not only would the design of the systems, in terms of interface be important, but also what kind of activities the systems support. The aim of this study was to identify an appropriate activity to support the cognitive processes for truck drivers, develop an interface for this activity, and evaluate it in driving situations. This was executed in three sub-studies: the Pre-study, the Design-study, and the Evaluation study.</p><p>In the Pre-study, the aim was to investigate for what kind of driver-related activity distribution and long haulage truck drivers need a driver support and interface. This was investigated via contribution from truck drivers, HMI/Ergonomics experts, as well as engineers. The activity chosen to support was detecting objects around the vehicle. However, reconsiderations were made due to constrains in the simulator. Suggested by Scania’s Vehicle Ergonomics group a holistic system was chosen; an interface approach enabling for more technologies to be included within the same interface, reducing the amount of modalities a driver can be exposed to.</p><p>The Design-study addressed the aim of designing an interface for the Holistic system with truck drivers’ cognitive workload in focus. A LED-prototype was built running along the window edges inside the cab of Shania’s Vehicle Ergonomics groups’ simulator, to create warning signal concepts. Literature findings, the LED-prototype, and the simulator were used in an iterative process to design and improve warning signal concepts, until two final concepts were created. The holistic system informs of hazards around and near the vehicle by lighting the area risky objects occurs to guide drivers’ attention and this was done either with 1) the informative display or, 2) the directional display. The Informative display conveys information of a hazard location and type, and the Directional display exclusively conveys information of the hazard location.</p><p>The Evaluation study explored how drivers were affected by, and how they perceived, the holistic interface design regarding mental workload and hazard detection. A user simulator test was designed to collect data within the areas of ‘Event detection’, ‘Workload’, ‘Driving performance’ and ‘Subjective opinion’. Fourteen professional truck drivers assessed three conditions: 1) Baseline (driving without a system), 2) the Informative display, and, 3) the Directional display, while being exposed to potential hazards. To further increase workload, a secondary task was performed at the end of each condition.</p><p>The results showed that the Informative display did not only result in more ‘Detection hits’, instances when a driver responded to a present hazard, but also significantly decreased reaction time to detect a hazard. However, in terms of acceptance, the two concepts were considered equally preferred. As the Informative display showed to be more efficient in terms of hazard detection, this should be investigated further. A holistic interface enables for more systems to be included within the same interface, reducing the amount of alarms and modalities drivers are exposed to if designed skillfully. Thus, more support systems can be included in future vehicles, without causing unnecessary distraction when applying a holistic interface approach.</p>

w='Shania' val={'c': 'Scania', 's': 'diva2:697402', 'n': 'error in original'}

corrected abstract:
<p>A human-centered approach when developing new support systems in vehicles has the potential to enable the driver to make safe decisions in the transition between manual and automatic control. However, careful considerations have to be taken. Not only would the design of the systems, in terms of interface be important, but also what kind of activities the systems support. The aim of this study was to identify an appropriate activity to support the cognitive processes for truck drivers, develop an interface for this activity, and evaluate it in driving situations. This was executed in three sub-studies: the Pre-study, the Design-study, and the Evaluation study.</p><p>In the Pre-study, the aim was to investigate for what kind of driver-related activity distribution and long haulage truck drivers need a driver support and interface. This was investigated via contribution from truck drivers, HMI/Ergonomics experts, as well as engineers. The activity chosen to support was detecting objects around the vehicle. However, reconsiderations were made due to constrains in the simulator. Suggested by Scania’s Vehicle Ergonomics group a holistic system was chosen; an interface approach enabling for more technologies to be included within the same interface, reducing the amount of modalities a driver can be exposed to.</p><p>The Design-study addressed the aim of designing an interface for the Holistic system with truck drivers’ cognitive workload in focus. A LED-prototype was built running along the window edges inside the cab of Shania’s Vehicle Ergonomics groups’ simulator, to create warning signal concepts. Literature findings, the LED-prototype, and the simulator were used in an iterative process to design and improve warning signal concepts, until two final concepts were created. The holistic system informs of hazards around and near the vehicle by lighting the area risky objects occurs to guide drivers’ attention and this was done either with 1) the informative display or, 2) the directional display. The Informative display conveys information of a hazard location and type, and the Directional display exclusively conveys information of the hazard location.</p><p>The Evaluation study explored how drivers were affected by, and how they perceived, the holistic interface design regarding mental workload and hazard detection. A user simulator test was designed to collect data within the areas of ‘Event detection’, ‘Workload’, ‘Driving performance’ and ‘Subjective opinion’. Fourteen professional truck drivers assessed three conditions: 1) Baseline (driving without a system), 2) the Informative display, and, 3) the Directional display, while being exposed to potential hazards. To further increase workload, a secondary task was performed at the end of each condition.</p><p>The results showed that the Informative display did not only result in more ‘Detection hits’, instances when a driver responded to a present hazard, but also significantly decreased reaction time to detect a hazard. However, in terms of acceptance, the two concepts were considered equally preferred. As the Informative display showed to be more efficient in terms of hazard detection, this should be investigated further. A holistic interface enables for more systems to be included within the same interface, reducing the amount of alarms and modalities drivers are exposed to if designed skillfully. Thus, more support systems can be included in future vehicles, without causing unnecessary distraction when applying a holistic interface approach.</p>
----------------------------------------------------------------------
In diva2:851832 abstract is: <p>The physiological process of new blood vessel development from pre-existing ones is referred to as angiogenesis, and it is one of the most important contributors to cancer cell survival, proliferation and metastasis. Anti-angiogenesis therapeutic strategies have therefore become popular and possess considerable potential for treating angiogenesis related diseases such as cancer and ophtalmic disorders. Biparatopic Affibody constructs which target the Vascular Endothelial Growth Receptor-2 (VEGFR2), an important regulator of angiogenesis, have previously been generated with the intent of therapeutic and imaging utility. The small size of the Affibody provides several advantages over larger proteins, such as faster tissues penetration, higher stability and possible use of alternative administrative routes, which in this case could be the use of eye drops instead of intravitreal eye injections. Here we study how varying the linker length between the two domains in the biparatopic Affibody construct affects the inhibitory ability and select a candidate to move forward with for further anti-angiogenic studies. Both human and murine VEGFR2 binding was observed for all Affibody constructs. The construct chosen for further studies demonstrated inhibition of VEGFR2 phosphorylation, angiogenesis sprout formation of HUVECs and also cell proliferation of 293/KDR cells. The results indicate that the biparatopic Affibody holds potential for in vivo therapeutic and imaging utilization. New constructs of the Affibody were therefore designed, produced and conjugated to chelators and fluorphores for future studies.</p>

w='fluorphores' val={'c': 'fluorophores', 's': 'diva2:851832', 'n': 'no full text'}
w='ophtalmic' val={'c': 'ophthalmic', 's': 'diva2:851832', 'n': 'no full text'}

corrected abstract:
<p>The physiological process of new blood vessel development from pre-existing ones is referred to as angiogenesis, and it is one of the most important contributors to cancer cell survival, proliferation and metastasis. Anti-angiogenesis therapeutic strategies have therefore become popular and possess considerable potential for treating angiogenesis related diseases such as cancer and ophthalmic disorders. Biparatopic Affibody constructs which target the Vascular Endothelial Growth Receptor-2 (VEGFR2), an important regulator of angiogenesis, have previously been generated with the intent of therapeutic and imaging utility. The small size of the Affibody provides several advantages over larger proteins, such as faster tissues penetration, higher stability and possible use of alternative administrative routes, which in this case could be the use of eye drops instead of intravitreal eye injections. Here we study how varying the linker length between the two domains in the biparatopic Affibody construct affects the inhibitory ability and select a candidate to move forward with for further anti-angiogenic studies. Both human and murine VEGFR2 binding was observed for all Affibody constructs. The construct chosen for further studies demonstrated inhibition of VEGFR2 phosphorylation, angiogenesis sprout formation of HUVECs and also cell proliferation of 293/KDR cells. The results indicate that the biparatopic Affibody holds potential for in vivo therapeutic and imaging utilization. New constructs of the Affibody were therefore designed, produced and conjugated to chelators and fluorophores for future studies.</p>
----------------------------------------------------------------------
In diva2:630125 abstract is: <p>Several studies suggest that the use of ultrasound in conjunction with microbubbles (MBs) can induce the lysis of the blood clots through acoustic cavitation and through the production of microjets and microstreaming. However, there is no accordance about the optimal ultrasound parameters that have to be considered in order to achieve the maximum thrombolytic effect, neither a clear agreement about the type of MBs that have to be used.</p><p>This project had two main goals: the design and optimization of an in-vitro set-up for the study of clot lysis within coronary arteries and its testing with ultrasound in conjunction with two different types of MBs. The MBs considered were the 3MiCRON MBs and the SonoVue MBs.</p><p>The ultrasound sequence was developed using a programmable ultrasound architecture (Verasonics, Inc) and was tested using commercially available clinical transducers.</p><p>Using the designed set-up and varying the ultrasound parameters (frequency, pulse length and pulse amplitude) it was possible to study the clot lysis effciency in conjunction with the two types of MBs. For the 3MiCRON MBs no increase in clot lysis was found with the implemented ultrasound parameters, while considering the SonoVue MBs, a 10% increase in clot lysis was found with 10ms long pulse delivered at 50V (peak-to peak value).</p><p>The obtained set-up had several aspects in common with the real situation of occluded coronary arteries, although some limitations were present and further optimizations are required.</p><p>Further work is required in order to assess if different combination of ultrasound parameters are able to lead to an increase in clot lysis when delivered with 3MiCRON or SonoVue MBs.</p>

w='effciency' val={'c': 'efficiency', 's': 'diva2:630125'}

corrected abstract:
<p>Several studies suggest that the use of ultrasound in conjunction with microbubbles (MBs) can induce the lysis of the blood clots through acoustic cavitation and through the production of microjets and microstreaming. However, there is no accordance about the optimal ultrasound parameters that have to be considered in order to achieve the maximum thrombolytic effect, neither a clear agreement about the type of MBs that have to be used.</p><p>This project had two main goals: the design and optimization of an in-vitro set-up for the study of clot lysis within coronary arteries and its testing with ultrasound in conjunction with two different types of MBs. The MBs considered were the 3MiCRON MBs and the SonoVue MBs.</p><p>The ultrasound sequence was developed using a programmable ultrasound architecture (Verasonics, Inc) and was tested using commercially available clinical transducers.</p><p>Using the designed set-up and varying the ultrasound parameters (frequency, pulse length and pulse amplitude) it was possible to study the clot lysis efficiency in conjunction with the two types of MBs. For the 3MiCRON MBs no increase in clot lysis was found with the implemented ultrasound parameters, while considering the SonoVue MBs, a 10% increase in clot lysis was found with 10ms long pulse delivered at 50V (peak-to peak value).</p><p>The obtained set-up had several aspects in common with the real situation of occluded coronary arteries, although some limitations were present and further optimizations are required.</p><p>Further work is required in order to assess if different combination of ultrasound parameters are able to lead to an increase in clot lysis when delivered with 3MiCRON or SonoVue MBs.</p>
----------------------------------------------------------------------
In diva2:1339368 abstract is: <p>Block copolymer nanoparticles (NPs) have gained great attention among researcher for various medical application mainly due to their extraordinary optical, chemical, and biological properties. The current thesis presents design of multifunctional polymeric NPs for imaging and drug delivery system (DDS) with an in-vitro study of their participation in drug release and cell viability. The NPs were synthesized using reversible addition chain fragmentation transfer (RAFT)-mediated emulsion polymerization via polymerization induce self-assembly (PISA) approach. The environment-friendly emulsion polymerization process of n-buytl acrylate (n-BA) in water is highly efficient. The process produced uniform NPs which would have control over the particle size and molecular weight of the compound. Herein we report a novel simultaneous encapsulation of camptothecin (CPT) and Nile red (NR) into poly(ethylene glycol) methyl ether methacrylate-co-N-hydroxyethyl acrylamide-b-poly n-buytlacrylate (PEGA-co-HEAA)-b-P(n-BA) during the particles formation with a small particle size of 66 nm, high conversion ~80% and encapsulation efficiency of ~50%. The In vitro drug release of the CPT from the NPs exhibited an initial burst (70-80%) within 6h. cell viability was evaluated for the NPs against RAW 264.7 cell line, which indicated the designed NPs are biocompatible and not toxic.</p><p><strong> </strong></p>

w='n-buytl' val={'c': 'n-butyl', 's': 'diva2:1339368', 'n': 'error in original'}
w='n-buytlacrylate' val={'c': 'n-butyl acrylate', 's': 'diva2:1339368', 'n': 'error in original'}
The above errors were left in the abstract - as they are in the original thesis.

no changes

----------------------------------------------------------------------
In diva2:1344757 abstract is: <p>Purpose: Magnetic Resonance Imaging (MRI) is a medical imaging technique used in radiology to produce high quality images of the anatomy and the physiological processes of the body. Cardiac gating is a triggering system used for the MRI scanner to synchronize the MRI scanner with the cardiac cycle of the patient while imaging. When applying cardiac gating, artifacts that results from small movement in the heart and blood ow are neglected. Recent MRI scanners uses Electrocardiogram (ECG) as a cardiac gating method, but with higher magnetic eld strength the ECG signal get distorted. In this thesis DUS signal will be examined as a replacement cardiac gating method for the MRI scanner. In theory the DUS signal should not be aected by the high magnetic eld strength.</p><p>Methods: Dierent sets of data were collected from three dierent subjects. The data contain a synchronized ECG and DUS signal without the eect of the MRI magnetic eld. A Filtering and peak detection algorithm were developed in MATLAB to process the DUS signal and the result was compared to the ECG signal as a reference method.</p><p>Results: The ltering algorithm showed good results in terms of being able to increase the signal to noise ration (SNR) of the signal to enable the processing phase. The peak detection algorithm was able to detect the peaks in the dierent data sets with low false positive (19 out of 24 data sets had lower FP errors then 10%) and false negative errors (17 out of 24 data sets had lower FN errors then 10%). Some data sets had low SNR even after the ltering phase, peak detection on those data sets were not functioning properly. When comparing the DUS signal to the ECG signal, an average delay was detected to be around 0.26 seconds for the forward signal and around 0.5 seconds for the backward signal.</p><p>Conclusion: The DUS signal shows promising results to be able to be used as a cardiac gating method for the MRI scanner.</p>

w='eect' val={'c': 'effect', 's': ['diva2:1344757', 'diva2:1229284'], 'n': 'missing ligature'}
w='ltering' val={'c': 'filtering', 's': 'diva2:1344757', 'n': 'missing ligature'}
w='aect' val={'c': 'affect', 's': 'diva2:1344757', 'n': 'missing ligature'}
w='Dierent' val={'c': 'Different', 's': ['diva2:1344757', 'diva2:949728'], 'n': 'missing ligature'}
There were also other missing ligatures for "ff", "fl", and "fi".
Also missing was the bold face of the headings.

corrected abstract:
<p><strong>Purpose:</strong> Magnetic Resonance Imaging (MRI) is a medical imaging technique used in radiology to produce high quality images of the anatomy and the physiological processes of the body. Cardiac gating is a triggering system used for the MRI scanner to synchronize the MRI scanner with the cardiac cycle of the patient while imaging. When applying cardiac gating, artifacts that results from small movement in the heart and bloodflow are neglected. Recent MRI scanners uses Electrocardiogram (ECG) as a cardiac gating method, but with higher magnetic field strength the ECG signal get distorted. In this thesis DUS signal will be examined as a replacement cardiac gating method for the MRI scanner. In theory the DUS signal should not be aFfected by the high magnetic field strength.</p><p><strong>Methods:</strong> Different sets of data were collected from three dierent subjects. The data contain a synchronized ECG and DUS signal without the effect of the MRI magnetic field. A Filtering and peak detection algorithm were developed in MATLAB to process the DUS signal and the result was compared to the ECG signal as a reference method.</p><p><strong>Results:</strong> The filtering algorithm showed good results in terms of being able to increase the signal to noise ration (SNR) of the signal to enable the processing phase. The peak detection algorithm was able to detect the peaks in the different data sets with low false positive (19 out of 24 data sets had lower FP errors then 10%) and false negative errors (17 out of 24 data sets had lower FN errors then 10%). Some data sets had low SNR even after the filtering phase, peak detection on those data sets were not functioning properly. When comparing the DUS signal to the ECG signal, an average delay was detected to be around 0.26 seconds for the forward signal and around 0.5 seconds for the backward signal.</p><p><strong>Conclusion:</strong> The DUS signal shows promising results to be able to be used as a cardiac gating method for the MRI scanner.</p>
----------------------------------------------------------------------
In diva2:1711725 abstract is: <p>DNA microscopy is a new field using the high information storage capacity of DNA to create and store information regarding the locations of structures or molecules currently visualized by highly resolute methods based on light microscopy, the increased resolution often facilitated by specialized use of fluorophores. Using DNA strands that can concatenate the unique molecular identifiers of two separate DNA strands, the generation of a mathematical network of neighbours would be possible. In turn, this would allow an image reconstruction of the individual molecules locations in space. The method that this project worked towards would use affinity proteins, specifically nanobodies, chemically conjugated to DNA as a way of anchoring DNA to proteins of interest. Nanobodies are a variable region fragment of heavy-chain only antibodies, with the ones used having two added tags, one for purification and on for tag-removal. Methods for producing tagged precursor nanobodies from bacterial stocks were performed and evaluated, as well as the parameters required for successful protease digestion into a mature nanobodies. A reducing agent, DTT, was shown to be crucial for efficient digestion. To establish the orientation of the tags on the nanobody and their possible effect on target binding, a homology model of the fusion-tagged nanboodies was also created. The model showed that the tags were close to the target binding regions of the nanobody, and could interfere with binding if not removed by protease digestion. An ELISA for establishing the activity of produced nanobodies was performed and evaluated, and showed the importance of controlling for leftover reactants when performing a chemical conjugation. A complete workflow for producing and assessing protein-DNA conjugates was performed, evaluated and showed possible successfully conjugated products however alternative methods are needed for confirmation.</p>

w='nanboodies' val={'c': 'nanobodies', 's': 'diva2:1711725', 'n': 'no full text'}

corrected abstract:
<p>DNA microscopy is a new field using the high information storage capacity of DNA to create and store information regarding the locations of structures or molecules currently visualized by highly resolute methods based on light microscopy, the increased resolution often facilitated by specialized use of fluorophores. Using DNA strands that can concatenate the unique molecular identifiers of two separate DNA strands, the generation of a mathematical network of neighbours would be possible. In turn, this would allow an image reconstruction of the individual molecules locations in space. The method that this project worked towards would use affinity proteins, specifically nanobodies, chemically conjugated to DNA as a way of anchoring DNA to proteins of interest. Nanobodies are a variable region fragment of heavy-chain only antibodies, with the ones used having two added tags, one for purification and on for tag-removal. Methods for producing tagged precursor nanobodies from bacterial stocks were performed and evaluated, as well as the parameters required for successful protease digestion into a mature nanobodies. A reducing agent, DTT, was shown to be crucial for efficient digestion. To establish the orientation of the tags on the nanobody and their possible effect on target binding, a homology model of the fusion-tagged nanobodies was also created. The model showed that the tags were close to the target binding regions of the nanobody, and could interfere with binding if not removed by protease digestion. An ELISA for establishing the activity of produced nanobodies was performed and evaluated, and showed the importance of controlling for leftover reactants when performing a chemical conjugation. A complete workflow for producing and assessing protein-DNA conjugates was performed, evaluated and showed possible successfully conjugated products however alternative methods are needed for confirmation.</p>
----------------------------------------------------------------------
In diva2:854001 abstract is: <p>The purpose of this study was to develop and evaluate a lateral flow immunoassay, as a point-of-care prototype for diagnosis of contagious bovine pleuropneumonia (CBPP). The assay was based on a panel of recombinant proteins spotted on a nitrocellulose membrane. The study included optimization of assay components, such as running buffer and protein microarray layout. The purpose was to obtain a clear discriminatory capacity between CBPP positive and CBPP negative sera, by developing and evaluating a lateral flow immunoassay that could be used on-site.</p><p>The discriminatory capacity between CBPP positive sera and CBPP negative sera in the lateral flow assay was statistically significant with p-values ≤0.05 for recombinant proteins to be printed in microarray spots both individually as well as proteins printed as mixtures. The sensitivity of the assay was 64% and the specificity 100%, which was comparable to current diagnostic methods for CBPP. From these rsults, four combinations of recombinant proteins were selected to print as a microarray, which was based on p-valuesas well as sera coverage. As the purpose of this study was to develop a prototype for on-site usage, a fieldtrip to Nairobi, Kenya, facilitated by the Swedish International Development Cooperation Agency, SIDA, was made to test the prototpyes as well as obtain deeper knowledge of user area. However, the lateral flow immunoassay did not deliver reproducible and stable results during the field trip to Kenya.</p><p></p><p></p>

w='prototpyes' val={'c': 'prototypes', 's': 'diva2:854001', 'n': 'no full text'}
w='rsults' val={'c': 'results', 's': 'diva2:854001', 'n': 'no full text'}

corrected abstract:
<p>The purpose of this study was to develop and evaluate a lateral flow immunoassay, as a point-of-care prototype for diagnosis of contagious bovine pleuropneumonia (CBPP). The assay was based on a panel of recombinant proteins spotted on a nitrocellulose membrane. The study included optimization of assay components, such as running buffer and protein microarray layout. The purpose was to obtain a clear discriminatory capacity between CBPP positive and CBPP negative sera, by developing and evaluating a lateral flow immunoassay that could be used on-site.</p><p>The discriminatory capacity between CBPP positive sera and CBPP negative sera in the lateral flow assay was statistically significant with p-values ≤0.05 for recombinant proteins to be printed in microarray spots both individually as well as proteins printed as mixtures. The sensitivity of the assay was 64% and the specificity 100%, which was comparable to current diagnostic methods for CBPP. From these results, four combinations of recombinant proteins were selected to print as a microarray, which was based on p-values as well as sera coverage. As the purpose of this study was to develop a prototype for on-site usage, a fieldtrip to Nairobi, Kenya, facilitated by the Swedish International Development Cooperation Agency, SIDA, was made to test the prototypes as well as obtain deeper knowledge of user area. However, the lateral flow immunoassay did not deliver reproducible and stable results during the field trip to Kenya.</p>
----------------------------------------------------------------------
In diva2:1586242 abstract is: <p>Transcription factor EB (TFEB) has been identified as a master regulator of the lysosomalautophagy pathway and as such, an important inducer of cellular clearance. It has therefore emerged as a promising target for diseases associated with reduced clearance or accumulation of toxic aggregates, including multiple neurodegenerative diseases. In addition, as a downstream target of the nutrient-sensing complex mTORC1, activating TFEB has been proposed as a potential mean to mimic the effects of calorie restriction and thereby promote longevity. To enable the identification of compounds able to activate TFEB, this project set out to develop a cell-based assay that can be used in a phenotypic high-throughput screen. The assay was based on detection of TFEB subcellular location, which is closely linked to its activation, enabled by expression of a GFP-tagged TFEB. NSC-34, RPE-,1 and HeLa cells were transfected with a construct encoding the fusion protein, but only NSC-34 and RPE-1 cells expressing the fusion protein at sufficient quantities were successfully developed. It could be demonstrated that NSC-34 GFP-TFEB clones responded as expected when treated with compounds known to induce TFEB nuclear translocation. The project however failed to convincingly show that these clones are responsive to amino acid starvation, which is known to cause nuclear translocation of TFEB. Regarding the stable RPE-1 GFP-TFEB cell line, its suitability as a model system to monitor TFEB nuclear translocation remains to be determined.</p>

w='lysosomalautophagy' val={'c': 'lysosomal autophagy', 's': 'diva2:1586242', 'n': 'no full text'}

corrected abstract:
<p>Transcription factor EB (TFEB) has been identified as a master regulator of the lysosomal autophagy pathway and as such, an important inducer of cellular clearance. It has therefore emerged as a promising target for diseases associated with reduced clearance or accumulation of toxic aggregates, including multiple neurodegenerative diseases. In addition, as a downstream target of the nutrient-sensing complex mTORC1, activating TFEB has been proposed as a potential mean to mimic the effects of calorie restriction and thereby promote longevity. To enable the identification of compounds able to activate TFEB, this project set out to develop a cell-based assay that can be used in a phenotypic high-throughput screen. The assay was based on detection of TFEB subcellular location, which is closely linked to its activation, enabled by expression of a GFP-tagged TFEB. NSC-34, RPE-,1 and HeLa cells were transfected with a construct encoding the fusion protein, but only NSC-34 and RPE-1 cells expressing the fusion protein at sufficient quantities were successfully developed. It could be demonstrated that NSC-34 GFP-TFEB clones responded as expected when treated with compounds known to induce TFEB nuclear translocation. The project however failed to convincingly show that these clones are responsive to amino acid starvation, which is known to cause nuclear translocation of TFEB. Regarding the stable RPE-1 GFP-TFEB cell line, its suitability as a model system to monitor TFEB nuclear translocation remains to be determined.</p>
----------------------------------------------------------------------
In diva2:744722 abstract is: <p>Peptide aggregation ccours during the progression of many neurodegenerative diseases such as Alzhaimer's disease. For this reason, administration of aggregation inhibitors has been suggested asa viable therapeutic strategy for the prevention and treatment of such diseases. The goal of this project was to develop a system for using flow cytometry in order to select for molecules that inhibit aggregation of a specific aggregation-prone molecule. The system utilizes the fact that enhanced green fluorescent protein (EGFP) can be used as s reporter molecule for determining whether or not a given peptide is aggregating; when EGFP is expressed fused with an aggregating peptide EGFP cannot fold correctly and thus will not fluoresce, but if aggregation is inhibited EGFP can fold correctly and will fluoresce. Thus by co-expressing an aggregation-prone peptide fused with EGFP and a potential aggregation inhibitor, the effectiveness of the inhibitor will be related to the intensity of EGFP fluorescence.</p><p>For the initial development and evaluation, the aggregation prone peptide used was amyloid-ß 42 (Aß42), a peptide whose aggregation is associated with Alzheimer's disease. For the aggregation inhibitor, an Affibody nolecule known to inhibit aggregation of Aß42 was used. It was shown that co-expression of the aggregation inhibitor and Aß42 resulted in an increased EGFP  fluorescence as compared to co-expression of a negative control and Aß42. This means that the system was capable of distinguishing the inhibitor from the non-inhibitor, indication that the system works in principle.</p>

w='ccours' val={'c': 'occurs', 's': 'diva2:744722', 'n': 'no full text'}
w='nolecule' val={'c': 'molecule', 's': 'diva2:744722', 'n': 'no full text'}
w='Alzhaimer' val={'c': 'Alzheimer', 's': 'diva2:744722'}

corrected abstract:
<p>Peptide aggregation occurs during the progression of many neurodegenerative diseases such as Alzheimer's disease. For this reason, administration of aggregation inhibitors has been suggested asa viable therapeutic strategy for the prevention and treatment of such diseases. The goal of this project was to develop a system for using flow cytometry in order to select for molecules that inhibit aggregation of a specific aggregation-prone molecule. The system utilizes the fact that enhanced green fluorescent protein (EGFP) can be used as s reporter molecule for determining whether or not a given peptide is aggregating; when EGFP is expressed fused with an aggregating peptide EGFP cannot fold correctly and thus will not fluoresce, but if aggregation is inhibited EGFP can fold correctly and will fluoresce. Thus by co-expressing an aggregation-prone peptide fused with EGFP and a potential aggregation inhibitor, the effectiveness of the inhibitor will be related to the intensity of EGFP fluorescence.</p><p>For the initial development and evaluation, the aggregation prone peptide used was amyloid-ß 42 (Aß42), a peptide whose aggregation is associated with Alzheimer's disease. For the aggregation inhibitor, an Affibody molecule known to inhibit aggregation of Aß42 was used. It was shown that co-expression of the aggregation inhibitor and Aß42 resulted in an increased EGFP  fluorescence as compared to co-expression of a negative control and Aß42. This means that the system was capable of distinguishing the inhibitor from the non-inhibitor, indication that the system works in principle.</p>
----------------------------------------------------------------------
In diva2:1602704 abstract is: <p>A 10-color prototype GeneXpert assay based on distinct mRNA expression profiles in ex-vivo antigen stimulated whole blood has been developed at Cepheid for diagnosis of M. Tuberculosis infection and separation of active and latent tuberculosis. An original larger set of mRNA biomarkers has been downselected to a set of 9 biomarkers constituting the mRNA signature in the 10-color assay. The downselection of biomarkers was based on probe screening and optimization studies performed with Cepheid’s “in cartridge” qPCR technology. The 10-color assay was evaluated for diagnostic performance by analyzing 380 banked antigen stimulated clinical samples classified as M. Tuberculosis infected, active tuberculosis, latent tuberculosis and not infected. The ability of the mRNA signature to correctly classify and separate active and latent tuberculosis was determined by implementing ROC curves using delta Ctvalues as test variables and resulted in an optimal AUC value of 0.784 indicating ability to separate active and latent tuberculosis.</p>

w='Ctvalues' val={'c': 'Ct-values', 's': 'diva2:1602704', 'n': 'correct in original'}
Included the hyphen in down-selected and down-selection.

corrected abstract:
<p>A 10-color prototype GeneXpert assay based on distinct mRNA expression profiles in ex-vivo antigen stimulated whole blood has been developed at Cepheid for diagnosis of M. Tuberculosis infection and separation of active and latent tuberculosis. An original larger set of mRNA biomarkers has been down-selected to a set of 9 biomarkers constituting the mRNA signature in the 10-color assay. The down-selection of biomarkers was based on probe screening and optimization studies performed with Cepheid’s “in cartridge” qPCR technology. The 10-color assay was evaluated for diagnostic performance by analyzing 380 banked antigen stimulated clinical samples classified as M. Tuberculosis infected, active tuberculosis, latent tuberculosis and not infected. The ability of the mRNA signature to correctly classify and separate active and latent tuberculosis was determined by implementing ROC curves using delta Ct-values as test variables and resulted in an optimal AUC value of 0.784 indicating ability to separate active and latent tuberculosis.</p>
----------------------------------------------------------------------
In diva2:1528289 abstract is: <p>The antibody Immunoglobulin G (IgG) main function is to protect and prevent the body from infections, and it is normally found in human serum. This study is about IgG glycosylation, which is associated with different types of diseases such as neurological diseases, cancers and immunodeficiency etc. This study attempts to optimize IgG glycopeptide enrichment in a 100 μL micropipette tip set up, and to separate the enriched glycopeptides using capillary electrophoresis (CE). Matrix-assisted laser desorption/ionization – mass spectrometry (MALDI-MS) was used for data acquisition and glycopeptide profiling. </p><p>In this study, loading solutions with different combinations of acetonitrile (ACN) and trifluoroacetic acid (TFA), together with various precondition and sample preparation procedures were evaluated on IgG digest samples. Best enrichment performance, particularly regarding the selectivity, was achieved using the parameters as follows: loading solution of 83% ACN/16% H2O/1% TFA, sample solution in H2O containing 83% ACN, using a 100 μL micropipette tip packed with 1 mg cotton wool. A re-enrichment step was carried out on enriched glycopeptide samples, and improved selectivity of glycopeptides could be observed. Enriched glycopeptides could be separated into three major groups by CE using an acidic background electrolyte of 50 mM formic acid and 50 mM acetic acid, pH 2.5. </p>

w='ACN/16' val={'c': '83% ACN/16% H2O/1% TFA', 's': 'diva2:1528289'}

corrected abstract:
<p>The antibody Immunoglobulin G (IgG) main function is to protect and prevent the body from infections, and it is normally found in human serum. This study is about IgG glycosylation, which is associated with different types of diseases such as neurological diseases, cancers and immunodeficiency etc. This study attempts to optimize IgG glycopeptide enrichment in a 100 μL micropipette tip set up, and to separate the enriched glycopeptides using capillary electrophoresis (CE). Matrix-assisted laser desorption/ionization – mass spectrometry (MALDI-MS) was used for data acquisition and glycopeptide profiling.</p><p>In this study, loading solutions with different combinations of acetonitrile (ACN) and trifluoroacetic acid (TFA), together with various precondition and sample preparation procedures were evaluated on IgG digest samples. Best enrichment performance, particularly regarding the selectivity, was achieved using the parameters as follows: loading solution of 83% ACN/16% H<sub>2</sub>O/1% TFA, sample solution in H<sub>2</sub>O containing 83% ACN, using a 100 μL micropipette tip packed with 1 mg cotton wool. A re-enrichment step was carried out on enriched glycopeptide samples, and improved selectivity of glycopeptides could be observed. Enriched glycopeptides could be separated into three major groups by CE using an acidic background electrolyte of 50 mM formic acid and 50 mM acetic acid, pH 2.5.</p>
----------------------------------------------------------------------
In diva2:779089 abstract is: <p>Thermally expandable microspheres are spherical particles around 5-­‐40 µm in size, consisting of a polymeric shell in which a blowing agent has been encapsulated. The microspheres are expanded upon heating, resulting in a particularly low density. Microspheres are therefore suitable to use as light weight filler or as foaming agent.</p><p>AkzoNobel is world leading in the production of expandable microspheres, which are commercialised under the name Expancel. Sustainability is a great focus at AkzoNobel and two issues that AkzoNobel works with today is to develop products free from chlorine and Me1. The aim with this thesis has been to investigate whether it is possible to produce microspheres free from these chemicals and to see if they can be a more sustainable alternative to one of the existing Expancel grades.</p><p>In this study, the microspheres have been produced through free radical suspension polymerisation and analysed by measuring mainly the particle size and expansion properties. The polymeric shell was composed of the monomers acrylonitrile, methacrylonitrile, and methyl acrylate. The main focus has been to evaluate the silica-­‐based stabilisation system, which stabilise the monomer droplets during the suspension polymerisation. The stabilisation is possible due to the formation of silica flocs that is adsorbed on the surface of the droplets. It has been investigating how different parameters, e.g. amount of stabiliser or mixing procedure, affects the formation of silica flocs and the stabilisation of monomer droplets.</p><p>For the silica-­‐based system, it was found that the mixing order, stirring rate, and amount of stabilisers affect the formation of flocs. It was also seen that the amount of stabiliser affect the stabilisation of droplets, and that some stabilisers is more significant than others.</p><p>Microspheres without chlorine and Me1 have successfully been produced in laboratory scale (50 mL and 1 L). The expansion and size of the microspheres produced in this study was relatively similar to one of the existing Expancel grades. However, the reproducibility of polymerisations in 1 litre reactors has been poor.</p>

w='silica-\xad‐based' val={'c': 'silica-based', 's': 'diva2:779089'}
Aslo fixed the similar dash between 5 and 40.

corrected abstract:
<p>Thermally expandable microspheres are spherical particles around 5-40 µm in size, consisting of a polymeric shell in which a blowing agent has been encapsulated. The microspheres are expanded upon heating, resulting in a particularly low density. Microspheres are therefore suitable to use as light weight filler or as foaming agent.</p><p>AkzoNobel is world leading in the production of expandable microspheres, which are commercialised under the name Expancel. Sustainability is a great focus at AkzoNobel and two issues that AkzoNobel works with today is to develop products free from chlorine and Me1. The aim with this thesis has been to investigate whether it is possible to produce microspheres free from these chemicals and to see if they can be a more sustainable alternative to one of the existing Expancel grades.</p><p>In this study, the microspheres have been produced through free radical suspension polymerisation and analysed by measuring mainly the particle size and expansion properties. The polymeric shell was composed of the monomers acrylonitrile, methacrylonitrile, and methyl acrylate. The main focus has been to evaluate the silica-based stabilisation system, which stabilise the monomer droplets during the suspension polymerisation. The stabilisation is possible due to the formation of silica flocs that is adsorbed on the surface of the droplets. It has been investigating how different parameters, e.g. amount of stabiliser or mixing procedure, affects the formation of silica flocs and the stabilisation of monomer droplets.</p><p>For the silica-based system, it was found that the mixing order, stirring rate, and amount of stabilisers affect the formation of flocs. It was also seen that the amount of stabiliser affect the stabilisation of droplets, and that some stabilisers is more significant than others.</p><p>Microspheres without chlorine and Me1 have successfully been produced in laboratory scale (50 mL and 1 L). The expansion and size of the microspheres produced in this study was relatively similar to one of the existing Expancel grades. However, the reproducibility of polymerisations in 1 litre reactors has been poor.</p>
----------------------------------------------------------------------
In diva2:804984 abstract is: <p>Contagious bovine pleuropneumonia is a severe lung disease effecting cattle that is enzootic to Sub-Saharan Africa. No satisfactory vaccine or treatment is yet available for treatment of this infectious disease caused by the bacterial pathogen <em>Mycoplasma mycoides</em> subsp. <em>mycoides (Mmm)</em>, which leads to its continuous spread over the continent with large socio-economic consequences as a result. By developing a diagnostic tool that is independent from laboratory equipment progress in eradicating this disease in the developing world can be made. In this investigation, construction of a lateral flow protein microarray with recombinant <em>Mmm</em> surface proteins was conducted and each step in the lateral flow assay was evaluated. A field trip to International Livestock Research Institue in Kenya was made in order to evaluate the prototype. The point-of-care prototype could discriminate between infected a non-infected cattle, with a sensitivity of 100% and specificity of 65%.</p>

w='Institue' val={'c': 'Institute', 's': 'diva2:804984', 'n': 'no full text'}

corrected abstract:
<p>Contagious bovine pleuropneumonia is a severe lung disease effecting cattle that is enzootic to Sub-Saharan Africa. No satisfactory vaccine or treatment is yet available for treatment of this infectious disease caused by the bacterial pathogen <em>Mycoplasma mycoides</em> subsp. <em>mycoides (Mmm)</em>, which leads to its continuous spread over the continent with large socio-economic consequences as a result. By developing a diagnostic tool that is independent from laboratory equipment progress in eradicating this disease in the developing world can be made. In this investigation, construction of a lateral flow protein microarray with recombinant <em>Mmm</em> surface proteins was conducted and each step in the lateral flow assay was evaluated. A field trip to International Livestock Research Institute in Kenya was made in order to evaluate the prototype. The point-of-care prototype could discriminate between infected a non-infected cattle, with a sensitivity of 100% and specificity of 65%.</p>
----------------------------------------------------------------------
In diva2:744718 abstract is: <p>Plasma and serum are believed to at some point in time contain all proteins of the human body, possibly containing information about every disease state. This makes plasma and serum widely utilized for biomarker discovery. Biomarker discovery is at the momen performed either by identifying proteins from the protein mass (i.e. amino acid sequence) or with affinity reagents or by employing a combination of these two. In the group of Biobank Profiling, Royal Institute of Technology, KTH, a Standard Assay (STA) for affinity based biomarker discovery with Suspension Bead Array has been applied. This STA is based on direct labelling of plasma samples, a method with potentially intrinsic challengens in distinguishing the target-protein signal from noise and background of off-target binding. To circumvent or reduce these problems, an idea based on a capture-recapture of the target protein, entitled Dual Capture Assay (DCA). This concept could enrich the arget protein and reduce the complexity of the sample for the second binding event. In thsi master thesis project, the DCA concept has been investigated in the aspects of elution, incubation times as the reported Median Fluorescent Intensity (MFI). When evaluating DCS, a system mimicking a biomarker discovery setting has been applied by spike-in of a non-human protein into human plasma. These experiments show that DCA create a less complex environment for the second capture, implied by a higher Signal to Noise ratio than for the STA. DCA was then applied in a disease study on Duchenne Muscular Dystrophy samples. In this application, the STA reports higher MFI signals than the DCA but the both assays show the same trend in separation of cases vs. controls, supporting previously achieved data. DCA also shows a reduction to background signals for 7 out of 10 positive controls in human blood while the STA yields high signals for all 10. Together with the other datapresented in this thesis, DCA appears to be a possible approach to complement STA because of the higher Signal to Noise ratios. With further optimizing and efforts in providing more rigid proof of the concept, DCA could probably be implemented in the standard work flow of this group, providing more reliable data due to the capture- recapture of the target protein.  </p>

w='challengens' val={'c': 'challenges', 's': 'diva2:744718', 'n': 'no full text'}
w='momen' val={'c': 'moment', 's': 'diva2:744718', 'n': 'no full text'}
w='thsi' val={'c': 'this', 's': 'diva2:744718', 'n': 'no full text'}
w='arget' val={'c': 'target', 's': 'diva2:744718', 'n': 'no full text'}
w='DCS' val={'c': 'DCA', 's': 'diva2:744718', 'n': 'no full text'}

corrected abstract:
<p>Plasma and serum are believed to at some point in time contain all proteins of the human body, possibly containing information about every disease state. This makes plasma and serum widely utilized for biomarker discovery. Biomarker discovery is at the moment performed either by identifying proteins from the protein mass (i.e. amino acid sequence) or with affinity reagents or by employing a combination of these two. In the group of Biobank Profiling, Royal Institute of Technology, KTH, a Standard Assay (STA) for affinity based biomarker discovery with Suspension Bead Array has been applied. This STA is based on direct labelling of plasma samples, a method with potentially intrinsic challenges in distinguishing the target-protein signal from noise and background of off-target binding. To circumvent or reduce these problems, an idea based on a capture-recapture of the target protein, entitled Dual Capture Assay (DCA). This concept could enrich the target protein and reduce the complexity of the sample for the second binding event. In this master thesis project, the DCA concept has been investigated in the aspects of elution, incubation times as the reported Median Fluorescent Intensity (MFI). When evaluating DCA, a system mimicking a biomarker discovery setting has been applied by spike-in of a non-human protein into human plasma. These experiments show that DCA create a less complex environment for the second capture, implied by a higher Signal to Noise ratio than for the STA. DCA was then applied in a disease study on Duchenne Muscular Dystrophy samples. In this application, the STA reports higher MFI signals than the DCA but the both assays show the same trend in separation of cases vs. controls, supporting previously achieved data. DCA also shows a reduction to background signals for 7 out of 10 positive controls in human blood while the STA yields high signals for all 10. Together with the other data presented in this thesis, DCA appears to be a possible approach to complement STA because of the higher Signal to Noise ratios. With further optimizing and efforts in providing more rigid proof of the concept, DCA could probably be implemented in the standard work flow of this group, providing more reliable data due to the capture- recapture of the target protein.</p>
----------------------------------------------------------------------
In diva2:854047 abstract is: <p>Immunoassays are widely used for the detection and quantification of proteins in medical diagnostics. In Medicine, sensitive and specific detection of a causative agent is crucial concerning early diagnosis of diseases and monitoring the disease status. Microfluidics platform in combination with immunoassay has beome demanding for point of care diagnostics as it provides faster, high throughput analysis with higher sensitivity and lowered cost per analysis. Recently, sensitivity has been enhanced by single molecule immunoassays also called digital ELISA, enable detection of low abundance proteins based on single molecule counting.</p><p>In this thesis work, micro devices were rabricated using UV cross-linking polymer for digital ELISA application. The digital ELISA was first developed in the bead based format. Bead based assay was developed by using streptavidin coated magnetic beads and protein analytes were capture. For detection, dual labeled gold nanoparticles aiming colorimetric detection through gold silver enhancement were used. LOD of IL-2 antigen concentration was demonstrated 0.008ug/ml. Micro devices consisting of array of microwells were fabricated by using a UV cross-linkable polymer. The developed bead based ELISA was integrated into digital ELISA format.</p>

w='beome' val={'c': 'become', 's': 'diva2:854047'}
w='rabricated' val={'c': 'fabricated', 's': 'diva2:854047', 'n': 'no full text'}

corrected abstract:
<p>Immunoassays are widely used for the detection and quantification of proteins in medical diagnostics. In Medicine, sensitive and specific detection of a causative agent is crucial concerning early diagnosis of diseases and monitoring the disease status. Microfluidics platform in combination with immunoassay has become demanding for point of care diagnostics as it provides faster, high throughput analysis with higher sensitivity and lowered cost per analysis. Recently, sensitivity has been enhanced by single molecule immunoassays also called digital ELISA, enable detection of low abundance proteins based on single molecule counting.</p><p>In this thesis work, micro devices were fabricated using UV cross-linking polymer for digital ELISA application. The digital ELISA was first developed in the bead based format. Bead based assay was developed by using streptavidin coated magnetic beads and protein analytes were capture. For detection, dual labeled gold nanoparticles aiming colorimetric detection through gold silver enhancement were used. LOD of IL-2 antigen concentration was demonstrated 0.008ug/ml. Micro devices consisting of array of microwells were fabricated by using a UV cross-linkable polymer. The developed bead based ELISA was integrated into digital ELISA format.</p>
----------------------------------------------------------------------
In diva2:1873319 abstract is: <p>Dual engagers are a class of protein therapeutics that contain two binding arms, one targeting atumor associated antigen (TAA) on a tumor cell and one arm binding to an activating receptor on an immune cell. Via simultaneous binding to both cell types, a dual engager can activate acellular immune response towards the tumor cell. The primary objective of this project was to investigate the activity of affibody-based dual engagers targeting the breast cancer-related receptors HER2, HER3, and B7-H3, as well as CD16a on NK cells, with a focus on the effect of affinity, epitope preference, binding valency, biparatopic and bispecific target binding. Twelve dual engager constructs of different designs were produced in <em>Escherichia coli</em>, and purified to allow for a comprehensive analysis. Through SDS-PAGE and mass spectrometry analyses, successful expression of proteins with reasonable purity and correct molecular weights after purification was confirmed. Subsequent binding experiments using surface plasmon resonance (SPR) technology experiments confirmed binding to the intended targets and affirmed the ability of the dual engagers to simultaneously bind to both TAAs and NK cell targets. Activity analyses using a breast cancer cell line (SK-BR-3) and an engineered Jurkat- CD16a luciferase reporter cell line provided insights into the functionality of the dual engagers in activating immune cells. These analyses showed that the majority of the investigated dual engagers were active, although with varying strengths. The results suggest that compared to dual engagers containing monovalent TAA binding units, head-to-tail homodimers of target binding units did not confer benefits over single units, suggesting that monomeric forms may offer greater advantages. Moving forward, additional breast cancer cell lines should be tested, as TAA expression may vary significantly. Further, some unexpected issues concerning the storage stability of the produced dual engager constructs should be addressed. Future investigations should focus on validating these findings through further empirical analyses in more complex systems, including<em> in vitro</em> cell killing experiments and<em> in vivo</em> studies. The findings in this project contribute to the understanding and optimization of dual engagers for potential advancements in cancer treatment.</p>

w='JurkatCD16a' val={'c': 'Jurkat CD16a', 's': 'diva2:1873319', 'n': 'no full text'}

corrected abstract:
<p>Dual engagers are a class of protein therapeutics that contain two binding arms, one targeting a tumor associated antigen (TAA) on a tumor cell and one arm binding to an activating receptor on an immune cell. Via simultaneous binding to both cell types, a dual engager can activate acellular immune response towards the tumor cell. The primary objective of this project was to investigate the activity of affibody-based dual engagers targeting the breast cancer-related receptors HER2, HER3, and B7-H3, as well as CD16a on NK cells, with a focus on the effect of affinity, epitope preference, binding valency, biparatopic and bispecific target binding. Twelve dual engager constructs of different designs were produced in <em>Escherichia coli</em>, and purified to allow for a comprehensive analysis. Through SDS-PAGE and mass spectrometry analyses, successful expression of proteins with reasonable purity and correct molecular weights after purification was confirmed. Subsequent binding experiments using surface plasmon resonance (SPR) technology experiments confirmed binding to the intended targets and affirmed the ability of the dual engagers to simultaneously bind to both TAAs and NK cell targets. Activity analyses using a breast cancer cell line (SK-BR-3) and an engineered Jurkat- CD16a luciferase reporter cell line provided insights into the functionality of the dual engagers in activating immune cells. These analyses showed that the majority of the investigated dual engagers were active, although with varying strengths. The results suggest that compared to dual engagers containing monovalent TAA binding units, head-to-tail homodimers of target binding units did not confer benefits over single units, suggesting that monomeric forms may offer greater advantages. Moving forward, additional breast cancer cell lines should be tested, as TAA expression may vary significantly. Further, some unexpected issues concerning the storage stability of the produced dual engager constructs should be addressed. Future investigations should focus on validating these findings through further empirical analyses in more complex systems, including <em>in vitro</em> cell killing experiments and<em> in vivo</em> studies. The findings in this project contribute to the understanding and optimization of dual engagers for potential advancements in cancer treatment.</p>
----------------------------------------------------------------------
In diva2:744699 abstract is: <p>Diabetes is a strong risk factor for premature and severe stroke. The dipeptidyl peptidase-4 (DPP-4) enzyme inhibitor Linagliptin is a drug for the treatment of type 2 diabetes (T2D) that may also have neurprotective effects. The aim of this study was to understand whether the systemic activation of Linagliptin is efficacious against stroke via stimulation of neuroprotection. This was done y using a diabetic animal model, a drug administration paradigm and a dose that mimic a diabetic patient on chronic Linagliptin therapy. We used glimepiride, which is an insulin secretagogue sulfonylurea, as a glycemic  comparator.</p><p>A total of 44 male C57B1 mice were divided in two sets of experiments. In the first sets of experiments, 21 eight-week-old mice were exposed to high-fat diet (HFD) for a total of 32 weeks so they would develop (T2D). In teh second set of experiments, 23 ten-week-old normal diet-fed mice were treated similarly to the first experiment. In both sets of experimentm, the animals were given 10mg/kg/bw Linagliptin per day, or 2 mg/kg/bw glimepiride per day, or vehicle for 4 weeks. They were then subjected to stroke by transient middle cerebral artery occlustion (MCAo) and the drug treatment continued for 3 weeks until sacrifice.</p><p>The severity of ischemic damage was measured by evaluation of stroke volume and by stereological counting of neurons in the striatum and cortex. We show a statistically significant anti-stroke efficacy in both T2D and non-diabetic mice on Linagliptin treatment. Our results provide an impetus for the further development of incretin-based drugs for the prevention and treatment of stroke in both diabetic and non-diabetic high-risk patients.</p>

w='experimentm' val={'c': 'experiment', 's': 'diva2:744699', 'n': 'no full text'}
w='neurprotective' val={'c': 'neuroprotective', 's': 'diva2:744699', 'n': 'no full text'}
w='occlustion' val={'c': 'occlusion', 's': 'diva2:744699', 'n': 'no full text'}
w='teh' val={'c': 'the', 's': ['diva2:744699', 'diva2:801776']}

corrected abstract:
<p>Diabetes is a strong risk factor for premature and severe stroke. The dipeptidyl peptidase-4 (DPP-4) enzyme inhibitor Linagliptin is a drug for the treatment of type 2 diabetes (T2D) that may also have neuroprotective effects. The aim of this study was to understand whether the systemic activation of Linagliptin is efficacious against stroke via stimulation of neuroprotection. This was done y using a diabetic animal model, a drug administration paradigm and a dose that mimic a diabetic patient on chronic Linagliptin therapy. We used glimepiride, which is an insulin secretagogue sulfonylurea, as a glycemic  comparator.</p><p>A total of 44 male C57B1 mice were divided in two sets of experiments. In the first sets of experiments, 21 eight-week-old mice were exposed to high-fat diet (HFD) for a total of 32 weeks so they would develop (T2D). In the second set of experiments, 23 ten-week-old normal diet-fed mice were treated similarly to the first experiment. In both sets of experiment, the animals were given 10mg/kg/bw Linagliptin per day, or 2 mg/kg/bw glimepiride per day, or vehicle for 4 weeks. They were then subjected to stroke by transient middle cerebral artery occlusion (MCAo) and the drug treatment continued for 3 weeks until sacrifice.</p><p>The severity of ischemic damage was measured by evaluation of stroke volume and by stereological counting of neurons in the striatum and cortex. We show a statistically significant anti-stroke efficacy in both T2D and non-diabetic mice on Linagliptin treatment. Our results provide an impetus for the further development of incretin-based drugs for the prevention and treatment of stroke in both diabetic and non-diabetic high-risk patients.</p>
----------------------------------------------------------------------
In diva2:1686258 abstract is: <p>Accumulation of ash on the Catalytic Diesel Particulate Filter (CDPF) is a well-known problem that results in elevated back pressure and affected vehicle performance. However, attention is seldom paid to the catalytic performance of the filter, and how it is affected by impurities from the exhaust. The objective of this study is to address the effect of ash on the catalytic performance of CDPF to oxidise pollutants that are harmful to the environment and human health. In addition, evaluating the feasibility to laboratory age CDPFs with different loads and compositions using the dip-coating method to simulate field aged filters. The filter characterisation was analysed by Optical Microscopy, Scanning Electron Microscopy (SEM) with Electron dispersive X-ray Spectroscopy (EDS), and X-ray Fluorescence (XRF). The catalytic performance was investigated based on NOx and C3H8 oxidation tests in a Synthetic Catalyst Activity Test (SCAT) rig. A fresh and field aged filter was compared with laboratory aged filters prepared with coatings of 22 g/L and 47 g/L CaSO4, and 79 g/L ash , respectively. The catalytic activity to oxidise NO and C3H8 was unaffected by the ash deposits in the laboratory aged filters, showing similar activity as the fresh filter. While the field aged filter had low catalytic performance to convert NO and was almost completely deactivated for the oxidation of C3H8. EDS mapping for the field aged filter and the laboratory ash coated sample showed that the elements Ca and S were present and had well-matched regions of deposit, respectively, indicating that CaSO4 was present. The microscopic analysis also indicated that both samples had a homogeneous distribution of ash on the filter channel surface. Despite the similarity in the physicochemical properties of the filter channel surface(field aged filter and the laboratory ash coated sample) the catalytic activity had significantly different results. This study suggests that the deactivationof the field aged filter might be caused by other types of ageing mechanism. The laboratory ageing by dip-coating is seen as a non-applicable method to simulate field aged filters. Further studies are required to evaluate both the method and the effects of ash on the catalytic activity.</p>

w='C3H8' val={'c': 'C<sub>3</sub>H<sub>8</sub>', 's': 'diva2:1686258'}
Also fixed CAS04

corrected abstract:
<p>Accumulation of ash on the Catalytic Diesel Particulate Filter (CDPF) is a well-known problem that results in elevated back pressure and affected vehicle performance. However, attention is seldom paid to the catalytic performance of the filter, and how it is affected by impurities from the exhaust. The objective of this study is to address the effect of ash on the catalytic performance of CDPF to oxidise pollutants that are harmful to the environment and human health. In addition, evaluating the feasibility to laboratory age CDPFs with different loads and compositions using the dip-coating method to simulate field aged filters. The filter characterisation was analysed by Optical Microscopy, Scanning Electron Microscopy (SEM) with Electron dispersive X-ray Spectroscopy (EDS), and X-ray Fluorescence (XRF). The catalytic performance was investigated based on NO<sub>x</sub> and C<sub>3</sub>H<sub>8</sub> oxidation tests in a Synthetic Catalyst Activity Test (SCAT) rig. A fresh and field aged filter was compared with laboratory aged filters prepared with coatings of 22 g/L and 47 g/L CaSO<sub>4</sub>, and 79 g/L ash , respectively. The catalytic activity to oxidise NO and C<sub>3</sub>H<sub>8</sub> was unaffected by the ash deposits in the laboratory aged filters, showing similar activity as the fresh filter. While the field aged filter had low catalytic performance to convert NO and was almost completely deactivated for the oxidation of C<sub>3</sub>H<sub>8</sub>. EDS mapping for the field aged filter and the laboratory ash coated sample showed that the elements Ca and S were present and had well-matched regions of deposit, respectively, indicating that CaSO<sub>4</sub> was present. The microscopic analysis also indicated that both samples had a homogeneous distribution of ash on the filter channel surface. Despite the similarity in the physicochemical properties of the filter channel surface (field aged filter and the laboratory ash coated sample) the catalytic activity had significantly different results. This study suggests that the deactivation of the field aged filter might be caused by other types of ageing mechanism. The laboratory ageing by dip-coating is seen as a non-applicable method to simulate field aged filters. Further studies are required to evaluate both the method and the effects of ash on the catalytic activity.</p>
----------------------------------------------------------------------
In diva2:856152 abstract is: <p>Dehydrins are proteins formed in plants in connection to different kinds of stress, such as cold and drought, and dehydrin gene expression can be used as a marker of freezing hardiness. It would be useful if the dehydrin content in young plants could be influenced by chemical treatment, which could simplify the inwintering process for conifer seedlings in plant nurseries. The effect of seed treatment with nicotinamide on the expression of the dehydrin 5 gene (Psdhn5) in <em>Pinus sylvestris</em> was studied.</p><p>Approximately 20% of the seeds in the batch of seeds used for this study were light coloured seeds and the rest dark brown or black. A potential connection between seed colour and gene expression was investigated. Psdhn5 expression was analyzed in seeds and in roots of one month old seedlings. Seeds were analyzed both directly after four hours of treatment with 2,5 mM nicotinamide or water (control), and after air drying over night. The Psdhn5 expression in control seeds, not dried over night, was approximately the same in light and dark coloured seeds. Nicotinamide treatment of seeds resulted in decreased Psdhn5 expression in seeds of both colours. In rotts from darj coloured seeds the expression was lower if seeds had been treated with nicotinamide, but also if the seeds had been left drying over night. However, in roots from light coloured seeds there was only a small decrease in expression after seed drying and only a small, if any, decrease after nicotinamide seed treatment. Compared to dark coloured seeds, the light coloured ones were slower to germinate, but was stimulated by nicotinamide treatment.</p><p>Overall, the results indicate a way to influence dehydrin expression in young pine seedlings by a simple seed treatment with a natural non-toxic compound. This knowledge may be useful in controlling the inwintering process for cold storage of seedlings in plant nurseries. The study also illustrates the impact of different phenotypes among pine seeds, which can be of importance in the production of healthy and stress tolerant seedlings within forestry. Furthermore, this study also points at sensitivity of gene expression in the growing plant regarding treatment and general environmental factos at the seed stage.  </p>

w='factos' val={'c': 'factors', 's': 'diva2:856152', 'n': 'no full text'}
w='darj' val={'c': 'dark', 's': 'diva2:856152', 'n': 'no full text'}
w='rotts' val={'c': 'roots', 's': 'diva2:856152', 'n': 'no full text'}

corrected abstract:
<p>Dehydrins are proteins formed in plants in connection to different kinds of stress, such as cold and drought, and dehydrin gene expression can be used as a marker of freezing hardiness. It would be useful if the dehydrin content in young plants could be influenced by chemical treatment, which could simplify the inwintering process for conifer seedlings in plant nurseries. The effect of seed treatment with nicotinamide on the expression of the dehydrin 5 gene (Psdhn5) in <em>Pinus sylvestris</em> was studied.</p><p>Approximately 20% of the seeds in the batch of seeds used for this study were light coloured seeds and the rest dark brown or black. A potential connection between seed colour and gene expression was investigated. Psdhn5 expression was analyzed in seeds and in roots of one month old seedlings. Seeds were analyzed both directly after four hours of treatment with 2,5 mM nicotinamide or water (control), and after air drying over night. The Psdhn5 expression in control seeds, not dried over night, was approximately the same in light and dark coloured seeds. Nicotinamide treatment of seeds resulted in decreased Psdhn5 expression in seeds of both colours. In roots from dark coloured seeds the expression was lower if seeds had been treated with nicotinamide, but also if the seeds had been left drying over night. However, in roots from light coloured seeds there was only a small decrease in expression after seed drying and only a small, if any, decrease after nicotinamide seed treatment. Compared to dark coloured seeds, the light coloured ones were slower to germinate, but was stimulated by nicotinamide treatment.</p><p>Overall, the results indicate a way to influence dehydrin expression in young pine seedlings by a simple seed treatment with a natural non-toxic compound. This knowledge may be useful in controlling the inwintering process for cold storage of seedlings in plant nurseries. The study also illustrates the impact of different phenotypes among pine seeds, which can be of importance in the production of healthy and stress tolerant seedlings within forestry. Furthermore, this study also points at sensitivity of gene expression in the growing plant regarding treatment and general environmental factors at the seed stage.</p>
----------------------------------------------------------------------
In diva2:1741842 abstract is: <p><em>Saprolegnia parasitica</em> is an opportunistic parasitic oomycete that infects fish eggs as well as juvenile and adult fish. The infection causes a disease called saprolegniasis, which is seen as patches on the host. This is a serious problem in fish farms since the conditions with high density populations and an increased stress level and risk of injury creates an environment in which the parasite can thrive.  Aquaculture contributes to almost half of the food fish industry, but a large part of the farmed fish is lost due to saprolegniasis. Other alternatives have been both explored and used, but they have been found to be ineffective or negatively affected the health of people and fish or the environment. Before this project, the lab group had performed a study, finding FDA approved drugs that could be re-purposed to target predicted essential proteins of the organism. Triclosan was found to have the lowest MIC<sub>100</sub> of 4 µg/mL and is the selected inhibitor in this the project. Here, we performed a mass spectrometry-based large-scale proteomics study comparing samples of <em>S. parasitica</em> grown with and without triclosan as an inhibitor. </p><p>The samples were cultivated in liquid media and examined using microscopy, which showed a high degree of branching of the hyphae in the treated samples while the ones in the control were long and smooth. This was followed by SDS-PAGE, in-gel digestion with trypsin and analysis with mass spectrometry. The raw data was then analyzed using several online programs, which detected a total of 1478 different proteins. Of these, 515 were selected and functionally annotated, which generated information about the biological processes they are involved in. The proteins were divided into 11 different categories of biological processes. The proteins within these were compared between the treated and untreated samples. The main groups that were recognized to differ were tyrosine-kinase-like (TKL) protein kinases, Ca2+/calmodulin-dependent protein kinases (CAMK) and aminopeptidases. The proteins were also categorized into different superfamilies: the NADB_Rossmann superfamily, the AdoMet_MTases superfamily, and the PknB_PASTA_kin superfamily had the highest number of proteins in the treated/untreated group, but none in the other. The proteins in these superfamilies were further investigated.</p>

w='MIC' val={'c': 'MIC<sub>100</sub>', 's': 'diva2:1741842', 'n': 'correct in original'}

corrected abstract:
<p><em>Saprolegnia parasitica</em> is an opportunistic parasitic oomycete that infects fish eggs as well as juvenile and adult fish. The infection causes a disease called saprolegniasis, which is seen as patches on the host. This is a serious problem in fish farms since the conditions with high density populations and an increased stress level and risk of injury creates an environment in which the parasite can thrive.  Aquaculture contributes to almost half of the food fish industry, but a large part of the farmed fish is lost due to saprolegniasis. Other alternatives have been both explored and used, but they have been found to be ineffective or negatively affected the health of people and fish or the environment. Before this project, the lab group had performed a study, finding FDA approved drugs that could be re-purposed to target predicted essential proteins of the organism. Triclosan was found to have the lowest MIC<sub>100</sub> of 4 µg/mL and is the selected inhibitor in this the project. Here, we performed a mass spectrometry-based large-scale proteomics study comparing samples of <em>S. parasitica</em> grown with and without triclosan as an inhibitor. </p><p>The samples were cultivated in liquid media and examined using microscopy, which showed a high degree of branching of the hyphae in the treated samples while the ones in the control were long and smooth. This was followed by SDS-PAGE, in-gel digestion with trypsin and analysis with mass spectrometry. The raw data was then analyzed using several online programs, which detected a total of 1478 different proteins. Of these, 515 were selected and functionally annotated, which generated information about the biological processes they are involved in. The proteins were divided into 11 different categories of biological processes. The proteins within these were compared between the treated and untreated samples. The main groups that were recognized to differ were tyrosine-kinase-like (TKL) protein kinases, Ca2+/calmodulin-dependent protein kinases (CAMK) and aminopeptidases. The proteins were also categorized into different superfamilies: the NADB_Rossmann superfamily, the AdoMet_MTases superfamily, and the PknB_PASTA_kin superfamily had the highest number of proteins in the treated/untreated group, but none in the other. The proteins in these superfamilies were further investigated.</p>
----------------------------------------------------------------------
In diva2:757429 abstract is: <p>The interpretation of the ECG is an important method in the diagnosis of abnormal heart conditions and can be used proactively to discover previ-ously unknown heart problems. Being able to easily measure the ECG and get it analyzed and presented in a clear manner without having to consult a doctor is improtant to satisfy consumer needs.</p><p>This report describes how an ECG signal is treated with different algo-rithms and methods to detect the heartbeat and its various parameters. This information is used to classify each heartbeat separately and thus determine whether the user has a normal or abnormal cardiac function. To achieve this a software prototype was developed in which the algorithms were implemented. A questionnaire survey was done in order to examine how the output of the software prototype should be presented for a user with no medical training.</p><p>Seven ECG files from MIT-BIH Arrhythmia database were used for validation of the algorithms. The developed algorithms could detect of if any abnormality of heart function occurred and informed the users to consult a physician. The presentation of the heart function was based on the result from the questioner.</p>

w='improtant' val={'c': 'important', 's': 'diva2:757429', 'n': 'error in original'}
w='previ-ously' val={'c': 'previously', 's': 'diva2:757429', 'n': 'hyphen at end of line'}
w='algo-rithms' val={'c': 'algo-rithms', 's': 'diva2:757429', 'n': 'hyphen at end of line'}

corrected abstract:
<p>The interpretation of the ECG is an important method in the diagnosis of abnormal heart conditions and can be used proactively to discover previously unknown heart problems. Being able to easily measure the ECG and get it analyzed and presented in a clear manner without having to consult a doctor is improtant to satisfy consumer needs.</p><p>This report describes how an ECG signal is treated with different algorithms and methods to detect the heartbeat and its various parameters. This information is used to classify each heartbeat separately and thus determine whether the user has a normal or abnormal cardiac function. To achieve this a software prototype was developed in which the algorithms were implemented. A questionnaire survey was done in order to examine how the output of the software prototype should be presented for a user with no medical training.</p><p>Seven ECG files from MIT-BIH Arrhythmia database were used for validation of the algorithms. The developed algorithms could detect of if any abnormality of heart function occurred and informed the users to consult a physician. The presentation of the heart function was based on the result from the questioner.</p>
----------------------------------------------------------------------
In diva2:1676731 abstract is: <p>The efficiency of boron-doped diamond electrodes (BDD) has been investigated by electrolysis of an aqueous solution containing sodium sulfate. A Synthesis StarterKit from Condias was used, which contained the BDD anode with an active surface area of 3.14 cm<sup>2</sup>, and electrosynthesis was performed in a batch mode reactor. In this report, the electrooxidation of sodium sulfate to sodium persulfate is well reported. The production of persulfate was studied at different cell voltages and electrolyte concentrations. The amount of persulfate produced was determined by the iodometric titration and itwas found that its concentration in the electrolyte was directly proportional to the persulfate concentration, i.e, a larger amount of persulfate could be obtained when the electrolyte was highly concentrated, up to 1 M. For each of the samples the amount of persulfate that theoretically is possible to produce was calculated and subsequently compared to the actual amount of persulfate that was formed, ie. current efficiencies. These current efficiencies were unexpectedly low for all experiments except for one data point. Hydrogen gas was also produced as a by-product at the cathode, but it couldn ot be collected in the present setup. The results and some possible improvements are discussed in the report.</p>

w='ot' val={'c': 'not', 's': 'diva2:1676731', 'n': 'counld not at end of line, the not should be on the next line'}

corrected abstract:
<p>The efficiency of boron-doped diamond electrodes (BDD) has been investigated by electrolysis of an aqueous solution containing sodium sulfate. A Synthesis StarterKit from Condias was used, which contained the BDD anode with an active surface area of 3.14 cm<sup>2</sup>, and electrosynthesis was performed in a batch mode reactor. In this report, the electrooxidation of sodium sulfate to sodium persulfate is well reported. The production of persulfate was studied at different cell voltages and electrolyte concentrations. The amount of persulfate produced was determined by the iodometric titration and it was found that its concentration in the electrolyte was directly proportional to the persulfate concentration, i.e, a larger amount of persulfate could be obtained when the electrolyte was highly concentrated, up to 1 M. For each of the samples the amount of persulfate that theoretically is possible to produce was calculated and subsequently compared to the actual amount of persulfate that was formed, ie. current efficiencies. These current efficiencies were unexpectedly low for all experiments except for one data point. Hydrogen gas was also produced as a by-product at the cathode, but it could not be collected in the present setup. The results and some possible improvements are discussed in the report.</p>
----------------------------------------------------------------------
In diva2:1205231 abstract is: <p>The invention of plastic has revolutionized people's life style not only by facilitating the storage and transportation of various goods but also by improving mechanical properties of mankind's technological advances. However, plastic is considered to be harmful, as it contains toxic chemical compounds which are accumulated due to its persistence. Therefore, plastic materials represent a threat to the wildlife, since its extensive use and disposal in landfills and natural habitats increased eminently. People's growing concern about the enviromental impact of pastic and its presence in the food chain led to reflections about designing biodegradable materials that potentially may replace a wide range of plastic materials. It has been found that materials made of specific proteins show plastic-like properties. These protein-based materials are both flexible and remarkable strong. Since proteins are available in great abundance and biodegradable, they are a promising source for creating such desired, environmental friendly materials. Although the physicochemical properties of protein-based material are well-established, the quality needs to be improved in order to make them viable for the market. For this purpose, the molecular structure which determines the physicochemical properties has to be analysed. Whey proteins for example form under non-physiological conditions highly ordered structures, called nanofibrils. These nanofibrils form fine-stranded networks which make the resulting material so flexible and remarkably strong. A well-established to determine the molecular structure of these nanofibrils is by means of Solid State Nuclear Magnetic Resonance (NMR) Spectroscopy. For example whey proteins are well known for forming nanofibrils due to their history of being associated with the development of degenerative diseases. For this reason, they may serve as excellent models for analyzing the molecular structure of amyloid fibrils. The aim of this research project is to determine the molecular structure of amyloid fibrils obtained from the whey protein β-lactoglobulin (β-lg). The major challenge hereby is to prepare isotopic laballed β-lg nanofibrils which is for Solid State NMR measurements required.</p>

w='laballed' val={'B2': 'Adjective', 's': 'diva2:1205231'}
w='pastic' val={'c': 'plastic', 's': 'diva2:1205231', 'n': 'error in original'}

corrected abstract:
<p>The invention of plastic has revolutionized people’s life style not only by facilitating the storage and transportation of various goods but also by improving mechanical properties of mankind’s technological advances. However, plastic is considered to be harmful, as it contains toxic chemical compounds which are accumulated due to its persistence. Therefore, plastic materials represent a threat to the wildlife, since its extensive use and disposal in landfills and natural habitats increased eminently.</p><p>People’s growing concern about the enviromental impact of pastic and its presence in the food chain led to reflections about designing biodegradable materials that potentially may replace a wide range of plastic materials.</p><p>It has been found that materials made of specific proteins show plastic-like properties. These protein-based materials are both flexible and remarkable strong. Since proteins are available in great abundance and biodegradable, they are a promising source for creating such desired, environmental friendly materials. Although the physicochemical properties of protein-based material are well-established, the quality needs to be improved in order to make them viable for the market. For this purpose, the molecular structure which determines the physicochemical properties has to be analysed. Whey proteins for example form under non-physiological conditions highly ordered structures, called nanofibrils. These nanofibrils form fine-stranded networks which make the resulting material so flexible and remarkably strong. A well-established to determine the molecular structure of these nanofibrils is by means of Solid State Nuclear Magnetic Resonance (NMR) Spectroscopy. For example whey proteins are well known for forming nanofibrils due to their history of being associated with the development of degenerative diseases. For this reason, they may serve as excellent models for analyzing the molecular structure of amyloid fibrils. The aim of this research project is to determine the molecular structure of amyloid fibrils obtained from the whey protein β-lactoglobulin (β-lg). The major challenge hereby is to prepare isotopic laballed β-lg nanofibrils which is for Solid State NMR measurements required.</p>
----------------------------------------------------------------------
In diva2:1322863 abstract is: <p>In fields such as sports science and entertainment, there’s occasionally a need to an- alyze a person's body pose in 3D. These needs may include analyzing a golf swing or enabling human interaction with games. Today, in order to reliably perform a human pose estimation, specialized hardware is usually required, which is often expensive and difficult to access. In recent years, multiple learning-based solutions have been developed that can perform the same kind of estimation on ordinary images. The purpose of this report has been to identify and compare popular learning-based so- lutions and to investigate whether any of these perform on par with an established hardware-based solution. To accomplish this, tools for testing have been developed, pose estimations have been conducted and result data for each test have been ana- lyzed. The result has shown that the solutions do not perform on par with Kinect and that they are currently not sufficiently well-developed to be used as a substitute for specialized hardware.</p>

w='lutions' val={'c': 'solutions', 's': 'diva2:1322863', 'n': 'correct in original, hyphen at end of line'}

corrected abstract:
<p>In fields such as sports science and entertainment, there’s occasionally a need to analyze a person's body pose in 3D. These needs may include analyzing a golf swing or enabling human interaction with games. Today, in order to reliably perform a human pose estimation, specialized hardware is usually required, which is often expensive and difficult to access. In recent years, multiple learning-based solutions have been developed that can perform the same kind of estimation on ordinary images. The purpose of this report has been to identify and compare popular learning-based solutions and to investigate whether any of these perform on par with an established hardware-based solution. To accomplish this, tools for testing have been developed, pose estimations have been conducted and result data for each test have been analyzed. The result has shown that the solutions do not perform on par with Kinect and that they are currently not sufficiently well-developed to be used as a substitute for specialized hardware.</p>
----------------------------------------------------------------------
In diva2:826713 abstract is: <p>Sweden and its municipal wastewater treatments plants, has as other Baltic countries, problem meeting the requirement on nitrogen compounds discharged via municipal wastewater. Raising the temperature of the wastewater, in the biological nitrogen removing process, is a possible alternative in order to achiev a higher reduction efficiency during the cold season when the wastewater has a bad cleaning outcome because of the low temperature.</p><p>Hammarby Sjöstadsverk has, together with Oskarshamns municipality, developed a project named ITEST,-( Increased Technology and Efficiency in Sewage Treatment), where the project is based on studying the temperature influence on the biological nitrogen reduction process.</p><p>The aim with this thesis, has been to study how the temperature, but also other important factors, influence the biological nitrogen reduction process, and then mainly the nitrification process. The main objective has been to run tests in a pilot plant and to do a literature review about the temperature influence on the nitrogen reduction process and the influence on the environment.</p><p>The performance of the project was at Sjöstadsverket where two testing line had been set up. The testing plant concisted of one line with adjusting temperature and another line acted as a reference line. Measurements have been performed from both lines, where the outgoing concentrations of ammonia, nitrate and totalnitrogen has been studied. From the studies a discussion of the temperature influence of the denitrification and the nitrification process has been fulfilled.</p><p>The result of the literature study part of thesis thesis shows that the nitrogen reduction rate is dependent on the temperature of the wastewater and it thought to be possible to get a better treatment in the biological nitrogen removal step if the temperature can be higher. The test in the pilot plant didn´t show a clear result. The nitrogen reduction rate showeds from both lines quite similar results despite the differences of both lines temperature. The difference of temperature in the two lines was probably too small to show a clear result.</p>

w='concisted' val={'c': 'consisted', 's': 'diva2:826713', 'n': 'error in original'}
w='showeds' val={'c': 'showed', 's': 'diva2:826713', 'n': 'error in original'}

corrected abstract:
<p>Sweden and its municipal wastewater treatments plants, has as other Baltic countries, problem meeting the requirement on nitrogen compounds discharged via municipal wastewater. Raising the temperature of the wastewater, in the biological nitrogen removing process, is a possible alternative in order to achiev a higher reduction efficiency during the cold season when the wastewater has a bad cleaning outcome because of the low temperature.</p><p>Hammarby Sjöstadsverk has, together with Oskarshamns municipality, developed a project named ITEST,-( Increased Technology and Efficiency in Sewage Treatment), where the project is based on studying the temperature influence on the biological nitrogen reduction process.</p><p>The aim with this thesis, has been to study how the temperature, but also other important factors, influence the biological nitrogen reduction process, and then mainly the nitrification process. The main objective has been to run tests in a pilot plant and to do a literature review about the temperature influence on the nitrogen reduction process and the influence on the environment.</p><p>The performance of the project was at Sjöstadsverket where two testing line had been set up. The testing plant concisted of one line with adjusting temperature and another line acted as a reference line. Measurements have been performed from both lines, where the outgoing concentrations of ammonia, nitrate and total nitrogen has been studied. From the studies a discussion of the temperature influence of the denitrification and the nitrification process has been fulfilled.</p><p>The result of the literature study part of thesis thesis shows that the nitrogen reduction rate is dependent on the temperature of the wastewater and it thought to be possible to get a better treatment in the biological nitrogen removal step if the temperature can be higher. The test in the pilot plant didn´t show a clear result. The nitrogen reduction rate showeds from both lines quite similar results despite the differences of both lines temperature. The difference of temperature in the two lines was probably too small to show a clear result.</p>
----------------------------------------------------------------------
In diva2:1045583 abstract is: <p>Stockholm Vatten introduced in januari 2014 a fee on biochemical oxygen demand (BOD) that is being releaed by industrial processes. Untreated BOD could have an oxygen depleting effect on recipients and thus affect aquatic life if the reciever is a lake or a stream.</p><p>The goal with this research thesis is to investigate the fluctuating patterns of specifically BOD7 in the industrial sewage of Coca-Cola Enterprises Sweden (CCES). Additionally to find possible BOD7 sources and investigate where, how and why these fluctuating patterns occur. Another objective for this research thesis is to discuss the correlation of chemical oxygen demand (COD) and BOD.</p><p>A large part of this research thesis consisted of analyzing data from the databases of CCES and Eurofins. Eurofins is an external company that analyzes samples taken from the industrial waste of CCES. During the research thesis samples were taken from the industrial waste and sent for analysis of BOD7 and CODcr.</p><p>After rebuilding at CCES production plant the dates which were looked at were 2015-01-01 to 2016-05-31. The analysis consisted of looking at what dates sampling was done of BOD7 and CODcr and correlating these with events happening in the production facility during these dates. The temporary events that were looked at were if discarding or dumping of beverage happened, what products were manufactured and how many changeovers took place during the sampling dates.</p><p>The results indicated that a high number of changeovers would generally give a high BOD7. Discarding and dumping of products would also give a high BOD7, which can be confirmed by the sampling of the industrial waste. The discarded beverage would also have the alternative of sending the it away to another company, SITA, that would use it to make biofuel. This however did not show any economic advantage regarding savings on the fee for BOD7 since CCES have to pay SITA for the transport of the discarded beverage.</p><p>The analysis of the existing data from Eurofins confirmed a positive correlation of BOD7 and CODcr. CODcr measure the chemical oxidizable material which means that in addition to measuring the organic oxidizable material it also measures the inorganic oxidizable material. This results in that CODcr will almost always be larger than the BOD7.</p>

w='releaed' val={'c': 'released', 's': 'diva2:1045583', 'n': 'error in original'}
w='reciever' val={'c': 'receiver', 's': 'diva2:1045583', 'n': 'error in original'}
w='CODcr' val={'c': 'COD<sub>cr</sub>', 's': 'diva2:1045583'}
similarly for BOD7

corrected abstract:
<p>Stockholm Vatten introduced in januari 2014 a fee on biochemical oxygen demand (BOD) that is being releaed by industrial processes. Untreated BOD could have an oxygen depleting effect on recipients and thus affect aquatic life if the reciever is a lake or a stream.</p><p>The goal with this research thesis is to investigate the fluctuating patterns of specifically BOD<sub>7</sub> in the industrial sewage of Coca-Cola Enterprises Sweden (CCES). Additionally to find possible BOD<sub>7</sub> sources and investigate where, how and why these fluctuating patterns occur. Another objective for this research thesis is to discuss the correlation of chemical oxygen demand (COD) and BOD.</p><p>A large part of this research thesis consisted of analyzing data from the databases of CCES and Eurofins. Eurofins is an external company that analyzes samples taken from the industrial waste of CCES. During the research thesis samples were taken from the industrial waste and sent for analysis of BOD<sub>7</sub> and COD<sub>cr</sub>.</p><p>After rebuilding at CCES production plant the dates which were looked at were 2015-01-01 to 2016-05-31. The analysis consisted of looking at what dates sampling was done of BOD<sub>7</sub> and COD<sub>cr</sub> and correlating these with events happening in the production facility during these dates. The temporary events that were looked at were if discarding or dumping of beverage happened, what products were manufactured and how many changeovers took place during the sampling dates.</p><p>The results indicated that a high number of changeovers would generally give a high BOD<sub>7</sub>. Discarding and dumping of products would also give a high BOD<sub>7</sub>, which can be confirmed by the sampling of the industrial waste. The discarded beverage would also have the alternative of sending the it away to another company, SITA, that would use it to make biofuel. This however did not show any economic advantage regarding savings on the fee for BOD<sub>7</sub> since CCES have to pay SITA for the transport of the discarded beverage.</p><p>The analysis of the existing data from Eurofins confirmed a positive correlation of BOD<sub>7</sub> and COD<sub>cr</sub>. COD<sub>cr</sub> measure the chemical oxidizable material which means that in addition to measuring the organic oxidizable material it also measures the inorganic oxidizable material. This results in that COD<sub>cr</sub> will almost always be larger than the BOD<sub>7</sub>.</p>
----------------------------------------------------------------------
In diva2:1273275 abstract is: <p>Storsjön and Lillsjön are two small lakes near Roslagsbanan and close to Lindholmen. The lakes together with the surrounding wetland makes a very nice natural park. The aim of this work is to investigate the impact of metals on the enviroment of the lakes. pH is very low(=6,7) and the water contains a lot of organic material. The area is seriously impacted by the nearby roads. Sodium cloride from road salt spread during the winter and metal ions fromthe cars are transported through this area and are partly stored in the sediments of the lakes. Dams should be constructed in the incoming creeks for sedimentation of metal ions.</p>

w='cloride' val={'c': 'chloride', 's': 'diva2:1273275', 'n': 'error in original'}

corrected abstract:
<p>Storsjön and Lillsjön are two small lakes near Roslagsbanan and close to Lindholmen. The lakes together with the surrounding wetland makes a very nice natural park. The aim of this work is to investigate the impact of metals on the enviroment of the lakes. pH is very low (=6,7) and the water contains a lot of organic material. The area is seriously impacted by the nearby roads. Sodium cloride from road salt spread during the winter and metal ions from the cars are transported through this area and are partly stored in the sediments of the lakes. Dams should be constructed in the incoming creeks for sedimentation of metal ions.</p>
----------------------------------------------------------------------
In diva2:441389 abstract is: <p>The degree project was done at Södra Cell Värö with the purpose to investigate how the use of energy for pulp drying in a pulp dryer could be made more effective to decrease the energy consumption or increase the capacity. The pulp dryer is one of the machines that consumes the most energy at SCV. The air that dryes the pulp is heated by low pressure steam, and since the amount low pressure steam was limited, the purpose with the project was to investigate how the low pressure steam best could be used. If the drying capacity could be improved it could enable for an increase in production or a decrease low pressure steam consumption.</p>
<p>The task was divided into:</p>
<p>1. Analysis of steam and condensate flows connected to the pulp dryer. Can they be adjusted to improve the drying capacity?</p>
<p>2. Investigation of possible sectors of application for hot air flows from vacuum pumps.</p>
<p>3. Investigation of the condensate system. Can condensate and flash steam be used in a better way to provide more steam to the pulp dryer?</p>
<p>For task 1, air and energy balances were made ove the pulp dryer, then temperature, flow and moisture content were measured for all air flows in and out. To investigate how the consumption low pressure steam in the pulp dryer depends on the air flows in to the pulp dryer, tests were made where the rotation speed for the fans and the temperature for the air were varied.</p>
<p>The result of measuring the air balance over the pulp dryer was that the same amount air was going in and out, which means that all the air was going in to the dryer preheated. The energy balance over the thermal recycling system showed that 40 % of the energy in outgoing air was being reused.</p>
<p>Increasing the rotation speed from 750 rpm to 1000 rpm was favourable when the production was high. Increasing the temperature of the air in to the pulp dryer showed that the consumption low pressure steam decreased. Recommended rotation speeds:</p>
<p>December – february:</p>
<p>1000 rpm, all levels of production</p>
<p>mars – november:</p>
<p>1000 rpm for high production (over 3 bar low pressure steam to pulp dryer)</p>
<p>750 rpm for low production (below 3 bar low pressure steam to pulp dryer)</p>
<p>For task 2, temperature, flow and moisture content were measured for all air flows out from the vacuum pumps.</p>
<p>The air flows out from the vacuum pumps had a temperature of 40-50 °C, which was too low to be used for preheating of air to pulp dryer.</p>
<p>For task 3, a mapping of the condensate system including all steam and condensate flows connected to the pulp dryer was made. The mapping was made in AutoCAD. Since the experiment with increased temperature of the air in to the pulp dryer showed that an increase in temperature caused the consumption low pressure steam to decrease, calculations of how much more the consumption low pressure steam could be decreased by switching to steam of a higher pressure for preaheating the drying air.</p>
<p>By using only steam of higher pressure for air preheating, the amount available low pressure steam to the pulp dryer could be increased with 6 tonnes/h.</p>


w='dryes' val={'c': 'dries', 's': 'diva2:441389', 'n': 'error in original'}
w='preaheating' val={'c': 'preheating', 's': 'diva2:441389', 'n': error in original'}

corrected abstract:
<p>The degree project was done at Södra Cell Värö with the purpose to investigate how the use of energy for pulp drying in a pulp dryer could be made more effective to decrease the energy consumption or increase the capacity. The pulp dryer is one of the machines that consumes the most energy at SCV. The air that dryes the pulp is heated by low pressure steam, and since the amount low pressure steam was limited, the purpose with the project was to investigate how the low pressure steam best could be used. If the drying capacity could be improved it could enable for an increase in production or a decrease low pressure steam consumption.</p><p>The task was divided into:</p><ol><li>Analysis of steam and condensate flows connected to the pulp dryer. Can they be adjusted to improve the drying capacity?</li><li>Investigation of possible sectors of application for hot air flows from vacuum pumps.</li><li>Investigation of the condensate system. Can condensate and flash steam be used in a better way to provide more steam to the pulp dryer?</li></ol><p>For task 1, air and energy balances were made ove the pulp dryer, then temperature, flow and moisture content were measured for all air flows in and out. To investigate how the consumption low pressure steam in the pulp dryer depends on the air flows in to the pulp dryer, tests were made where the rotation speed for the fans and the temperature for the air were varied.</p><p>The result of measuring the air balance over the pulp dryer was that the same amount air was going in and out, which means that all the air was going in to the dryer preheated. The energy balance over the thermal recycling system showed that 40 % of the energy in outgoing air was being reused.</p><p>Increasing the rotation speed from 750 rpm to 1000 rpm was favourable when the production was high. Increasing the temperature of the air in to the pulp dryer showed that the consumption low pressure steam decreased. Recommended rotation speeds:</p><p>December – february:</p><p>1000 rpm, all levels of production</p><p>mars – november:</p><p>1000 rpm for high production (over 3 bar low pressure steam to pulp dryer)</p><p>750 rpm for low production (below 3 bar low pressure steam to pulp dryer)</p><p>For task 2, temperature, flow and moisture content were measured for all air flows out from the vacuum pumps.</p><p>The air flows out from the vacuum pumps had a temperature of 40-50 °C, which was too low to be used for preheating of air to pulp dryer.</p><p>For task 3, a mapping of the condensate system including all steam and condensate flows connected to the pulp dryer was made. The mapping was made in AutoCAD. Since the experiment with increased temperature of the air in to the pulp dryer showed that an increase in temperature caused the consumption low pressure steam to decrease, calculations of how much more the consumption low pressure steam could be decreased by switching to steam of a higher pressure for preaheating the drying air.</p><p>By using only steam of higher pressure for air preheating, the amount available low pressure steam to the pulp dryer could be increased with 6 tonnes/h.</p>
----------------------------------------------------------------------

In diva2:1673998 abstract is: <p>Determining structures of proteins is important to understand protein functions, and a rapidly evolving technique in this field is cryogen electron microscopy. However, size limitations are preventing wider applications of the technique because small proteins have poor signal to noise ratios and are not possible to distinguish in single-particle images. The hypothesis of this project is that it is possible to image very small proteins, bypassing the conventional size limitations of single-particle cryo-EM, by utilizing a carrier protein-scaffold (Putrescine Aminotransferase; YgjG) connected through helical fusion to an affibody (Zwt) that can bind to a small protein of interest. The complex provides a sufficient size, symmetry, and rigidity for successful electron microscopy also of the non-covalently bound small protein of interest. To characterise the proposed scaffold, thermal stability through CD, binding of target protein in SPR, purity through SEC and experiments towards proof-of-concept in cryo-EM will be performed. The small protein of interest to be imaged in the proof-of-concept setup is another affibody, called Z963, that would be the smallest protein ever solved with cryo-EM. The results show that the investigated tetrameric protein scaffold is a highly stable protein (Tm~85oC) that can tolerate affibody fusion with retained binding function of multiple sites. The protein can be recombinantly expressed and purified in high yield and forms tetramers also when fused to affibody. The cryo-EM results are still pending, but promising grids have been created and in an initial particle selection clear 2-D classes that also reveal the small bound protein of interest have been generated. To conclude, biophysical characterization indicates that YgjG is a promising base structure for an imaging scaffold and preliminary single-particle cryo-EM analyses show that the proposed strategy to investigate structures of small proteins of interest is feasible.</p>

w='Tm~85oC' val={'c': 'T<sub>m</sub> ~85ᵒC', 's': 'diva2:1673998', 'n': 'correct in original'}

corrected abstract:
<p>Determining structures of proteins is important to understand protein functions, and a rapidly evolving technique in this field is cryogen electron microscopy. However, size limitations are preventing wider applications of the technique because small proteins have poor signal to noise ratios and are not possible to distinguish in single-particle images. The hypothesis of this project is that it is possible to image very small proteins, bypassing the conventional size limitations of single-particle cryo-EM, by utilizing a carrier protein-scaffold (Putrescine Aminotransferase; YgjG) connected through helical fusion to an affibody (Zwt) that can bind to a small protein of interest. The complex provides a sufficient size, symmetry, and rigidity for successful electron microscopy also of the non-covalently bound small protein of interest. To characterise the proposed scaffold, thermal stability through CD, binding of target protein in SPR, purity through SEC and experiments towards proof-of-concept in cryo-EM will be performed. The small protein of interest to be imaged in the proof-of-concept setup is another affibody, called Z963, that would be the smallest protein ever solved with cryo-EM. The results show that the investigated tetrameric protein scaffold is a highly stable protein (T<sub>m</sub> ~85ᵒC) that can tolerate affibody fusion with retained binding function of multiple sites. The protein can be recombinantly expressed and purified in high yield and forms tetramers also when fused to affibody. The cryo-EM results are still pending, but promising grids have been created and in an initial particle selection clear 2-D classes that also reveal the small bound protein of interest have been generated. To conclude, biophysical characterization indicates that YgjG is a promising base structure for an imaging scaffold and preliminary single-particle cryo-EM analyses show that the proposed strategy to investigate structures of small proteins of interest is feasible.</p>
----------------------------------------------------------------------
In diva2:1574726 abstract is: <p>The anaerobic thermophilic bacteria <em>Clostridium thermocellum</em> has a great ability to hydrolyze and ferment cellulose into ethanol. This makes <em>C. thermocellum</em> a promising candidate for second-generation biofuel production using consolidating bioprocessing. To achieve a competitive process, the ethanol yield and titer must be higher than what is natively produced in <em>C. thermocellum</em>. Despite numerous engineering attempts to increase ethanol production, no strain able to produce the required industrial amounts of ethanol has yet been constructed. However, modification of the central metabolism is a possible solution to overcome this problem. Therefore, the aim of this study was to use state-of-the-art techniques for gene knock-outs and overexpression in the central fermentative metabolism of <em>C. themocellum</em>. Physiological and biochemical characterization revealed successful redirection of carbon and electrons. Further engineering of this strain proved to be difficult and strategies to overcome this problem were proposed. The unsuccessful strain construction together with the physiological characterizations provided insightful information that can be used as a guide for future attempts to engineer <em>C. thermocellum</em>. </p>

w='themocellum' val={'c': 'thermocellum', 's': 'diva2:1574726', 'n': 'no full text'}

corrected abstract:
<p>The anaerobic thermophilic bacteria <em>Clostridium thermocellum</em> has a great ability to hydrolyze and ferment cellulose into ethanol. This makes <em>C. thermocellum</em> a promising candidate for second-generation biofuel production using consolidating bioprocessing. To achieve a competitive process, the ethanol yield and titer must be higher than what is natively produced in <em>C. thermocellum</em>. Despite numerous engineering attempts to increase ethanol production, no strain able to produce the required industrial amounts of ethanol has yet been constructed. However, modification of the central metabolism is a possible solution to overcome this problem. Therefore, the aim of this study was to use state-of-the-art techniques for gene knock-outs and overexpression in the central fermentative metabolism of <em>C. thermocellum</em>. Physiological and biochemical characterization revealed successful redirection of carbon and electrons. Further engineering of this strain proved to be difficult and strategies to overcome this problem were proposed. The unsuccessful strain construction together with the physiological characterizations provided insightful information that can be used as a guide for future attempts to engineer <em>C. thermocellum</em>. </p>
----------------------------------------------------------------------
In diva2:1095499 abstract is: <p>This thesis reports the effect of silica-particles on conductive polymer nanocomposites based on polymethylmethacrylate <strong>(PMMA) </strong>and carbon nanotubes (CNTs). The goal was to lower the percolation threshold by adding silica and therefore achieving a higher conductivity in finished composites at the same CNT concentration. Different sizes of silica were investigated, from nano - to micromet er-sized, where only the nano-silica was found to disperse and not sediment within the matrix. Addition of neat silica-particles of any size, without the use of a surfactant, to the conductive composite did not improve the conductive properties. However with the addition of an anionic surfactant in combination with the nano -sil ica a shift of the percolation threshold to lower concentrations of CNT was seen, which improved the conductivity with ca 1 order of magnitude in the finished composites compared to that of composites containing only CNTs. The characterization by microscopy revealed a segregation of the nano-sized silica into clusters where no CNTs were observed. The detailed analysis revealed that the nano -sized silica therefore occupied a larger volume fraction in the matrix than expected, thus concentrating the CNTs into the remaining volume which resulted in a much greater conductivity than explained by theory.</p>

w='micromet' val={'c': 'micrometer-sized', 's': 'diva2:1095499', 'n': 'no full text'}

corrected abstract:
<p>This thesis reports the effect of silica-particles on conductive polymer nanocomposites based on polymethylmethacrylate <strong>(PMMA) </strong>and carbon nanotubes (CNTs). The goal was to lower the percolation threshold by adding silica and therefore achieving a higher conductivity in finished composites at the same CNT concentration. Different sizes of silica were investigated, from nano - to micrometer-sized, where only the nano-silica was found to disperse and not sediment within the matrix. Addition of neat silica-particles of any size, without the use of a surfactant, to the conductive composite did not improve the conductive properties. However with the addition of an anionic surfactant in combination with the nano -sil ica a shift of the percolation threshold to lower concentrations of CNT was seen, which improved the conductivity with ca 1 order of magnitude in the finished composites compared to that of composites containing only CNTs. The characterization by microscopy revealed a segregation of the nano-sized silica into clusters where no CNTs were observed. The detailed analysis revealed that the nano-sized silica therefore occupied a larger volume fraction in the matrix than expected, thus concentrating the CNTs into the remaining volume which resulted in a much greater conductivity than explained by theory.</p>
----------------------------------------------------------------------
In diva2:1765032 abstract is: <p>Waste management and waste generation are large contributors to global climate change and in modern times it has become evident that corporations play an important role in environmental sustainability. The purpose of this thesis was to investigate Viking Line’s waste management system and to identify liquid and residual waste streams associated with daily activities onboard. With the aim of identifying potential areas of improvement and opportunities to save both emissions and monetary resources. Finally, to give recommendations with the goal of enabling Viking Line to climb the waste hierarchy. This report is based upon a literature review, internal documents provided by responsible at Viking Line and on semi-structured interviews with focus on their waste management system. Investigation took place using data collected from their vessel named Cinderella. Through examination, it was found that combustible waste, corrugated cardboard, coloured glass packaging, clear glass packaging, landfill (unsorted mainteance waste) and frag scrap were the six largest waste streams, during 2019 and 2022. When investigating the waste generation from all vessels and corresponding treatment, it was found that energy recovery was the most common treatment method of residual waste, resulting in a scoring value of 395 out of 1000 when applying the waste hierarchy scoring system.</p><p>Low degree of sorting is prohibiting increased recycling and a move towards the top of the waste hierarchy. It was found that the passengers have little to no opportunity to sort waste onboard. Previous attempts at trying to increase sorting among passengers were unsuccessful and one possible explanation is lack of motive, understanding and awareness for the purpose of sorting. To increase awareness, the recommendation is to educate on the topic of waste management, visualize the positive impacts from sorting and enable sorting by providing clear instructions and availability. Viking Line could also investigate innovative technology, design and sustainable materials with their suppliers and stakeholders to reduce waste generation.</p>

w='mainteance' val={'c': 'maintenance', 's': 'diva2:1765032', 'n': 'correct in original'}

corrected abstract:
<p>Waste management and waste generation are large contributors to global climate change and in modern times it has become evident that corporations play an important role in environmental sustainability. The purpose of this thesis was to investigate Viking Line’s waste management system and to identify liquid and residual waste streams associated with daily activities onboard. With the aim of identifying potential areas of improvement and opportunities to save both emissions and monetary resources. Finally, to give recommendations with the goal of enabling Viking Line to climb the waste hierarchy. This report is based upon a literature review, internal documents provided by responsible at Viking Line and on semi-structured interviews with focus on their waste management system.</p><p>Investigation took place using data collected from their vessel named Cinderella. Through examination, it was found that combustible waste, corrugated cardboard, coloured glass packaging, clear glass packaging, landfill (unsorted maintenance waste) and frag scrap were the six largest waste streams, during 2019 and 2022. When investigating the waste generation from all vessels and corresponding treatment, it was found that energy recovery was the most common treatment method of residual waste, resulting in a scoring value of 395 out of 1000 when applying the waste hierarchy scoring system.</p><p>Low degree of sorting is prohibiting increased recycling and a move towards the top of the waste hierarchy. It was found that the passengers have little to no opportunity to sort waste onboard. Previous attempts at trying to increase sorting among passengers were unsuccessful and one possible explanation is lack of motive, understanding and awareness for the purpose of sorting. To increase awareness, the recommendation is to educate on the topic of waste management, visualize the positive impacts from sorting and enable sorting by providing clear instructions and availability. Viking Line could also investigate innovative technology, design and sustainable materials with their suppliers and stakeholders to reduce waste generation.</p>
----------------------------------------------------------------------
In diva2:1678492 abstract is: <p>Glycosylation is a common post-translational modification of proteins, where glycans are covalently attached to the polypeptide backbone. Their presence affects protein function and abnormal variations in glycosylation has been linked to diseases such as Alzheimer’s and cancers. These variations could be studied and then used as clinical biomarkers for disease, however, analysis is often difficult and pre-treatments such as enrichment is generally required. One common choice of enrichment method is hydrophilic interaction liquid chromatography (HILIC) which uses a polar affinity material to retain the hydrophilic glycopeptides. </p><p>In this project, glass wool, which is a cheap and accessible product, was evaluated as an affinity material in a HILIC setup. The glycoprotein used was Immunoglobulin G, the most common antibody in humans, which was digested using trypsin. An enrichment protocol was developed and optimized, and the concentrated samples were analysed using MALDI-TOF mass spectrometry. The solutions chosen were loading solution 88% ACN 0.1% TFA, washing solution 88% ACN 0.01% TFA followed by 88% ACN 0.02% TFA, and H2O as elution solution. The binding capacity was assumed to be 2.5 μg/mg and the selectivity for IgG glycopeptides was seen to decrease when other protein was present in sample. The performance of glass wool tips seemed promising when compared to the performance of commercial HIILIC tips for the less hydrophilic glycopeptides, thus glass wool tips could potentially be used as a cheap and easy to use alternative when enriching glycopeptides.</p>

w='HIILIC' val={'c': 'HILIC', 's': 'diva2:1678492', 'n': 'no full text'}

corrected abstract:
<p>Glycosylation is a common post-translational modification of proteins, where glycans are covalently attached to the polypeptide backbone. Their presence affects protein function and abnormal variations in glycosylation has been linked to diseases such as Alzheimer’s and cancers. These variations could be studied and then used as clinical biomarkers for disease, however, analysis is often difficult and pre-treatments such as enrichment is generally required. One common choice of enrichment method is hydrophilic interaction liquid chromatography (HILIC) which uses a polar affinity material to retain the hydrophilic glycopeptides. </p><p>In this project, glass wool, which is a cheap and accessible product, was evaluated as an affinity material in a HILIC setup. The glycoprotein used was Immunoglobulin G, the most common antibody in humans, which was digested using trypsin. An enrichment protocol was developed and optimized, and the concentrated samples were analysed using MALDI-TOF mass spectrometry. The solutions chosen were loading solution 88% ACN 0.1% TFA, washing solution 88% ACN 0.01% TFA followed by 88% ACN 0.02% TFA, and H2O as elution solution. The binding capacity was assumed to be 2.5 μg/mg and the selectivity for IgG glycopeptides was seen to decrease when other protein was present in sample. The performance of glass wool tips seemed promising when compared to the performance of commercial HILIC tips for the less hydrophilic glycopeptides, thus glass wool tips could potentially be used as a cheap and easy to use alternative when enriching glycopeptides.</p>
----------------------------------------------------------------------
In diva2:845712 abstract is: <p>DNA methylation is an important feature of the aging process. Recent studies have revealed the association between DNA methylation and age. Also, the Methylation patterns of many CpG sites and genes have been reported to be age related. This study aims to study DNA methylation data from midroarray and to perform statistical analysis to identify age-related CpG sites. We processed Swedish twin cohort data from Illumina 450k methylation array and identified 417 significant CpG sites from the Cpigenome-Wide Association Study.</p>


w='midroarray' val={'c': 'microarray', 's': 'diva2:845712', 'n': 'no full text'}

corrected abstract:
<p>DNA methylation is an important feature of the aging process. Recent studies have revealed the association between DNA methylation and age. Also, the Methylation patterns of many CpG sites and genes have been reported to be age related. This study aims to study DNA methylation data from microarray and to perform statistical analysis to identify age-related CpG sites. We processed Swedish twin cohort data from Illumina 450k methylation array and identified 417 significant CpG sites from the Cpigenome-Wide Association Study.</p>
----------------------------------------------------------------------
In diva2:738455 abstract is: <p><strong>Background</strong>: The Swedish population is aging [1] and malnutrition is a common problem among elderly people [2]. A method called food registration is used to monitor the patients’ nutritional intake, with the purpose of reducing the risk of patients deteriorating in nutritional status during hospitalisation. However, the current method is not satisfactory and many food registration lists are incomplete. [3-6] Thus, patients do not receive a qualitative nutritional care since the assessment of the patients’ nutritional status is based on incomplete documentation.</p><p><strong>Purpose</strong>: The purpose of this master thesis is to increase the quality in the nutrition monitoring process in the geriatric units B72 and B74 at Karolinska University Hospital in Huddinge, Sweden. This should be done by increasing the prospects of providing caregivers with correct information about the patients’ nutritional intake and make the food registration more complete by introducing a new working system based on Lean Healthcare principles. The new working system should consist of a prototype of a food registration application and a new working approach with clear and standardised responsibilities for all occupational groups and shifts.</p><p><strong>Problem</strong>: Is it possible to create a visual and standardised working system that will increase the effectiveness and safety of the nutritional monitoring process?</p><p><strong>Methods</strong>: The current nutrition monitoring process was investigated by observations, interviews and timings. In addition, 100 registration lists were scrutinized and errors and missing information were noted and prototype of a food registration application was created using usability inspection methods. The prototype was tested in a usability test and evaluated in a survey that contained a standardised usability index called The System Usability Scale (SUS).</p><p><strong>Results</strong>: The review of old registration lists revealed that 59 per cent of the food registration lists missed information on how much the patient had been served and/or had consumed at one or more occasions. On average, each list contained 9.4 errors or points of missinged information. The SUS test generated a usability score of 86.75 out of 100, which is equivalent to an A in the A-F grading system. [7] During the usability test an average of 8.2 errors per test were made, though as 4.4 of these were corrected the net average was 3.8 errors per test. The three most common errors, which accounted for 69.5 per cent of all 82 errors, were to forget to specify quantity (36.6 per cent), to press ‘done’ before everything had been registered (18.3 per cent) and to not find the correct item (14.6 per cent).</p><p><strong>Conclusion</strong>: Hopefully, in the future, more food registrations will be correctly performed due to the built in mistake proofing systems of the food registration application and the introduction of the new working approach. The hope is that the new working system will provide caregivers with accurate information on the patients’ nutritional intake so that the patients’ nutritional care can be based on complete documentation. In this way, the quality and safety of the nutrition monitoring process will be increased.</p>

w='missinged' val={'c': 'missing', 's': 'diva2:738455', 'n': 'correct in original'}

corrected abstract:
<p><strong>Background</strong>: The Swedish population is aging [1] and malnutrition is a common problem among elderly people [2]. A method called food registration is used to monitor the patients’ nutritional intake, with the purpose of reducing the risk of patients deteriorating in nutritional status during hospitalisation. However, the current method is not satisfactory and many food registration lists are incomplete. [3-6] Thus, patients do not receive a qualitative nutritional care since the assessment of the patients’ nutritional status is based on incomplete documentation.</p><p><strong>Purpose</strong>: The purpose of this master thesis is to increase the quality in the nutrition monitoring process in the geriatric units B72 and B74 at Karolinska University Hospital in Huddinge, Sweden. This should be done by increasing the prospects of providing caregivers with correct information about the patients’ nutritional intake and make the food registration more complete by introducing a new working system based on Lean Healthcare principles. The new working system should consist of a prototype of a food registration application and a new working approach with clear and standardised responsibilities for all occupational groups and shifts.</p><p><strong>Problem</strong>: Is it possible to create a visual and standardised working system that will increase the effectiveness and safety of the nutritional monitoring process?</p><p><strong>Methods</strong>: The current nutrition monitoring process was investigated by observations, interviews and timings. In addition, 100 registration lists were scrutinized and errors and missing information were noted and prototype of a food registration application was created using usability inspection methods. The prototype was tested in a usability test and evaluated in a survey that contained a standardised usability index called The System Usability Scale (SUS).</p><p><strong>Results</strong>: The review of old registration lists revealed that 59 per cent of the food registration lists missed information on how much the patient had been served and/or had consumed at one or more occasions. On average, each list contained 9.4 errors or points of missing information. The SUS test generated a usability score of 86.75 out of 100, which is equivalent to an A in the A-F grading system. [7] During the usability test an average of 8.2 errors per test were made, though as 4.4 of these were corrected the net average was 3.8 errors per test. The three most common errors, which accounted for 69.5 per cent of all 82 errors, were to forget to specify quantity (36.6 per cent), to press ‘done’ before everything had been registered (18.3 per cent) and to not find the correct item (14.6 per cent).</p><p><strong>Conclusion</strong>: Hopefully, in the future, more food registrations will be correctly performed due to the built in mistake proofing systems of the food registration application and the introduction of the new working approach. The hope is that the new working system will provide caregivers with accurate information on the patients’ nutritional intake so that the patients’ nutritional care can be based on complete documentation. In this way, the quality and safety of the nutrition monitoring process will be increased.</p>
----------------------------------------------------------------------
In diva2:1763572 abstract is: <p>The aim of this thesis is to improve the knowledgebase regarding the technical lifetime of radio circuit boards. The purpose is to create opportunities to facilitate the design of products that live up to the customers' requirements and to Ericsson's sustainable responsibility goals. The problem at hand was to look for the average temperature that has contribiuted to the performance degradation of circuit boards used in the field. The method was to measure the performance degradation of capacitors used in a power distribution function on the circuit board and combining those values with the time the circuit board has been in operation, to derive the average operation temperature with the help of the Arrhenius equation. The radio circuit board used in the field showed signs of performance degradation that could be interpreted as having reached the end of its technical lifetime after being used for 31 800 hours (3,6 years) at an average temperature of 78 °C to 79 °C. For it to be possible to interpret the temperature results of the unit used in the field, the three parameters: measured performance degradation, time of operation and average temperature at that time, had to be collected. All three parameters are key when evaluating technical lifetime since they need to be interpreted in relation to each other. The temperature results exists in a thermodynamic system that includes the capacitors, circuit board, the temperature of the surrounding environment, and the global climate. The temperature results and the measured performance degradation is a consequence of how the radio was operated, which is linked to the desired performance it was designed for. The radio circuit boards are also designed to comply with customers' requirements and the requirements based in the companys's sustainability responsibility goals. It is likely that the desired performance of radios in the future needs to be balanced with the company's sustainability responisibility goals, and the temperatures created as an effect of climate change.</p>

w='contribiuted' val={'c': 'contributed', 's': 'diva2:1763572', 'n': 'error in original'}
w='responisibility' val={'c': 'responsibility', 's': 'diva2:1763572', 'n': 'correct in original'}
Note that there were many wording difference between the DiVA abstract and the actual abstract.

corrected abstract:
<p>The aim of this thesis is to improve the knowledgebase regarding the technical lifetime of radio circuit boards. The purpose is to create opportunities to facilitate the design of products that live up to the customers' requirements and to Ericsson’s sustainable responsibility goals. The problem at hand was to look for the average temperature that has contributed to the performance degradation of circuit boards used in the field. The method was to measure the performance degradation of capacitors used in a power distribution function on the circuit board and combining those values with the time the circuit board has been in operation, to derive the average operation temperature with the help of the Arrhenius equation. The radio circuit board unit used in the field showed signs of performance degradation that could be interpreted as having reached the end of its technical life after being used for 31 800 hours (3,6 years) at an average temperature of approximately 78 °C to 79 °C. For it to be possible to interpret the temperature results of the unit used in the field, the three parameters: measured performance degradation, time of operation and average temperature at that time, had to be collected. All three parameters are key when evaluating technical lifetime since they need to be interpreted in relation to each other. The temperature results exist in a thermodynamic system that includes the capacitors, circuit board, the temperature of the surrounding environment, and the global climate. The temperature results and the measured performance degradation is a consequence of how the radio was operated, which is linked to the desired performance it was designed for. The radio circuit boards are also designed to comply with customers’ requirements and the requirements based in the company’s sustainability responsibility goals. It is likely that the desired performance of radios in the future needs to be balanced with the company’s sustainable responsibility goals, and temperatures created as an effect of climate change.</p>
----------------------------------------------------------------------
In diva2:460449 abstract is: <p>In Sweden the industries releases about 50 TWh / year of low temperature waste heat [1], often in the form of humid air flows. Today, conventional flue gas condensation is only exploiting a minor part of the energy from these flows. It is a well-established and profit­able way of improving the efficiency of district heating plants and other boilers for wet fuels. How­ever, the condensation is only applicable when the dew point of the flue gas is above the temperature demand for the heating net. The paper industry gives a good illustration of the limitations for conventional condensation: several MW of wet air streams with dew points of 60-65 ̊<sup> </sup>C are released but cannot be recovered since the tempe­ra­ture demand is 70-80 ̊ C for the heating net. Different technologies for more advanced waste heat recovery are developing and this report is evaluating a demonstration plant for “hygro­scopic condenser”, which uses a hygroscopic solution that allows condensation above the dew point. The hygro­scopic solution is potassium formate, which enables condensation to start about 20 ̊ C above the dew point and is sufficiently non-toxic and non-corrosive.</p><p>The objective of this work is to evaluate both the equipment and the process during some initial tests at the paper mill at Holmen, Braviken. The aim is also to suggest improvements of the process, the components and the additional equipment for future continuous operation. </p><p>The equipment consists of two main parts:  a hygroscopic absorption stage and a regene­ration stage. The major part of the humid air is led into an absorption column where vapor is absorbed by the formate solution and rises its temperature. This recovered waste heat is transferred to the heating net by a plate heat exchanger. The regeneration unit is used to maintain the hygroscopic concentrations by evaporation of vapor from the formate solution (amount of absorbed vapor = amount of evaporated vapor). The regene­rator is driven by process steam from the existing 3.5 bar net. The evaporated vapor is led to a conventional condenser where the regeneration energy can be recovered and the condensate is bled off.  The recovered heat from the process (hygroscopic absorber + regeneration condenser) is used in the heating net at Holmen, Braviken (VVG-net).   </p><p>The initial tests have been made during 15 hours of initial operation, when the equipment has delivered about 3 MWh in total. The tests show a good temperature performance since the dew point of the humid air has been lowered from about 60<sup>o</sup>C to 47<sup>o</sup>C. The reco­vered heat was used for heating from 65<sup>o</sup>C to about 80<sup>o</sup>C. During the initial tests the capacity has not yet reached the design values. As an example the delivered heat was measured to 280 kW where­of 46kW from absorbed vapor, 129 kW from the sensible heat in the incoming humid air and 105 kW from the regeneration. The bottle-neck parts of the equipment have been localized and will be overseen during the summer of 2011 and the process is planned to be in use during the autumn with an output capacity of 500kW. The coefficient of performance (COP) is calculated to just below 2 during normal operation but was about 2.7 in the test runs due to the high portion of sensible heat.</p><p>[1] <em>Förekomst av industriellt spillvärme vid låga temperaturer,  Ingrid Nyström, Per-Åke Franck, Industriell Energianalys AB, 2002-04-15</em></p>

w='hygro\xadscopic' val={'c': 'hygroscopic', 's': 'diva2:460449'}
w='profit\xadable' val={'c': 'profitable', 's': 'diva2:460449', 'n': 'correct in original'}
w='regene\xadration' val={'c': 'regeneration', 's': 'diva2:460449'}
w='regene\xadrator' val={'c': 'regenerator', 's': 'diva2:460449'}
w='where\xadof' val={'c': 'whereof', 's': 'diva2:460449'}
w='reco\xadvered' val={'c': 'recovered', 's': 'diva2:460449'}
w='How\xadever' val={'c': 'However', 's': 'diva2:460449'}

corrected abstract:
<p>In Sweden the industries releases about 50 TWh /year of low temperature waste heat<sup>1</sup>, often in the form of humid air flows. Today, conventional flue gas condensation is only exploiting a minor part of the energy from these flows. It is a well-established and profitable way of improving the efficiency of district heating plants and other boilers for wet fuels. However, the condensation is only applicable when the dew point of the flue gas is above the temperature demand for the heating net. The paper industry gives a good illustration of the limitations for conventional condensation: several MW of wet air streams with dew points of 60-65 ̊°C are released but cannot be recovered since the temperature demand is 70-80 ̊°C for the heating net. Different technologies for more advanced waste heat recovery are developing and this report is evaluating a demonstration plant for “hygroscopic condenser”, which uses a hygroscopic solution that allows condensation above the dew point. The hygroscopic solution is potassium formate, which enables condensation to start about 20 ̊°C above the dew point and is sufficiently non-toxic and non-corrosive.</p><p>The objective of this work is to evaluate both the equipment and the process during some initial tests at the paper mill at Holmen, Braviken. The aim is also to suggest improvements of the process, the components and the additional equipment for future continuous operation.</p><p>The equipment consists of two main parts: a hygroscopic absorption stage and a regeneration stage. The major part of the humid air is led into an absorption column where vapor is absorbed by the formate solution and rises its temperature. This recovered waste heat is transferred to the heating net by a plate heat exchanger. The regeneration unit is used to maintain the hygroscopic concentrations by evaporation of vapor from the formate solution (amount of absorbed vapor = amount of evaporated vapor). The regenerator is driven by process steam from the existing 3.5 bar net. The evaporated vapor is led to a conventional condenser where the regeneration energy can be recovered and the condensate is bled off.  The recovered heat from the process (hygroscopic absorber + regeneration condenser) is used in the heating net at Holmen, Braviken (VVG-net).</p><p>The initial tests have been made during 15 hours of initial operation, when the equipment has delivered about 3 MWh in total. The tests show a good temperature performance since the dew point of the humid air has been lowered from about 60°C to 47°C. The recovered heat was used for heating from 65°C to about 80°C. During the initial tests the capacity has not yet reached the design values. As an example the delivered heat was measured to 280 kW whereof 46kW from absorbed vapor, 129 kW from the sensible heat in the incoming humid air and 105 kW from the regeneration. The bottle-neck parts of the equipment have been localized and will be overseen during the summer of 2011 and the process is planned to be in use during the autumn with an output capacity of 500kW. The coefficient of performance (COP) is calculated to just below 2 during normal operation but was about 2.7 in the test runs due to the high portion of sensible heat.</p><p><sup>1</sup> <em>Förekomst av industriellt spillvärme vid låga temperaturer,  Ingrid Nyström, Per-Åke Franck, Industriell Energianalys AB, 2002-04-15</em></p>
----------------------------------------------------------------------
In diva2:744738 abstract is: <p>Sanfilippo syndrome is a lycosomal storage disease and a third type of mucopolysaccharidosis (MPS III type) caused by the deficiency of three lysosomal enzymes. Sulfamidase (SGSH) is involved in the degradation of glycosaminoglycan heparan sulphate. The defect of the enzyme leads to accumulation of heparan sulphate and MPS IIIA; a disease where the central nervous system (CNS) is damaged. The involvement of CNS has obstructed the development of an effective treatment. An approach of treatment is enzyme replacement therapy (ERT); already established for efficient treatment of many other lysosomal storage diseases. The challenge is the cross-over of protein across the blood-brain barrier (BBB) and avoidance of rapid clearance of the protein by renal and hepatic elimination and also the decrease of enzyme endocytosis through receptors involved in the uptake of the protein. A potential way to come across these difficulties is by extending the serum jalf-life of SGSH Such effect would potentially reduce clearance rates in peripheral tissues while still mediate a desired receptor-mediated uptake in cells on the other side of the BBB.</p><p>In this study four recombinant variants of SGSH have been constructed, where the protein is attached to four lengths of XTEN; a hydrophilic and unstructured ciopolymer developed by Amunix to extend the serum half-lives of drugs. The constructs have been expressed in eukaryotic cells and purified. Characterization of SGSH-XTEN variant's propensity for cellularuptake and potency of the proteins have been investigated using already established in vitro assays. Experimental procedures have been performed in order to analyze the uptake of the recombinant proteins in mouse fibroblasts (MEF-1 cells) and protein distribution in MPS IIIA patient cells.</p><p>The attachment of longer XTEN polymers to the C-terminal of SGSH appear to enhance the protein expression in HEK-293 Freestyle cells. All four constructs had a low uptake in MEF-1 cells, indicating that XTEN greatly affects and modulates the cellular uptake. However only the variants of SGSH with shorter length of XTEN appeared to have a significant uptake compared to the background/control. The uptake of these variants differed in MPS IIIA fibroblasts, where their uptake were above of what was expected. This is probably due to higher expression of mannose-6-phosphate receptor. Based on the results, the SGSH constructs with longer XTEN should be further assessed. The ultimate evaluation of the usefulness of the polypeptide fused to the C-terminal of SGSH is to perform a pharmacokinetic study <em>in vivo</em>.</p>

w='ciopolymer' val={'c': 'copolymer', 's': 'diva2:744738', 'n': 'no full text'}
w='jalf-life' val={'c': 'half-life', 's': 'diva2:744738', 'n': 'no full text'}
w='lycosomal' val={'c': 'lysosomal', 's': 'diva2:744738', 'n': 'no full text'}
w='Freestyle' val={'c': 'FreeStyle™', 's': 'diva2:744738', 'n': 'no full text - abstract missting trademark'}

corrected abstract:
<p>Sanfilippo syndrome is a lysosomal storage disease and a third type of mucopolysaccharidosis (MPS III type) caused by the deficiency of three lysosomal enzymes. Sulfamidase (SGSH) is involved in the degradation of glycosaminoglycan heparan sulphate. The defect of the enzyme leads to accumulation of heparan sulphate and MPS IIIA; a disease where the central nervous system (CNS) is damaged. The involvement of CNS has obstructed the development of an effective treatment. An approach of treatment is enzyme replacement therapy (ERT); already established for efficient treatment of many other lysosomal storage diseases. The challenge is the cross-over of protein across the blood-brain barrier (BBB) and avoidance of rapid clearance of the protein by renal and hepatic elimination and also the decrease of enzyme endocytosis through receptors involved in the uptake of the protein. A potential way to come across these difficulties is by extending the serum half-life of SGSH Such effect would potentially reduce clearance rates in peripheral tissues while still mediate a desired receptor-mediated uptake in cells on the other side of the BBB.</p><p>In this study four recombinant variants of SGSH have been constructed, where the protein is attached to four lengths of XTEN; a hydrophilic and unstructured copolymer developed by Amunix to extend the serum half-lives of drugs. The constructs have been expressed in eukaryotic cells and purified. Characterization of SGSH-XTEN variant's propensity for cellular uptake and potency of the proteins have been investigated using already established in vitro assays. Experimental procedures have been performed in order to analyze the uptake of the recombinant proteins in mouse fibroblasts (MEF-1 cells) and protein distribution in MPS IIIA patient cells.</p><p>The attachment of longer XTEN polymers to the C-terminal of SGSH appear to enhance the protein expression in HEK-293 FreeStyle™ cells. All four constructs had a low uptake in MEF-1 cells, indicating that XTEN greatly affects and modulates the cellular uptake. However only the variants of SGSH with shorter length of XTEN appeared to have a significant uptake compared to the background/control. The uptake of these variants differed in MPS IIIA fibroblasts, where their uptake were above of what was expected. This is probably due to higher expression of mannose-6-phosphate receptor. Based on the results, the SGSH constructs with longer XTEN should be further assessed. The ultimate evaluation of the usefulness of the polypeptide fused to the C-terminal of SGSH is to perform a pharmacokinetic study <em>in vivo</em>.</p>
----------------------------------------------------------------------
In diva2:1809085 abstract is: <p>Nanoparticles (NPs) offer unique possibilities for medical applications, including the controlled release of cancer drugs, the use as imaging contrast during imaging procedures or the hyperthermic treatment of cancer cells. Flash nanoprecipitation (FNP) produces NPs to combine these applications in a fast, cheap, and scalable coating process. The use of FNP with a Multi-Inlet Vortex Mixer (MIVM) is a promising method to easily coat hydrophobic oleic acid iron oxide NPs (IONPs) with various biocompatible block-copolymers. Amphiphilic block-copolymers based on hydrophilic polyethylene glycol (PEG) and hydrophobic poly(lactic acid) (PLA), poly(lactic-co-glycolic acid) (PLGA) or poly(caprolactone) (PCL) were successfully synthesized. The organic catalyst 1,8-diazabicyclo[5.4.0]undec-7-ene (DBU) was used to increase biocompatibility of the resulting polymers PEG-PLA, PEG-PL7.5KG2.5KA and PEG2K-PCL2K. The synthesis of hydroxyl terminated poly(acrylic acid) (PAA-OH) followed by the polymerization with PLGA was attempted. The amphiphilic block-copolymers were used in combination with the stabilizer polysorbate 80 (Tween80®) in FNP to form bare polymeric NPs using a MIVM as the reactor. DLS and STEM confirmed particle sizes between 50 - 100 nm. The addition of 13 ± 2 nm hydrophobic oleic acid IONPs yielded an increase in particle size as well as increase in particle stability over time. STEM images showed attachment of single IONPs to the outside of the polymeric NPs. Hydrophobic interactions between the polymer and oleic acid IONPs are possible. To achieve encapsulation of the oleic acid IONPs, adjustments to the process parameters of FNP should be considered in future research. Additional experiments are required to explore possible drug addition, release mechanisms and hyperthermia behavior of the polymer coated IONPs particles.</p>

w='PEG-PL7.5KG2.5KA' val={'c': 'PEG-PL<sub>7.5K</sub>G<sub>2.5K</sub>A', 's': 'diva2:1809085', 'n': 'correct in original'}
w='PEG2K-PCL2K' val={'c': 'PEG<sub>2K</sub>-PCL<sub>2Kz/sub>', 's': 'diva2:1809085', 'n': 'correct in original'}


The DiVA abstract does not match the content of the abstract in the thesis. I've included the abstract in the thesis below.

corrected abstract:
<p>Nanoparticles (NPs) offer unique possibilities for medical applications, including the controlled release of cancer drugs, the use as imaging contrast during imaging procedures or the hyperthermic treatment of cancer cells. Flash nanoprecipitation (FNP) produces NPs to combine these applications in a fast, cheap, and scalable coating process. Until now, FNP was successfully used to encapsulate hydrophobic, organic anti-cancer drugs with blockcopolymers [1, 2]. The combination of hydrophobic oleic acid iron oxide NPs (IONPs) with amphiphilic block-copolymers offers promising theranostic abilities when modified with targeting ligands [3]. The use of FNP with a Multi-Inlet Vortex Mixer (MIVM) is a promising method to easily coat IONPs with block-copolymers. The FNP coating process needs yet to be tested and understood for various biocompatible block-copolymers (Figure 1).</p><p>Amphiphilic block-copolymers based on hydrophilic polyethylene glycol (PEG) and hydrophobic poly(lactic acid) (PLA), poly(lactic-co-glycolic acid) (PLGA) or poly(caprolactone) (PCL) were successfully synthesized. The organic catalyst 1,8-diazabicyclo[5.4.0]undec-7-ene (DBU) was used to increase biocompatibility of the resulting polymers PEG-PLA, PEG-PL<sub>7.5K</sub>G<sub>2.5K</sub>A and PEG<sub>2K</sub>-PCL<sub>2K</sub>. The synthesis of hydroxyl terminated poly(acrylic acid) (PAA-OH) followed by the polymerization with PLGA was attempted.</p><p>The amphiphilic block-copolymers were used in combination with the stabilizer polysorbate 80 (Tween80®) in FNP to form bare polymeric NPs using a MIVM as the reactor. DLS and STEM confirmed particle sizes between 50 - 100 nm. The addition of 13 ± 2 nm hydrophobic oleic acid IONPs yielded an increase in particle size as well as increase in particle stability over time. STEM images showed attachment of single IONPs to the outside of the polymeric NPs. Hydrophobic interactions between the polymer and oleic acid IONPs are possible.</p><p>To achieve encapsulation of the oleic acid IONPs, adjustments to the process parameters of FNP should be considered in future research. Additional experiments are required to explore possible drug addition, release mechanisms and hyperthermia behavior of the polymer coated IONPs particles.</p>

<!-- insert Figure 1 - available as CBH-thesis-1809085-figure1.png -->

<p><strong>Figure 1:</strong> Graphical Abstract: Synthesis of biocompatible block-copolymers followed by the
fabrication of polymeric nanoparticles and addition of hydrophobic oleic acid IONPs through FNP.</p>
----------------------------------------------------------------------
In diva2:854043 abstract is: <p>The presence of cell-free DNA (cfDNA) in blood has been known for some time but it was just recently that certain applications for it materialized. First it was shown that elevated levels of cell-free DNA in cancer patients had the potential of serving as a biomarker and later, fetal DNA was found in cfDNA in maternal blood and could therefore be used for non-invasive prenatal diagnostics. Several other diseases such a s heart attack and tissue injures after stroke and trauma have also showed increased levels of cfDNA. The concentration of cfDNA in blood is often low and varies between individuals, which requires reliable and efficient methods for isolation of the DNA before it can be used for diagnosis. Three isolation methods were evaluated in this work and compared in terms of yield and the potential of becoming automated. All methods were based on the same principle where cfDNA binds to a solid support, either a filter or magnetic beads, when right bindingconditions are obtained. We found that all three methods were capable of isolating cfDNA with comparable yields. In addition, although all methods use Proteinase K in the original lysis step, we found thta one of the methods managed without Proteinase K, which in combination with the discovery that the same method also worked with plasma centrifuged only once (instead of tow times that is standard). These findings are promising when considering an automated cfDNA isolation procedure, as plasma transfer steps and enzymatic incubations are eliminated.</p>

w='thta' val={'c': 'that', 's': 'diva2:854043', 'n': 'no full text'}

corrected abstract:
<p>The presence of cell-free DNA (cfDNA) in blood has been known for some time but it was just recently that certain applications for it materialized. First it was shown that elevated levels of cell-free DNA in cancer patients had the potential of serving as a biomarker and later, fetal DNA was found in cfDNA in maternal blood and could therefore be used for non-invasive prenatal diagnostics. Several other diseases such a s heart attack and tissue injures after stroke and trauma have also showed increased levels of cfDNA. The concentration of cfDNA in blood is often low and varies between individuals, which requires reliable and efficient methods for isolation of the DNA before it can be used for diagnosis. Three isolation methods were evaluated in this work and compared in terms of yield and the potential of becoming automated. All methods were based on the same principle where cfDNA binds to a solid support, either a filter or magnetic beads, when right binding conditions are obtained. We found that all three methods were capable of isolating cfDNA with comparable yields. In addition, although all methods use Proteinase K in the original lysis step, we found that one of the methods managed without Proteinase K, which in combination with the discovery that the same method also worked with plasma centrifuged only once (instead of tow times that is standard). These findings are promising when considering an automated cfDNA isolation procedure, as plasma transfer steps and enzymatic incubations are eliminated.</p>
----------------------------------------------------------------------
In diva2:1096827 abstract is: <p>The aim of the study is to evaluate the ability of non-noble metal catalysts to function as the commercially used noble metal catalyst. The exhaust gas that was used in the project is generated from a heater developed by ReformTech AB with diesel as fuel. The compound that was focused on is carbon monoxide that has a concentration of 300-750 ppm. The catalysts that were tested are MnO/CeO<sub>2</sub>, CuO/CeO<sub>2</sub> and a Pt/CeO<sub>2</sub> catalyst used to compare the non-noble metal catalyst with. The sensitivity against sulfur poisoning was also analyzed by mixing sulfur into the fuel. Analysis of the exhaust gas was done with a micro-GC and the catalysts were also analyzed with SEM before and after exposure of sulfur.</p><p> </p><p>The manganese catalyst with a loading of 7 wt-% did not show any activity against carbon monoxide oxidation. The copper catalysts contained two different loadings of active material, 7 and 14 wt-% and monoliths with 400 and 600 cpsi were used. Both loadings showed good activity against carbon monoxide oxidation.</p><p> </p><p>The most prominent catalyst was the 14 wt-% CuO/CeO<sub>2</sub> catalyst with a 600 cpsi monolith because of an increase in surface area. The SEM analysis showed that sulfur was present on the surface when the heater was using diesel with 300 ppm sulfur. The sulfur caused complete deactivation of the non-noble metal catalysts and a small decrease in activity was shown on the noble metal Pt catalyst.</p>


w='MnO/CeO' val={'c': 'MnO/CeO<sub>2</sub>', 's': 'diva2:1096827', 'n': 'correct in original'}
w='CuO/CeO' val={'c': 'CuO/CeO<sub>2</sub>', 's': 'diva2:1096827', 'n': 'correct in original'}
w='Pt/CeO' val={'c': 'Pt/CeO<sub>2</sub>', 's': 'diva2:1096827', 'n': 'correct in original'}

Note that "aginast" is speed correctly and incorrectly in the 2nd paragraph.

corrected abstract:
<p>The aim of the study is to evaluate the ability of non-noble metal catalysts to function as the commercially used noble metal catalyst. The exhaust gas that was used in the project is generated from a heater developed by ReformTech with diesel as fuel. The compound that was focused on is carbon monoxide that has a concentration of 300-750 ppm. The catalysts that were tested are MnO/CeO<sub>2</sub>, CuO/CeO<sub>2</sub> and a Pt/CeO<sub>2</sub> catalyst used to compare the non-noble metal catalyst with. The sensitivity against sulfur poisoning was also analysed by mixing sulfur into the fuel. Analysis of the exhaust gas was done with a micro-GC and the catalysts were also analysed with SEM before and after exposure of sulfur.</p><p>The manganese catalyst with a loading of 7 wt-% did not show any activity against carbon monoxide oxidation. The copper catalysts contained two different loadings of active material, 7 and 14 wt-% and monoliths with 400 and 600 cpsi were used. Both loadings showed good activity agains carbon monoxide oxidation.</p><p>The most prominent catalyst was the 14 wt-% CuO/CeO<sub>2</sub> catalyst with a 600 cpsi monolith because of an increase in surface area. The SEM analysis showed that sulfur was present on the surface when the heater was using diesel with 300 ppm sulfur. The sulfur caused complete deactivation of the non-noble metal catalysts and a small decrease in activity was shown on the noble metal Pt catalyst.</p>
----------------------------------------------------------------------
In diva2:1322207 abstract is: <p>The hierarchal distributed database, Domain Name System (DNS) provides name resolution for network applications, which is vital for the functionality of the Internet. An important part of the system design is to distribute the administration by delegation of domains. To maintain the availability of domains it is important that configuration is accurately per-formed. Zonemaster is a tool used for validating the quality of a DNS delegation by testing of misconfigurations. This thesis evaluates Zonemaster and how it relates to other DNS tools. The evaluation was conducted by identifying and designing configuration errors re-garding DNS delegation. These errors were tested by each tool. Their responses together with the extent of documentation regarding test specification was analyzed. The analyzed data was then divided into three categories: number of implemented tests, detected errors and if the test specification referred to a reliable source. The results showed that Zonemas-ter had all the tests implemented, detected and referred to 91,7% of them.</p>


w='Zonemas-ter' val={'c': 'Zonemaster', 's': 'diva2:1322207'}

corrected abstract:
<p>The hierarchal distributed database, Domain Name System (DNS) provides name resolution for network applications, which is vital for the functionality of the Internet. An important part of the system design is to distribute the administration by delegation of domains. To maintain the availability of domains it is important that configuration is accurately performed. Zonemaster is a tool used for validating the quality of a DNS delegation by testing of misconfigurations. This thesis evaluates Zonemaster and how it relates to other DNS tools. The evaluation was conducted by identifying and designing configuration errors regarding DNS delegation. These errors were tested by each tool. Their responses together with the extent of documentation regarding test specification was analyzed. The analyzed data was then divided into three categories: number of implemented tests, detected errors and if the test specification referred to a reliable source. The results showed that Zonemaster had all the tests implemented, detected and referred to 91,7% of them.</p>
----------------------------------------------------------------------
In diva2:1148715 abstract is: <p>In the face of global warming and shrinking resources of fossil fuels</p><p>the interest in solar energy has increased in recent years. However,</p><p>the low energy and cost efficiency of current solar cells has up to</p><p>this date hindered solar energy from playing a major role on the</p><p>energy market. Photon upconversion is the process in which light</p><p>of low energy is converted to high energy photons. Lately, this</p><p>phenomenon has attracted renewed interest and ongoing research</p><p>in this field mainly focuses on solar energy applications, solar cells</p><p>in particular. The aim of this study was to investigate and evaluate</p><p>amyloid fibrils as nanotemplates for an upconversion system</p><p>based on the dyes platinum octaetylporphyrin (PtOEP) and 9,10-</p><p>diphenylanthracene (DPA). This well-known pair of organic dyes</p><p>upconverts light in the visible spectrum through a mechanism</p><p>known as sensitized triplet-triplet annihilation. Amyloid fibrils</p><p>are β-sheet rich protein fibril structures, formed by self-assembly</p><p>of peptides.</p><p>Amyloid fibrils were prepared from whey protein isolate using heat</p><p>and acidic solutions. Dyes were incorporated according to a wellestablished</p><p>technique, in which dyes are grinded together with the</p><p>protein in solid state prior to fibrillization. Photophysical properties</p><p>of pure fibrils and dye-incorporated fibrils were studied using</p><p>UV-VIS spectroscopy and fluorescence spectroscopy. Atomic force</p><p>microscopy was further employed to confirm the presence of amyloid</p><p>fibrils as well as to study fibril structure. Results indicate</p><p>that amyloid fibrils may not be the optimal host material for the</p><p>upconversion system PtOEP/DPA. It was found that the absorption</p><p>and emission spectra of this system overlap to a great deal</p><p>with that of the fibrils. Though no upconverted emission clearly</p><p>generated by the dye system was recorded, anti-Stokes emission</p><p>was indeed observed. Interestingly, this emission appears to be</p><p>strongly enhanced by the presence of dyes. It is suggested that</p><p>this emission may be attributed to the protein residues rather than</p><p>the amyloid structure. Future studies are encouraged to further</p><p>investigate these remarkable findings.</p>

w='octaetylporphyrin' val={'c': 'octaethylporphyrin', 's': 'diva2:1148715', 'n': 'error in original'}

corrected abstract:
<p>In the face of global warming and shrinking resources of fossil fuels the interest in solar energy has increased in recent years. However, the low energy and cost efficiency of current solar cells has up to this date hindered solar energy from playing a major role on the energy market. Photon upconversion is the process in which light of low energy is converted to high energy photons. Lately, this phenomenon has attracted renewed interest and ongoing research in this field mainly focuses on solar energy applications, solar cells in particular. The aim of this study was to investigate and evaluate amyloid fibrils as nanotemplates for an upconversion system based on the dyes platinum octaetylporphyrin (PtOEP) and 9,10- diphenylanthracene (DPA). This well-known pair of organic dyes upconverts light in the visible spectrum through a mechanism known as sensitized triplet-triplet annihilation. Amyloid fibrils are β-sheet rich protein fibril structures, formed by self-assembly of peptides.</p><p>Amyloid fibrils were prepared from whey protein isolate using heat and acidic solutions. Dyes were incorporated according to a well established technique, in which dyes are grinded together with the protein in solid state prior to fibrillization. Photophysical properties of pure fibrils and dye-incorporated fibrils were studied using UV-VIS spectroscopy and fluorescence spectroscopy. Atomic force microscopy was further employed to confirm the presence of amyloid fibrils as well as to study fibril structure. Results indicate that amyloid fibrils may not be the optimal host material for the upconversion system PtOEP/DPA. It was found that the absorption and emission spectra of this system overlap to a great deal with that of the fibrils. Though no upconverted emission clearly generated by the dye system was recorded, anti-Stokes emission was indeed observed. Interestingly, this emission appears to be strongly enhanced by the presence of dyes. It is suggested that this emission may be attributed to the protein residues rather than the amyloid structure. Future studies are encouraged to further investigate these remarkable findings.</p>
----------------------------------------------------------------------
In diva2:1303137 abstract is: <p>Video is a large part of today’s society where surveillance cameras represent the biggest source of big data, and real-time entertainment is the largest network traffic category. There is currently a large interest in analysing the contents of video where video analysis is mainly conducted by people. This increase in video has for instance made it difficult for professional editors to analyse movies and series in a scalable way, and alternative solutions are needed. The media technology company June, want to explore scalable alternatives for extracting metadata from video. With recent advances in Machine Learning and the rise of machine-learning-asa-service platforms, June wished more specifically to explore how these Machine Learning services can be utilised for extracting metadata from videos, and from it construct a summary regarding its contents. This work examined Machine Learning as an option for scalable video summarisation which resulted in developing and evaluating an application that utilised transcription, summarisation, and translation services to produce a text based summarisation of video. Furthermore to examine the services current state of affairs, multiple services from different providers were tested, evaluated and compared to each other. Lastly, in order to evaluate the summarisation services an evaluation model was developed. The test results showed that the translation services were the only service that produced good results. Transcription and summarisation performed poorly in the tests which renders the suggested solution of combining the three services for video summarisation as impractical.</p>

w='machine-learning-asa-service' val={'c': 'machine-learning-as-a-service', 's': 'diva2:1303137', 'n': 'hyphen at end of line in original'}
Note: also separated the merged paragraphs.

corrected abstract:
<p>Video is a large part of today’s society where surveillance cameras represent the biggest source of big data, and real-time entertainment is the largest network traffic category. There is currently a large interest in analysing the contents of video where video analysis is mainly conducted by people. This increase in video has for instance made it difficult for professional editors to analyse movies and series in a scalable way, and alternative solutions are needed.</p><p>The media technology company June, want to explore scalable alternatives for extracting metadata from video. With recent advances in Machine Learning and the rise of machine-learning-as-a-service platforms, June wished more specifically to explore how these Machine Learning services can be utilised for extracting metadata from videos, and from it construct a summary regarding its contents.</p><p>This work examined Machine Learning as an option for scalable video summarisation which resulted in developing and evaluating an application that utilised transcription, summarisation, and translation services to produce a text based summarisation of video. Furthermore to examine the services current state of affairs, multiple services from different providers were tested, evaluated and compared to each other. Lastly, in order to evaluate the summarisation services an evaluation model was developed.</p><p>The test results showed that the translation services were the only service that produced good results. Transcription and summarisation performed poorly in the tests which renders the suggested solution of combining the three services for video summarisation as impractical.</p>
----------------------------------------------------------------------
In diva2:1875773 abstract is: <p>Adipose tissue is a critical regulator of metabolism, exhibiting a complex cellular architecture that influences various physiological and pathological processes. Its heterogeneous nature is relatively unstructured, mainly formed by fragile fatty adipocytes and immune cells. These intricacies complicate the study of its microarchitecture – crucial for understanding its behaviour – which has recently benefitted from spatially resolved technologies, that enable the study of genomic profiles while keeping the information from the tissue.</p><p>This work explores the chromatin dynamics of adipose tissue using the newly developed Spatial Assay for Transposase-Accessible Chromatin with high throughput sequencing (Spatial ATAC-seq). Focusing on subcutaneous white adipose tissue, samples were collected from an individual suffering from obesity before and five years after bariatric surgery to study changes associated with significant weight loss. The study comprises details for both experimental protocols and advanced computational tools for data analysis, including the use of a development version of Semla package to integrate spatial and chromatin accessibility data. The analysis revealed a diverse cellular architecture and distinct genomic features across the tissue, highlighting the presence of specific cell types such as <em>Adipo<sup>LEP</sup></em>-like adipocytes and infiltrating immune cells. This study demonstrated the feasibility of applying Spatial ATAC-seq in investigating the molecular mechanisms of adipose tissue underlying metabolic health and disease, particularly in the context of obesity and weight loss.</p>

w='Adipo' val={'c': 'Adipo<sup>LEP</sup>', 's': 'diva2:1875773'}

corrected abstract:
<p>Adipose tissue is a critical regulator of metabolism, exhibiting a complex cellular architecture that influences various physiological and pathological processes. Its heterogeneous nature is relatively unstructured, mainly formed by fragile fatty adipocytes and immune cells. These intricacies complicate the study of its microarchitecture – crucial for understanding its behaviour – which has recently benefitted from spatially resolved technologies, that enable the study of genomic profiles while keeping the information from the tissue.</p><p>This work explores the chromatin dynamics of adipose tissue using the newly developed Spatial Assay for Transposase-Accessible Chromatin with high throughput sequencing (Spatial ATAC-seq). Focusing on subcutaneous white adipose tissue, samples were collected from an individual suffering from obesity before and five years after bariatric surgery to study changes associated with significant weight loss. The study comprises details for both experimental protocols and advanced computational tools for data analysis, including the use of a development version of Semla package to integrate spatial and chromatin accessibility data. The analysis revealed a diverse cellular architecture and distinct genomic features across the tissue, highlighting the presence of specific cell types such as <em>Adipo<sup>LEP</sup></em>-like adipocytes and infiltrating immune cells. This study demonstrated the feasibility of applying Spatial ATAC-seq in investigating the molecular mechanisms of adipose tissue underlying metabolic health and disease, particularly in the context of obesity and weight loss.</p>
----------------------------------------------------------------------
In diva2:625004 abstract is: <p><strong>Presentation and aim</strong><strong>:</strong> Brominated flame retardants (BFRs) are used in many materials and products. Polybrominated diphenyl ethers (PBDEs) are the most well known and because of their structures, similar to that of PCB, they have been proposed to possibly affect health and environment. Occupational exposure has been an issue for years and this study investigated BRF in aircraft mainte­nance. The aim of this study was to assess the presence and levels of BRFs in dust, air and blood (serum) and compare the results with different references.</p><p><strong>Method</strong><strong>:</strong> Dust and air were sampled in different phases of the work with the air­craft and analysis of a spectrum of BFR was performed. In addition, serum from employees was sampled and analyzed for the presence of PBDEs.</p><p><strong>Result</strong><strong>:</strong> PBDEs and other BFRs were found in dust and air samples at high con­centrations. Serum concentrations in the technicians were slightly higher to those observed in the general Swedish population and in the same magnitude as some highly exposed occupational references. Years of work, age or time spent in the aircraft before giving blood seems not to have an impact on the level of BFRs in serum.</p><p><strong>Conclusion:</strong> This study shows high concentrations of BFR in dust and air in the aircraft and the concentrations of PBDEs in serum were high compared to most reference groups. To minimize the risk for exposure, it is important to focus on industrial hygiene improvements to minimize the amount of dust in working areas. The personal hygiene and use of personal safety equipment can also be improved and the routines should be described and included in the appropriate documentation regarding work environment.</p>

w='air\xadcraft' val={'c': 'aircraft', 's': 'diva2:625004'}
w='con\xadcentrations' val={'c': 'concentrations', 's': 'diva2:625004'}
w='mainte\xadnance' val={'c': 'maintenance', 's': 'diva2:625004', 'n': 'hyphen at end of line in original'}
w='BRF' val={'c': 'BFR', 's': 'diva2:625004', 'n': 'error in original - it should be referring to Brominated flame retardant'}
w='BRFs' val={'c': 'BFRs', 's': 'diva2:625004', 'n': 'error in original - it should be referring to Brominated flame retardant'}

corrected abstract:
<p><strong>Presentation and aim:</strong> Brominated flame retardants (BFRs) are used in many materials and products. Polybrominated diphenyl ethers (PBDEs) are the most well known and because of their structures, similar to that of PCB, they have been proposed to possibly affect health and environment. Occupational exposure has been an issue for years and this study investigated BRF in aircraft maintenance. The aim of this study was to assess the presence and levels of BRFs in dust, air and blood (serum) and compare the results with different references.</p><p><strong>Method:</strong> Dust and air were sampled in different phases of the work with the aircraft and analysis of a spectrum of BFR was performed. In addition, serum from employees was sampled and analyzed for the presence of PBDEs.</p><p><strong>Result:</strong> PBDEs and other BFRs were found in dust and air samples at high concentrations. Serum concentrations in the technicians were slightly higher to those observed in the general Swedish population and in the same magnitude as some highly exposed occupational references. Years of work, age or time spent in the aircraft before giving blood seems not to have an impact on the level of BFRs in serum.</p><p><strong>Conclusion:</strong> This study shows high concentrations of BFR in dust and air in the aircraft and the concentrations of PBDEs in serum were high compared to most reference groups. To minimize the risk for exposure, it is important to focus on industrial hygiene improvements to minimize the amount of dust in working areas. The personal hygiene and use of personal safety equipment can also be improved and the routines should be described and included in the appropriate documentation regarding work environment.</p>
----------------------------------------------------------------------
In diva2:1876708 abstract is: <p>Extracellular vesicles (EVs) serve as vital mediators of intercellular communication, shuttling essential cargo such as proteins, metabolites, and nucleic acids between cells. While EVs can interact with recipient cells through various uptake mechanisms, receptor-mediated endocytosis emerges as a critical pathway, involving the membrane proteins tetraspanins, and most notably CD63. To harness the therapeutic potential of EVs, this study focuses on engineering EVs with nanobodies targeting the prostate- specific membrane antigen (PSMA), which is abundant in prostate cancer (PCa). By fusing nanobodies at different sites of CD63, this research aims to optimize EV uptake efficiency and specificity, particularly in PCa cells. Five nanobody constructs were newly designed through in silico cloning, including nanobodies A7 and PSMANb9, both of which have shown an affinity towards PSMA. Additionally, two nanobody constructs from previous studies were included. HEK293T cells were transfected to produce lentiviruses to then further infect the cell line COLO205 to secrete EVs. The vesicles were harvested and purified, by ultracentrifugation, and quantified using the high-throughput quantification EVQuant assay. The uptake of the vesicles was tested in two PSMA positive and two PSMA negative cell lines using flow cytometry. Transfection of HEK293T cells was successful for all the constructs, but infection of COLO205 cells was effective for only a few of them. The uptake experiments showed an increased uptake with EVs tagged with nanobody A7 in PSMA positive cells, compared to EVs tagged with NoBi (nonspecific binding nanobody). This indicated that it is possible to target uptake with nanobody-modified EVs in PSMA-expressing cell lines.</p>


w='prostatespecific' val={'c': 'prostate-specific', 's': 'diva2:1876708', 'n': 'no full text'}
I think that "PSMANb9" should be "PSMA Nb9" - but would have to see the actual thesis to be sure.


corrected abstract:
<p>Extracellular vesicles (EVs) serve as vital mediators of intercellular communication, shuttling essential cargo such as proteins, metabolites, and nucleic acids between cells. While EVs can interact with recipient cells through various uptake mechanisms, receptor-mediated endocytosis emerges as a critical pathway, involving the membrane proteins tetraspanins, and most notably CD63. To harness the therapeutic potential of EVs, this study focuses on engineering EVs with nanobodies targeting the prostate- specific membrane antigen (PSMA), which is abundant in prostate cancer (PCa). By fusing nanobodies at different sites of CD63, this research aims to optimize EV uptake efficiency and specificity, particularly in PCa cells. Five nanobody constructs were newly designed through in silico cloning, including nanobodies A7 and PSMANb9, both of which have shown an affinity towards PSMA. Additionally, two nanobody constructs from previous studies were included. HEK293T cells were transfected to produce lentiviruses to then further infect the cell line COLO205 to secrete EVs. The vesicles were harvested and purified, by ultracentrifugation, and quantified using the high-throughput quantification EVQuant assay. The uptake of the vesicles was tested in two PSMA positive and two PSMA negative cell lines using flow cytometry. Transfection of HEK293T cells was successful for all the constructs, but infection of COLO205 cells was effective for only a few of them. The uptake experiments showed an increased uptake with EVs tagged with nanobody A7 in PSMA positive cells, compared to EVs tagged with NoBi (nonspecific binding nanobody). This indicated that it is possible to target uptake with nanobody-modified EVs in PSMA-expressing cell lines.</p>
----------------------------------------------------------------------
In diva2:1555594 abstract is: <p>The energy company Stockholm Exergy (SE) has set the goal of running a climate positive business by 2025. To meet the environmental goal, the company plans to build a BECCS plant (Bio Energy Carbon Capture and Storage) for the combined heat and power plant KVV8. The carbon dioxide of the plants flue gases will then be separated with HPC absorption (Hot Potassium Carbonates). The separated carbon dioxide is then to be liquefied, shipped and stored in a deep sea bottom. To liquefy the separated carbon dioxide a cooling plant is being built in connection to the BECCS facility. </p><p>The liquefaction plant is expected to not be operating between the summer months of June through August. Summer time is also the when the demand on district cooling is at its highest. To increase the redundancy of cooling capacity during high demand periods the possibility of using the liquefaction plant for district cooling production has been investigated. </p><p>The design of a liquefaction plant with the final conditions of 7 bar and -50 ̊C is yet to be fully developed. The study therefore investigates how three of the most researched liquefaction types could be used for district cooling production which is a CO2-NH3 cascade cycle (Case 1), an NH3 external cycle (Case 2) and a CO2 internal cycle (Case 3). The model for the cascade cycle is developed in by Alabdulkarem et al. (2012) as well as Dopazo and Fernández-Seara (2010). The models for the NH3 external and CO2 internal cooling cycles has been developed by Adhikari et al. (2014) and Øi et al. (2016). The liquefaction plants have been simulated in Chemcad with uniform process conditions as well as SE’s CO2 input and product conditions. Subsequently, a proposal on how each of the three cases can be used for district cooling production were developed. </p><p>For proposals on district cooling production for Case 1-3 are expected to be generating a cooling effect of 22.2, 15.6 and 13.1 MW. The COP for the cooling cycles was calculated to be 4.6, 5.8 and 4.1. </p><p>The investment capital is expected to be high, mainly as a result of piping and ground work for a seawater pipeline to supply the district cooling plant with cooling water. The total investment capital for Case 1, 2 and 3 were estimated to be approximately 52.7, 50.6 and 54.2 MSEK. The sea water pipeline accounts for almost half of the total investment capital. Since the investment capital has been reviewed at an early stage typical percentages such as unforeseen costs were set high for the project. </p><p>As an alternative to district cooling production where the liquefaction plant and the district cooling network are directly connected, an intermediate circuit has also been reviewed (Case 4). A proposal on how the intermediate circuit could be designed and dimensioned has been developed by Energy Engineering trainee Nasim Rafieyan (2020) under the supervision of Combustion Engineer Hans P. Larsson (SE). The intermediate circuit has been dimensioned using three different refrigerants; ethanol, methanol and a methanol/water solution. </p>

w='CO2-NH3' val={'c': 'CO<sub>2</sub>-NH<sub>3</sub>', 's': 'diva2:1555594', 'n': 'error in original'}
w='̊C' val={'c': '℃', 's': 'diva2:1555594'}

corrected abstract:
<p>The energy company Stockholm Exergy (SE) has set the goal of running a climate positive business by 2025. To meet the environmental goal, the company plans to build a BECCS plant (Bio Energy Carbon Capture and Storage) for the combined heat and power plant KVV8. The carbon dioxide of the plants flue gases will then be separated with HPC absorption (Hot Potassium Carbonates). The separated carbon dioxide is then to be liquefied, shipped and stored in a deep sea bottom. To liquefy the separated carbon dioxide a cooling plant is being built in connection to the BECCS facility.</p><p>The liquefaction plant is expected to not be operating between the summer months of June through August. Summer time is also the when the demand on district cooling is at its highest. To increase the redundancy of cooling capacity during high demand periods the possibility of using the liquefaction plant for district cooling production has been investigated.</p><p>The design of a liquefaction plant with the final conditions of 7 bar and -50 ℃ is yet to be fully developed. The study therefore investigates how three of the most researched liquefaction types could be used for district cooling production which is a CO2-NH3 cascade cycle (Case 1), an NH3 external cycle (Case 2) and a CO2 internal cycle (Case 3). The model for the cascade cycle is developed in by Alabdulkarem et al. (2012) as well as Dopazo and Fernández-Seara (2010). The models for the NH3 external and CO2 internal cooling cycles has been developed by Adhikari et al. (2014) and Øi et al. (2016). The liquefaction plants have been simulated in Chemcad with uniform process conditions as well as SE’s CO2 input and product conditions. Subsequently, a proposal on how each of the three cases can be used for district cooling production were developed.</p><p>For proposals on district cooling production for Case 1-3 are expected to be generating a cooling effect of 22.2, 15.6 and 13.1 MW. The COP for the cooling cycles was calculated to be 4.6, 5.8 and 4.1.</p><p>The investment capital is expected to be high, mainly as a result of piping and ground work for a seawater pipeline to supply the district cooling plant with cooling water. The total investment capital for Case 1, 2 and 3 were estimated to be approximately 52.7, 50.6 and 54.2 MSEK. The sea water pipeline accounts for almost half of the total investment capital. Since the investment capital has been reviewed at an early stage typical percentages such as unforeseen costs were set high for the project.</p><p>As an alternative to district cooling production where the liquefaction plant and the district cooling network are directly connected, an intermediate circuit has also been reviewed (Case 4). A proposal on how the intermediate circuit could be designed and dimensioned has been developed by Energy Engineering trainee Nasim Rafieyan (2020) under the supervision of Combustion Engineer Hans P. Larsson (SE). The intermediate circuit has been dimensioned using three different refrigerants; ethanol, methanol and a methanol/water solution.</p>
----------------------------------------------------------------------
In diva2:1154619 abstract is: <p>This thesis is a part of the D-factory project, which aims to develop a sustainable bio refinery from the carotenoid-rich microalgae <em>Dunaliella </em><em>salina.</em></p><p>An extract, containing approximately 30% carotenoids where the majority is -carotene, from the microalgae has been incorporated into o/w nanoemulsions and suspensions of nanostructured lipid carriers (NLC) , which are supposed to be administrated orally and topically respectively. The nanoemulsions are to be added as a nutritional concentrate in a smoothie, where one dose (approximately 3-SmL) contains a daily intake of -carotene with respect to its pro vitamin A activity, while the NLC suspensions are to be used in a treating skin cream.</p><p>2.5% <strong>(w/w) </strong>extract was successfully incorporated in the oil phase of nanoemulsions, based on medium chain triglycerides, MCT, in water with Lipoid LPC as emulsifier, that were physically stable at room temperature for over 4 weeks storage time . Two different emulsions  with  different oil compositions were chosen; one containing 10% (w/ w) oil and  one that contained 40% (w/w) oil. Nanostructured lipid carriers with a lipid matrix based on stearic acid and MCT and also carnauba wax and MCT, both with 2.5% (w/w) extract in the lipid phase, that were  stable  for more than 4 weeks of storage at  room temperature  were also developed.</p><p>Besides the formulation of the mentioned delivery systems, their stability  has  also  been evaluated, both with regards to their physical stability and also their ability to preserve the carotenoids in the extract from degradation, over  time  at  three  different  storage  conditions; room temperature, UV-exposure at room temperature and 40°C (protected from UV-light). No significant differences in particle size were seen in any of the delivery systems at room temperature, however a significant increase in particle size was observed for the nanostructured lipid carrier suspensions at 40°C. Out of the four delivery systems evaluated, the emulsion containing 40% <strong>(w/w) </strong>oil showed the best chemical stability at all storage conditions. The worst stability was noted at the suspensions, which is not in agreement with previous findings  that have shown  the opposite.</p><p> </p><p> </p>

w='emuls' val={'c': 'emulsifier', 's': 'diva2:1154619', 'n': 'no full text'}
w='ifie' val={'c': 'emulsifier', 's': 'diva2:1154619', 'n': 'no full text'}
w='triglycer' val={'c': 'triglycerides', 's': 'diva2:1154619', 'n': 'no full text'}

corrected abstract:
<p>This thesis is a part of the D-factory project, which aims to develop a sustainable bio refinery from the carotenoid-rich microalgae <em>Dunaliella salina.</em></p><p>An extract, containing approximately 30% carotenoids where the majority is -carotene, from the microalgae has been incorporated into o/w nanoemulsions and suspensions of nanostructured lipid carriers (NLC) , which are supposed to be administrated orally and topically respectively. The nanoemulsions are to be added as a nutritional concentrate in a smoothie, where one dose (approximately 3-SmL) contains a daily intake of -carotene with respect to its pro vitamin A activity, while the NLC suspensions are to be used in a treating skin cream.</p><p>2.5% <strong>(w/w) </strong>extract was successfully incorporated in the oil phase of nanoemulsions, based on medium chain triglycerides, MCT, in water with Lipoid LPC as emulsifier, that were physically stable at room temperature for over 4 weeks storage time . Two different emulsions with different oil compositions were chosen; one containing 10% (w/ w) oil and  one that contained 40% (w/w) oil. Nanostructured lipid carriers with a lipid matrix based on stearic acid and MCT and also carnauba wax and MCT, both with 2.5% (w/w) extract in the lipid phase, that were  stable  for more than 4 weeks of storage at  room temperature  were also developed.</p><p>Besides the formulation of the mentioned delivery systems, their stability  has  also  been evaluated, both with regards to their physical stability and also their ability to preserve the carotenoids in the extract from degradation, over  time  at  three  different  storage  conditions; room temperature, UV-exposure at room temperature and 40°C (protected from UV-light). No significant differences in particle size were seen in any of the delivery systems at room temperature, however a significant increase in particle size was observed for the nanostructured lipid carrier suspensions at 40°C. Out of the four delivery systems evaluated, the emulsion containing 40% <strong>(w/w)</strong> oil showed the best chemical stability at all storage conditions. The worst stability was noted at the suspensions, which is not in agreement with previous findings that have shown the opposite.</p>
----------------------------------------------------------------------
In diva2:855383 abstract is: <p>Due to the need to find alternatives for the current fossil-based chemical industry, the use of blo-sourced platform chemicals as building blocks for the synthesis of a wide range of industrial and consumer chemicals has gained significantly in importance over the past years.</p><p>In this context, 5-(chloromethyl)furfural (CMF) could become a very interesting target for  the implementation of a sugar-based biorefinery. Originating from the dehydration of cellulose,  hemlcellulose or sugar, it can for example be a precursor of bio-fuels and/or fuel additives. However, there are very few published examples on the CMF synthesis as well as its ability to be further converted in interesting derivatives.</p><p>A simple procedure for the conversion of sugar-based feedstocks to CMF, using flow chemistry, is reported in this  master  thesis.  Sucrose  and  High  Fructose  Corn  Syrup  (HFCS-90  and  HFCS-55)  were  shown  to  be suitable feedstocks. The use of HFCS-90 has been demonstrated to be particularly promising, as it could be converted in a stable process which yields 70% of CMF.</p><p>As a proof of concept, condensation reactions of 5-methylfurfural (MF) and CMF were performed with amine compounds, thus expanding the existing pool of CMF derivatives. MF condensations could be performed under harsh conditions, whereas CMF required milder treatments. Very high conversions were observed, especially when using aliphatic amines as starting materials.</p>

w='blo-sourced' val={'c': 'bio-sourced', 's': 'diva2:855383'}
w='hemlcellulose' val={'c': 'hemicellulose', 's': 'diva2:855383', 'n': 'no full text'}

corrected abstract:
<p>Due to the need to find alternatives for the current fossil-based chemical industry, the use of bio-sourced platform chemicals as building blocks for the synthesis of a wide range of industrial and consumer chemicals has gained significantly in importance over the past years.</p><p>In this context, 5-(chloromethyl)furfural (CMF) could become a very interesting target for  the implementation of a sugar-based biorefinery. Originating from the dehydration of cellulose,  hemicellulose or sugar, it can for example be a precursor of bio-fuels and/or fuel additives. However, there are very few published examples on the CMF synthesis as well as its ability to be further converted in interesting derivatives.</p><p>A simple procedure for the conversion of sugar-based feedstocks to CMF, using flow chemistry, is reported in this  master  thesis.  Sucrose  and  High  Fructose  Corn  Syrup  (HFCS-90  and  HFCS-55)  were  shown  to  be suitable feedstocks. The use of HFCS-90 has been demonstrated to be particularly promising, as it could be converted in a stable process which yields 70% of CMF.</p><p>As a proof of concept, condensation reactions of 5-methylfurfural (MF) and CMF were performed with amine compounds, thus expanding the existing pool of CMF derivatives. MF condensations could be performed under harsh conditions, whereas CMF required milder treatments. Very high conversions were observed, especially when using aliphatic amines as starting materials.</p>
----------------------------------------------------------------------
In diva2:458457 abstract is: <p>The background to this report is the author’s ambition to understand individuals’ work situation, how it’s formed in interaction between the individual and factors in the work-setting. Such an ambition is well in line with the concept of Human-Technology-Organization (HTO), a cross-scientific approach that puts a system-oriented perspective on how human, technolog-ical and organizational factors interact within work systems.</p>
<p>The aim of this report was to explore managers’ work-situation and generate an understanding of it from an HTO-perspective by using an organizational model as a framework for an interview guide and to analyze the work situation from an HTO-perspective following the ques-tions at issue:</p>
<p>1. How does the managers experience their work situation?</p>
<p>2. How can an HTO-perspective contribute to the understanding and potential of im-provement of the work situation?</p>
<p>The organizational model used in the report is Porras &amp; Robertson’s (1992) "Factors constituting the organizational work setting". The model summarizes and describes the organiza-tional, social, physical and technological factors that up until 1992 had been identified by contemporary research within the "Organizational Development" field as important for organizational change.</p>
<p>Four managers’ work situation was studied at the company Södra Cell Mörrum. The report’s methodological approach is of an explorative qualitative nature and has been conducted by the design of an interview guide based on the organizational model described above and semi structured interviews to explore the managers’ work situation.</p>
<p>The results give a detailed description of how the managers experience their work situation och how it is affected by the interaction of the factors human-technlogy-organization. In analysis, it’s illustrated how:</p>
<p>- the organization’s strategies and structure in interaction with the work flow and tech-nology contributes to the managers’ exposure of stress in their work-situation.</p>
<p>- the organizational structure interacts with the individual and vice versa, how the managers’ fuzzy work-description contributes to how they themselves in interaction with other parts of the organization define it and creates informal organizational structure an interaction processes.</p>
<p>- the use of technology is affected by its design, by how the organization reinforces its use and by individual characteristics. It has been shown how technology affects the organization’s strategical effort and disposition.</p>
<p>- organizational factors such as goals and reward systems interacts with and affect behaviors at the workplace. The analysis also shows how informal aspects of the organization, such as it’s history and culture affects behaviors.</p>
<p>The conclusions of the report are:</p>
<p>-that an HTO-perspective is a suitable approach to create a broad understanding of the factors which interaction affects work systems. Such an understanding constitutes a good starting point for development and improvement of work systems.</p>
<p>- that the result encourages further use and development of the methodological approach of using Porras &amp; Robertsons’ model as a framework for an understanding of the individuals’ work situation.</p>

w='organiza-tional' val={'c': 'organiza-tional', 's': 'diva2:458457'}

corrected abstract:
<p>The background to this report is the author’s ambition to understand individuals’ work situation, how it’s formed in interaction between the individual and factors in the work-setting. Such an ambition is well in line with the concept of Human-Technology-Organization (HTO), a cross-scientific approach that puts a system-oriented perspective on how human, technological and organizational factors interact within work systems.</p><p>The aim of this report was to explore managers’ work-situation and generate an understanding of it from an HTO-perspective by using an organizational model as a framework for an interview guide and to analyze the work-situation from an HTO-perspective following the questions at issue:<ol><li>How does the managers experience their work situation?</li><li>How can an HTO-perspective contribute to the understanding and potential of improvement of the work situation?</li></ol></p><p>The organizational model used in the report is Porras &amp; Robertson’s (1992) ”Factors constituting the organizational work setting”. The model summarizes and describes the organizational, social, physical and technological factors that up until 1992 had been identified by contemporary research within the “Organizational Development” field as important for organizational change.</p><p>Four managers’ work situation was studied at the company Södra Cell Mörrum. The report’s methodological approach is of an explorative qualitative nature and has been conducted by the design of an interview guide based on the organizational model described above and semi structured interviews to explore the managers’ work situation.</p><p>The results give a detailed description of how the managers experience their work situation och how it is affected by the interaction of the factors human-technlogy-organization. In analysis, it’s illustrated how:<ul><li>the organization’s strategies and structure in interaction with the work flow and technology contributes to the managers’ exposure of stress in their work-situation.</li><li>the organizational structure interacts with the individual and vice versa, how the managers’ fuzzy work-description contributes to how they themselves in interaction with other parts of the organization define it and creates informal organizational structure an interaction processes.</li><li>the use of technology is affected by its design, by how the organization reinforces its use and by individual characteristics. It has been shown how technology affects the organization’s strategical effort and disposition.</li><li>organizational factors such as goals and reward systems interacts with and affect behaviors at the workplace. The analysis also shows how informal aspects of the organization, such as it’s history and culture affects behaviors.</li></ul></p><p>The conclusions of the report are:<ul><li>that an HTO-perspective is a suitable approach to create a broad understanding of the factors which interaction affects work systems. Such an understanding constitutes a good starting point for development and improvement of work systems.</li><li>that the result encourages further use and development of the methodological approach of using Porras &amp; Robertsons’ model as a framework for an understanding of the individuals’ work situation.</li></ul></p>
----------------------------------------------------------------------
In diva2:854686 abstract is: <p>Notch signaling is a highly conserved signaling pathway important in the transcriptional control expecially during cell fate choice and tumorigenesis. For examplem breast cancer is isusally associated with dysregulated Notch signaling. Therefore, understanding the transcriptional control by Notcvh is fundamental to understand tumor biology of brest cancer. The action of Notch is very contect dependent, but not much is known about the underpinning mechanisms of how different transcriptional controls are achieved. Besides, there were yet to be an accurate genome-wide binding profile of CSL. Here, we have generated CSL null breast cancer cell line and aim to perform accurate binding profile analysis by introducing a tagged version of exogenous CSL. This will shed light on the mechanism of transcriptional control by canonical Notch signaling, ecpecially in the context how it relates to cancer progression in human breast cancer.</p>

w='brest' val={'c': 'breast', 's': 'diva2:854686'}
w='ecpecially' val={'c': 'especially', 's': 'diva2:854686', 'n': 'no full text'}
w='expecially' val={'c': 'especially', 's': 'diva2:854686', 'n': 'no full text'}
w='examplem' val={'c': 'example', 's': 'diva2:854686', 'n': 'no full text'}
w='usally' val={'c': 'usually', 's': 'diva2:854686', 'n': 'no full text'}

corrected abstract:
<p>Notch signaling is a highly conserved signaling pathway important in the transcriptional control especially during cell fate choice and tumorigenesis. For example breast cancer is usually associated with dysregulated Notch signaling. Therefore, understanding the transcriptional control by Notcvh is fundamental to understand tumor biology of breast cancer. The action of Notch is very contect dependent, but not much is known about the underpinning mechanisms of how different transcriptional controls are achieved. Besides, there were yet to be an accurate genome-wide binding profile of CSL. Here, we have generated CSL null breast cancer cell line and aim to perform accurate binding profile analysis by introducing a tagged version of exogenous CSL. This will shed light on the mechanism of transcriptional control by canonical Notch signaling, especially in the context how it relates to cancer progression in human breast cancer.</p>
----------------------------------------------------------------------
Note that this is a scanned document and there are no fonts, just images of pages.

In diva2:1159711 abstract is: <p>Glucose, as the most plentiful sugar in nature, is a renewable resource and possesses excellent record in health safety. Levulinic acid is a platform chemical which plays an important role  in  biomass transformation and reactive intermediates. Both glucose and levulinic acid can be produced by biomass conversion with green processing techno logies.</p><p>Due to the rising needs for bio-based, eco-friendly and non-toxic plasticizers, glucose levulinates as bio­ plasticizers were synthesized from glucose and levulinic acid, by utilizing microwave radiation or conventional condensation reaction (direct-heating method ). Acid number for the reaction liquor was measured by acid-base titration to follow the decrease of acid groups due to the reaction and the trend in  the acid number within reaction time displayed the process of esterification and possible sensitivity of the reaction rate to reaction scale. It showed that microwave radiation had superior ability in  enhancing reaction speed but it was also more sensitive to reaction scale and generated more diverse prod ucts  than the direct-heating method. Besides, the process of reaction and formation  of ester  bonds was  followed  and confirmed by FT IR.</p><p>The achieved levulinate products were extracted by 2-pro panol and ethyl acetate. The practices showed several serio us problems in 2-propanol extraction, including high dosage required  for  NaCl and solvent and difficulties in purification. The ethyl acetate proved to be a suitable solvent for this study and the  extrac ted  product s  from  the Con-24hrs  and Micro-3/4/5/6/7hrs  were  characterized  by  1H  NMR,  13C N :tvlR. and LDI-MS. The results from spectrum suggested the presence of GL,. and G J .'l. type of levulinates. That means the glucose levulinates were  successfully  synthesized  although  the  dehydration side reaction of glucose was inevitable leading to the generation of glucosidic bonds. In addition, BG (mixture of glucose and glycosidic levulinates) was evaluated by so lution casting of starch and PVC. In order to minimize the microbial contaminations in solution casting of  starch, a  modified  method  was raised and applied. The results showed that 40% BG had goo d miscibility with starch and the conclusion was further proved by DSC measurements, while the BG performed poor miscibility with  PVC.</p><p> </p>

w='lution' val={'c': 'solution', 's': 'diva2:1159711', 'n': 'correct in original'}
w='logies' val={'B2': 'technologies', 's': 'diva2:1159711', 'n': 'correct in original'}
w='panol' val={'c': '2-propanol', 's': 'diva2:1159711'}
w='serio' val={'c': 'serious', 's': 'diva2:1159711', 'n': 'correct in original'}
w='ucts' val={'c': 'products', 's': 'diva2:1159711'}
It is likely that the original DiVA text was obtained by OCR and hence there were a lot of errors.

corrected abstract:
<p>Glucose, as the most plentiful sugar in nature, is a renewable resource and possesses excellent record in health safety. Levulinic acid is a platform chemical which plays an important role in biomass transformation and reactive intermediates. Both glucose and levulinic acid can be produced by biomass conversion with green processing technologies.</p><p>Due to the rising needs for bio-based, eco-friendly and non-toxic plasticizers, glucose levulinates as bio­plasticizers were synthesized from glucose and levulinic acid, by utilizing microwave radiation or conventional condensation reaction (direct-heating method). Acid number for the reaction liquor was measured by acid-base titration to follow the decrease of acid groups due to the reaction and the trend in the acid number within reaction time displayed the process of esterification and possible sensitivity of the reaction rate to reaction scale. It showed that microwave radiation had superior ability in enhancing reaction speed but it was also more sensitive to reaction scale and generated more diverse products than the direct-heating method. Besides, the process of reaction and formation of ester bonds was followed and confirmed by FTIR.</p><p>The achieved levulinate products were extracted by 2-propanol and ethyl acetate. The practices showed several serious problems in 2-propanol extraction, including high dosage required for NaCl and solvent and difficulties in purification. The ethyl acetate proved to be a suitable solvent for this study and the extracted products from the Con-24hrs and Micro-3/4/5/6/7hrs were characterized by <sup>1</sup>H NMR, <sup>13</sup>C NMR, and LDI-MS. The results from spectrum suggested the presence of GL<sub>x</sub> and G<sub>x</sub>L<sub>y</sub>. type of levulinates. That means the glucose levulinates were successfully synthesized although the dehydration side reaction of glucose was inevitable leading to the generation of glucosidic bonds. In addition, BG (mixture of glucose and glycosidic levulinates) was evaluated by solution casting of starch and PVC. In order to minimize the microbial contaminations in solution casting of starch, a modified method was raised and applied. The results showed that 40% BG had good miscibility with starch and the conclusion was further proved by DSC measurements, while the BG performed poor miscibility with PVC.</p>
----------------------------------------------------------------------
In diva2:1764820 abstract is: <p>In today's society, energy efficiency is an important parameter when discussing sustaina-bility. Many buildings lack technical solutions to effectively monitor and manage energy consumption. To address this need, companies like iquest strive to digitize and automate energy monitoring. Currently, iquest faces issues of inefficiency and bottlenecks when up-loading large amounts of data into their current graph database. Through a thorough eval-uation, the thesis project has identified suitable alternatives for iquest to consider.During the investigation, the graph databases Neo4j, Stardog, Allegrograph, Amazon Nep-tune, GraphDB, BlazingGraph, and OrientDB were presented. Based on the characteristics and features of these graph databases, it was determined that Neo4j, Stardog, Allegro-graph, Amazon Neptune, and GraphDB meet the requirements for a suitable graph data-base.The implementation of the graph databases was limited by time constraints, and only Neo4j, Stardog, Allegrograph, and GraphDB could be implemented and subjected to test-ing. Despite conducting tests with reduced data volumes and using the free versions of the databases, the results showed that two of the implemented databases successfully passed all the tests.</p>


corrected abstract:
<p>In today's society, energy efficiency is an important parameter when discussing sustaina-bility. Many buildings lack technical solutions to effectively monitor and manage energy consumption. To address this need, companies like iquest strive to digitize and automate energy monitoring. Currently, iquest faces issues of inefficiency and bottlenecks when up-loading large amounts of data into their current graph database. Through a thorough evaluation, the thesis project has identified suitable alternatives for iquest to consider.</p><p>During the investigation, the graph databases Neo4j, Stardog, Allegrograph, Amazon Neptune, GraphDB, BlazingGraph, and OrientDB were presented. Based on the characteristics and features of these graph databases, it was determined that Neo4j, Stardog, Allegrograph, Amazon Neptune, and GraphDB meet the requirements for a suitable graph database.</p><p>The implementation of the graph databases was limited by time constraints, and only Neo4j, Stardog, Allegrograph, and GraphDB could be implemented and subjected to testing. Despite conducting tests with reduced data volumes and using the free versions of the databases, the results showed that two of the implemented databases successfully passed all the tests.</p>
----------------------------------------------------------------------
In diva2:1454460 abstract is: <p>Groundwater (GW) accounting for most of the freshwater available around the World, finding sustainable techniques to depollute it is of crucial importance for safe drinking water supply. The extensive use of fertilizers in the agriculture, as well as other anthropogenic activities, are contributing to the excessive nitrate levels in some aquifers. These levels need to be reduced to obtain potable water. Bioelectrochemical systems (BES), using microorganisms to catalyze a desired electrochemical reaction, recently proved to be a very promising technology for water remediation. Groundwater denitrification using Microbial Electrolysis Cell (MEC) needs to be improved for further scaled-up on-site system. The advantages conferred by fluidized bed reactor (FBR), as well as the outstanding electrochemical properties of reduced graphene oxide (rGO), are two potential enhancements of such bioelectrochemical denitrification system that were investigated in this thesis.</p><p>Some essential parameters could be determined during the preliminary steps' experiments. The fluidization trials gave us a clear insight that Coconut-based Activated Carbon (CAC) particles were resistant carrier particles, nicely fluidized within a 39.27cm3 circular cathodic chamber for a flow rate ranging between 450ml/min to 590ml/min. For the same flow rate of 500ml/min, we could obtain CAC particles fluidization for the upstream fluidized configuration, and still bed particles for the fixed bed downstream configuration, which would be very useful for later unbiased comparison. The denitrifying bacteria showed during their enrichment, a nitrate removal rate of up to 1.986ppm NO3-N/h in serum bottles, with an average of 0.38ppm NO2-N/h accumulation. The parallel running of fixed bed versus fluidized bed denitrifying reactor in order to compare their denitrification performances, was planned, but could not be performed due to COVID-19.</p><p>The graphene oxide (GO) batch experiments showed a good biocompatibility between GO/rGO and our autotrophic denitrifying bacteria. A change of morphology within about 20 hours was observed, probably suggesting the reduction of GO to rGO by the bacteria. During a first test, the presence of GO led to a 2.7 folds less efficient denitrification performance as compared with the GO/rGO-free condition, likely due to the competition between nitrate and GO for being reduced. However, the denitrification rate in presence of GO/rGO increased up to 1.873ppm NO3-N/h after the second pulse of groundwater and flush with H2/CO2 gas, which is almost 2.3 folds higher than initially in the same condition. This suggests that GO needs some time to get fully reduced to rGO, and the denitrification rate might reach the same or higher levels as in the GO/rGO-free conditions, when GO is fully reduced. Improved denitrification would indicate that rGO facilitates the electron transfer between bacteria and nitrate, as it can be expected from its electrochemical properties previously studied. This would be worth being investigated in the scope of a longer experience.</p>

w='H2/CO2' val={'c': 'H<sub>2</sub>/CO<sub>2</sub>', 's': 'diva2:1454460', 'n': 'correct in original'}
Note that other chemical formulas needed subscripts and the cubic centimeters needed a superscript.

corrected abstract:
<p>Groundwater (GW) accounting for most of the freshwater available around the World, finding sustainable techniques to depollute it is of crucial importance for safe drinking water supply. The extensive use of fertilizers in the agriculture, as well as other anthropogenic activities, are contributing to the excessive nitrate levels in some aquifers. These levels need to be reduced to obtain potable water. Bioelectrochemical systems (BES), using microorganisms to catalyze a desired electrochemical reaction, recently proved to be a very promising technology for water remediation. Groundwater denitrification using Microbial Electrolysis Cell (MEC) needs to be improved for further scaled-up on-site system. The advantages conferred by fluidized bed reactor (FBR), as well as the outstanding electrochemical properties of reduced graphene oxide (rGO), are two potential enhancements of such bioelectrochemical denitrification system that were investigated in this thesis.</p><p>Some essential parameters could be determined during the preliminary steps' experiments. The fluidization trials gave us a clear insight that Coconut-based Activated Carbon (CAC) particles were resistant carrier particles, nicely fluidized within a 39.27cm<sup>3</sup> circular cathodic chamber for a flow rate ranging between 450ml/min to 590ml/min. For the same flow rate of 500ml/min, we could obtain CAC particles fluidization for the upstream fluidized configuration, and still bed particles for the fixed bed downstream configuration, which would be very useful for later unbiased comparison. The denitrifying bacteria showed during their enrichment, a nitrate removal rate of up to 1.986ppm NO<sub>3</sub>-N/h in serum bottles, with an average of 0.38ppm NO<sub>2</sub>-N/h accumulation. The parallel running of fixed bed versus fluidized bed denitrifying reactor in order to compare their denitrification performances, was planned, but could not be performed due to COVID-19.</p><p>The graphene oxide (GO) batch experiments showed a good biocompatibility between GO/rGO and our autotrophic denitrifying bacteria. A change of morphology within about 20 hours was observed, probably suggesting the reduction of GO to rGO by the bacteria. During a first test, the presence of GO led to a 2.7 folds less efficient denitrification performance as compared with the GO/rGO-free condition, likely due to the competition between nitrate and GO for being reduced. However, the denitrification rate in presence of GO/rGO increased up to 1.873ppm NO<sub>3</sub>-N/h after the second pulse of groundwater and flush with H<sub>2</sub>/CO<sub>2</sub> gas, which is almost 2.3 folds higher than initially in the same condition. This suggests that GO needs some time to get fully reduced to rGO, and the denitrification rate might reach the same or higher levels as in the GO/rGO-free conditions, when GO is fully reduced. Improved denitrification would indicate that rGO facilitates the electron transfer between bacteria and nitrate, as it can be expected from its electrochemical properties previously studied. This would be worth being investigated in the scope of a longer experience.</p>
----------------------------------------------------------------------
In diva2:537187 abstract is: <p><strong>Abstract</strong><strong></strong><em>Introduction</em>: Hand Arm Risk Assessment Method (HARM) is a tool to assess the risks of deve­lo­ping complaints of the arm, neck or shoulders during manual work. The method was developed in the Netherlands primarily for employers, but is also used as an aid to work environment in­spec­tors. The purpose of this study was to evaluate the HARM-method for assessment of biomecha­nical exposure of the upper limbs when performing manual tasks as well as its suitability to be used within work environment inspection. <strong></strong></p>
<p><em>Methods</em>: Ten labour inspectors conducted assessments of five video-recorded work tasks. Assess­ments made with HARM were compared with those made with the Assessment of Re­pe­titive Tasks (ART) tool and with ACGIH Hand Actvity Level (HAL) - both methods for assess­ment of bio­mecha­nical exposure of the upper limbs - and with the model for the assessment of re­pe­titive work in the pro­visions of the Swedish Work Environ­ment Autho­rity on ergonomics for the prevention of mus­culo­skeletal disorders, AFS 1998:1. HAL is based on a threshold limit value for hand activity. The method combines the assessment of hand activity with per­ceived effort in the hand and forearm. The assess­ments were made twice, two weeks apart. Following each assessment the inspec­tors answered questions about the suitability of each method. Three experts (X) made the same assess­­ments, first individually just like the inspectors’, and then they agreed upon a consensus estimation. Head and upper arm position were registered by inclinometer, wrist movements with electro goniometer. The observers' assessments were com­pared with a “gold standard” that was created by the results of the technical measure­ments which re­placed the consensus esti­mates for the head and arm positions as well as wrist motions in the HARM, ART and HAL assessments.<strong></strong></p>
<p><em>Results:</em> The inspectors' assessment of HARM and ART showed in comparison with the res­pective “gold standard” some under­esti­ma­tion of risks. Conformity in the test-retest was 68 % at appraisal with HARM and 66 % with ART. Based on the inspectors' observations it was re­vealed that force and frequency were experienced as the most difficult to assess. On the other hand the indi­vi­dual assess­ments indicated that the work position of the hand and forearm showed the lar­gest deviation. Hand activity was both over and undervalued in comparison with tech­nical measurements, suggesting that it is difficult to simply assess hand activity by ob­ser­va­tion. Fur­thermore, the model for identifying repetitive work in AFS 1998:1 was perceived to be the most difficult to use for performing assess­ments, as it has few criteria and no support for the assessment of hand/arm and hand inten­si­ve move­ments. This under­lines that there is a need for other models as a supplement to the pro­visions.<strong></strong></p>
<p><em>Conclusions</em>: The results showed that the HARM and ART are relatively similar in content and struc­ture and provided relatively similar results. HARM is more detailed than ART as it takes into account the vibration exposure as a single factor and shows more consideration to the duration of exposure. The HARM-method provides support for the assessment; it is easy to use, it needs pen and paper only and is in that sense readily available, it is fast and takes into account the whole of the assessment of biomechanical exposure of the upper limbs. ART is very similar to HARM; HAL is more limited and can be used as a rapid screening of hand load. <strong></strong></p>

partal corrected: diva2:537187: <p><strong>Abstract</strong><strong></strong><em>Introduction</em>: Hand Arm Risk Assessment Method (HARM) is a tool to assess the risks of deve­lo­ping complaints of the arm, neck or shoulders during manual work. The method was developed in the Netherlands primarily for employers, but is also used as an aid to work environment in­spec­tors. The purpose of this study was to evaluate the HARM-method for assessment of biomecha­nical exposure of the upper limbs when performing manual tasks as well as its suitability to be used within work environment inspection. <strong></strong></p>
<p><em>Methods</em>: Ten labour inspectors conducted assessments of five video-recorded work tasks. Assess­ments made with HARM were compared with those made with the Assessment of Re­pe­titive Tasks (ART) tool and with ACGIH Hand Actvity Level (HAL) - both methods for assess­ment of bio­mecha­nical exposure of the upper limbs - and with the model for the assessment of re­pe­titive work in the pro­visions of the Swedish Work Environ­ment Autho­rity on ergonomics for the prevention of mus­culo­skeletal disorders, AFS 1998:1. HAL is based on a threshold limit value for hand activity. The method combines the assessment of hand activity with per­ceived effort in the hand and forearm. The assess­ments were made twice, two weeks apart. Following each assessment the inspec­tors answered questions about the suitability of each method. Three experts (X) made the same assess­­ments, first individually just like the inspectors’, and then they agreed upon a consensus estimation. Head and upper arm position were registered by inclinometer, wrist movements with electro goniometer. The observers' assessments were com­pared with a “gold standard” that was created by the results of the technical measure­ments which re­placed the consensus esti­mates for the head and arm positions as well as wrist motions in the HARM, ART and HAL assessments.<strong></strong></p>
<p><em>Results:</em> The inspectors' assessment of HARM and ART showed in comparison with the res­pective “gold standard” some under­esti­ma­tion of risks. Conformity in the test-retest was 68 % at apprais al with HARM and 66 % with ART. Based on the inspectors' observations it was re­vealed that force and frequency were experienced as the most difficult to assess. On the other hand the indi­vi­dual assess­ments indicated that the work position of the hand and forearm showed the lar­gest deviation. Hand activity was both over and undervalued in comparison with tech­nical measurements, suggesting that it is difficult to simply assess hand activity by ob­ser­va­tion. Fur­thermore, the model for identifying repetitive work in AFS 1998:1 was perceived to be the most difficult to use for performing assess­ments, as it has few criteria and no support for the assessment of hand/arm and hand inten­si­ve move­ments. This under­lines that there is a need for other models as a supplement to the pro­visions.<strong></strong></p>
<p><em>Conclusions</em>: The results showed that the HARM and ART are relatively similar in content and struc­ture and provided relatively similar results. HARM is more detailed than ART as it takes into account the vibration exposure as a single factor and shows more consideration to the duration of exposure. The HARM-method provides support for the assessment; it is easy to use, it needs pen and paper only and is in that sense readily available, it is fast and takes into account the whole of the assessment of biomechanical exposure of the upper limbs. ART is very similar to HARM; HAL is more limited and can be used as a rapid screening of hand load. <strong></strong></p>
w='under\xadesti\xadma\xadtion' val={'c': 'under\xadestimation', 's': 'diva2:537187'}
w='com\\xadpared' val={'c': 'compared', 's': 'diva2:537187'}
w='bio\xadmecha\xadnical' val={'c': 'biomechanical', 's': 'diva2:537187'}
w='com\xadpared' val={'c': 'compared', 's': 'diva2:537187'}
w='esti\xadmates' val={'c': 'estimates', 's': 'diva2:537187'}
w='deve\xadlo\xadping' val={'c': 'developing', 's': 'diva2:537187'}
w='indi\xadvi\xaddual' val={'c': 'individual', 's': 'diva2:537187'}
w='inspec\xadtors' val={'c': 'inspectors', 's': 'diva2:537187'}
w='inten\xadsi\xadve' val={'c': 'intensive', 's': 'diva2:537187'}
w='in\xadspec\xadtors' val={'c': 'inspectors', 's': 'diva2:537187'}
w='lar\xadgest' val={'c': 'largest', 's': 'diva2:537187', 'n': 'correct in original'}
w='move\xadments' val={'c': 'movements', 's': 'diva2:537187'}
w='measure\xadments' val={'c': 'measurements', 's': 'diva2:537187'}
w='ob\xadser\xadva\xadtion' val={'c': 'ob\xadser\xadva\xadtion', 's': 'diva2:537187'}
w='mus\xadculo\xadskeletal' val={'c': 'musculoskeletal', 's': 'diva2:537187'}
w='per\xadceived' val={'c': 'perceived', 's': 'diva2:537187'}
w='res\xadpective' val={'c': 'respective', 's': 'diva2:537187'}
w='struc\xadture' val={'c': 'structure', 's': 'diva2:537187'}
w='tech\xadnical' val={'c': 'technical', 's': 'diva2:537187'}
w='Autho\xadrity' val={'c': 'Authority', 's': 'diva2:537187'}
w='Environ\xadment' val={'c': 'Environment', 's': 'diva2:537187'}
w='re\xadpe\xadtitive' val={'c': 'repetitive', 's': 'diva2:537187'}
w='re\xadplaced' val={'c': 'replaced', 's': 'diva2:537187'}
w='re\xadvealed' val={'c': 'revealed', 's': 'diva2:537187'}
w='pro\xadvisions' val={'c': 'provisions', 's': 'diva2:537187'}
w='Re\xadpe\xadtitive' val={'c': 'Repetitive', 's': 'diva2:537187'}
w='Fur\xadthermore' val={'c': 'Furthermore', 's': 'diva2:537187', 'n': 'the abstract is full of \xad soft hyphens'}
w='Actvity' val={'c': 'Activity', 's': 'diva2:537187', 'n': 'error in original'}
w='Assess\xadments' val={'c': 'Assessments', 's': 'diva2:537187'}

corrected abstract:
<p><em>Introduction</em>: Hand Arm Risk Assessment Method (HARM) is a tool to assess the risks of developing complaints of the arm, neck or shoulders during manual work. The method was developed in the Netherlands primarily for employers, but is also used as an aid to work environment inspectors. The purpose of this study was to evaluate the HARM-method for assessment of biomechanical exposure of the upper limbs when performing manual tasks as well as its suitability to be used within work environment inspection.</p><p><em>Methods</em>: Ten labour inspectors conducted assessments of five video-recorded work tasks. Assessments made with HARM were compared with those made with the Assessment of Repetitive Tasks (ART) tool and with ACGIH Hand Actvity Level (HAL) - both methods for assessment of biomechanical exposure of the upper limbs - and with the model for the assessment of repetitive work in the provisions of the Swedish Work Environment Authority on ergonomics for the prevention of musculoskeletal disorders, AFS 1998:1. HAL is based on a threshold limit value for hand activity. The method combines the assessment of hand activity with perceived effort in the hand and forearm. The assessments were made twice, two weeks apart. Following each assessment the inspectors answered questions about the suitability of each method. Three experts (X) made the same assessments, first individually just like the inspectors’, and then they agreed upon a consensus estimation. Head and upper arm position were registered by inclinometer, wrist movements with electro goniometer. The observers' assessments were compared with a “gold standard” that was created by the results of the technical measurements which replaced the consensus estimates for the head and arm positions as well as wrist motions in the HARM, ART and HAL assessments.</p><p><em>Results</em>: The inspectors' assessment of HARM and ART showed in comparison with the respective “gold standard” some underestimation of risks. Conformity in the test-retest was 68 % at appraisal with HARM and 66 % with ART. Based on the inspectors' observations it was revealed that force and frequency were experienced as the most difficult to assess. On the other hand the individual assessments indicated that the work position of the hand and forearm showed the largest deviation. Hand activity was both over and undervalued in comparison with technical measurements, suggesting that it is difficult to simply assess hand activity by observation. Furthermore, the model for identifying repetitive work in AFS 1998:1 was perceived to be the most difficult to use for performing assessments, as it has few criteria and no support for the assessment of hand/arm and hand intensive movements. This underlines that there is a need for other models as a supplement to the provisions.</p><p><em>Conclusions</em>: The results showed that the HARM and ART are relatively similar in content and structure and provided relatively similar results. HARM is more detailed than ART as it takes into account the vibration exposure as a single factor and shows more consideration to the duration of exposure. The HARM-method provides support for the assessment; it is easy to use, it needs pen and paper only and is in that sense readily available, it is fast and takes into account the whole of the assessment of biomechanical exposure of the upper limbs. ART is very similar to HARM; HAL is more limited and can be used as a rapid screening of hand load.</p>
----------------------------------------------------------------------
In diva2:853705 abstract is: <p>The aim of the thesis was to develop an innovative high-throughput approach for charaterization of primary B cells repertoires. This system combining droplet-based microfluidics and next generation Illumina sequencing technology, will allow the heavy chain and light chan gene pairs of targeted antibodies to be identified and sequenced.</p><p> </p><p>Droplet-based microfluidics involves generating highly monodisperse aqueuous droplets in the range of picoliter volume that function as independent biocompatible microreactors for chemical or biological assays. In the thesis, droplet-based microfluidics is used to compartmentalize a single cell inside a droplet, capture the mRNA released by the B cells and labelled it with a unique barcode DNA sequence. Cells are encapsulated with reverse transcription reagents and a single bead carrying a unique CNA barcode sequence and a specific sequense targeting the heavy chain and light chain genes of the antibodies o finterest. Up tp 85 millipn pf DNA barcodes are generated by combinatorial indexing method using beads carriers. A split-pool sythesis method allows the construction of a unique 4 indices barcode onto the beads with an additional gene specific sequence tareting the light chain or heavy chain gene sequences of the antibodies. After incubation, droplets are collected, broken and the generated cDNA  is exponentially amplified by PCR prior to sequencing. Bioinformatic analysis will allow the identification of VL and VH genes from the same antibody as they bear the same barcode.</p><p>This master thesis was realized in the Laboratory of Biochemistry (LBC) directed by Pr. Andrew Griffiths at EXPCI ParisTech and is part of the antibody –screening project developed in collaboration between the LBC and HiFiBIO, a spin-off company hosted at ESPCI and based on single cells technologies for drug discovery.</p><p> </p>


w='aqueuous' val={'c': 'aqueous', 's': 'diva2:853705'}
w='charaterization' val={'c': 'characterization', 's': 'diva2:853705', 'n': 'no full text'}
w='finterest' val={'c': 'interest', 's': 'diva2:853705', 'n': 'no full text'}
w='millipn' val={'c': 'millipn', 's': 'diva2:853705', 'n': 'no full text'}
w='pf' val={'c': 'of', 's': 'diva2:853705', 'n': 'no full text'}
w='sequense' val={'c': 'sequence', 's': 'diva2:853705', 'n': 'no full text'}
w='tareting' val={'c': 'targeting', 's': 'diva2:853705', 'n': 'no full text'}
w='EXPCI' val={'c': 'ESPCI', 's': 'diva2:853705', 'n': 'no full text'}

corrected abstract:
<p>The aim of the thesis was to develop an innovative high-throughput approach for characterization of primary B cells repertoires. This system combining droplet-based microfluidics and next generation Illumina sequencing technology, will allow the heavy chain and light chan gene pairs of targeted antibodies to be identified and sequenced.</p><p> </p><p>Droplet-based microfluidics involves generating highly monodisperse aqueous droplets in the range of picoliter volume that function as independent biocompatible microreactors for chemical or biological assays. In the thesis, droplet-based microfluidics is used to compartmentalize a single cell inside a droplet, capture the mRNA released by the B cells and labelled it with a unique barcode DNA sequence. Cells are encapsulated with reverse transcription reagents and a single bead carrying a unique CNA barcode sequence and a specific sequence targeting the heavy chain and light chain genes of the antibodies of interest. Up tp 85 million of DNA barcodes are generated by combinatorial indexing method using beads carriers. A split-pool sythesis method allows the construction of a unique 4 indices barcode onto the beads with an additional gene specific sequence targeting the light chain or heavy chain gene sequences of the antibodies. After incubation, droplets are collected, broken and the generated cDNA  is exponentially amplified by PCR prior to sequencing. Bioinformatic analysis will allow the identification of VL and VH genes from the same antibody as they bear the same barcode.</p><p>This master thesis was realized in the Laboratory of Biochemistry (LBC) directed by Pr. Andrew Griffiths at ESPCI ParisTech and is part of the antibody –screening project developed in collaboration between the LBC and HiFiBIO, a spin-off company hosted at ESPCI and based on single cells technologies for drug discovery.</p>
----------------------------------------------------------------------
title: "Homogeneous and heterogeneous decomposition ofhypochlorite - A study of the oxygen evolving sidereaction using mass spectrometry"
==>    "Homogeneous and heterogeneous decomposition of hypochlorite - A study of the oxygen evolving side reaction using mass spectrometry"

In diva2:765822 abstract is: <p>Oxygen evolution from homogenous and heterogenous decomposition of hypochlorite is a</p><p>small but nonetheless important side reaction in the electrolytic production of chlorate. In</p><p>this diploma work, a method using mass spectrometry for analyzing the amount of oxygen</p><p>formed in a hypochlorite containing electrolyte has been developed, and some preliminary</p><p>experiments have been made. The method works satisfactory for initial screening, but for</p><p>use in further studies, needs to be developed to include measurement of concentrations in</p><p>the electrolyte and the ability to maintain a constant pH during experiments.</p><p>Based on results from the limited experiments made, some preliminary conclusions can</p><p>be drawn. The amount of oxygen evolved was measured with the initial pH of 7, 8, and 9, and</p><p>three dierent types of aqueous electrolytes at initial pH 7; NaOCl(0.19M), NaCl(1.8M/2.7M)+</p><p>NaOCl(0.19M), and NaClO</p><p>4(1.8M) + NaOCl(0.19M). DSA (Dimensionally Stable Anode)</p><p>particles, two types of cerium salts, and a cobalt salt were tried as catalysts, the concentration</p><p>of the salts were 0.018 mM in all cases. The DSA particles and the cobalt used in</p><p>this study catalyze the oxygen evolution reaction, while cerium does not. Both hypochlorous</p><p>acid and hypochlorite ion seem to decompose separately into oxygen in the presence of catalyst,</p><p>while the uncatalyzed decomposition mechanism require the presence of both species as</p><p>no oxygen is detected outside of the pH range where they are both present (approximately</p><p>6&lt;pH&lt;10). The rate of oxygen formation has a maximum around neutral pH for both catalyzed</p><p>and uncatalyzed decomposition, and the rate increases with a decrease in pH in the</p><p>approximate interval 7&lt;pH&lt;10, below which it decreases. No clear eects of ionic medium</p><p>or ionic strength were noticed in this study.</p>

w='eects' val={'c': 'effects', 's': 'diva2:765822', 'n': 'missing ligature'}
Fixed other missing ligatures and removed the unnecessary paragraphs for each line of the text!

corrected abstract:
<p>Oxygen evolution from homogenous and heterogenous decomposition of hypochlorite is a small but nonetheless important side reaction in the electrolytic production of chlorate. In this diploma work, a method using mass spectrometry for analyzing the amount of oxygen formed in a hypochlorite containing electrolyte has been developed, and some preliminary experiments have been made. The method works satisfactory for initial screening, but for use in further studies, needs to be developed to include measurement of concentrations in the electrolyte and the ability to maintain a constant pH during experiments.</p><p>Based on results from the limited experiments made, some preliminary conclusions can be drawn. The amount of oxygen evolved was measured with the initial pH of 7, 8, and 9, and three different types of aqueous electrolytes at initial pH 7; NaOCl(0.19M), NaCl(1.8M/2.7M)+NaOCl(0.19M), and NaClO<sub>4</sub>(1.8M) + NaOCl(0.19M). DSA (Dimensionally Stable Anode) particles, two types of cerium salts, and a cobalt salt were tried as catalysts, the concentration of the salts were 0.018 mM in all cases. The DSA particles and the cobalt used in this study catalyze the oxygen evolution reaction, while cerium does not. Both hypochlorous acid and hypochlorite ion seem to decompose separately into oxygen in the presence of catalyst, while the uncatalyzed decomposition mechanism require the presence of both species as no oxygen is detected outside of the pH range where they are both present (approximately 6&lt;pH&lt;10). The rate of oxygen formation has a maximum around neutral pH for both catalyzed and uncatalyzed decomposition, and the rate increases with a decrease in pH in the approximate interval 7&lt;pH&lt;10, below which it decreases. No clear effects of ionic medium or ionic strength were noticed in this study.</p>
----------------------------------------------------------------------
In diva2:1701736 abstract is: <p>Motor control, including locomotion, strongly depends on the gravitational field. Recent developments like lower-body positive pressure treadmills (LBPPTs) have enabled Earth- based studies on the effects of reduced body weight (BW) on walking and running. Yet, the observed adaptations to simulated hypogravity are not optimal. The present project aims at improving them by adding visual information mimicking a martian environment during running sessions on a LBPPT.</p><p>Twenty-nine participants performed three sessions of four successive five-minute runs at preferred speed, alternating Earth- or Mars-like gravity (100 or 38% BW). They were displayed visual scenes using a virtual reality headset in order to assess the effects of the presence or absence of visual flow and vertical head oscillations during running. The gait was analysed using vertical ground reaction force, foot and sacrum accelerations. Evaluation of the visual vertical and of the sensitivity to the illusion of self-motion helped investigate the adaptations in the perception of gravity and motion.</p><p>It was found that body weight reduction induces biomechanical adaptations independently of the visual context. Impact peak force and stance time decrease, the latter increasing flight time. Strong inter-individual differences in braking and push-off times appear at 38% BW, unobserved in previous studies at 60 and 80% BW. Additionally, the weight given to visual cues in the perceptual integration of gravity diminishes in hypogravity, in favour of increased reliance on the body long axis as a reference for verticality, all the more when visual scenes and locomotor activity are coherent. In that case, self-motion perception decreases without being influenced by hypogravity.</p><p>Thus, locomotor and perceptual adaptations are impacted differently depending on a given sensory context. Motor control and spatial perception can then be updated independently relative to the gravitational and visual environment.</p>

w='Earthbased' val={'c': 'Earth-based', 's': 'diva2:1701736', 'n': 'hyphen at end of line'}
w='Earth- based' val={'c': 'Earth-based', 's': 'diva2:1701736', 'n': 'hyphen at end of line'}

corrected abstract:
<p>Motor control, including locomotion, strongly depends on the gravitational field. Recent developments like lower-body positive pressure treadmills (LBPPTs) have enabled Earth-based studies on the effects of reduced body weight (BW) on walking and running. Yet, the observed adaptations to simulated hypogravity are not optimal. The present project aims at improving them by adding visual information mimicking a martian environment during running sessions on a LBPPT.</p><p>Twenty-nine participants performed three sessions of four successive five-minute runs at preferred speed, alternating Earth- or Mars-like gravity (100 or 38% BW). They were displayed visual scenes using a virtual reality headset in order to assess the effects of the presence or absence of visual flow and vertical head oscillations during running. The gait was analysed using vertical ground reaction force, foot and sacrum accelerations. Evaluation of the visual vertical and of the sensitivity to the illusion of self-motion helped investigate the adaptations in the perception of gravity and motion.</p><p>It was found that body weight reduction induces biomechanical adaptations independently of the visual context. Impact peak force and stance time decrease, the latter increasing flight time. Strong inter-individual differences in braking and push-off times appear at 38% BW, unobserved in previous studies at 60 and 80% BW. Additionally, the weight given to visual cues in the perceptual integration of gravity diminishes in hypogravity, in favour of increased reliance on the body long axis as a reference for verticality, all the more when visual scenes and locomotor activity are coherent. In that case, self-motion perception decreases without being influenced by hypogravity.</p><p>Thus, locomotor and perceptual adaptations are impacted differently depending on a given sensory context. Motor control and spatial perception can then be updated independently relative to the gravitational and visual environment.</p>
----------------------------------------------------------------------
In diva2:1741341 abstract is: <p>For companies in the financial industry, it is important that work routines are carried out correctly and that they follow rules from the Swedish FSA (Financial Supervisory Authority). A process that needs to be ensured is that the company classifies its cus- tomers based on how big the risk is believed to be of being used for money laundering and terrorist financing. This is a case study that shows how automation of the process for risk classification at a Swedish asset manager affects the time consumption and the manual risk involved in this work routine, and how this can lead to decreased costs. </p><p>The report answers the question: To what extent does automation of the risk classi- fication of customers at a Swedish asset manager lead to lower costs, through re- duced time consumption and lower manual risk? </p><p>Through time measurement, interviews with employees and document study of in- cident reports, information is gathered that is compiled into the results of the study. The results show that there are big cost savings to be gained. Reduced manual work through automation leads to lower personnel costs and thus large cost savings. It is also explained in the report why the manual risks could not be linked to direct costs, but need further research into the subject. </p>

w='cus- tomers' val={'c': 'customers', 's': 'diva2:1741341'}
Fixed other unnecessary hypens and added the correct emphasis.

corrected abstract:
<p>For companies in the financial industry, it is important that work routines are carried out correctly and that they follow rules from the Swedish FSA (Financial Supervisory Authority). A process that needs to be ensured is that the company classifies its customers based on how big the risk is believed to be of being used for money laundering and terrorist financing. This is a case study that shows how automation of the process for risk classification at a Swedish asset manager affects the time consumption and the manual risk involved in this work routine, and how this can lead to decreased costs.</p><p>The report answers the question: <em>To what extent does automation of the risk classification of customers at a Swedish asset manager lead to lower costs, through reduced time consumption and lower manual risk?</em></p><p>Through time measurement, interviews with employees and document study of incident reports, information is gathered that is compiled into the results of the study. The results show that there are big cost savings to be gained. Reduced manual work through automation leads to lower personnel costs and thus large cost savings. It is also explained in the report why the manual risks could not be linked to direct costs, but need further research into the subject.</p>
----------------------------------------------------------------------
In diva2:744700 abstract is: <p>The ability to egnineer defined 3D neural circuits within artifical materials is expected to bring new tools for understanding basic biology, for screening pharmacologic molecules important in neurogeneration as well as for developing neural tissue engineering applications.</p><p>Here we show that the Michael addition of thiols on maleimides can be used to form PEG-based hydrogels entrapping neurons. This addition reaction is already in wide use for bioconjugation because of its selectivity, high kinetics, and biocompatibility. But its use in this context is original. In such a system, the gelation time is higy tunable due to the PH-dependence, ranging from nearly instantaneous at pG 7.4 to around 30min at pH 5.0. A solution containing neurons can be stabilized to any pH in this range by adding a small amount of 2(N-morpholino)ethanesulfonic (MES) buffer.</p><p>Peptides with oe or two thiol-containing cysteine residues could be readily incorporated into the hydrogel. This alloed us to produce PEG gels with a matrix metalo-proteinase (MMP-cleavable peptide as a cross-linker and(or laminin peptides (RGD, IKVAV, YIGSE) as attachment cues. Among all these peptides, IKVAV was the only one that was found to promote cell adhesion and extensive neurite outgrowth from embryonic day 18 (E18) rat primary hippocampal neurons, while gels without any peptide prevented neurite outgrowth completely. Gels cross-linked with MMP-cleavable peptides degraded in a few days due to enzymatic degradation, while gels cross-linked with PEG-dithiol degraded lver the course of a few weeks, which dould be attributed to the reversibility of the addition of thiols on maleimides. Both gels promoted neurite outgrowth for macromer content below 1.6%(w/v). For the future, we envision that inductive and repulsive hydrogels can be patterned in three dimensional space, thereby providing a system for creating complex neural networks.</p>

w='egnineer' val={'c': 'engineer', 's': 'diva2:744700', 'n': 'no full text'}
w='dould' val={'c': 'could', 's': 'diva2:744700', 'n': 'no full text'}
w='higy' val={'c': 'highly', 's': 'diva2:744700', 'n': 'no full text'}
w='lver' val={'c': 'over', 's': 'diva2:744700', 'n': 'no full text'}
w='pG' val={'c': 'pH', 's': 'diva2:744700', 'n': 'no full text'}
w='metalo-proteinase' val={'c': 'metalloproteinase', 's': 'diva2:744700', 'n': 'no full text'}
w='oe' val={'c': 'one', 's': 'diva2:744700', 'n': 'no full text'}
w='alloed' val={'c': 'allowed', 's': 'diva2:744700', 'n': 'no full text'}

corrected abstract:
<p>The ability to engineer defined 3D neural circuits within artifical materials is expected to bring new tools for understanding basic biology, for screening pharmacologic molecules important in neurogeneration as well as for developing neural tissue engineering applications.</p><p>Here we show that the Michael addition of thiols on maleimides can be used to form PEG-based hydrogels entrapping neurons. This addition reaction is already in wide use for bioconjugation because of its selectivity, high kinetics, and biocompatibility. But its use in this context is original. In such a system, the gelation time is highly tunable due to the PH-dependence, ranging from nearly instantaneous at pH 7.4 to around 30min at pH 5.0. A solution containing neurons can be stabilized to any pH in this range by adding a small amount of 2(N-morpholino)ethanesulfonic (MES) buffer.</p><p>Peptides with one or two thiol-containing cysteine residues could be readily incorporated into the hydrogel. This allowed us to produce PEG gels with a matrix metalo-proteinase (MMP-cleavable peptide as a cross-linker and(or laminin peptides (RGD, IKVAV, YIGSE) as attachment cues. Among all these peptides, IKVAV was the only one that was found to promote cell adhesion and extensive neurite outgrowth from embryonic day 18 (E18) rat primary hippocampal neurons, while gels without any peptide prevented neurite outgrowth completely. Gels cross-linked with MMP-cleavable peptides degraded in a few days due to enzymatic degradation, while gels cross-linked with PEG-dithiol degraded over the course of a few weeks, which could be attributed to the reversibility of the addition of thiols on maleimides. Both gels promoted neurite outgrowth for macromer content below 1.6%(w/v). For the future, we envision that inductive and repulsive hydrogels can be patterned in three dimensional space, thereby providing a system for creating complex neural networks.</p>
----------------------------------------------------------------------
In diva2:856188 abstract is: <p>It has been shown in many studies that alternative splicing plays an important role in functional regulation and is associated with many neurological disorders and cancers. Current methods to detect splice variants are computational prediction by ESTs clustering and experimental approaches such as RNA probe microarray. These methods are only focusing on the the RNA level. But due to post transcriptional modification on pre-mRNA molecules, there is a weak correlation between mRNA abundance and protein expressin. Therefore, it is important to find evidence of these functioinally important splice variants at the protein level. Shotgun proteomics has become a favorable tool to do large scale identification of proteins. Recently, a tool called Protein Quantification and Peptide Quality Control (PQPQ) was develope to enhance information output from shotgun proteomics data and to detect protein variants. Here, we present a downstream program, SpliceView, a tool for analyzing and visualizing the output from PQPQ for finding splice variants. Proteomics data of A431 cell line was used to exemplify available functions of SpliceView.</p>


w='functioinally' val={'c': 'functionally', 's': 'diva2:856188', 'n': 'no full text'}

corrected abstract:
<p>It has been shown in many studies that alternative splicing plays an important role in functional regulation and is associated with many neurological disorders and cancers. Current methods to detect splice variants are computational prediction by ESTs clustering and experimental approaches such as RNA probe microarray. These methods are only focusing on the the RNA level. But due to post transcriptional modification on pre-mRNA molecules, there is a weak correlation between mRNA abundance and protein expression. Therefore, it is important to find evidence of these functioinally important splice variants at the protein level. Shotgun proteomics has become a favorable tool to do large scale identification of proteins. Recently, a tool called Protein Quantification and Peptide Quality Control (PQPQ) was develope to enhance information output from shotgun proteomics data and to detect protein variants. Here, we present a downstream program, SpliceView, a tool for analyzing and visualizing the output from PQPQ for finding splice variants. Proteomics data of A431 cell line was used to exemplify available functions of SpliceView.</p>
----------------------------------------------------------------------
In diva2:854845 abstract is: <p>Gene expression profiling techniques such as RNA sequencing has greatly contributed to our understanding of physiological and disease processes in the brain. Wen applied to celllular complex brain tissue samples, these techniques do not account for cell type specific expression changes and the underlying biological pathways of cell types. Aberrations in cell type gene expression patterns have been documented in brain diseases such as depression, schizophrenia, Alzheimer's among others. Therefore, gene expression at cell type resolution migh be critical to understand disease processes and biological pathways. In the recent years, several cell isolation techniques such as laser capture micro-dissection and fluorescence-activated cell sorting have been couple with microarrays for this purpose. Hoever, these methods are technially highly challengin, tim-and-resource consuming and may be limited because of potential isolation artefacts, mRNA length and abundance biases. For these combined technical issues, gene expression profiling with tissues samples is still the most widely applied approach in brain.</p><p>In this study, we aim at establishing a transcriptome database for gene expression profiles and identify marker genes that are devoid of these biases, from mousederived in vitro oliodendrocytes, microaglia, astrocytes and neurons. To this end, a modified deep sequencing method that enriches for 3' mRNA reads was used for expression profiling. This study identified numerous cell type-specific gene markers that can potentially be usedd to characterize cell types and even estimate that proportion of cell types in tissue samples. Additionally, a novel strategy based on RNA abundance to compare the pathway enrichment between the cell types was developed and pathways that are particularly enriched in individual cell types were identified. Thus, this transcriptome database of digital RNA sequencing data generated for the major cell types of the brain can be used as reference information for cell type specific gene expression profiles to overcome some limitations of expression studies from brain tissues.</p>

w='celllular' val={'c': 'cellular', 's': 'diva2:854845'}
w='challengin' val={'c': 'challenging', 's': 'diva2:854845', 'n': 'no full text - note additional error in title'}
w='microaglia' val={'c': 'microglia', 's': 'diva2:854845', 'n': 'no full text'}
w='migh' val={'c': 'might', 's': 'diva2:854845', 'n': 'no full text'}
w='usedd' val={'c': 'used', 's': 'diva2:854845', 'n': 'no full text'}
w='technially' val={'c': 'technically', 's': 'diva2:854845', 'n': 'no full text'}
w='Hoever' val={'c': 'However', 's': 'diva2:854845', 'n': 'no full text'}

corrected abstract:
<p>Gene expression profiling techniques such as RNA sequencing has greatly contributed to our understanding of physiological and disease processes in the brain. Wen applied to cellular complex brain tissue samples, these techniques do not account for cell type specific expression changes and the underlying biological pathways of cell types. Aberrations in cell type gene expression patterns have been documented in brain diseases such as depression, schizophrenia, Alzheimer's among others. Therefore, gene expression at cell type resolution might be critical to understand disease processes and biological pathways. In the recent years, several cell isolation techniques such as laser capture micro-dissection and fluorescence-activated cell sorting have been couple with microarrays for this purpose. However, these methods are technically highly challenging, time-and-resource consuming and may be limited because of potential isolation artefacts, mRNA length and abundance biases. For these combined technical issues, gene expression profiling with tissues samples is still the most widely applied approach in brain.</p><p>In this study, we aim at establishing a transcriptome database for gene expression profiles and identify marker genes that are devoid of these biases, from mouse derived in vitro oliodendrocytes, microglia, astrocytes and neurons. To this end, a modified deep sequencing method that enriches for 3' mRNA reads was used for expression profiling. This study identified numerous cell type-specific gene markers that can potentially be used to characterize cell types and even estimate that proportion of cell types in tissue samples. Additionally, a novel strategy based on RNA abundance to compare the pathway enrichment between the cell types was developed and pathways that are particularly enriched in individual cell types were identified. Thus, this transcriptome database of digital RNA sequencing data generated for the major cell types of the brain can be used as reference information for cell type specific gene expression profiles to overcome some limitations of expression studies from brain tissues.</p>
----------------------------------------------------------------------
In diva2:1595081 abstract is: <p>MRCKβ (Cdc42bpb) is a downstream effector of the Rho GTPase CDC42. The information known about the MRCK proteins is limited. This project aimed to gain a better understanding of MRCKβ´s function by identifying potential interactors for MRCKβ. It is known that MRCK regulates the lamellar actomyosin dynamics and interacts with other proteins to promote F-actin stability, circumferential actin bundle formation, and dendritic outgrowth. The MRCK proteins seem to be necessary for mesenchymal cell migration, in a process independent on the similar ROCK proteins. Those interactions are all connected to the cytoskeleton and cell migration. This project utilized a proximity-labeling method called BioID to identify potential interactors for MRCKβ in a HEK239FT cell line. During this project 26 potential interactors were identified by the BioID experiment. Out of the 26 potential interactors, A, B, and C were selected for further analysis by co-immunoprecipitation. Protein A was identified as a possible interactor for MRCKβ by co-immunoprecipitation, while the CoIP could not confirm a direct interaction between B or C and MRCKβ.</p>


w='MRCK' val={'c': 'MRCKβ', 's': 'diva2:1595081', 'n': 'no full text'}
The beta was in some places, but not in others. The acronmys is now used uniformly.

corrected abstract:
<p>MRCKβ (Cdc42bpb) is a downstream effector of the Rho GTPase CDC42. The information known about the MRCKβ proteins is limited. This project aimed to gain a better understanding of MRCKβ´s function by identifying potential interactors for MRCKβ. It is known that MRCKβ regulates the lamellar actomyosin dynamics and interacts with other proteins to promote F-actin stability, circumferential actin bundle formation, and dendritic outgrowth. The MRCKβ proteins seem to be necessary for mesenchymal cell migration, in a process independent on the similar ROCK proteins. Those interactions are all connected to the cytoskeleton and cell migration. This project utilized a proximity-labeling method called BioID to identify potential interactors for MRCKβ in a HEK239FT cell line. During this project 26 potential interactors were identified by the BioID experiment. Out of the 26 potential interactors, A, B, and C were selected for further analysis by co-immunoprecipitation. Protein A was identified as a possible interactor for MRCKβ by co-immunoprecipitation, while the CoIP could not confirm a direct interaction between B or C and MRCKβ.</p>
----------------------------------------------------------------------
In diva2:744661 abstract is: <p>Application areas for synthetic polymers are constantly increasing hence demand of improved polymer properties, where functionalization of polymer surface is one approach. The currently utilized methods have large environmental impacts and decrease the quality of polymers resulting in an interest of alternative methods, like enzymatic treatment. The main limitation with enzymatic treatment is slow processes since synthetic polymers are unnatural substrate for enzymes. Genetic engineering is in general applied to increase enzymatic efficiency by enlarging the are around the active site and/or modify the active site. Upcoming is fusion of binding modules to enzymes to facilitate adsorption on insoluble substrates. In this study, has a hydrophobin (hfb4jecorina) originating from Trichoderma reesei been fused to a cutinase from Thermobifida cellulosilytica DSM444535 (Thc_Cut1+Hyd). Additionally, has the carbohydrate binding module from cellobiohydrolase I from T. reesei and the substrate binding domain origin from Ralstonia ickettii T1 been fused to a codon optimized adsorption behavior on insoluble synthetic polymers. The fusion proteins were cloned, expressed in Escherichia coli, characterized and compared to the native enzymes reagarding activity on polymers. Thc_Cut1+Hyd showed decreases activity on soluble substrates but increased activity on amorphous polyethylene terephthalate compared to the native enzyme. rNfpolyA showed no activity on polyamides but on a polyurethane/polyester copolymer where rNfpolyA+CBM and rNfpolyA+PBM showed higher activity. The study shows promising possibilities to enable an increase adsorption and hence improved enzymatic acitivty on insoluble polymers by fusion of binding domains to already active enzymes. The wide affinity diversity of binding domains in combination with the broad enzyme specificity enables limitless possibilities of enhanced enzymatic treatment by targeting enzymes to insoluble substrates.</p>

w='hfb4jecorina' val={'c': 'HFB4 jecorina', 's': 'diva2:744661', 'n': 'likely correction - no full text'}
See for example: https://febs.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1742-4658.2007.05636.x
w='ickettii' val={'c': 'pickettii', 's': 'diva2:744661', 'n': 'no full text'}
w='reagarding' val={'c': 'regarding', 's': 'diva2:744661', 'n': 'no full text'}
w='acitivty' val={'c': 'activity', 's': 'diva2:744661'}
w='DSM444535' val={'c': 'DSM44535', 's': 'diva2:744661', 'n': 'no full text'}


corrected abstract:
<p>Application areas for synthetic polymers are constantly increasing hence demand of improved polymer properties, where functionalization of polymer surface is one approach. The currently utilized methods have large environmental impacts and decrease the quality of polymers resulting in an interest of alternative methods, like enzymatic treatment. The main limitation with enzymatic treatment is slow processes since synthetic polymers are unnatural substrate for enzymes. Genetic engineering is in general applied to increase enzymatic efficiency by enlarging the are around the active site and/or modify the active site. Upcoming is fusion of binding modules to enzymes to facilitate adsorption on insoluble substrates. In this study, has a hydrophobin (<em>HFB4 jecorina</em>) originating from <em>Trichoderma reesei>/em> been fused to a cutinase from <em>Thermobifida cellulosilytica</em> DSM44535 (Thc_Cut1+Hyd). Additionally, has the carbohydrate binding module from cellobiohydrolase I from <em>T. reesei</em> and the substrate binding domain origin from <em>Ralstonia pickettii</em> T1 been fused to a codon optimized adsorption behavior on insoluble synthetic polymers. The fusion proteins were cloned, expressed in Escherichia coli, characterized and compared to the native enzymes regarding activity on polymers. Thc_Cut1+Hyd showed decreases activity on soluble substrates but increased activity on amorphous polyethylene terephthalate compared to the native enzyme. rNfpolyA showed no activity on polyamides but on a polyurethane/polyester copolymer where rNfpolyA+CBM and rNfpolyA+PBM showed higher activity. The study shows promising possibilities to enable an increase adsorption and hence improved enzymatic activity on insoluble polymers by fusion of binding domains to already active enzymes. The wide affinity diversity of binding domains in combination with the broad enzyme specificity enables limitless possibilities of enhanced enzymatic treatment by targeting enzymes to insoluble substrates.</p>
----------------------------------------------------------------------
In diva2:744706 abstract is: <p>Dosgin of two different chemical amendments was evaluated to counter problems with poorly settling activated sludge in separate treatment trains at Henriksdal wastewater treatmen plant, Stockholm. Flocculating cationic polyacrylamide (CPAM) was added to the secondary clarifiers to evoke an immediate improvment of sludge settling. Fine chalk (Otical 20) was added to the aeration baisins t improve the activated sludge floc structure and enable more efficient nitrogen removal due to the alkalinity increasing property of the substance.</p><p>Reductions of SVI and turbidity were confirmed in lab scale with dose concetnrations of 1-2 mg/l of CPAM. Dose concentrations of 1-3 mg/l in the treatment plant showed no improvement of the quality of the effluent, while ability to control the height of the sludge blankets was proven towards the end of the study at extremely high SVI (&gt;250 ml/g) and high amount of filaments in the activated sludge.</p><p>Dosing of Optical 20 was inconclusive regarding improvement of floc structure and settling properties of the activated sludge, likely die to an insufficient average dose concentration. A slight increase of the alkalinity in the biological treatment stage was possibly shown towards the end of the dosing period.</p><p>Revised dosing strategies for full scale applications and a rough financial evaluation of the two methods conclude the report.</p><p></p>

w='concetnrations' val={'c': 'concentrations', 's': 'diva2:744706'}
w='treatmen' val={'c': 'treatment', 's': 'diva2:744706', 'n': 'no full text'}
w='Dosgin' val={'c': 'Dosing', 's': 'diva2:744706', 'n': 'no full text'}
w='Otical' val={'c': 'OPTICAL®', 's': 'diva2:744706', 'n': 'no full text'}

corrected abstract:
<p>Dosing of two different chemical amendments was evaluated to counter problems with poorly settling activated sludge in separate treatment trains at Henriksdal wastewater treatment plant, Stockholm. Flocculating cationic polyacrylamide (CPAM) was added to the secondary clarifiers to evoke an immediate improvment of sludge settling. Fine chalk (OPTICAL® 20) was added to the aeration baisins t improve the activated sludge floc structure and enable more efficient nitrogen removal due to the alkalinity increasing property of the substance.</p><p>Reductions of SVI and turbidity were confirmed in lab scale with dose concentrations of 1-2 mg/l of CPAM. Dose concentrations of 1-3 mg/l in the treatment plant showed no improvement of the quality of the effluent, while ability to control the height of the sludge blankets was proven towards the end of the study at extremely high SVI (&gt;250 ml/g) and high amount of filaments in the activated sludge.</p><p>Dosing of Optical 20 was inconclusive regarding improvement of floc structure and settling properties of the activated sludge, likely die to an insufficient average dose concentration. A slight increase of the alkalinity in the biological treatment stage was possibly shown towards the end of the dosing period.</p><p>Revised dosing strategies for full scale applications and a rough financial evaluation of the two methods conclude the report.</p>
----------------------------------------------------------------------
In diva2:1679390 abstract is: <p>The marine shipping industry is dominated by fossil fuel driven propulsion. Electrification of marine vessels is one of the main strategies to enable emission-free propulsion. Hydrogen is an excellent energy carrier to meet the power demand of a marine vessel. Proton exchange membrane fuel cells (PEMFC) is a commercially available alternative for converting hydrogen into electricity. However, durability issues of the PEMFC is a constraint with the technology which limits technical lifetime. Research around ripple currents impact on degradation of PEMFC is scarce and the reported results are ambiguous and lack clear correlation between the effects of the ripple current on the lifetime of a PEMFC.</p><p>This master thesis evaluates the impact on degradation of a single cell PEMFC by imposing a sinusoidal (70 Hz, 50 % amplitude) AC ripple to a dynamic load cycle. The dynamic load cycle is designed to simulate typical operating conditions of a marine vessel. Constant load cycling at 0.4 A/cm2 with the same ripple characteristics was also conducted to verify the dynamic load cycling impact on the performance losses of the PEMFC.</p><p>The in-situ characterization showed performance losses both during the dynamic and constant load cycling, for the ripple current and reference tests. To conclude, no significant effects on degradation by the sinusoidal ripple current of 70 Hz and 50% amplitude is found when applied to a single cell PEMFC despite of performance losses for all cases.</p>

w='A/cm2' val={'c': 'A/cm<sup>2</sup>', 's': 'diva2:1679390', 'n': 'correct in original'}

corrected abstract:
<p>The marine shipping industry is dominated by fossil fuel driven propulsion. Electrification of marine vessels is one of the main strategies to enable emission-free propulsion. Hydrogen is an excellent energy carrier to meet the power demand of a marine vessel. Proton exchange membrane fuel cells (PEMFC) is a commercially available alternative for converting hydrogen into electricity. However, durability issues of the PEMFC is a constraint with the technology which limits technical lifetime. Research around ripple currents impact on degradation of PEMFC is scarce and the reported results are ambiguous and lack clear correlation between the effects of the ripple current on the lifetime of a PEMFC.</p><p>This master thesis evaluates the impact on degradation of a single cell PEMFC by imposing a sinusoidal (70 Hz, 50 % amplitude) AC ripple to a dynamic load cycle. The dynamic load cycle is designed to simulate typical operating conditions of a marine vessel. Constant load cycling at 0.4 A/cm<sup>2</sup> with the same ripple characteristics was also conducted to verify the dynamic load cycling impact on the performance losses of the PEMFC.</p><p>The in-situ characterization showed performance losses both during the dynamic and constant load cycling, for the ripple current and reference tests. To conclude, no significant effects on degradation by the sinusoidal ripple current of 70 Hz and 50 % amplitude is found when applied to a single cell PEMFC despite of performance losses for all cases.</p>
----------------------------------------------------------------------
In diva2:1438811 abstract is: <p>With the growing impact of climate change on both political decisions and how companies develop their products, it is increasingly important to find environmentally friendly alternatives to fossil-based materials. One of the more interesting materials in this respect is cellulose, which is the world's most naturally occurring polymer and can be used in a variety of applications. One way to modify the fibers and to change their properties is to use a method called Layer-by-Layer (LbL) treatment where two polymers of opposite charge are alternately adsorbed to the fiber surface. Another possibility is to oxidize the cellulose in the fibers to obtain a higher charge. This means that the fibers can adsorb a higher amount of cationic additives and that the fibers swell more which means that they are plasticized and can create stronger joints between the fiber surfaces in the dry state, which results in stronger dry fiber networks. However, wood-based fibers are small and inhomogeneous, both chemically and morphologically, which means that it is necessary to use model systems to be able to clarify, in detail, how treated and untreated surfaces interact with each other at a molecular level.</p><p>One model system that can be used to investigate how cellulose fibers are affected by coating using the LbL method is to use spherical beads made from regenerated cellulose. In the present work, these beads were treated with polyallylamine hydrochloride (PAH) and hylauronic acid (HA) as well as with PAH and alginate (Alg) before being allowed to dry together and then subjected to tensile testing to clarify the adhesion between the surfaces. The beads were treated with five and ten bi-layers of these polymers, respectively, and then dried together on an AKD-coated surface, to avoid adhesion to the underlying surface, to examine the adhesion between the beads. The adhesion increased when treated with LbL and became higher upon adsorption of multiple layers of polyelectrolytes and the Alg / PAH system showed the greatest increase. Ten-layer samples of Alg / PAH were also tested after a solution of calcium chloride was added during drying, resulting in poorer adhesion between the beads.</p>

w='hylauronic' val={'c': 'hyaluronic', 's': 'diva2:1438811', 'n': 'error in original'}

corrected abstract:
<p>With the growing impact of climate change on both political decisions and how companies develop their products, it is increasingly important to find environmentally friendly alternatives to fossil-based materials. One of the more interesting materials in this respect is cellulose, which is the world's most naturally occurring polymer and can be used in a variety of applications. One way to modify the fibers and to change their properties is to use a method called Layer-by-Layer (LbL) treatment where two polymers of opposite charge are alternately adsorbed to the fiber surface. Another possibility is to oxidize the cellulose in the fibers to obtain a higher charge. This means that the fibers can adsorb a higher amount of cationic additives and that the fibers swell more which means that they are plasticized and can create stronger joints between the fiber surfaces in the dry state, which results in stronger dry fiber networks. However, wood-based fibers are small and inhomogeneous, both chemically and morphologically, which means that it is necessary to use model systems to be able to clarify, in detail, how treated and untreated surfaces interact with each other at a molecular level.</p><p>One model system that can be used to investigate how cellulose fibers are affected by coating using the LbL method is to use spherical beads made from regenerated cellulose. In the present work, these beads were treated with polyallylamine hydrochloride (PAH) and hylauronic acid (HA) as well as with PAH and alginate (Alg) before being allowed to dry together and then subjected to tensile testing to clarify the adhesion between the surfaces. The beads were treated with five and ten bi-layers of these polymers, respectively, and then dried together on an AKD-coated surface, to avoid adhesion to the underlying surface, to examine the adhesion between the beads. The adhesion increased when treated with LbL and became higher upon adsorption of multiple layers of polyelectrolytes and the Alg / PAH system showed the greatest increase. Ten-layer samples of Alg / PAH were also tested after a solution of calcium chloride was added during drying, resulting in poorer adhesion between the beads.</p>
----------------------------------------------------------------------
In diva2:819109 abstract is: <p>The GPS is used today for personal navigation, however GPS performs less well in an in-door environment due to the dependency of line of sight. The IPS, Indoor Positioning Sys-tem, will complement the function of the GPS and open doors for new applications. In this thesis studies of methods and techniques are made to enable IPS for a portable device. These techniques and methods are verified that they could be implemented in a smartphone.A prototype was designed to examine the possibility to navigate in an indoor environment. The prototype was built on a RSSI-based radio fingerprinting method over Bluetooth. This method was integrated with a dead reckoning system using an IMU to follow the motions of the user. Tests and simulations are performed along with the results presented in tables and figures. The result shows that it is possible to navigate using the prototype with a mean error of 0.26m. Results are also showing that two methods are to prefer, as they will com-plement each other’s weaknesses.</p>


w='com-plement' val={'c': 'complement', 's': 'diva2:819109'}
w='in-door' val={'c': 'indoor', 's': 'diva2:819109'}
w='Sys-tem' val={'c': 'System', 's': 'diva2:819109'}

corrected abstract:
<p>The GPS is used today for personal navigation, however GPS performs less well in an indoor environment due to the dependency of line of sight. The IPS, <em>Indoor Positioning System</em>, will complement the function of the GPS and open doors for new applications. In this thesis studies of methods and techniques are made to enable IPS for a portable device. These techniques and methods are verified that they could be implemented in a smartphone.</p><p>A prototype was designed to examine the possibility to navigate in an indoor environment. The prototype was built on a RSSI-based radio fingerprinting method over Bluetooth. This method was integrated with a dead reckoning system using an IMU to follow the motions of the user. Tests and simulations are performed along with the results presented in tables and figures. The result shows that it is possible to navigate using the prototype with a mean error of 0.26m. Results are also showing that two methods are to prefer, as they will complement each other’s weaknesses.</p>
----------------------------------------------------------------------
In diva2:1672863 abstract is: <p>Human cervical mucus is a native protein hydrogel secreted inside the female reproductive tract and defined by its secretion inside the cervix. The mucus plays an important role as an immunological and bioactive barrier, while regulating the selection and migration of spermatozoa. As such, cervical mucus plays an important role in fertility, however a lot of previous studies into its rheological properties suffer from inconsistent results and aged methodology. Understanding the rheological properties of cervical mucus would allow additional insight into the treatment of mucus-related fertility issues, as well as allow for the development of next generation female contraceptives.</p><p>The project achieved structural determination of cervical mucus as a porous filamentous mesh with help of scanning electron microscopy. The results showed evidence of potential discrepancy between cervical mucus of low and high quality. Low quality mucus displayed significantly smaller interfilamentous spacing (mean: 44.4 ± 20.6 nm versus 32.0 ± 13.3 nm) at 40k magnification, and thicker filaments (mean: 13 ± 4 nm versus 19 ± 6 nm). Larger pores observed at 10k magnification showed no difference in interfilamentous spacing with an average of 81.1 ± 35.0 nm. Furthermore, PEGylation of 2 µm large fluorescent polystyrene microsphere was achieved to ζ-potential of −7.6 mV and demonstrated impact of PEGylation on mean square displacement of particles in varying mucus concentrations. Bulk rheology measurements in 90% cervical mucus displayed an apparent pore diameter of 560 ± 97 nm and microviscosity of 28 ± 9 mPa*s. Additional highlight of differences in the behaviour of the particles in different concentrations of cervical mucus was achieved. Lastly, entrapment of particles was accomplished inside the optical trap generated by an optical tweezer, demonstrating the pattern of viscoelastic moduli in cervical mucus treated and untreated with chitosan. Potential evidence of altered structure upon chitosan treatment was shown via scanning electron microscopy.</p>

w='Pa*s' val={'c': 'mPa·s', 's': 'diva2:1672863', 'n': 'no full text'}

corrected abstract:
<p>Human cervical mucus is a native protein hydrogel secreted inside the female reproductive tract and defined by its secretion inside the cervix. The mucus plays an important role as an immunological and bioactive barrier, while regulating the selection and migration of spermatozoa. As such, cervical mucus plays an important role in fertility, however a lot of previous studies into its rheological properties suffer from inconsistent results and aged methodology. Understanding the rheological properties of cervical mucus would allow additional insight into the treatment of mucus-related fertility issues, as well as allow for the development of next generation female contraceptives.</p><p>The project achieved structural determination of cervical mucus as a porous filamentous mesh with help of scanning electron microscopy. The results showed evidence of potential discrepancy between cervical mucus of low and high quality. Low quality mucus displayed significantly smaller interfilamentous spacing (mean: 44.4 ± 20.6 nm versus 32.0 ± 13.3 nm) at 40k magnification, and thicker filaments (mean: 13 ± 4 nm versus 19 ± 6 nm). Larger pores observed at 10k magnification showed no difference in interfilamentous spacing with an average of 81.1 ± 35.0 nm. Furthermore, PEGylation of 2 µm large fluorescent polystyrene microsphere was achieved to ζ-potential of −7.6 mV and demonstrated impact of PEGylation on mean square displacement of particles in varying mucus concentrations. Bulk rheology measurements in 90% cervical mucus displayed an apparent pore diameter of 560 ± 97 nm and microviscosity of 28 ± 9 mPa·s. Additional highlight of differences in the behaviour of the particles in different concentrations of cervical mucus was achieved. Lastly, entrapment of particles was accomplished inside the optical trap generated by an optical tweezer, demonstrating the pattern of viscoelastic moduli in cervical mucus treated and untreated with chitosan. Potential evidence of altered structure upon chitosan treatment was shown via scanning electron microscopy.</p>
----------------------------------------------------------------------
In diva2:543884 abstract is: <p>The cities of Stockholm and Uppsala take its drinking water from lake Mälaren and its tributaries. An increase in concentration of carbon in water is likely to follow a warmer and more humid climate, which may be a future scenario. This may lead to that the current methods of purification that are used today are going to be insufficient and have to be developed further. Organic matter in drinking water can cause a variety of problems such as disinfection by-products, biological growth in distribution systems and also odor, bad taste and unwanted color. A promising method of reducing organic matter from drinking water has proved to be the anion exchange process MIEX<sup>®</sup> (Magnetic Ion Exchange resin process).</p><p>The purpose with this project was to examine MIEX<sup>®</sup> capacity of reducing dissolved organic matter (DOC) from water in presence of anions (Cl<sup>-</sup>, HCO<sub>3</sub><sup>-</sup> and SO<sub>4</sub><sup>2-</sup>). This was done through performing experiments with water containing different combinations of above-mentioned anions. The main objective was to create a model in PHREEQC from experimental data such as TOC/DOC, anions, alkalinity, pH and UV-absorbance. These parameters determine the concentration of chemical compounds in a solvent. The values could be used to calculate how the different anions bind to MIEX<sup>®</sup>.</p><p>MIEX<sup>®</sup> ability to purify dissolved organic matter from water deteriorate when high concentrations of sulfate ions are present in the solvent. The sulfate ions and the organic matter contest about positions on the ion-exchanger and the equilibrium will displace and more organic carbon will be left in the water. To evaluate how much Cl<sup>-</sup> and HCO<sub>3</sub><sup>-</sup> that binds to MIEX<sup>®</sup> requires further experiments since sufficient data to validate the result is unavailable.</p><p>A mathematical model in PHREEQC showed that sulfate binds almost 3,5 times stronger to MIEX<sup>®</sup> than DOC. If the concentration of DOC is low, the reaction rate will increase compared to if the concentration of DOC is high.</p>

The supersciprt in 'MIEX<sup>®</sup>' is unnecessary asn the symbol is already in a raised position.


corrected abstract:
<p>The cities of Stockholm and Uppsala take its drinking water from lake Mälaren and its tributaries. An increase in concentration of carbon in water is likely to follow a warmer and more humid climate, which may be a future scenario. This may lead to that the current methods of purification that are used today are going to be insufficient and have to be developed further. Organic matter in drinking water can cause a variety of problems such as disinfection by-products, biological growth in distribution systems and also odor, bad taste and unwanted color. A promising method of reducing organic matter from drinking water has proved to be the anion exchange process MIEX® (Magnetic Ion Exchange resin process).</p><p>The purpose with this project was to examine MIEX® capacity of reducing dissolved organic matter (DOC) from water in presence of anions (Cl<sup>-</sup>, HCO<sub>3</sub><sup>-</sup> and SO<sub>4</sub><sup>2-</sup>). This was done through performing experiments with water containing different combinations of above-mentioned anions. The main objective was to create a model in PHREEQC from experimental data such as TOC/DOC, anions, alkalinity, pH and UV-absorbance. These parameters determine the concentration of chemical compounds in a solvent. The values could be used to calculate how the different anions bind to MIEX®.</p><p>MIEX® ability to purify dissolved organic matter from water deteriorate when high concentrations of sulfate ions are present in the solvent. The sulfate ions and the organic matter contest about positions on the ion-exchanger and the equilibrium will displace and more organic carbon will be left in the water. To evaluate how much Cl<sup>-</sup> and HCO<sub>3</sub><sup>-</sup> that binds to MIEX® requires further experiments since sufficient data to validate the result is unavailable.</p><p>A mathematical model in PHREEQC showed that sulfate binds almost 3,5 times stronger to MIEX® than DOC. If the concentration of DOC is low, the reaction rate will increase compared to if the concentration of DOC is high.</p>
----------------------------------------------------------------------
In diva2:855825 abstract is: <p>As the researches and dvelopments in DNA nanotechnology is increasing steadily, the demands for the research tools associated also started to rise gradually. To make CNA nanotechnology a viable technology for therapeutics, diagnostics or life science research, the above requirements have to be met and the component materials should be made affordable. There is a high need for producing single stranded DNA cheaply, abundantly and with very high quality. Prior experiments demonstrated the produciton of clonal template through a highly scalable enzymatic process called Monoclonal StoichiometrIC (MOSIC) method for producing such single stranded DNA oligonucleotides perfectly in a controlled relative stoichiometric ratio. This MOSIC mehtod utilizes the helper phages and bacteria to produce ssDNA in large quantity very cheaply which is highly appreciable. But after the revovery of ssDNA from phages, when they are cut with restriction enzymes to release the DNA oligonucleotides, some contaminations were seen in the digested mixture die to ineffective digestion. The aim of this project is to investigate the factors affecting the production of oligonucleotides by amplification of phagemid vector via helper phages and to find out the reason for the ineffective digestion.</p>

w='dvelopments' val={'c': 'developments', 's': 'diva2:855825', 'n': 'no full text'}
w='mehtod' val={'c': 'method', 's': 'diva2:855825', 'n': 'no full text'}
w='revovery' val={'c': 'recovery', 's': 'diva2:855825', 'n': 'no full text'}
w='produciton' val={'c': 'production', 's': 'diva2:855825', 'n': 'no full text'}

corrected abstract:
<p>As the researches and developments in DNA nanotechnology is increasing steadily, the demands for the research tools associated also started to rise gradually. To make CNA nanotechnology a viable technology for therapeutics, diagnostics or life science research, the above requirements have to be met and the component materials should be made affordable. There is a high need for producing single stranded DNA cheaply, abundantly and with very high quality. Prior experiments demonstrated the production of clonal template through a highly scalable enzymatic process called Monoclonal StoichiometrIC (MOSIC) method for producing such single stranded DNA oligonucleotides perfectly in a controlled relative stoichiometric ratio. This MOSIC method utilizes the helper phages and bacteria to produce ssDNA in large quantity very cheaply which is highly appreciable. But after the recovery of ssDNA from phages, when they are cut with restriction enzymes to release the DNA oligonucleotides, some contaminations were seen in the digested mixture die to ineffective digestion. The aim of this project is to investigate the factors affecting the production of oligonucleotides by amplification of phagemid vector via helper phages and to find out the reason for the ineffective digestion.</p>
----------------------------------------------------------------------
In diva2:1471149 abstract is: <p>The insect cell line Sf9, which is derived from the fall armyworm Spodoptera frugiperda(Sf), has been used as a host platform for recombinant protein production since the early 2000s. Unlike mammalian cells, Sf9 cells are cholesterol auxotrophs. Their culture requires cell culture medium supplemented with a cholesterol source. Cell culture media designed to support the growth of insect cells already exist on the market, however, they often contain animal-derived cholesterol and use an encapsulation technology with limited capacity to admit cholesterol molecules.</p><p>In the present work, two cholesterol derivatives, CST-DERIV-1 and CST-DERIV-2, were investigated for their potential to supplement animal component-free and chemically defined media for Sf9 cells. Both cholesterol conjugates proved to be bioavailable in Sf9 cells. Commercially available insect cells media are commonly supplemented with xtarget mg/Lof cholesterol. It was demonstrated that cholesterol supplementation of xmin mg/L or xmax mg/L did not display a significant change in cell performance. Additionally, a cholesterol quantification method based on enzymatic reactions and spectrophotometric detection was tested on solutions of CST-derivatives in different solvent systems. Quantification was achieved for CST-DERIV-1 and CST-DERIV-2 solutions in assay buffer and PBS. However, most of the experimental results deviated from theoretical concentrations and could not be reproduced. Lastly, this study included the testing of three different qPCR strategies for the detection of Sf-rhabdovirus, an adventitious agent in Sf9 cells. The sensitivity of the most promising strategy was evaluated by testing decreasing amount of starting RNA extracted from Sf9 cells.</p>

w='xtarget' val={'c': 'x target', 's': 'diva2:1471149', 'n': 'error in original - no full text of the thesis - only the abstract and cover are in the full text'}

corrected abstract:
<p>The insect cell line Sf9, which is derived from the fall armyworm Spodoptera frugiperda (Sf), has been used as a host platform for recombinant protein production since the early 2000s. Unlike mammalian cells, Sf9 cells are cholesterol auxotrophs. Their culture requires cell culture medium supplemented with a cholesterol source. Cell culture media designed to support the growth of insect cells already exist on the market, however, they often contain animal-derived cholesterol and use an encapsulation technology with limited capacity to admit cholesterol molecules.</p><p>In the present work, two cholesterol derivatives, CST-DERIV-1 and CST-DERIV-2, were investigated for their potential to supplement animal component-free and chemically defined media for Sf9 cells. Both cholesterol conjugates proved to be bioavailable in Sf9 cells. Commercially available insect cells media are commonly supplemented with xtarget mg/L of cholesterol. It was demonstrated that cholesterol supplementation of xmin mg/L or xmax mg/L did not display a significant change in cell performance. Additionally, a cholesterol quantification method based on enzymatic reactions and spectrophotometric detection was tested on solutions of CST-derivatives in different solvent systems. Quantification was achieved for CST-DERIV-1 and CST-DERIV-2 solutions in assay buffer and PBS. However, most of the experimental results deviated from theoretical concentrations and could not be reproduced. Lastly, this study included the testing of three different qPCR strategies for the detection of Sf-rhabdovirus, an adventitious agent in Sf9 cells. The sensitivity of the most promising strategy was evaluated by testing decreasing amount of starting RNA extracted from Sf9 cells.</p>
----------------------------------------------------------------------
In diva2:1690568 abstract is: <p>A new Os(II)-based complex, namely Osmium(II) tris[4,4'-bis (1-adamantantylmethyl)- 2,2'- bipyridine] hexafluorophosphate, has been synthesized and investigated as a redox-active mediator of ion transfer processes across voltammetric thin membranes, which work on the principle of interconnected electron transfer and ion transfer processes. The redox behavior of the complex was explored in different medium: the Os(II) center exhibited excellent redox feature and stability and demonstrated a great potential towards its implementation in thin polymeric ion-selective membranes, which are commonly used in electrochemical ion sensors. Effectively, the membrane can be formulated to create either cation or anion transfer at the sample-membrane interface, displaying a voltammetric peak whose position depends on the ion concentration in the sample solution. Advantageously, this behavior can be exploited for ion determination. Having demonstrated the possibility of both cation and anion transfer, our research was focused on the investigation on membranes that allow for cation detection, i.e., membranes based on the cation exchanger Sodium tetrakis[3,5-bis(trifluoromethyl)phenyl]borate (NaTFPB). The experimental results showed that the optimal response of thin-layer membranes was achieved for compositions comprising 50 mmol kg-1 of NaTFPB and 10 mmol kg-1 of the Os(II) compound. In essence, a reversible and repeatable Gaussian-shaped voltammetric cation transfer wave was obtained. Furthermore, the thin-layer behavior was confirmed (linear relationship of peak current with the scan rate, R2&gt;0.9942), and the corresponding width at the half height of the peak was only about 100 mV, which is narrower than previously reported mediators. The importance of this behavior is two-fold. On one hand, the membrane composition prevents the components’ leaching, as they are in relatively low amounts. Moreover, the further incorporation of an ionophore is totally feasible. On the other hand, the simplicity of incorporating the redox mediator in the membrane, rather than creating a separate layer (i.e., electropolymerized conducting polymers or self-assembling monolayers), together with the narrow voltammetric peaks are expected to facilitate the further development of nano-sensors using nanotip-based electrodes for multi-ion detection.</p>

w='phenyl]borate' val={'c': 'phenyl borate', 's': 'diva2:1690568', 'n': 'part of "Sodium tetrakis[3,5-bis(trifluoromethyl)phenyl]borate" - the split on the right bracket is wrong for a checmical name'}
w='tetrakis[3,5-bis' val={'c': 'tetrakis[3,5-bis(trifluoromethyl)phenyl]borate', 's': 'diva2:1690568'}

corrected abstract:
<p>A new Os(II)-based complex, namely Osmium(II) tris[4,4'-bis (1-adamantantylmethyl)- 2,2'- bipyridine] hexafluorophosphate, has been synthesized and investigated as a redox-active mediator of ion transfer processes across voltammetric thin membranes, which work on the principle of interconnected electron transfer and ion transfer processes. The redox behavior of the complex was explored in different medium: the Os(II) center exhibited excellent redox feature and stability and demonstrated a great potential towards its implementation in thin polymeric ion-selective membranes, which are commonly used in electrochemical ion sensors. Effectively, the membrane can be formulated to create either cation or anion transfer at the sample-membrane interface, displaying a voltammetric peak whose position depends on the ion concentration in the sample solution. Advantageously, this behavior can be exploited for ion determination. Having demonstrated the possibility of both cation and anion transfer, our research was focused on the investigation on membranes that allow for cation detection, i.e., membranes based on the cation exchanger Sodium tetrakis[3,5-bis(trifluoromethyl)phenyl]borate (NaTFPB). The experimental results showed that the optimal response of thin-layer membranes was achieved for compositions comprising 50 mmol kg-1 of NaTFPB and 10 mmol kg-1 of the Os(II) compound. In essence, a reversible and repeatable Gaussian-shaped voltammetric cation transfer wave was obtained. Furthermore, the thin-layer behavior was confirmed (linear relationship of peak current with the scan rate, R<sup>2</sup> &gt; 0.9942), and the corresponding width at the half height of the peak was only about 100 mV, which is narrower than previously reported mediators. The importance of this behavior is two-fold. On one hand, the membrane composition prevents the components’ leaching, as they are in relatively low amounts. Moreover, the further incorporation of an ionophore is totally feasible. On the other hand, the simplicity of incorporating the redox mediator in the membrane, rather than creating a separate layer (i.e., electropolymerized conducting polymers or self-assembling monolayers), together with the narrow voltammetric peaks are expected to facilitate the further development of nano-sensors using nanotip-based electrodes for multi-ion detection.</p>
----------------------------------------------------------------------
In diva2:856313 abstract is: <p>Water has always been important for people and other living organisms. Earlier, when the the springs where important sources of water, the extracted water often had a good quality due to the natural cleansing processes. Furthermore, the springs are important habitats for animals and plants.  Mostly, springwater is excellent drinking water, and may also have a historic value. Therefore, several springs were localised and their water qualities were examined  A survey of  the springs in Täby and Vallentuna municipalities has been made. Relevant water quality parameters were investigated, both in the field and in the laboratory.  The parameters examined were: flow, temperature, pH, conductivity, alkalinity, chlorinity, taste, smell, color, dissolved gases, COD<sub>Mn</sub>, phosphorus, sulfur, and the most common metals with ICP-OES.  Furthermore a survey of the geology and the human impact has been performed.</p><p>In total 16 springs were visited, of which all but one were investigated. All springs had a normal temperature of spring water. The flow was estimated or calculated to be under 0,5  l/s in all springs. The pH-values were between six and eight in all springs and the alkalinity was high in most springs. Springs with high alkalinities also had high contents of the dominating positive ions sodium, potassium, calcium and magnesium. In some springs with low alkalinities iron, manganese and copper occurred in high concentrations. Taste, odor and color varied froma  good crystal clear water to a brown and smelly water.  Most of the springs contain high quantities of  oxygen gas and carbon dioxide.  Some springs contained high concentrations of chloride, phosphorus and copper, which indicate a human impact.</p><p>Five of the springs have good  water qualities and these springs are excellent goals for excursions.  The water qualities of these springs meet the requirements for drinking water in Sweden. By proper signs at each spring, the public can be informed about the water quality and also about the historical traditions of the spring.  </p><p>In five of the springs, the water quality is poor and the water does not satisfy the requirements for the drinking water quality. Some of these springs could be restored and cared for by ceaning away debris, leaves and branches.  Old crumbling structures should also be removed. </p>

w='ceaning' val={'c': 'cleaning', 's': 'diva2:856313', 'n': 'no full text'}

corrected abstract:
<p>Water has always been important for people and other living organisms. Earlier, when the the springs where important sources of water, the extracted water often had a good quality due to the natural cleansing processes. Furthermore, the springs are important habitats for animals and plants.  Mostly, springwater is excellent drinking water, and may also have a historic value. Therefore, several springs were localised and their water qualities were examined  A survey of  the springs in Täby and Vallentuna municipalities has been made. Relevant water quality parameters were investigated, both in the field and in the laboratory.  The parameters examined were: flow, temperature, pH, conductivity, alkalinity, chlorinity, taste, smell, color, dissolved gases, COD<sub>Mn</sub>, phosphorus, sulfur, and the most common metals with ICP-OES.  Furthermore a survey of the geology and the human impact has been performed.</p><p>In total 16 springs were visited, of which all but one were investigated. All springs had a normal temperature of spring water. The flow was estimated or calculated to be under 0,5  l/s in all springs. The pH-values were between six and eight in all springs and the alkalinity was high in most springs. Springs with high alkalinities also had high contents of the dominating positive ions sodium, potassium, calcium and magnesium. In some springs with low alkalinities iron, manganese and copper occurred in high concentrations. Taste, odor and color varied from a good crystal clear water to a brown and smelly water.  Most of the springs contain high quantities of oxygen gas and carbon dioxide.  Some springs contained high concentrations of chloride, phosphorus and copper, which indicate a human impact.</p><p>Five of the springs have good water qualities and these springs are excellent goals for excursions.  The water qualities of these springs meet the requirements for drinking water in Sweden. By proper signs at each spring, the public can be informed about the water quality and also about the historical traditions of the spring.</p><p>In five of the springs, the water quality is poor and the water does not satisfy the requirements for the drinking water quality. Some of these springs could be restored and cared for by cleaning away debris, leaves and branches.  Old crumbling structures should also be removed.</p>
----------------------------------------------------------------------
In diva2:801905 abstract is: <p>An altered cancer cell metabolis is today regarded as a hallmark of cancer and several metabolic pathways, including the one-carbon metabolism, are being investigated for their role in cancerl cell transformation. Recent findins have shown that one of the metabolic enzymes of the one-carbon metabolism is upregulated ina number of different cancer tumors, but is absen in most normal adult tissues. Furthermore, this metabolic enzyme has been demonstrated to have an importatnt role in suporting cancer cell proliferation and survival, but the mechanism behind this remains unclear. Here, we aim to characterize the role of this metabolic enzyme in cancer cell prolifertion and survival in order to gain a deeper understanding of the enzyme for its future use as a potential therapeutical terget. First, we show evidence that the enzyme localizes to the nucleus using confocal microscopy in tow cancer cell lines of different origin. Secondly, we were able to demonstrate an increased cellular proliferation when overexpressing the enzyme in cancer cell lines and by that validate its role in supporting cancer cell proliferation. Thirdly, we find that overexpression of the protein stimulates cancer cell proliferation independent of its enzymatoc activity. Taken togther, we here provid important data that will aid in future evaluation of the enzyme as a potential drug target in cancer treatment. </p>

w='enzymatoc' val={'c': 'enzymatic', 's': 'diva2:801905', 'n': 'no full text'}
w='cancerl' val={'c': 'cancer', 's': 'diva2:801905', 'n': 'no full text'}
w='findins' val={'c': 'findings', 's': 'diva2:801905', 'n': 'no full text'}
w='importatnt' val={'c': 'important', 's': 'diva2:801905', 'n': 'no full text'}
w='metabolis' val={'c': 'metabolism', 's': 'diva2:801905', 'n': 'no full text'} # not all cases are misspelled
w='suporting' val={'c': 'supporting', 's': 'diva2:801905', 'n': 'no full text'}
w='togther' val={'c': 'together', 's': 'diva2:801905', 'n': 'no full text'}
w='terget' val={'c': 'target', 's': 'diva2:801905', 'n': 'no full text'}
w='prolifertion' val={'c': 'proliferation', 's': 'diva2:801905', 'n': 'no full text'}
w='absen' val={'c': 'absent', 's': 'diva2:801905', 'n': 'no full text'}

corrected abstract:
<p>An altered cancer cell metabolism is today regarded as a hallmark of cancer and several metabolic pathways, including the one-carbon metabolism, are being investigated for their role in cancer cell transformation. Recent findings have shown that one of the metabolic enzymes of the one-carbon metabolism is upregulated in a number of different cancer tumors, but is absent in most normal adult tissues. Furthermore, this metabolic enzyme has been demonstrated to have an important role in supporting cancer cell proliferation and survival, but the mechanism behind this remains unclear. Here, we aim to characterize the role of this metabolic enzyme in cancer cell proliferation and survival in order to gain a deeper understanding of the enzyme for its future use as a potential therapeutical target. First, we show evidence that the enzyme localizes to the nucleus using confocal microscopy in tow cancer cell lines of different origin. Secondly, we were able to demonstrate an increased cellular proliferation when overexpressing the enzyme in cancer cell lines and by that validate its role in supporting cancer cell proliferation. Thirdly, we find that overexpression of the protein stimulates cancer cell proliferation independent of its enzymatic activity. Taken together, we here provid important data that will aid in future evaluation of the enzyme as a potential drug target in cancer treatment. </p>
----------------------------------------------------------------------
In diva2:801899 abstract is: <p>The pro-inflammatory cytokine tumor necrosis factor (TNF) has long been associated with the pathogenesis of several inflammatory and autoimmune disorders, prompting the development of TNF inhibitors. Although these ihibitors have greatly revolutionized the treatment of such diseases, a subset of patients fails to respond to anti-TNF treatment. With the discovery of other central proinflammatory cytokines also implicated in the pathogenesis of inflammatory and autoimmune disorders, several inhibitors have been developed and tested in clinical trials with promising results. In the present study, the inhibitory potential of two antibody-Affibody fusion proteins (AffiMabs) targeting TNF and a second cytokine were examined. The two bispecific AffiMabs; consisting of adalimumab (an anti-TNF antibody) with two Affibody molecules fused C-terminally on either the heavy or light chain have been analyzed within the scope of this thesis. The analyzes of the AffiMabs were performed in a normal human dermal fibroblast assay using TNF and/or the second cytokine as stimuli and secretion of two different interleukins was used as readout. The analyses reveal that the AffiMabs are equally potent to adalimumab  with regards to TNF inhibition, and have excellent inhibitory properties of the second cytokine. The profound inhibitory capacity of the AffiMabs can be attributed to the bivalent nature of these compounds. In addition, of the two AffiMabs examined, fusion of the Affibody to the C terminus of the light chain of adalimumab showed stronger binding to the second cytokine, while fusion of the Affibody molecule to the C terminus of heavy chain of adalimumab indicated an increased binding to the neonatal Fc receptor (FcRn) at physolological pH. These promising results make the bispecific AffiMabs studied here attractive to use in therapy for a range of indications within inflammatory and autoimmune disorders, as they inhibit the otherwise synergistic interactions between these two cytokines. </p>

w='ihibitors' val={'c': 'inhibitors', 's': 'diva2:801899', 'n': 'no full text'}
w='physolological' val={'c': 'physiological', 's': 'diva2:801899', 'n': 'no full text'}

corrected abstract:
<p>The pro-inflammatory cytokine tumor necrosis factor (TNF) has long been associated with the pathogenesis of several inflammatory and autoimmune disorders, prompting the development of TNF inhibitors. Although these inhibitors have greatly revolutionized the treatment of such diseases, a subset of patients fails to respond to anti-TNF treatment. With the discovery of other central proinflammatory cytokines also implicated in the pathogenesis of inflammatory and autoimmune disorders, several inhibitors have been developed and tested in clinical trials with promising results. In the present study, the inhibitory potential of two antibody-Affibody fusion proteins (AffiMabs) targeting TNF and a second cytokine were examined. The two bispecific AffiMabs; consisting of adalimumab (an anti-TNF antibody) with two Affibody molecules fused C-terminally on either the heavy or light chain have been analyzed within the scope of this thesis. The analyzes of the AffiMabs were performed in a normal human dermal fibroblast assay using TNF and/or the second cytokine as stimuli and secretion of two different interleukins was used as readout. The analyses reveal that the AffiMabs are equally potent to adalimumab  with regards to TNF inhibition, and have excellent inhibitory properties of the second cytokine. The profound inhibitory capacity of the AffiMabs can be attributed to the bivalent nature of these compounds. In addition, of the two AffiMabs examined, fusion of the Affibody to the C terminus of the light chain of adalimumab showed stronger binding to the second cytokine, while fusion of the Affibody molecule to the C terminus of heavy chain of adalimumab indicated an increased binding to the neonatal Fc receptor (FcRn) at physiological pH. These promising results make the bispecific AffiMabs studied here attractive to use in therapy for a range of indications within inflammatory and autoimmune disorders, as they inhibit the otherwise synergistic interactions between these two cytokines. </p>
----------------------------------------------------------------------
In diva2:846119 abstract is: <p>The aim of the  study  was to study  the  performance of nickel-based  catalysts  in the  production  of synthetic  natural  gas from synthesis gas (CO and H,). Nickel-based catalysts are preeminent catalysts in  this  reaction   due  to  their   high activity  and  selectivity  to  methane.  Nevertheless,   these   are threatened by severe  intrinsic deactivation  phenomena. In  more  detail, the goal of this work is to study   the   influence   of  different   catalyst   properties  on   the   three    most  severe   deactivation mechanisms  (nickel carbonyl formation,  carbon formation  and thermal  sintering). For that  purpose,  alumina-supported nickel catalysts containing  30 wt% Ni and promoted with Zr02, MgO, CaO and BaO were  prepared  and tested  in a high pressure  catalytic reactor.   The experimental work was divided in 3 parts according to specific study goals.</p><p>The first  part of the  work consisted  in the  study  of the  catalyst  pellet size on deactivation due  to nickel carbonyl  formation. For that  purpose,  the  non-promoted  alumina-supported  nickel catalyst</p><p>was tested  at 20 bar and 310 oc using three  different  pellet sizes. It was found  and  proved that  the</p><p>use of large catalyst  pellet sizes significantly suppresses this  kind of deactivation. The results  were scientifically explained  by the influence of internal mass transfer phenomena. In order  to support this conclusion,  COMSOL multihpysics computations were  performed  to estimate internal  temperature and concentration profiles for the different  pellet sizes.</p><p>The second  part of the  work was focused  on the effect of the selected  promoters on the  resistance towards  carbon  formation. For that  purpose, the different  catalysts  were tested  at 310 oc, 1 bar and</p><p>a H,/C0=3; conditions  at which the carbon formation  is favorable. The spent  samples  were analyzed by means  of temperature-programmed hydrogenation  analyses  in order  to quantify the  amount  of carbon  formed. It was found  that  the  use of Zr02,  in particular,  significantly reduces  the  rate  of carbon formation.</p><p>The third  part  of  the  work  consisted  in studying  the  effect  of these   promoters on  the  catalyst resistance  towards thermal  sintering.  For that  purpose,  the  different  catalysts  were  exposed  to an</p><p>accelerated aging  procedure (690 oc and  H20/H2=2). The intrinsic catalytic activity of the  catalysts</p><p>was  then  determined for  both  the  fresh  and  aged  samples   by testing  these  at  non-deactivating conditions  (300 oc, 1 bar and  H,jC0=9).  No clear  improvement neither  decline  in catalyst  stability</p><p>was observed for any of the promoted  catalyst samples.</p>

w='multihpysics' val={'c': 'multiphysics', 's': 'diva2:846119', 'n': 'no full text'}

corrected abstract:
<p>The aim of the  study  was to study  the  performance of nickel-based  catalysts  in the  production  of synthetic  natural  gas from synthesis gas (CO and H,). Nickel-based catalysts are preeminent catalysts in  this  reaction   due  to  their   high activity  and  selectivity  to  methane.  Nevertheless,   these   are threatened by severe  intrinsic deactivation  phenomena. In  more  detail, the goal of this work is to study   the   influence   of  different   catalyst   properties  on   the   three    most  severe   deactivation mechanisms  (nickel carbonyl formation,  carbon formation  and thermal  sintering). For that  purpose,  alumina-supported nickel catalysts containing  30 wt% Ni and promoted with Zr02, MgO, CaO and BaO were  prepared  and tested  in a high pressure  catalytic reactor.   The experimental work was divided in 3 parts according to specific study goals.</p><p>The first  part of the  work consisted  in the  study  of the  catalyst  pellet size on deactivation due  to nickel carbonyl  formation. For that  purpose,  the  non-promoted  alumina-supported  nickel catalyst</p><p>was tested  at 20 bar and 310 oc using three  different  pellet sizes. It was found  and  proved that  the</p><p>use of large catalyst  pellet sizes significantly suppresses this  kind of deactivation. The results  were scientifically explained  by the influence of internal mass transfer phenomena. In order  to support this conclusion,  COMSOL multiphysics computations were  performed  to estimate internal  temperature and concentration profiles for the different  pellet sizes.</p><p>The second  part of the  work was focused  on the effect of the selected  promoters on the  resistance towards  carbon  formation. For that  purpose, the different  catalysts  were tested  at 310 oc, 1 bar and</p><p>a H,/C0=3; conditions  at which the carbon formation  is favorable. The spent  samples  were analyzed by means  of temperature-programmed hydrogenation  analyses  in order  to quantify the  amount  of carbon  formed. It was found  that  the  use of Zr02,  in particular,  significantly reduces  the  rate  of carbon formation.</p><p>The third  part  of  the  work  consisted  in studying  the  effect  of these   promoters on  the  catalyst resistance  towards thermal  sintering.  For that  purpose,  the  different  catalysts  were  exposed  to an</p><p>accelerated aging  procedure (690 oc and  H20/H2=2). The intrinsic catalytic activity of the  catalysts</p><p>was  then  determined for  both  the  fresh  and  aged  samples   by testing  these  at  non-deactivating conditions  (300 oc, 1 bar and  H,jC0=9).  No clear  improvement neither  decline  in catalyst  stability</p><p>was observed for any of the promoted  catalyst samples.</p>
----------------------------------------------------------------------
In diva2:846859 abstract is: <p>Gasification of biomass is today facing several problems with the high amount of tar</p><p>produced and compounds such as alkali that can harm the catalyst in catalytic tar reformation.</p><p>This is why the focus in this master thesis study was to create a catalyst for secondary tar</p><p>steam reforming. The aim was to create a catalyst that was suitable for tar steam reforming</p><p>and also evaluate the effect that alkali had on the catalyst.</p><p>A catalyst composed of 20%Bronzes –ZrO2 impregnated with nickel was prepared in this</p><p>study and characterised with XRD and BET. The bronzes consisted of K0.25WO3 and was</p><p>prepared with two different methods and analysed with XRD to see if there was some</p><p>difference in the structure and purity. Three different weight loads of nickel: 5-,10- and 15</p><p>wt% , was prepared for each catalyst that was named Method 1 and Method 2. In total six</p><p>catalysts was tested in an experimental test rig that was situated at the Royal Institute of</p><p>Technology in Stockholm. In addition a blank test was performed for comparison of the</p><p>catalytic activity.</p><p>For the experiments 1-methylnaphthalene was decided to be used as a simulated tar. The</p><p>experiments were divided into two parts where in Part 1 a S/C ratio of 4 was used and Part 2 a</p><p>S/C ratio of 6 was used. The experiments were conducted at reactor temperatures of 700 °C</p><p>and 800 °C with or without alkali aerosols. Other parameters changed in the experiments were</p><p>the catalyst load, 1-methylnaphthalene flow and the gas hourly space velocity. Results were</p><p>analysed with 4 micro gas chromatographs and solid phase adsorption.</p><p>Results from the catalyst characterisation indicated that the wanted catalyst had been prepared</p><p>however in Method 2 a higher purity of the bronzes was reached compared to Method 1. The</p><p>results from the BET analysis gave a surface area of between 40-46 m2/g for the different</p><p>catalysts.</p><p>In the experiments from Part 1 a very high gas hourly space velocity was used and the results</p><p>indicated that there was almost tar reduction compared to the blank test. In Part 2 the gas</p><p>hourly space velocity was lowered and a higher tar reduction and was obtained. One test was</p><p>also conducted at 900 °C where the highest tar reduction was obtained, almost 40 %.</p><p>From the results it could be seen tar reduction and 1-MN/naphthalene ratio was increasing</p><p>with higher temperatures and nickel loadings. Catalysts prepared from Method 2 also showed</p><p>a higher tar reduction and 1-MN/naphthalene ratio compared to Method 1which could</p><p>indicate that it was more stable and had a higher purity of the bronzes. The results from</p><p>atomic absorption spectrophotometer showed that the mass of potassium in the catalyst before</p><p>the experiment decreased between 3-29 % compared to after the experiment. From the rather</p><p>low decrease in potassium the hexagonal structure of the bronzes clearly protects the</p><p>potassium from evaporating within the bulk. Also introducing alkali aerosols had a positive</p><p>effect on the tar reduction.</p>

w='K0.25WO3' val={'c': 'K<sub>0.25</sub>WO<<sub>3</sub>', 's': 'diva2:846859', 'n': 'correct in original'}

corrected abstract:
<p>Gasification of biomass is today facing several problems with the high amount of tar produced and compounds such as alkali that can harm the catalyst in catalytic tar reformation. This is why the focus in this master thesis study was to create a catalyst for secondary tar steam reforming. The aim was to create a catalyst that was suitable for tar steam reforming and also evaluate the effect that alkali had on the catalyst.</p><p>A catalyst composed of 20%Bronzes –ZrO2 impregnated with nickel was prepared in this study and characterised with XRD and BET. The bronzes consisted of K<sub>0.25</sub>WO<sub>3</sub> and was prepared with two different methods and analysed with XRD to see if there was some difference in the structure and purity. Three different weight loads of nickel: 5-,10- and 15 wt% , was prepared for each catalyst that was named Method 1 and Method 2. In total six catalysts was tested in an experimental test rig that was situated at the Royal Institute of Technology in Stockholm. In addition a blank test was performed for comparison of the catalytic activity.</p><p>For the experiments 1-methylnaphthalene was decided to be used as a simulated tar. The experiments were divided into two parts where in Part 1 a S/C ratio of 4 was used and Part 2 a S/C ratio of 6 was used. The experiments were conducted at reactor temperatures of 700 °C and 800 °C with or without alkali aerosols. Other parameters changed in the experiments were the catalyst load, 1-methylnaphthalene flow and the gas hourly space velocity. Results were analysed with 4 micro gas chromatographs and solid phase adsorption.</p><p>Results from the catalyst characterisation indicated that the wanted catalyst had been prepared however in Method 2 a higher purity of the bronzes was reached compared to Method 1. The results from the BET analysis gave a surface area of between 40-46 m<sup>2</sup>/g for the different catalysts.</p><p>In the experiments from Part 1 a very high gas hourly space velocity was used and the results indicated that there was almost tar reduction compared to the blank test. In Part 2 the gas hourly space velocity was lowered and a higher tar reduction and was obtained. One test was also conducted at 900 °C where the highest tar reduction was obtained, almost 40 %.</p><p>From the results it could be seen tar reduction and 1-MN/naphthalene ratio was increasing with higher temperatures and nickel loadings. Catalysts prepared from Method 2 also showed a higher tar reduction and 1-MN/naphthalene ratio compared to Method 1which could indicate that it was more stable and had a higher purity of the bronzes. The results from atomic absorption spectrophotometer showed that the mass of potassium in the catalyst before the experiment decreased between 3-29 % compared to after the experiment. From the rather low decrease in potassium the hexagonal structure of the bronzes clearly protects the potassium from evaporating within the bulk. Also introducing alkali aerosols had a positive effect on the tar reduction.</p>
----------------------------------------------------------------------
In diva2:801776 abstract is: <p>Droplet microfluidics is an emerging technology for single cell canalysis that allows compartmentalization of single cells in monodisperse picoliter sized water droplets in an immiscible oil carrier phase at rates over 1000 droplets per second. An application using the high throughput of droplet microfluidics is directed evolution of industrial enzyme production hosts.</p><p>The oxygen conditions in the droplets during cell incubation are not fully studied. Strict aerobes cannot grow without oxygen and facultative anaerobic cells such as S.cerevisiae change their phenotype based on the presence or absence of oxygen. Therefore it is of importance to nderstand, and if possible control, the oxygen levels in the droplets during incubation of cells.</p><p>In this master's thesis project the oxygen conditions in microfluidic droplet during incubation of cells was studied by comparing metabolite concentrations produced by S.cerevisiae in various incubation formats over time.</p><p>The concentrations of the metabolites produced by S.cerevisiae incubated in droplets were similar to the metabolite profiles in the non-droplet control formats. This indicates that incubating cells in droplets does not impact cell metabolism significantly. Furthermore, the metabolite concentraions in droplets incubated in a syringe had a similar profile as in teh anaerobic control culture, indicating the conditions in droplets in syringe being anaerobic or oxygen limited. A new droplet incubation format, droplets in wide tube, was engineered aiming to provide more oxygen to the droplets. The concentrations of the metabolites in this format were more similar to the concentrations in the aerobic control format, indicating aerobic conditions.</p><p>Incubation of B.subtili, whish is generally considered being a strictly aerobic bacterium, in droplets and contral formats was ised as an additional indicator of oxygenation. In the droplets in wide tube and aerobic control formats B.subtilis was shown to proliferate, unlike in droplets in syringe and anaerobic control.</p>

w='concentraions' val={'c': 'concentrations', 's': 'diva2:801776'}
w='canalysis' val={'c': 'analysis', 's': 'diva2:801776', 'n': 'no full text'}
w='contral' val={'c': 'control', 's': 'diva2:801776', 'n': 'no full text'}
w='ised' val={'c': 'used', 's': 'diva2:801776', 'n': 'no full text'}
w='nderstand' val={'c': 'understand', 's': 'diva2:801776', 'n': 'no full text'}
w='teh' val={'c': 'the', 's': ['diva2:744699', 'diva2:801776']}
Note "S. cerevisiae" is "Saccharomyces cerevisiae"
and "B. subtilis" is "Bacillus subtilis"
I assumed the usual italics for Latin words.

corrected abstract:
<p>Droplet microfluidics is an emerging technology for single cell analysis that allows compartmentalization of single cells in monodisperse picoliter sized water droplets in an immiscible oil carrier phase at rates over 1000 droplets per second. An application using the high throughput of droplet microfluidics is directed evolution of industrial enzyme production hosts.</p><p>The oxygen conditions in the droplets during cell incubation are not fully studied. Strict aerobes cannot grow without oxygen and facultative anaerobic cells such as S.cerevisiae change their phenotype based on the presence or absence of oxygen. Therefore it is of importance to understand, and if possible control, the oxygen levels in the droplets during incubation of cells.</p><p>In this master's thesis project the oxygen conditions in microfluidic droplet during incubation of cells was studied by comparing metabolite concentrations produced by <em>S. cerevisiae</em> in various incubation formats over time.</p><p>The concentrations of the metabolites produced by <em>S. cerevisiae</em> incubated in droplets were similar to the metabolite profiles in the non-droplet control formats. This indicates that incubating cells in droplets does not impact cell metabolism significantly. Furthermore, the metabolite concentrations in droplets incubated in a syringe had a similar profile as in the anaerobic control culture, indicating the conditions in droplets in syringe being anaerobic or oxygen limited. A new droplet incubation format, droplets in wide tube, was engineered aiming to provide more oxygen to the droplets. The concentrations of the metabolites in this format were more similar to the concentrations in the aerobic control format, indicating aerobic conditions.</p><p>Incubation of <em>B. subtilis</em>, whish is generally considered being a strictly aerobic bacterium, in droplets and control formats was used as an additional indicator of oxygenation. In the droplets in wide tube and aerobic control formats <em>B. subtilis</em> was shown to proliferate, unlike in droplets in syringe and anaerobic control.</p>
----------------------------------------------------------------------
In diva2:935159 abstract is: <p>Internet of things is today a development that is strongly moving forward to make society more con-trolled by technology, modern and it will be easier for many people. This technology can be used in many diffrent industries and work areas, such as elevator monitoring. This report evaluates the benefits with Internet of things within elevator monitoring. The company Safeline Sweden AB has developed a product that will make it easier to monitor elevators from a distance. An investigation of this product has been done and what their advantages were. After that, a buisness model was formed for Safelines new product IMS. By interviewing potential customers, the information about the sitaution today and the approach for new technology was discovered. The information from the interviews was then used to create a good and sustainable business model for Safeline.A buisness model was created for Safeline Sweden AB and the produkt IMS. The business model describes how the company should launch the new product, how the customer relations should be created, developed and retained. The information from the interviews made it clear that the IMS should be sold as a system solution because it is considered to be more attractive and be more valu-able for the customers.</p>

w='buisness' val={'c': 'business', 's': 'diva2:935159', 'n': 'error in original'}
w='diffrent' val={'c': 'different', 's': ['diva2:1186252', 'diva2:935159'], 'n': 'error in original'}
w='sitaution' val={'c': 'situation', 's': 'diva2:935159', 'n': 'error in original'}

corrected abstract:
<p>Internet of things is today a development that is strongly moving forward to make society more controlled by technology, modern and it will be easier for many people. This technology can be used in many diffrent industries and work areas, such as elevator monitoring. This report evaluates the benefits with Internet of things within elevator monitoring. The company Safeline Sweden AB has developed a product that will make it easier to monitor elevators from a distance. An investigation of this product has been done and what their advantages were. After that, a buisness model was formed for Safelines new product IMS. By interviewing potential customers, the information about the sitaution today and the approach for new technology was discovered. The information from the interviews was then used to create a good and sustainable business model for Safeline.</p><p>A buisness model was created for Safeline Sweden AB and the produkt IMS. The business model describes how the company should launch the new product, how the customer relations should be created, developed and retained. The information from the interviews made it clear that the IMS should be sold as a system solution because it is considered to be more attractive and be more valuable for the customers.</p>
----------------------------------------------------------------------
In diva2:839241 abstract is: <p>A new type of natural, antimicrobial, biocompatible agent is presented. Lanasol--a brominated cyclic compound that can be extracted from red sea algae and occurs naturally was applied onto the three most commonly available fabrics: cotton, wool and fleece. Samples of the fabrics were tested for a series of properties such Antibacterial effectiveness, durability to laundering and flammability.</p><p> </p><p>The treated fabric surface and the Lanasol percentage present at the surface of the fabric were evaluated by performing SEM, IR and Density Archimedes. The antimicrobial activity of the coated fibres against fungi Fussarium, was assessed using qualitative Agar diffusion test.</p><p> </p><p>The Lanasol treated samples displayed very good antimicrobial properties compared to the untreated control samples.  It was observes that antibacterial effectives increased significantly with higher Lanasols solutions of 10% compared to 2,5%. Antibacterial properties remained effective after 10 washes but decreased visibly after 20 washes.  Anti-flammability properties were strongest in unwashed samples with considerable ignition time differences to the untreated control samples.</p><p> </p><p>The results obtained from the current study demonstrate that Lanasol, which has previously not been tested for textile durability, has significant potential as a new antibacterial agent. This concept opens the door to further research into the increasing the antimicrobials properties as well as enhancing the success of commercial application of Lanasol for textile treatment.</p>

partal corrected: diva2:839241: <p>A new type of natural, antimicrobial, biocompatible agent is presented. Lanasol--a bromin ated cyclic compound that can be extracted from red sea algae and occurs naturally was applied onto the three most commonly available fabrics: cotton, wool and fleece. Samples of the fabrics were tested for a series of properties such Antibacterial effectiveness, durability to laundering and flammability.</p><p> </p><p>The treated fabric surface and the Lanasol percentage present at the surface of the fabric were evaluated by performing SEM, IR and Density Archimedes. The antimicrobial activity of the coated fibres against fungi Fussarium, was assessed using qualitative Agar diffusion test.</p><p> </p><p>The Lanasol treated samples displayed very good antimicrobial properties compared to the untreated control samples.  It was observes that antibacterial effectives increased significantly with higher Lanasols solutions of 10% compared to 2,5%. Antibacterial properties remained effective after 10 washes but decreased visibly after 20 washes.  Anti-flammability properties were strongest in unwashed samples with considerable ignition time differences to the untreated control samples.</p><p> </p><p>The results obtained from the current study demonstrate that Lanasol, which has previously not been tested for textile durability, has significant potential as a new antibacterial agent. This concept opens the door to further research into the increasing the antimicrobials properties as well as enhancing the success of commercial application of Lanasol for textile treatment.</p>
w='effectives' val={'c': 'effectiveness', 's': 'diva2:839241', 'n': 'no full text'}

corrected abstract:
<p>A new type of natural, antimicrobial, biocompatible agent is presented. Lanasol--a brominated cyclic compound that can be extracted from red sea algae and occurs naturally was applied onto the three most commonly available fabrics: cotton, wool and fleece. Samples of the fabrics were tested for a series of properties such Antibacterial effectiveness, durability to laundering and flammability.</p><p> </p><p>The treated fabric surface and the Lanasol percentage present at the surface of the fabric were evaluated by performing SEM, IR and Density Archimedes. The antimicrobial activity of the coated fibres against fungi Fussarium, was assessed using qualitative Agar diffusion test.</p><p> </p><p>The Lanasol treated samples displayed very good antimicrobial properties compared to the untreated control samples.  It was observes that antibacterial effectiveness increased significantly with higher Lanasols solutions of 10% compared to 2,5%. Antibacterial properties remained effective after 10 washes but decreased visibly after 20 washes.  Anti-flammability properties were strongest in unwashed samples with considerable ignition time differences to the untreated control samples.</p><p> </p><p>The results obtained from the current study demonstrate that Lanasol, which has previously not been tested for textile durability, has significant potential as a new antibacterial agent. This concept opens the door to further research into the increasing the antimicrobials properties as well as enhancing the success of commercial application of Lanasol for textile treatment.</p>
----------------------------------------------------------------------
In diva2:1677054 abstract is: <p>Cellulose nanofibrils (CNFs) are one of nature’s most fundamental building blocks, providing incredible strength and stiffness to natural materials, such as the outer cell wall layer in wood. By mimicking the architecture of wood, possibilities opens up for the fabrication of new, biobased, light-weight structural materials with mechanical properties exceeding that of glassfibers, metals and alloys. However, the engineering challenge lies in successfully managing to translate the desirable mechanical properties of the CNFs into filaments that can be used in everyday life materials. Throughout the process of spinning the extracted CNFs into filaments, many factors and parameters affect the ultimate functionality and performance of the resulting filaments. Measuring the dimensions of the spun filaments is a crucial step in further optimizing process parameters. The width of the filament especially, impacts its mechanical performance. The characterization of the cellulose filament width is currently very time-consuming as each filament is manually measured using optical microscopy. The primary goal of this project is to make the current characterization process more effective, with respect to both accuracy and speed of measurement, by using laser scattering. In this report, we demonstrate a reduction by more than a half in measurement time using a 3D-printed laser scattering setup instead of an optical microscope when measuring filament width. Our results indicate that the certainty in measurement is generally higher for lase rscattering compared to optical microscopy. The mean standard deviations (SD) for the smallest widths estimated with optical microscopy and the two curve fitting methods used for the laser measurements are reported to be 1.62, 0.85 (Curve fit) and 1.59 (Minima matching) respectively. However, standard deviations for the thinnest width does not correlate directly to the accuracy of the methods since the spun filaments show a large variation in width along the length. A closer comparison between measurement values for matched points at ideal and non-uniform filaments demonstrate that the accuracy of the laser measurements are dependent on the uniformity of the filaments, with non-uniform filaments negatively impacting the accuracy. Our overall results supports the fact that a thinner filament gives a better resolution and smaller error when measuring with laser. Our results provide evidence for the great potential of laser scattering as a more efficient method for cellulose filament width determination.</p>

w='rscattering' val={'c': 'scattering', 's': 'diva2:1677054', 'n': 'correct in original; the "r" comves from the end of the previous word "laser"'}

corrected abstract:
<p>Cellulose nanofibrils (CNFs) are one of nature’s most fundamental building blocks, providing incredible strength and stiffness to natural materials, such as the outer cell wall layer in wood. By mimicking the architecture of wood, possibilities opens up for the fabrication of new, biobased, light-weight structural materials with mechanical properties exceeding that of glass fibers, metals and alloys. However, the engineering challenge lies in successfully managing to translate the desirable mechanical properties of the CNFs into filaments that can be used in everyday life materials. Throughout the process of spinning the extracted CNFs into filaments, many factors and parameters affect the ultimate functionality and performance of the resulting filaments. Measuring the dimensions of the spun filaments is a crucial step in further optimizing process parameters. The width of the filament especially, impacts its mechanical performance. The characterization of the cellulose filament width is currently very time-consuming as each filament is manually measured using optical microscopy. The primary goal of this project is to make the current characterization process more effective, with respect to both accuracy and speed of measurement, by using laser scattering. In this report, we demonstrate a reduction by more than a half in measurement time using a 3D-printed laser scattering setup instead of an optical microscope when measuring filament width. Our results indicate that the certainty in measurement is generally higher for laser scattering compared to optical microscopy. The mean standard deviations (SD) for the smallest widths estimated with optical microscopy and the two curve fitting methods used for the laser measurements are reported to be 1.62, 0.85 (Curve fit) and 1.59 (Minima matching) respectively. However, standard deviations for the thinnest width does not correlate directly to the accuracy of the methods since the spun filaments show a large variation in width along the length. A closer comparison between measurement values for matched points at ideal and non-uniform filaments demonstrate that the accuracy of the laser measurements are dependent on the uniformity of the filaments, with non-uniform filaments negatively impacting the accuracy. Our overall results supports the fact that a thinner filament gives a better resolution and smaller error when measuring with laser. Our results provide evidence for the great potential of laser scattering as a more efficient method for cellulose filament width determination.</p>
----------------------------------------------------------------------
In diva2:745531 abstract is: <p>A power cable is a device that carries or conducts power to an electrical appliance and it is used for the transmission or distribution of electricity. Power cables are formed by conductor strands of high conductivity metals such as aluminum or copper, surrounded by polymeric insulated materials. Among the polymeric insulated materials are the conductor screen, insulation material, insulation screen, sheathing Material and sometimes, aged in the type of cable, a few other layers of materials.</p><p> </p><p>All the materials that form a power cable are plastic, except for the conductor. The plastic materials age over time, generating as a result failure, break or cease of its function. The most important processes that decrease the lifespan of a cable are the termooxidative degradation and electrical treeing. In the termooxidative degradation, the most important factors that affect the lifespan are the type of plastic materials, the presence of other chemicals in the cable system, the temperature and contact with liquids.</p><p>  </p><p>One of the objectives of this work is to study the lifespan of different conductor screen materials, regarding their compatibility in different oils present in the production process.</p><p> </p><p>Likewise, the influence of the degree of crosslinking on the lifespan of conductor screen materials was studied, considering the ageing in oil. For this determination, two levels of crosslinking degrees were studied and compared.</p><p> </p><p>Finally, the influence of water trees in the structure or constitution of the conductor screen, XLPE insulation and the interface between them was evaluated, after the electrical ageing of cables. The influence was observed on the surface of electrically aged cables, using a Scanning Electron Microscope.</p>

w='termooxidative' val={'c': 'thermooxidative', 's': 'diva2:745531', 'n': 'no full text'}
Also eliminated the empty paragraphs.

corrected abstract:
<p>A power cable is a device that carries or conducts power to an electrical appliance and it is used for the transmission or distribution of electricity. Power cables are formed by conductor strands of high conductivity metals such as aluminum or copper, surrounded by polymeric insulated materials. Among the polymeric insulated materials are the conductor screen, insulation material, insulation screen, sheathing Material and sometimes, aged in the type of cable, a few other layers of materials.</p><p>All the materials that form a power cable are plastic, except for the conductor. The plastic materials age over time, generating as a result failure, break or cease of its function. The most important processes that decrease the lifespan of a cable are the thermooxidative degradation and electrical treeing. In the thermooxidative degradation, the most important factors that affect the lifespan are the type of plastic materials, the presence of other chemicals in the cable system, the temperature and contact with liquids.</p><p>One of the objectives of this work is to study the lifespan of different conductor screen materials, regarding their compatibility in different oils present in the production process.</p><p>Likewise, the influence of the degree of crosslinking on the lifespan of conductor screen materials was studied, considering the ageing in oil. For this determination, two levels of crosslinking degrees were studied and compared.</p><p>Finally, the influence of water trees in the structure or constitution of the conductor screen, XLPE insulation and the interface between them was evaluated, after the electrical ageing of cables. The influence was observed on the surface of electrically aged cables, using a Scanning Electron Microscope.</p>
----------------------------------------------------------------------
In diva2:442833 abstract is: <p>Polymeric seals are widely installed in nuclear power plants. Since the seals are used to operational safety, it is very important to assess long-term behaviour.</p>
<p>In this study, the polymeric material used is Ethylene-Propylene-diene rubber (EPDM) because it has outstanding mechanical property and the characteristic of having high resistance to heat. An elevated temperature causes its ageing and therefore its degradation. It is great value that the condition of the seal is determined.  This was achieved by studying samples that underwent accelerated ageing by different analytical methods; such as tensile testing, indenter modulus measurements, thermogravimetric analysis and infrared spectroscopy.</p>
<p>Two EPDM samples were examined which came from the Swedish nuclear power plant company OKG; one had been exposed for 3 years (EPDM aged) and the other one was unaged (EPDM unaged). The lifetime prediction was evaluated by Young’s modulus (the data increased  with ageing time), strain at break (the values decreased as ageing is accelerated) and indenter modulus (the data followed the trend of Young’s modulus). From mechanical testing, there was not difference between EPDM unaged and aged. Therefore, the most important degradation is happened inside the oven at different temperatures. The mechanical data showed Arrhenius temperature dependence with activation energies at about of 50 kJ/mol for Young’s modulus and strain at break and of 40 kJ/mol for indenter modulus. These activation energies were used to extrapolate the lifetimes to dofferent temperatures (30°C, 40°C and 50°C). The composition of EPDM seal was determined by TGA and IR whereas the growing number of crosslinks was tested by density testing and swelling test.</p>

w='dofferent' val={'c': 'different', 's': 'diva2:442833', 'n': 'no full text'}

corrected abstract:
<p>Polymeric seals are widely installed in nuclear power plants. Since the seals are used to operational safety, it is very important to assess long-term behaviour.</p><p>In this study, the polymeric material used is Ethylene-Propylene-diene rubber (EPDM) because it has outstanding mechanical property and the characteristic of having high resistance to heat. An elevated temperature causes its ageing and therefore its degradation. It is great value that the condition of the seal is determined.  This was achieved by studying samples that underwent accelerated ageing by different analytical methods; such as tensile testing, indenter modulus measurements, thermogravimetric analysis and infrared spectroscopy.</p><p>Two EPDM samples were examined which came from the Swedish nuclear power plant company OKG; one had been exposed for 3 years (EPDM aged) and the other one was unaged (EPDM unaged). The lifetime prediction was evaluated by Young’s modulus (the data increased  with ageing time), strain at break (the values decreased as ageing is accelerated) and indenter modulus (the data followed the trend of Young’s modulus). From mechanical testing, there was not difference between EPDM unaged and aged. Therefore, the most important degradation is happened inside the oven at different temperatures. The mechanical data showed Arrhenius temperature dependence with activation energies at about of 50 kJ/mol for Young’s modulus and strain at break and of 40 kJ/mol for indenter modulus. These activation energies were used to extrapolate the lifetimes to different temperatures (30°C, 40°C and 50°C). The composition of EPDM seal was determined by TGA and IR whereas the growing number of crosslinks was tested by density testing and swelling test.</p>
----------------------------------------------------------------------
In diva2:862341 abstract is: <p>At the School of Technology and Health (STH) on KTH, a micro- CT is beeing build. The micro-CT-system need to be aligned in order to function correctly. The purpose of this bachelor thesis was to create an application that, from the micro-CT images, could calculate all the necessary parameters to make the alignment pos- sible. During the projekt, LabView-code for aligning a micro-CT system has been modified to being compatible with the micro-CT at the School of Technology and Health (STH) on KTH. The code has also partly been translated into the open-source and platform independent programming language Java. Due to delays in building the micro-CT at STH, neither the LabView- or Javacode could be validated with it. However, successful attempts have been made with images from another micro-CT and on simulated images of a phantom in a micro-CT. </p>

w='beeing' val={'c': 'being', 's': ['diva2:862341', 'diva2:739841', 'diva2:1455922']}

corrected abstract:
<p>At the School of Technology and Health (STH) on KTH, a micro-CT is being build. The micro-CT-system need to be aligned in order to function correctly. The purpose of this bachelor thesis was to create an application that, from the micro-CT images, could calculate all the necessary parameters to make the alignment possible. During the projekt, LabView-code for aligning a micro-CT system has been modified to being compatible with the micro-CT at the School of Technology and Health (STH) on KTH. The code has also partly been translated into the open-source and platform independent programming language Java. Due to delays in building the micro-CT at STH, neither the LabView- or Javacode could be validated with it. However, successful attempts have been made with images from another micro-CT and on simulated images of a phantom in a micro-CT.</p>
----------------------------------------------------------------------
In diva2:1235942 abstract is: <p><em>Background</em>: Individuals with cerebral palsy (CP) are prone to sedentary life with limited recreational- and physical activity. RaceRunning (RR) is a sport for individuals with disabilities such as motor impairment that allow them to move independently and enjoy physical activity. The aim of the study was to investigate muscle fatigue of RR athletes with different classiffcation of CP during six minuteRaceRunning test (6MRRT). The Cerebral Palsy International Sports and RecreationAssociation has the goal to make RR a Paralympic sport. For a sport to be included in the Paralympics a classiffcation system has to be made. Information about muscle function during RR could give vital physical information about athletes with CP and contribute to the development of such classiffcation system.</p><p><em>Method</em>: Median frequency of the power spectrum from surface electromyography(sEMG) of four muscles (vastus lateralis, biceps femoris, gluteus medius, gastrocnemius lateralis) was computed along with amplitude changes (root mean square) in order to assess muscle fatigue.</p><p><em>Results</em>: Indications of muscle fatigue were observed from one out of five athletes for athlete's less affected side. Further sEMG studies with more subjects have to be conducted on CP RR athletes to give more conclusive results of muscle fatigue.</p>

w='classiffcation' val={'c': 'classification', 's': 'diva2:1235942', 'n': 'missing ligature'}
There were also other missing ligatures.


corrected abstract:
<p><em>Background</em>: Individuals with cerebral palsy (CP) are prone to sedentary life with limited recreational- and physical activity. RaceRunning (RR) is a sport for individuals with disabilities such as motor impairment that allow them to move independently and enjoy physical activity. The aim of the study was to investigate muscle fatigue of RR athletes with different classification of CP during six minute RaceRunning test (6MRRT). The Cerebral Palsy International Sports and Recreation Association has the goal to make RR a Paralympic sport. For a sport to be included in the Paralympics a classification system has to be made. Information about muscle function during RR could give vital physical information about athletes with CP and contribute to the development of such classification system.</p><p><em>Method</em>: Median frequency of the power spectrum from surface electromyography (sEMG) of four muscles (vastus lateralis, biceps femoris, gluteus medius, gastrocnemius lateralis) was computed along with amplitude changes (root mean square) in order to assess muscle fatigue.</p><p><em>Results</em>: Indications of muscle fatigue were observed from one out of five athletes for athlete's less affected side. Further sEMG studies with more subjects have to be conducted on CP RR athletes to give more conclusive results of muscle fatigue.</p>
----------------------------------------------------------------------
In diva2:826703 abstract is: <p>Industrial processes in the food industry very often produce airstreams of contaminated air. The impurities can be both particulates and gaseous compounds. Controlling these emissions is of a great concern both regarding governmental and social requirements.</p><p>The overall aim with the thesis was to construct an easily portable, pilot scale air purification system. The main purpose with the pilot scale system was to enable testing of contaminated air generated from industries. The results from the industry tests with the pilot scale system would reveal valuable information that could be used later for the design of a large scale cleaning system.</p><p>The work was assigned by Ozone Tech Systems. The company is specialized in ozone treat-ments intended for industrial and domestic uses. The work done within the thesis project was performed in close relations with equipment suppliers and Ozone Tech System personnel.</p><p>The design of the pilot scale air purification system was based on contaminated air, generated from fish smokehouses. The smoke that is involved in the hot smoking process is derived in conjunction with alder chips smoldering. Although the pilot scale system was designed for purifying air generated from fish smokehouses, it should also suit other processes that gener-ate similar air emissions. Emission factors, literature data and measured values played an im-portant role for estimating the air composition in the air generated from the specific process. The treatment objective was determined by European regulations as well as a desired reduc-tion of unwanted odors. The odor was found to mainly be caused by volatile organic com-pounds (VOC).</p><p>The pilot system was predefined to consist of a primary step of particle separation, oxidation with ozone, particle separation consisting of a HEPA 13 filtration stage and at last adsorption with activated carbon. During the design process it was decided to use a venturi scrubber as the primary particle separator and to add a pre-filter stage before the HEPA 13 filter.</p><p>The work has resulted in a theoretical design of the air purification system. The units are de-tachable which provides a simple transportation as well as the possibility to use an adaptable combination of the units.</p><p>The purchase of equipment was not a part of the thesis, but the coordination of the purchase has been completed ensuring that the purchase could be done after the end of the project. In order for the system to work, a fan needs to be installed and pipe connections between the units have to be designed and purchased, but this was not a part in the project.</p>

w='treat-ments' val={'c': 'treatments', 's': 'diva2:826703'}
w='reduc-tion' val={'c': 'reduction', 's': 'diva2:826703'}

corrected abstract:
<p>Industrial processes in the food industry very often produce airstreams of contaminated air. The impurities can be both particulates and gaseous compounds. Controlling these emissions is of a great concern both regarding governmental and social requirements.</p><p>The overall aim with the thesis was to construct an easily portable, pilot scale air purification system. The main purpose with the pilot scale system was to enable testing of contaminated air generated from industries. The results from the industry tests with the pilot scale system would reveal valuable information that could be used later for the design of a large scale cleaning system.</p><p>The work was assigned by Ozone Tech Systems. The company is specialized in ozone treatments intended for industrial and domestic uses. The work done within the thesis project was performed in close relations with equipment suppliers and Ozone Tech System personnel.</p><p>The design of the pilot scale air purification system was based on contaminated air, generated from fish smokehouses. The smoke that is involved in the hot smoking process is derived in conjunction with alder chips smoldering. Although the pilot scale system was designed for purifying air generated from fish smokehouses, it should also suit other processes that generate similar air emissions. Emission factors, literature data and measured values played an important role for estimating the air composition in the air generated from the specific process. The treatment objective was determined by European regulations as well as a desired reduction of unwanted odors. The odor was found to mainly be caused by volatile organic compounds (VOC).</p><p>The pilot system was predefined to consist of a primary step of particle separation, oxidation with ozone, particle separation consisting of a HEPA 13 filtration stage and at last adsorption with activated carbon. During the design process it was decided to use a venturi scrubber as the primary particle separator and to add a pre-filter stage before the HEPA 13 filter.</p><p>The work has resulted in a theoretical design of the air purification system. The units are detachable which provides a simple transportation as well as the possibility to use an adaptable combination of the units.</p><p>The purchase of equipment was not a part of the thesis, but the coordination of the purchase has been completed ensuring that the purchase could be done after the end of the project. In order for the system to work, a fan needs to be installed and pipe connections between the units have to be designed and purchased, but this was not a part in the project.</p>
----------------------------------------------------------------------
In diva2:1061386 abstract is: <p>Indoor air quality is directly affected by the number of people and is a problematic area in school environments. The number of students is expected to increase in coming years in the Stockholm schools and there is a risk that existing ventilation system is not enough efficient.</p><p>Aim of this study was to investigate and analyse the teachers and students working environment on the air quality. Questions of extraordinary interest were for example: how does CO2 concentrations, temperature and humidity vary during lessons; are measuring instruments and method used in municipal project sufficiently reliable; and can schools handle the increased student population on the basis of maintaining air quality.</p><p>Theory analysis in this study showed that many schools does not pass the OVKcontrols and that they have deficient ventilation. It will be difficult for the schools to cope with an increased number of students, without improving the ventilation to be able to meet the requirements for ventilation from the Swedish Work Environment Authority.</p><p>The method used was measurements that were collected and analysed air quality quantitatively. The results of the measurements showed that the temperature and humidity is at a acceptable level. Overall, the CO2 concentrations did not exceed 1 000 ppm but accumulate during the day which require actions, yet a few measured concentrations did exceed the limit of 1000 ppm. Some schools that were included in the analysis will not be able to maintaining air quality show at an increasing population of students, more action will be required by the municipality.</p><p>Analysis of the measurement technique used shows that the validity of the technique can be improved as well as the need for a number of measurement points to be used in the classroom. Measurement of outside airflow needed for a classroom can be made by emanuate from the requirements of Swedish Work Environment Authority, but it is not always possible to increase outside airflow in the existing ventilation system.</p><p>This work points at a few options for increased airflow when the ventilation system is found to be under-dimensioned for the existing number of students.</p>

w='emanuate' val={'c': 'emanate', 's': 'diva2:1061386'}

corrected abstract:
<p>Indoor air quality is directly affected by the number of people and is a problematic area in school environments. The number of students is expected to increase in coming years in the Stockholm schools and there is a risk that existing ventilation system is not enough efficient.</p><p>Aim of this study was to investigate and analyse the teachers and students working environment on the air quality. Questions of extraordinary interest were for example: how does CO<sub>2</sub> concentrations, temperature and humidity vary during lessons; are measuring instruments and method used in municipal project sufficiently reliable; and can schools handle the increased student population on the basis of maintaining air quality.</p><p>Theory analysis in this study showed that many schools does not pass the OVK-controls and that they have deficient ventilation. It will be difficult for the schools to cope with an increased number of students, without improving the ventilation to be able to meet the requirements for ventilation from the Swedish Work Environment Authority.</p><p>The method used was measurements that were collected and analysed air quality quantitatively. The results of the measurements showed that the temperature and humidity is at a acceptable level. Overall, the CO<sub>2</sub> concentrations did not exceed 1 000 ppm but accumulate during the day which require actions, yet a few measured concentrations did exceed the limit of 1000 ppm. Some schools that were included in the analysis will not be able to maintaining air quality show at an increasing population of students, more action will be required by the municipality.</p><p>Analysis of the measurement technique used shows that the validity of the technique can be improved as well as the need for a number of measurement points to be used in the classroom.</p><p>Measurement of outside airflow needed for a classroom can be made by emanate from the requirements of Swedish Work Environment Authority, but it is not always possible to increase outside airflow in the existing ventilation system.</p><p>This work points at a few options for increased airflow when the ventilation system is found to be under-dimensioned for the existing number of students.</p>
----------------------------------------------------------------------
In diva2:1876173 abstract is: <p>A large, conformationally flexible protein complex known as the Mediator controls transcription initiation by acting as a transcriptional activator and facilitating the assembly of the preinitiation complex (PIC). The Mediator is involved in several different activities, such as transcription initiation, elongation, pausing, enhancer-promoter loop formation, and chromatin architectural organization. The objective of the project was to investigate the localization of the Mediator subunits MED1 and CDK8 by analyzing their bindng to enhancers and promoters across the human genome. Computational analysis of available CDK8 and MED1 ChIP-seq data from the ENCODE consortium [31] was carried out during initial phase of the project. The second part was lab work aimed to conduct ChIP and generate novel MED1 ChIP-seq data in human K562 cells. This part was done up to DNA extraction and Qubit measurement. Computational analysis support both CDK8 and MED1 subunits to play an important role in the formation of PIC due to a large number of binding sites present along promoters in the human genome. A considerable number of MED1 binding sites were present in closed chromatin, as assessed from ATAC-seq peak files. Instead, CDK8 binding sites were more often found at accessible chromatin. This promoted the idea of the distinct roles of MED1 and CDK8 in controlling RNA Polymerase II (Pol II) and possible enhancer-promoter loop formation. MED1 and CDK8 binding at untranscribed regulatory regions indicated that some MED1 binding sites showed almost no transcriptional activity, analysed from Precision Run-On sequencing (PRO-seq) data. The lack of ATAC-seq peaks and engaged Pol II complexes suggests that MED1 can occupy untranscribed regulatory regions or be present at enhancers prior to their transcriptional activation. Comparison of CDK8 and MED1 binding sites to ENCODE datasets further supported the CDK8 occupying open and transcriptionally active chromatin, while MED1 was also found on untranscribed regions. These findings suggest that there might be distinct states of activity at Mediator-occupied regions: 1) MED1-occupied closed and untransribed chromatin, 2) MED1 occupied sites with enriched H3K4me1 and 3) MED1 and CDK8 occupied enriched on H3K4me3, H3K27Ac and Pol II.</p>

w='untransribed' val={'c': 'untranscribed', 's': 'diva2:1876173'}

corrected abstract:
<p>A large, conformationally flexible protein complex known as the Mediator controls transcription initiation by acting as a transcriptional activator and facilitating the assembly of the preinitiation complex (PIC). The Mediator is involved in several different activities, such as transcription initiation, elongation, pausing, enhancer-promoter loop formation, and chromatin architectural organization. The objective of the project was to investigate the localization of the Mediator subunits MED1 and CDK8 by analyzing their binding to enhancers and promoters across the human genome. Computational analysis of available CDK8 and MED1 ChIP-seq data from the ENCODE consortium [31] was carried out during initial phase of the project. The second part was lab work aimed to conduct ChIP and generate novel MED1 ChIP-seq data in human K562 cells. This part was done up to DNA extraction and Qubit measurement. Computational analysis support both CDK8 and MED1 subunits to play an important role in the formation of PIC due to a large number of binding sites present along promoters in the human genome. A considerable number of MED1 binding sites were present in closed chromatin, as assessed from ATAC-seq peak files. Instead, CDK8 binding sites were more often found at accessible chromatin. This promoted the idea of the distinct roles of MED1 and CDK8 in controlling RNA Polymerase II (Pol II) and possible enhancer-promoter loop formation. MED1 and CDK8 binding at untranscribed regulatory regions indicated that some MED1 binding sites showed almost no transcriptional activity, analysed from Precision Run-On sequencing (PRO-seq) data. The lack of ATAC-seq peaks and engaged Pol II complexes suggests that MED1 can occupy untranscribed regulatory regions or be present at enhancers prior to their transcriptional activation. Comparison of CDK8 and MED1 binding sites to ENCODE datasets further supported the CDK8 occupying open and transcriptionally active chromatin, while MED1 was also found on untranscribed regions. These findings suggest that there might be distinct states of activity at Mediator-occupied regions: 1) MED1-occupied closed and untranscribed chromatin, 2) MED1 occupied sites with enriched H3K4me1 and 3) MED1 and CDK8 occupied enriched on H3K4me3, H3K27Ac and Pol II.</p>
----------------------------------------------------------------------
In diva2:1823407 abstract is: <p>The production of ethanol using <em>Saccharomyces cerevisiae</em> is important for the global fuel market and has the possibility to help cut the dependence on fossil fuels, thereby reducing greenhouse gas emissions. However, reaching the theoretical maximum yields during alcoholic fermentation remains challenging due to the production of glycerol, a by-product required to balance the excess NADH generated from cell growth. It has previously been shown that glycerol production can be removed through the deletion of <em>GPD1 </em>and <em>GPD2</em>, encoding for glycerol 3-phosphate dehydrogenases, and simultaneous overexpression of the Calvin-Cycle enzymes phophosribulokinase (PRK) and ribulose-1,5-bisphosphate carboxylase (Rubisco). Although glycerol production was eliminated, nine chromosomal copies of Rubisco were required due to its catalytic inefficiency, leading to a large transcriptional/translational burden for the cells. Therefore, this thesis seeks to identify more active Rubisco variants by investigating ten previously published highly active Rubisco variants through single copy expression in <em>S. cerevisiae</em>. The thesis has discovered three enzymes that resulted in higher growth rates than the previously used variant. The highest identified growth rate of 0.23 (h<sup>-1</sup>) was however not found to correlate with the highest identified enzymatic activity of Rubisco. Further it was discovered that laboratory evolution increased the growth rate of strain with initially lower growth rates from 0.09 to 0.22(h<sup>-1</sup>), though no significant increase in growth rate was observed in the fastest growing strain. Lastly the activity of PRK was measured and could be excluded as a limiting factor of the pathway. This thesis therefore acts as a steppingstone for continued investigations as it has established areas of interest to further investigate while establishing that the activity of Rubisco does not seem to be the limiting factor.</p>

w='phophosribulokinase' val={'c': 'phosphoribulokinase', 's': 'diva2:1823407', 'n': 'no full text'}

corrected abstract:
<p>The production of ethanol using <em>Saccharomyces cerevisiae</em> is important for the global fuel market and has the possibility to help cut the dependence on fossil fuels, thereby reducing greenhouse gas emissions. However, reaching the theoretical maximum yields during alcoholic fermentation remains challenging due to the production of glycerol, a by-product required to balance the excess NADH generated from cell growth. It has previously been shown that glycerol production can be removed through the deletion of <em>GPD1 </em>and <em>GPD2</em>, encoding for glycerol 3-phosphate dehydrogenases, and simultaneous overexpression of the Calvin-Cycle enzymes phophosribulokinase (PRK) and ribulose-1,5-bisphosphate carboxylase (Rubisco). Although glycerol production was eliminated, nine chromosomal copies of Rubisco were required due to its catalytic inefficiency, leading to a large transcriptional/translational burden for the cells. Therefore, this thesis seeks to identify more active Rubisco variants by investigating ten previously published highly active Rubisco variants through single copy expression in <em>S. cerevisiae</em>. The thesis has discovered three enzymes that resulted in higher growth rates than the previously used variant. The highest identified growth rate of 0.23 (h<sup>-1</sup>) was however not found to correlate with the highest identified enzymatic activity of Rubisco. Further it was discovered that laboratory evolution increased the growth rate of strain with initially lower growth rates from 0.09 to 0.22(h<sup>-1</sup>), though no significant increase in growth rate was observed in the fastest growing strain. Lastly the activity of PRK was measured and could be excluded as a limiting factor of the pathway. This thesis therefore acts as a steppingstone for continued investigations as it has established areas of interest to further investigate while establishing that the activity of Rubisco does not seem to be the limiting factor.</p>
----------------------------------------------------------------------
In diva2:744723 abstract is: <p>The advent of single-cell transcriptomics has enabled the study of cellular eterogeneity within and among populations. Cuurent methods are only able to process a small number of cells. A promising method for high-througput spatially resolved gene expression analysis with close to songle-cell resolution os currently being developed under the concept of spatial transcriptomics. Work is currently carried out to create bioinformatics tools to enable efficient analysis and integration of the data produced by the method.</p><p>This thesis describes an automatized pipeline that has been developed for the integration and post-processing of spatial transcriptomics cell line imaging and sequencing data. The pipeline was applied to data from two different cell lines derived from a four-stage cancer model study. Suitable pipeline parameters for the analysis of these cell lines are proposed.</p>

w='eterogeneity' val={'c': 'heterogeneity', 's': 'diva2:744723', 'n': 'no full text'}
w='high-througput' val={'c': 'high-throughput', 's': 'diva2:744723', 'n': 'no full text'}
w='songle-cell' val={'c': 'single-cell', 's': 'diva2:744723', 'n': 'no full text'}
w='Cuurent' val={'c': 'Current', 's': 'diva2:744723', 'n': 'no full text'}

corrected abstract:
<p>The advent of single-cell transcriptomics has enabled the study of cellular heterogeneity within and among populations. Current methods are only able to process a small number of cells. A promising method for high-throughput spatially resolved gene expression analysis with close to single-cell resolution os currently being developed under the concept of spatial transcriptomics. Work is currently carried out to create bioinformatics tools to enable efficient analysis and integration of the data produced by the method.</p><p>This thesis describes an automatized pipeline that has been developed for the integration and post-processing of spatial transcriptomics cell line imaging and sequencing data. The pipeline was applied to data from two different cell lines derived from a four-stage cancer model study. Suitable pipeline parameters for the analysis of these cell lines are proposed.</p>
----------------------------------------------------------------------
In diva2:911592 abstract is: <p>This thesis was done at Innventia and commissioned by Bioisolator. Bioisolator is an SME, small and medium sized company which markets cellulose-based materials for covering walls and ceilings. Bioisolator has developed a new and unique building material, Acustica Spackel, for sound insulation. The material acustica can be applied and mounted on any surface and has other characteristics as well, such as being heat-insulating and being resistant to moisture, mold, etc. This application is done by hand using a spatula, which is both time-consuming and exhausting. The aim of this method is to compare different methods and find the one that is more time efficient and requires less effort.</p><p>To investigate different methods and see what's on the market, a literature study was done in which various alternative methods were presented and described in more detail. Two main methods were used for examination and comparison; these are the wallpaper method and the spraying method. For each respective method, different procedures were given and gathered in a table. From the spraying method, screw pump and elmyggan were chosen and from the wallpaper method, Stratex was chosen, to be tested for the practical attempt in this thesis.</p><p>The result was that the screw pump was not suitable for this purpose. A few adjustments such as: in acustica pulp such as removal of flakesen, which are small circular plastic-like shapes which can clog the tubing, for the elmyggan was deemed more suitable. The StratEx trial is not finished yet because the experiments which were carried out in this project are considered to be insufficient to establish with certainty. Bioisolator, participated in certain moments, was satisfied with the result obtained; they tested the method elmygga then at home with approved results.</p>

w='flakesen' val={'c': 'flakes', 's': 'diva2:911592', 'n': 'error in original'}
w='Stratex' val={'c': 'StratEx', 's': 'diva2:911592', 'n': 'correct in oroginal'}
It appears as both 'Stratex' and 'StratEx' in the original!

Note that the "[.]" indicates there should be a period, but there is not actually one in the thesis.


corrected abstract:
<p>This thesis was done at Innventia and commissioned by Bioisolator. Bioisolator is an SME, small and medium sized company which markets cellulose-based materials for covering walls and ceilings. Bioisolator has developed a new and unique building material, Acustica Spackel, for sound insulation. The material acustica can be applied and mounted on any surface and has other characteristics as well, such as being heat-insulating and being resistant to moisture, mold, etc. This application is done by hand using a spatula, which is both time-consuming and exhausting. The aim of this method is to compare different methods and find the one that is more time efficient and requires less effort[.]</p><p>To investigate different methods and see what's on the market, a literature study was done in which various alternative methods were presented and described in more detail. Two main methods were used for examination and comparison; these are the wallpaper method and the spraying method. For each respective method, different procedures were given and gathered in a table. From the spraying method, screw pump and elmyggan were chosen and from the wallpaper method, Stratex was chosen, to be tested for the practical attempt in this thesis.</p><p>The result was that the screw pump was not suitable for this purpose. A few adjustments such as: in acustica pulp such as removal of flakesen, which are small circular plastic-like shapes which can clog the tubing, for the elmyggan was deemed more suitable. The StratEx trial is not finished yet because the experiments which were carried out in this project are considered to be insufficient to establish with certainty. Bioisolator, participated in certain moments, was satisfied with the result obtained; they tested the method elmygga then at home with approved results.</p>
----------------------------------------------------------------------
title: "MFA för att öka produktiviteten av 3HB av rekombinant E.coli"
==>    "Metabolic flux analysis for improved productivity of 3-hydroxybutyric acid in recombinant <em>Escherichia coli</em>"

In diva2:802022 abstract is: <p>Computer modeling has gained increasin attention as a quick method to investigate the effect of genetic changes on the metabolic networks of well-known organisms. This study has focused on the use of genome scale metabolic modelin of Escherichia coli metabolism to find ways of improving 3-hydroxybutyric acid (3HB) productivity. Standard flyx balance and flux variability analysis has been uswed, as well as thermodynamics-based metabolix flux analysis. The models led to a number of suggestions on how to improve the production.  Knockouts in AF1000 pTrcT3Rx were constructed from these predictions and tested for the effect on 3HB yield. The knockouts did not improve the yield of product but they allowed some insight to the limiting factors of 3HB sythesis. Based on this information, further suggestions were made on how to improve 3HB production.</p><p>To increase yield and productivity of 3HB, the growth rate of E. coli has to be limited. This can be done either through phosphorous or nitrogen limitation, or through an elevated pH. Since the production pathway is likely NADPH dependent, the glycolysis has to be steered towards the pentose phosphate pathway or the Entner-Doudoroff pathway for supply of the correct redox cofactor. This can be achieved by overexpression or knockouts of the correct genes. The use of lignocellulostic hydrolysates could potentially be a favorable substrate for an organism with modified glycolysis. In addition, such a substrate would open up possibilities for a cheaper and potentially greener process.</p>

w='flyx' val={'c': 'flux', 's': 'diva2:802022', 'n': 'correct in the original'}
w='increasin' val={'c': 'increasing', 's': 'diva2:802022', 'n': 'correct in original'}
w='metabolix' val={'c': 'metabolic', 's': 'diva2:802022'}
w='sythesis' val={'c': 'synthesis', 's': 'diva2:802022', 'n': 'correct in original'}
w='uswed' val={'c': 'used', 's': 'diva2:802022', 'n': 'correct in original'}

corrected abstract:
<p>Computer modeling has gained increasing attention as a quick method to investigate the effect of genetic changes on the metabolic networks of well-known organisms. This study has focused on the use of genome scale metabolic modeling of <em>Escherichia coli</em> metabolism to find ways of improving 3-hydroxybutyric acid (3HB) productivity. Standard flux balance and flux variability analysis has been used, as well as thermodynamics-based metabolic flux analysis. The models led to a number of suggestions on how to improve the production.  Knockouts in AF1000 pTrcT3Rx were constructed from these predictions and tested for the effect on 3HB yield. The knockouts did not improve the yield of product but they allowed some insight to the limiting factors of 3HB synthesis. Based on this information, further suggestions were made on how to improve 3HB production.</p><p>To increase yield and productivity of 3HB, the growth rate of E. coli has to be limited. This can be done either through phosphorous or nitrogen limitation, or through an elevated pH. Since the production pathway is likely NADPH dependent, the glycolysis has to be steered towards the pentose phosphate pathway or the Entner-Doudoroff pathway for supply of the correct redox cofactor. This can be achieved by overexpression or knockouts of the correct genes. The use of lignocellulostic hydrolysates could potentially be a favorable substrate for an organism with modified glycolysis. In addition, such a substrate would open up possibilities for a cheaper and potentially greener process.</p>
----------------------------------------------------------------------
In diva2:744701 abstract is: <p>The demand for new and healthy fermented dairy products is increasing around the world. To increase the amount of milk proteins in future dairy products is a promising part of the striving to fulfill the puzzle aiming to meet this need. Proteins are a way to go because proteins in bovine milk contain several amino acids essential for mankind. However, a more profound understanding of how the various components of the milk interact with each other and how they are affected during heat treatment and fermentation is necessarily, in order to cope with the rheology and taste challenges, which is imminent.</p><p>With the goal of creating a product containing 8 wt-% milk proteins, satisfactory viscosity and sense of taste, yoghurts and shots were produced based on the knowledge found within the cooperation of Arla Foods an in the literature. Three different bacterial cultures and nine different protein fractions combined resulted in nine an 12 different short-and yoghurt products, respectively. Then, rheological and sensory testing on selected products was performed.</p><p>The results from the analyzing showed that the rheological properties could be successfully modified with different compositions of milk protein fractions. This project showed successfully that it is possible to produce yoghurt and shots with 8 wt-% protein content with both satisfactory viscosity and taste.</p><p>Of the tested bacterial cultures for Arla mild low fat yoghurt, the most satisfying culture (concerning taste and texture) were the existing culture used at the dairies today. The yoghurt products with the best rheology and sensory results in combination contained (a) the non-commercialized protein fraction 1 and (b) the whey protein concentrate 1 and the whey protein concentrate 2. The results from the shot products showed that there still are taste challenges to overcome, although a successful product containing the protein 2 with both satisfying taste and texture were obtained.</p>

w='short-and' val={'B1': 'short- and', 's': 'diva2:744701', 'n': 'no full text'}

corrected abstract:
<p>The demand for new and healthy fermented dairy products is increasing around the world. To increase the amount of milk proteins in future dairy products is a promising part of the striving to fulfill the puzzle aiming to meet this need. Proteins are a way to go because proteins in bovine milk contain several amino acids essential for mankind. However, a more profound understanding of how the various components of the milk interact with each other and how they are affected during heat treatment and fermentation is necessarily, in order to cope with the rheology and taste challenges, which is imminent.</p><p>With the goal of creating a product containing 8 wt-% milk proteins, satisfactory viscosity and sense of taste, yoghurts and shots were produced based on the knowledge found within the cooperation of Arla Foods an in the literature. Three different bacterial cultures and nine different protein fractions combined resulted in nine an 12 different short-and yoghurt products, respectively. Then, rheological and sensory testing on selected products was performed.</p><p>The results from the analyzing showed that the rheological properties could be successfully modified with different compositions of milk protein fractions. This project showed successfully that it is possible to produce yoghurt and shots with 8 wt-% protein content with both satisfactory viscosity and taste.</p><p>Of the tested bacterial cultures for Arla mild low fat yoghurt, the most satisfying culture (concerning taste and texture) were the existing culture used at the dairies today. The yoghurt products with the best rheology and sensory results in combination contained (a) the non-commercialized protein fraction 1 and (b) the whey protein concentrate 1 and the whey protein concentrate 2. The results from the shot products showed that there still are taste challenges to overcome, although a successful product containing the protein 2 with both satisfying taste and texture were obtained.</p>
----------------------------------------------------------------------
In diva2:1703746 abstract is: <p>Security breaches caused by hackers are a significant issue for businesses. This illustrates the need for protection against these attacks. Using a Security Operations Center (SOC) solution to detect attacks against ones corporation is an essential step in doing that. How should businesses deploy their SOC?</p><p>This thesis compares traditional and modern SOC both by means of a literature study and hands-on experimentation, to evaluate which approach is appropriate for the current situation. The SOC solutions were set up to monitor a simulated office environment, using only free, open-source software.</p><p>This thesis sheds light on both subtle and significant differences betweenthe two solutions.</p><p>This thesis also concludes that the time for establishing a traditional SOC has passed. The advantages of utilizing a virtual SOC and its accompanying tool sare too significant to ignore.</p>

w='sare' val={'c': 'are', 's': 'diva2:1703746', 'n': 'the "s" belongs with the previous word "tools"'}

corrected abstract:
<p>Security breaches caused by hackers are a significant issue for businesses. This illustrates the need for protection against these attacks. Using a Security Operations Center (SOC) solution to detect attacks against ones corporation is an essential step in doing that. How should businesses deploy their SOC?</p><p>This thesis compares traditional and modern SOC both by means of a literature study and hands-on experimentation, to evaluate which approach is appropriate for the current situation. The SOC solutions were set up to monitor a simulated office environment, using only free, open-source software.</p><p>This thesis sheds light on both subtle and significant differences between the two solutions.</p><p>This thesis also concludes that the time for establishing a traditional SOC has passed. The advantages of utilizing a virtual SOC and its accompanying tools are too significant to ignore.</p>
----------------------------------------------------------------------
In diva2:1472049 abstract is: <p>The technical feasibility of gasoline and diesel range hydrocarbons production through oligomerisation of olefins, starting from biomass with the intermediary steps of gasfication, water-gas shift reaction and syngas-to-olefins synthesis was investigated, through mathematical modelling and simulation on Matlab.</p><p>The model for the gasifier was based on minimisation of Gibbs free energy and its results showed that higher carbon efficiencies could be achieved at lower pressures and steam inlet, and more inlet energy, by pre-heating the gasifying agents.</p><p>The water-gas shift reactor was used to increase the ratio of hydrogen to carbon monoxide from the gasifier, before entering the syngas-to-olefins process. A 1-D model was employed to determine the concentration, temperature, and pressure profiles in the reactor. High inlet pressure and temperature were shown to be beneficial, by requiring smaller reactors for the desired ratios to be reached.</p><p>Experimental data from scientific literature was used for empirical modelling of the Fischer-Tropsch reactor. Partial pressure of CO and H2 amounting to 1 bar, high temperature and H2/CO showed better production of the low olefins.</p><p>A reaction mechanism and accordingly, rate equations were developed and employed in a plug-flow type reactor model, calculating the concentration profile of the olefins up to C20. High pressures were favourable for the production of heavier fractions, while elevated temperatures showed to cause more cracking of heavy hydrocarbons and consequently, less conversion.</p><p>Based on the results of individual reactors, an integrated process flow diagram was suggested and optimised for maximum production of low olefins to the oligomerisation reactor (C2-4). The optimisation showed overall carbon efficiency of the process to be around 20%. The reason for this was associated with the choice of catalyst in the FTO process, due to its high selectivity to carbon dioxide.</p>

w='gasfication' val={'c': 'gasification', 's': 'diva2:1472049', 'n': 'correct in original'}

corrected abstract:
<p>The technical feasibility of gasoline and diesel range hydrocarbons production through oligomerisation of olefins, starting from biomass with the intermediary steps of gasification, water-gas shift reaction and syngas-to-olefins synthesis was investigated, through mathematical modelling and simulation on Matlab.</p><p>The model for the gasifier was based on minimisation of Gibbs free energy and its results showed that higher carbon efficiencies could be achieved at lower pressures and steam inlet, and more inlet energy, by pre-heating the gasifying agents.</p><p>The water-gas shift reactor was used to increase the ratio of hydrogen to carbon monoxide from the gasifier, before entering the syngas-to-olefins process. A 1-D model was employed to determine the concentration, temperature and pressure profiles in the reactor. High inlet pressure and temperature were shown to be beneficial, by requiring smaller reactors for the desired ratios to be reached.</p><p>Experimental data from scientific literature was used for empirical modelling of the Fischer-Tropsch reactor. Partial pressure of CO and H<sub>2</sub> amounting to 1 bar, high temperature and H<sub>2</sub>/CO showed better production of the low olefins.</p><p>A reaction mechanism and accordingly, rate equations were developed and employed in a plug-flow type reactor model, calculating the concentration profile of the olefins up to C<sub>20</sub>. High pressures were favourable for the production of heavier fractions, while elevated temperatures showed to cause more cracking of heavy hydrocarbons and consequently, less conversion.</p><p>Based on the results of individual reactors, an integrated process flow diagram was suggested and optimised for maximum production of low olefins to the oligomerisation reactor (C<sub>2-4</sub>). The optimisation showed overall carbon efficiency of the process to be low, around 20%. The reason for this was associated with the choice of catalyst in the FTO process, due to its high selectivity to carbon dioxide.</p>
----------------------------------------------------------------------
The measurements made earlier have shown clear results of high moisture and casein contents. </p><p>Casein is a protein found in milk products and is harmless when present in a dry environment. It was used in joists as early in the 1970s when it was used in surface putty. The result was a putty with good flow properties, and it is simpler for construction workers ergonomically which made it easier without the need to smooth the concrete while crouching. However, problems arise when floating putty is exposed to moisture, because casein breaks down in the presence of water under the alkaline conditions in the concrete and forms among other things, the gas ammonia, which affects the human health negatively. </p><p>Over the years, several defensive measures have been taken so that the residents can cope with the indoor environment, as the problems still persist, the property owner decided to evacuate the residents and demolish the property in order to build a new, healthy house for the residents. On behalf of AFA Fastigheter, Toolgate gets to lead the entire project from start to finished product. They want to recycle casein infected concrete in the best possible way, optimize the use of resources and enable circular material use. </p><p>According to statistics about 35% of all waste in Sweden is construction waste. They want to reduce this and be able to enable the recycling and reuse 100% of all construction waste that arises when demolishing and erecting new homes. The result of the optimization contributes to a positive impact on ecological sustainability, a healthy environment and great benefit for future projects. </p><p>The purpose of this report is to suggest possible methods for recycling casein infected concrete in the best possible sustainable way. </p><p>The question that will be the focus of the work is- Can casein infected concrete be recycled or reused? </p><p>This project is aimed at AFA Fastigheter as well as other managers, property owners and consultants who will hopefully be able to take advantage of useful information after this project is complete. </p><p>This work is limited to the neighborhood Silverskopan and is connected to the preliminary project that is ongoing before the demolition, but this will begin in 2024. Further limitations for the work aret hat it is done in the form of a case study where the neighborhood is in focus. </p><p>According to the conducted literature study you can see there is a clear lack of knowledge regarding the handling of casein containing concrete and how it can be recycled or reused. As we have relatively large natural resources in Sweden most actors prefer to buy new material instead of reusing the old. This is because it can entail a certain risk in terms of bearing capacity in new properties, while it is a rather unproven method. However, there may be grate opportunities for the use of recycled concrete after forming casein containing floating putty. One should then preferably use the material outdoors where it does not adverserly affect human health, at the same time that the present content of ammonia is aerated and diluted into the air. </p><p>Finally, it is a question of profitability for the property owner. Removing casein can mean high costs, a lot of time and energy. Leaving everything to a waste receiver can be a cheaper option. However, it can mean a great resource savings and environmental awarenedd to remove casein if it has not penetrated too deeply into the concrete. </p>

w='is- Can' val={'c': 'is - Can', 's': 'diva2:1786728'}
w='isCan' val={'c': 'is - Can', 's': 'diva2:1786728'}
w='aret' val={'c': 'are', 's': 'diva2:1786728', 'n': 'error in original'}
Note that "grate" appears in the original rather than "great".
Note also that "awarenedd" appears iin the original rather than "awareness".

corrected abstract:
<p>The neighborhood Silverskopan in central Stockholm has long had problems with moisture in the properties. The measurements made earlier have shown clear results of high moisture and casein contents.</p><p>Casein is a protein found in milk products and is harmless when present in a dry environment. It was used in joists as early in the 1970s when it was used in surface putty. The result was a putty with good flow properties, and it is simpler for construction workers ergonomically which made it easier without the need to smooth the concrete while crouching. However, problems arise when floating putty is exposed to moisture, because casein breaks down in the presence of water under the alkaline conditions in the concrete and forms among other things, the gas ammonia, which affects the human health negatively.</p><p>Over the years, several defensive measures have been taken so that the residents can cope with the indoor environment, as the problems still persist, the property owner decided to evacuate the residents and demolish the property in order to build a new, healthy house for the residents. On behalf of AFA Fastigheter, Toolgate gets to lead the entire project from start to finished product. They want to recycle casein infected concrete in the best possible way, optimize the use of resources and enable circular material use.</p><p>According to statistics about 35% of all waste in Sweden is construction waste. They want to reduce this and be able to enable the recycling and reuse 100% of all construction waste that arises when demolishing and erecting new homes. The result of the optimization contributes to a positive impact on ecological sustainability, a healthy environment and great benefit for future projects.</p><p>The purpose of this report is to suggest possible methods for recycling casein infected concrete in the best possible sustainable way.</p><p>The question that will be the focus of the work is - Can casein infected concrete be recycled or reused?</p><p>This project is aimed at AFA Fastigheter as well as other managers, property owners and consultants who will hopefully be able to take advantage of useful information after this project is complete.</p><p>This work is limited to the neighborhood Silverskopan and is connected to the preliminary project that is ongoing before the demolition, but this will begin in 2024. Further limitations for the work aret hat it is done in the form of a case study where the neighborhood is in focus.</p><p>According to the conducted literature study you can see there is a clear lack of knowledge regarding the handling of casein containing concrete and how it can be recycled or reused. As we have relatively large natural resources in Sweden most actors prefer to buy new material instead of reusing the old. This is because it can entail a certain risk in terms of bearing capacity in new properties, while it is a rather unproven method. However, there may be grate opportunities for the use of recycled concrete after forming casein containing floating putty. One should then preferably use the material outdoors where it does not adverserly affect human health, at the same time that the present content of ammonia is aerated and diluted into the air.</p><p>Finally, it is a question of profitability for the property owner. Removing casein can mean high costs, a lot of time and energy. Leaving everything to a waste receiver can be a cheaper option. However, it can mean a great resource savings and environmental awarenedd to remove casein if it has not penetrated too deeply into the concrete.</p>
----------------------------------------------------------------------
In diva2:1694001 abstract is: <p>Paper machine 11 in Hallsta paper mill produces high-gloss paper that is bleached with hydrogen peroxide. If the hydrogen peroxide that remains after the bleaching, called residual peroxide, gets out on the paper machine, this wears down the thermal rollers, which affects the final product.  There is a bacterial culture at the mill, consisting of the genus Tepidiphilus, which breaks down the hydrogen peroxide with the help of catalase and prevents it from getting out to the machine. If the bacteria are killed, for example by unfavourable conditions, this is noticeable by an increase in residual peroxide. The goal of this project is to try to identify which factors affect the bacteria and put forward a proposal for how to prevent the occurrence of residual peroxide. To carry this out, principal component analysis was primarily used, and how different factors changed in relation to the appearance of residual peroxide was studied. Several parameters could be identified based on the available data. However, no conclusive conclusions could be drawn as it is not possible to confirm any theories with lab tests or by manipulating the process. The parameters believed to have the greatest influence are coloration of mechanically cleaned water (mech. water), residual aluminium, turbidity, manganese content of the water, pH for tank K0203, the returning pulp suspension as well as the headbox and the chlorine levels for mech. water total chlorine, RVV1 free chlorine, hot water total chlorine and mech. water free chlorine. These parameters should therefore be monitored going forward, and if residual peroxide occurs again, their possible interactions and actual impact on the well-being of the bacteria can either be confirmed or denied</p>

w='mech' val={'c': 'mechanically', 's': 'diva2:1694001', 'n': 'part of "mechanically cleaned water (mech. water)"'}

corrected abstract:
<p>Paper machine 11 in Hallsta paper mill produces high-gloss paper that is bleached with hydrogen peroxide. If the hydrogen peroxide that remains after the bleaching, called residual peroxide, gets out on the paper machine, this wears down the thermal rollers, which affects the fin al product.  There is a bacterial culture at the mill, consisting of the genus Tepidiphilus, which breaks down the hydrogen peroxide with the help of catalase and prevents it from getting out to the machine. If the bacteria are killed, for example by unfavourable conditions, this is noticeable by an increase in residual peroxide. The goal of this project is to try to identify which factors affect the bacteria and put forward a proposal for how to prevent the occurrence of residual peroxide. To carry this out, principal component analysis was primarily used, and how different factors changed in relation to the appearance of residual peroxide was studied. Several parameters could be identified based on the available data. However, no conclusive conclusions could be drawn as it is not possible to confirm any theories with lab tests or by manipulating the process. The parameters believed to have the greatest influence are coloration of mechanically cleaned water (mech. water), residual aluminium, turbidity, manganese content of the water, pH for tank K0203, the returning pulp suspension as well as the headbox and the chlorine levels for mech. water total chlorine, RVV1 free chlorine, hot water total chlorine and mech. water free chlorine. These parameters should therefore be monitored going forward, and if residual peroxide occurs again, their possible interactions and actual impact on the well-being of the bacteria can either be confirmed or denied.</p>
----------------------------------------------------------------------
In diva2:1150933 abstract is: <p>The inclusion of nanoparticles has opened new possibilities to enhance and tune the properties of biobased materials. We here explored an approach to create protein homo-nanocomposites in which the nanoparticles and matrix are made from the same raw material. Whey protein isolate, as the raw material, are known to self-assemble into highly ordered nanofibrils under certain conditions. 2.5 - 15 % of protein nanofi brils solution was mixed with non-fibrillar matrix and 33 % glycerol as plasticizer. Tensile tests indicated increased elastic modulus and decreased  maximal elongation as function of increasing amounts of nanofibrils. The ultimate strength was less affected. It can be explained by the growth of  strongly  hydrogen-bonded peptide group in P-s heets observed by  IR  spectroscopy.  The  undulating  surface  and exifoliated P-sheets a lso have influence on the mechanical properties of the films.  Taken together, our study shows that it is possible to control the mechanical properties  of protein­  based materials  by addition  of protein nanofibrils.</p>

w='brils' val={'c': 'nanofibrils', 's': 'diva2:1150933'}
w='exifoliated' val={'c': 'exfoliated', 's': 'diva2:1150933', 'n': 'no full text'}
w='heets' val={'c': 'P-sheets', 's': 'diva2:1150933', 'n': 'no full text'}
w='lso' val={'c': 'also', 's': 'diva2:1150933', 'n': 'no full text'}
w='nanofi' val={'c': 'nanofibrils', 's': 'diva2:1150933'}

corrected abstract:
<p>The inclusion of nanoparticles has opened new possibilities to enhance and tune the properties of biobased materials. We here explored an approach to create protein homo-nanocomposites in which the nanoparticles and matrix are made from the same raw material. Whey protein isolate, as the raw material, are known to self-assemble into highly ordered nanofibrils under certain conditions. 2.5 - 15 % of protein nanofibrils solution was mixed with non-fibrillar matrix and 33 % glycerol as plasticizer. Tensile tests indicated increased elastic modulus and decreased maximal elongation as function of increasing amounts of nanofibrils. The ultimate strength was less affected. It can be explained by the growth of strongly hydrogen-bonded peptide group in P-sheets observed by IR spectroscopy. The undulating surface and exfoliated P-sheets also have influence on the mechanical properties of the films. Taken together, our study shows that it is possible to control the mechanical properties of protein­ based materials by addition of protein nanofibrils.</p>
----------------------------------------------------------------------
Note that this is a scanned document and there are no fonts, just images of pages.

In diva2:1223497 abstract is: <p>Graphene oxide (GO) is a promising candidate as nano-filler material in scaffolds for bone regeneration. It has been demonstrated to enhance the biological compatibility and osteogenic performance of polymer-based scaffolds, aside from its substantial contribution to the improvement of the material's mechanical properties. In this work, nano-graphene oxide (nGO) was covalently grafted to the surface of poly( e-caprolactone) (PCL) by first modifying the polymer surface via aminolysis. Using 1,6-hexanediamine/isopropanol, free amine groups were successfully introduced to the PCL surface for the subsequent immobilization of nGO. An optimized grafting pathway, which implements the solvent-assisted method and uses water as a solvent, was developed to covalently attach nGO using initial concentrations of 0.5 and 1 mg/mL. Fourier transform infrared spectroscopy (FTIR) and thermogravimetric analysis (TGA) both verified the successful attachment of nGO through the free amines. Scanning electron microscopy (SEM) depicts a homogeneous dispersion of nGO over the polymer matrix. Mechanical tests were performed and demonstrate a 50 and 21 % increase in compressive strength for the scaffolds grafted using initial nGO concentrations of 0.5 and 1 mglmL. In vitro mineralization tests showed the formation of mineral precipitates on the surface of the scaffolds that increased in size with higher nGO content. The potential of nGO as a nano-carrier of an antibiotic drug was also explored in this work. As it comprises of an abundance of chemical functionalities, nGO is able to efficiently adsorb compounds through various secondary interactions. In this study, these secondary interactions were optimized by controlling the solution pH for the maximum adsorption of ciprofloxacin, a broad-spectrum antibiotic used in the treatment of osteomyelitis. Ciprofloxacin was found to be adsorbed most strongly in its cationic form at pH 5, in which 1t-1t electron-donor acceptor (EDA) interactions predominate. Overall, the results presented in this work validate the potential of nGO as nano-enhancer and drug carrier in tissue engineering scaffold applications.</p>

w='mglmL' val={'c': 'mg/mL', 's': 'diva2:1223497', 'n': 'correct in original'}
Inserted missing lower case Greek letters.

corrected abstract:
<p>Graphene oxide (GO) is a promising candidate as nano-filler material in scaffolds for bone regeneration. It has been demonstrated to enhance the biological compatibility and osteogenic performance of polymer-based scaffolds, aside from its substantial contribution to the improvement of the material's mechanical properties. In this work, nano-graphene oxide (nGO) was covalently grafted to the surface of poly(ε-caprolactone) (PCL) by first modifying the polymer surface via aminolysis. Using 1,6-hexanediamine/isopropanol, free amine groups were successfully introduced to the PCL surface for the subsequent immobilization of nGO. An optimized grafting pathway, which implements the solvent-assisted method and uses water as a solvent, was developed to covalently attach nGO using initial concentrations of 0.5 and 1 mg/mL. Fourier transform infrared spectroscopy (FTIR) and thermogravimetric analysis (TGA) both verified the successful attachment of nGO through the free amines. Scanning electron microscopy (SEM) depicts a homogeneous dispersion of nGO over the polymer matrix. Mechanical tests were performed and demonstrate a 50 and 21 % increase in compressive strength for the scaffolds grafted using initial nGO concentrations of 0.5 and 1 mg/mL. <em>In vitro</em> mineralization tests showed the formation of mineral precipitates on the surface of the scaffolds that increased in size with higher nGO content. The potential of nGO as a nano-carrier of an antibiotic drug was also explored in this work. As it comprises of an abundance of chemical functionalities, nGO is able to efficiently adsorb compounds through various secondary interactions. In this study, these secondary interactions were optimized by controlling the solution pH for the maximum adsorption of ciprofloxacin, a broad-spectrum antibiotic used in the treatment of osteomyelit is. Ciprofloxacin was found to be adsorbed most strongly in its cationic form at pH 5, in which π-π electron-donor acceptor (EDA) interactions predominate. Overall, the results presented in this work validate the potential of nGO as nano-enhancer and drug carrier in tissue engineering scaffold applications.</p>
----------------------------------------------------------------------
In diva2:1788558 abstract is: <p>Na+/K+-ATPase is an essential ion pump protein in a host of physiological functions as it maintains the electrochemical gradient across cell membranes. Additionally, its dysfunction is implicated in several neurological diseases. The protein is a heterodimer of α and β subunits, occasionally associated with a third γ (FXYD) subunit, which makes studying its higher order organization in the cell membrane difficult using conventional, relatively large scale labeling probes such as antibodies. Non-canonical amino acid incorporation is an emerging field which offers a solution. Via CuAAC and SPIEDAC click conjugation reactions, organic fluorophores can be specifically attached to the side chains of residues of the ion pump with corresponding reactive moieties, creating a small and noninvasive probe for fluorescence microscopy imaging. In order to specifically image all three subunits concurrently, three color labeling is required. The objective of this project was to achieve three color labeling via non-canonical amino acid incorporation to aid in the study of the cell membrane localization of the subunits of Na+/K+-ATPase. Fluorescence microscopy of transiently transfected and live cell labeled HEK293T cells was complemented by in gel fluorescence imaging and immunoblotting. Coexpression and two color labeling of all nonsense codon subunit mutants in combination was shown in gel, of which only α and β had previously been coexpressed. α/γ dual labeling proved successful when cotransfected with wild type β. An autofluorescent effect in one of the color channels compromised the microscopy results. Three color labeling was not observed in gel, and expression of the subunits (including a substitute for α) was middling to absent. It remains unclear whether three color labeling or triple coexpression is a possibility with the bioorthogonal translation systems used in this project.</p>

w='autofluorescense' val={'c': 'autofluorescence', 's': ['diva2:1788558', 'diva2:1741440']}
Fixed missing superscripts.


corrected abstract:
<p>Na<sup>+</sup>/K<sup>+</sup>-ATPase is an essential ion pump protein in a host of physiological functions as it maintains the electrochemical gradient across cell membranes. Additionally, its dysfunction is implicated in several neurological diseases. The protein is a heterodimer of α and β subunits, occasionally associated with a third γ (FXYD) subunit, which makes studying its higher order organization in the cell membrane difficult using conventional, relatively large scale labeling probes such as antibodies. Non-canonical amino acid incorporation is an emerging field which offers a solution. Via CuAAC and SPIEDAC click conjugation reactions, organic fluorophores can be specifically attached to the side chains of residues of the ion pump with corresponding reactive moieties, creating a small and noninvasive probe for fluorescence microscopy imaging. In order to specifically image all three subunits concurrently, three color labeling is required. The objective of this project was to achieve three color labeling via non-canonical amino acid incorporation to aid in the study of the cell membrane localization of the subunits of Na<sup>+</sup>/K<sup>+</sup>-ATPase. Fluorescence microscopy of transiently transfected and live cell labeled HEK293T cells was complemented by in gel fluorescence imaging and immunoblotting. Coexpression and two color labeling of all nonsense codon subunit mutants in combination was shown in gel, of which only α and β had previously been coexpressed. α/γ dual labeling proved successful when cotransfected with wild type β. An autofluorescent effect in one of the color channels compromised the microscopy results. Three color labeling was not observed in gel, and expression of the subunits (including a substitute for α) was middling to absent. It remains unclear whether three color labeling or triple coexpression is a possibility with the bioorthogonal translation systems used in this project.</p>
----------------------------------------------------------------------
In diva2:1290440 abstract is: <p>Clinical ultrasound imaging techniques can be greatly improved by the use of ultrasound contrast agents (UCAs). While microbubbles (MBs) without shell are unstable and cannot be used for practical applications,a shell produced from biocompatible polyvinylalcohol (PVA) significantly improves chemical versatility and stability. The oscillation characteristics of a UCA are strongly dependent on concentration, applied pressure and viscoelastic parameters of the shell. Modifications in the shell as incorporation of antibodies or targeted molecules affect the bubble oscillation and resonance frequency of the MB suspension. In this presented work a tool for systematic characterization of UCAs is developed. Linear acoustic behaviour of PVA shelled MBs is examined. The acoustic driving pressure is kept below 100 kPa. The MB concentration is 1·10^{6} ml^{-1}. Attenuation and phase velocity profiles of ultrasound waves propagating through the UCA are measured using six narrow-band single crystal transducers that cover a frequency range between 1 and 15 MHz. The oscillation of a single bubble is modeled as a linear oscillator adapting HOFF’s model suitable for allshell thicknesses. The suspension is modeled through superposition of single bubbles. Knowing all parameters the resonance frequency of a MB suspension can be predicted. The model is fitted to experimental data to determine the viscoelastic shell parameters. The shell thickness is challenging to determine exactly and assumed to be either proportional to the outer shell radius or constant. Assuming a proportional shell thickness the calculated resulting shell parameters were shear modulus G_s = 14.5 MPa, shear viscosity η_s = 0.322 Pa·s and shell thickness d_s = 16 % of the outer radius. When instead assuming a constant shell thickness the determined parameters were in similar order of magnitude. Resonance frequency of the suspension was determined to 11.6 MHz. The developed tool can be used to characterize MBs with a modified shell independently of shell thickness and to predict resonance frequency of gas or air filled UCAs with known shell parameters.</p>

w='HOFF’s' val={'c': 'HOFF’s', 's': 'diva2:1290440', 'n': 'error in original'}
w='G_s' val={'c': 'G<sub>s</sub>', 's': 'diva2:1290440', 'n': 'correct in original'}
w='d_s' val={'c': 'd<sub>sz/sub>', 's': 'diva2:1290440', 'n': 'correct in original'}
The use of LaTeX math might have worked if the equations had been put inside a pair of dollar signs, e.g., $G:s$


corrected abstract:
<p>Clinical ultrasound imaging techniques can be greatly improved by the use of ultrasound contrast agents (UCAs). While microbubbles (MBs) without shell are unstable and cannot be used for practical applications, a shell produced from biocompatible polyvinylalcohol (PVA) significantly improves chemical versatility and stability. The oscillation characteristics of a UCA are strongly dependent on concentration, applied pressure and viscoelastic parameters of the shell. Modifications in the shell as incorporation of antibodies or targeted molecules affect the bubble oscillation and resonance frequency of the MB suspension. In this presented work a tool for systematic characterization of UCAs is developed. Linear acoustic behaviour of PVA shelled MBs is examined. The acoustic driving pressure is kept below 100 kPa. The MB concentration is 1·10<sup>6</sup> ml<sup>-1</sup>. Attenuation and phase velocity profiles of ultrasound waves propagating through the UCA are measured using six narrow-band single crystal transducers that cover a frequency range between 1 and 15 MHz. The oscillation of a single bubble is modeled as a linear oscillator adapting HOFF’s model suitable for all shell thicknesses. The suspension is modeled through superposition of single bubbles. Knowing all parameters the resonance frequency of a MB suspension can be predicted. The model is fitted to experimental data to determine the viscoelastic shell parameters. The shell thickness is challenging to determine exactly and assumed to be either proportional to the outer shell radius or constant. Assuming a proportional shell thickness the calculated resulting shell parameters were shear modulus <em>G<sub>s</sub></em> = 14.5 MPa, shear viscosity <em>η<sub>s</sub></em> = 0.322 Pa·s and shell thickness <em>d<sub>s</sub></em> = 16 % of the outer radius. When instead assuming a constant shell thickness the determined parameters were in similar order of magnitude. Resonance frequency of the suspension was determined to 11.6 MHz. The developed tool can be used to characterize MBs with a modified shell independently of shell thickness and to predict resonance frequency of gas or air filled UCAs with known shell parameters.</p>
----------------------------------------------------------------------
In diva2:1264229 abstract is: <p>Superaggregates are clusters formed by diverse aggregation mechanisms at diﬀerent scales. They can be found in ﬂuidized nanoparticles and soot formation. An aggregate, with a single aggregation mechanism, can be described by the fractal dimension, df , which is the measure of the distribution and conﬁguration of primary particles into the aggregates. Similarly, a su-peraggregate can be analyzed by the diﬀerent fractal dimensions that are found at each scale. In a fractal structure aggregate, a self-similarity can be identiﬁed at diﬀerent scales and it has a power law relation between the mass and aggregate size, which can be related to properties like density or light scattering. The fractal dimension, df , can be inﬂuenced by aggregation mechanism, particles concentration, temperature, residence time, among other variables. More-over, this parameter can help on the estimation of aggregates’ properties which can help on the design of new processes, analyze health issues and characterize new materials.A multi-dimensional soot aggregate was simulated with the following approach. The ﬁrst aggregation stage was modeled with a Diﬀusion Limited cluster-cluster aggregation (DLCA) mechanism, where primary clusters with a fractal dimension, df1, close to 1.44 were obtained. Then, the second aggregation stage was speciﬁed by Ballistic Aggregation (BA) mechanism, where the primary clusters generated in the ﬁrst stage were used to form a superaggregate. All the models were validated with reported data on diﬀerent experiments and computer models. Using the Ballistic Aggregation (BA) model with primary particles as the building blocks, the fractal dimension, df2, was close to 2.0, which is the expected value reported by literature. However, a decrease on this parameter is appreciated using primary clusters, from a DLCA model, as the building blocks because there is a less compact distribution of primary particles in the superaggregate’s structure.On the second aggregation stage, the fractal dimension, df2, increases when the superaggre-gate size increases, showing an asymptotic behavior to 2.0, which will be developed at higher scales. Partial reorganization was implemented in the Ballistic Aggregation (BA) mechanism where two contact points between primary clusters were achieved for stabilization purposes. This implementation showed a faster increase on the fractal dimension, df2, than without par-tial reorganization. This behavior is the result of a more packed distribution of primary clusters in a short range scales, but it does not aﬀect the scaling behavior of multi-dimensional fractal structures. Moreover, the same results were obtained with diﬀerent scenarios where the building block sizes were in the range from 200 to 300 and 700 to 800 primary particles.The obtained results demonstrate the importance of fractal dimension, df , for aggregate characterization. This parameter is powerful, universal and accurate since the identiﬁcation of the diﬀerent aggregation stages in the superaggregate can increase the accuracy of the estimation of properties, which is crucial in physics and process modeling.</p>

w='su-peraggregate' val={'c': 'superaggregate', 's': 'diva2:1264229', 'n': 'hyphen at end of line in original'}
w='superaggre-gate' val={'c': 'superaggregate', 's': 'diva2:1264229', 'n': 'correct in original'}
Replaced many ligatures and added subscripts for the df, df1, and df2.

corrected abstract:
<p>Superaggregates are clusters formed by diverse aggregation mechanisms at different scales. They can be found in fluidized nanoparticles and soot formation. An aggregate, with a single aggregation mechanism, can be described by the fractal dimension, d<sub>f</sub>, which is the measure of the distribution and configuration of primary particles into the aggregates. Similarly, a superaggregate can be analyzed by the different fractal dimensions that are found at each scale. In a fractal structure aggregate, a self-similarity can be identified at different scales and it has a power law relation between the mass and aggregate size, which can be related to properties like density or light scattering. The fractal dimension, d<sub>f</sub>, can be influenced by aggregation mechanism, particles concentration, temperature, residence time, among other variables. Moreover, this parameter can help on the estimation of aggregates’ properties which can help on the design of new processes, analyze health issues and characterize new materials.</p><p>A multi-dimensional soot aggregate was simulated with the following approach. The first aggregation stage was modeled with a Diffusion Limited cluster-cluster aggregation (DLCA) mechanism, where primary clusters with a fractal dimension, d<sub>f1</sub>, close to 1.44 were obtained. Then, the second aggregation stage was specified by Ballistic Aggregation (BA) mechanism, where the primary clusters generated in the first stage were used to form a superaggregate. All the models were validated with reported data on different experiments and computer models. Using the Ballistic Aggregation (BA) model with primary particles as the building blocks, the fractal dimension, d<sub>f2</sub>, was close to 2.0, which is the expected value reported by literature. However, a decrease on this parameter is appreciated using primary clusters, from a DLCA model, as the building blocks because there is a less compact distribution of primary particles in the superaggregate’s structure.</p><p>On the second aggregation stage, the fractal dimension, d<sub>f2</sub>, increases when the superaggregate size increases, showing an asymptotic behavior to 2.0, which will be developed at higher scales. Partial reorganization was implemented in the Ballistic Aggregation (BA) mechanism where two contact points between primary clusters were achieved for stabilization purposes. This implementation showed a faster increase on the fractal dimension, d<sub>f2</sub>, than without partial reorganization. This behavior is the result of a more packed distribution of primary clusters in a short range scales, but it does not affect the scaling behavior of multi-dimensional fractal structures. Moreover, the same results were obtained with different scenarios where the building block sizes were in the range from 200 to 300 and 700 to 800 primary particles.</p><p>The obtained results demonstrate the importance of fractal dimension, d<sub>f</sub>, for aggregate characterization. This parameter is powerful, universal and accurate since the identification of the different aggregation stages in the superaggregate can increase the accuracy of the estimation of properties, which is crucial in physics and process modeling.</p>
----------------------------------------------------------------------
In diva2:1561106 abstract is: <p>Today, it is common for events to be filmed without the use of a professional video photographer. It can be the little league football game, conference meetings, teaching or YouTube clips. To film without a cameraman, you can use something called object tracking cameras. It is a camera that can follow an object's position without a cameraman.This thesis describes how object tracking works as well as comparison between ob-ject tracking cameras with computer vision and a cameraman.</p><p>In order to compare them against each other, a prototype has been developed. The prototype consists of a Raspberry Pi 4B with MOSSE which is an object tracking algorithm and SSD300 which is a detection algorithm in computer vision. The steering consists of a gimbal consisting of three brushless motors that control the camera with a regulator.</p><p>The result was a prototype capable of following a person walking at a maximum speed 100 pixels per second or 1 meter per second in full screen, with a maximum distance of 11.4 meters outdoors. While a cameraman managed to follow a person at 300-800 pixels per second or 3 meters per second. The prototype is not as good as a cameraman but can be used to follow a person who teaches and walks slowly. Under basis that the prototype is robust, which is not the case. To get better results, stronger processor and better algorithms are needed than used with the prototype. That’s because a big problem was that the refresh rate was low for the detection algorithm.</p>

w='ob-ject' val={'c': 'object', 's': 'diva2:1561106', 'n': 'hyphen at end of line in original'}

corrected abstract:
<p>Today, it is common for events to be filmed without the use of a professional video photographer. It can be the little league football game, conference meetings, teaching or YouTube clips. To film without a cameraman, you can use something called object tracking cameras. It is a camera that can follow an object's position without a cameraman.</p><p>This thesis describes how object tracking works as well as comparison between object tracking cameras with computer vision and a cameraman. In order to compare them against each other, a prototype has been developed. The prototype consists of a Raspberry Pi 4B with MOSSE which is an object tracking algorithm and SSD300 which is a detection algorithm in computer vision. The steering consists of a gimbal consisting of three brushless motors that control the camera with a regulator.</p><p>The result was a prototype capable of following a person walking at a maximum speed 100 pixels per second or 1 meter per second in full screen, with a maximum distance of 11.4 meters outdoors. While a cameraman managed to follow a person at 300-800 pixels per second or 3 meters per second. The prototype is not as good as a cameraman but can be used to follow a person who teaches and walks slowly. Under basis that the prototype is robust, which is not the case. To get better results, stronger processor and better algorithms are needed than used with the prototype. That’s because a big problem was that the refresh rate was low for the detection algorithm.</p>
----------------------------------------------------------------------
In diva2:730237 abstract is: <p>Two key routes of the oxidation of ammonia is the reaction between the aminyl radical with molecular oxygen and superoxide.</p><p>Fundamental insights in its oxidation is of great importance to both our understanding of atmospheric chemistry, biological effects as well as the safety assessments in nuclear industry. The gas phase reactions differ largely in rate compared to the aqueous reactions.</p><p>In this work, the mechanism for the aqueous reactions have been studied computationally using ab initio quantum chemical calcultions.</p><p>Reaction barriers have been quantified and compared with experimental data in the litterature.</p><p>Also, the absorbtion spectrum of one of the postulated intermediates is verified using TDDFT.</p><p>Furthermore, ã-radiolysis experiments have been conducted to test a model for aqueous radical oxidation of ammonia</p><p>by investigating the yield of one of the final products, peroxynitrite.</p><p>The model consists of a large part of reported reactions relevant to the aqueous radiolysis of nitrogen containing solutions (with an emphasis on ammonia).</p><p>The dependence of the yield on the chosen experimental conditions is compared to that calculated using the model.</p><p>The results of the quantum chemical computations for the reaction with oxygen is in agreement with earlier experimentally reported absorption spectrum and rate of decomposition of a key intermediate, the aminyl peroxide radical.</p><p>Both properties, however, only agree when a large enough number of explicit water molecules are included in the computations.</p><p>The dependence of the results on the chosen number of water molecules as well as the level of theory is discussed.</p><p>The evaluation of the model for radiolysis of aqueous ammonia shows that the current understanding of oxygenated systems where superoxide is in excess with respect to hydroxyl radical is good.</p><p>For systems deficient in superoxide, the model fails to accurately predict the yield of the final product under investigation, peroxynitrite, and it raises the question whether some unknown process has been overlooked in earlier studies.</p>

w='calcultions' val={'c': 'calculations', 's': 'diva2:730237', 'n': 'error in original'}
w='litterature' val={'c': 'literature', 's': 'diva2:730237', 'n': 'error in original'}
w='ã-radiolysis' val={'c': 'γ-radiolysis', 's': 'diva2:730237'}

corrected abstract:
<p>Two key routes of the oxidation of ammonia is the reaction between the aminyl radical with molecular oxygen and superoxide. Fundamental insights in its oxidation is of great importance to both our understanding of atmospheric chemistry, biological effects as well as the safety assessments in nuclear industry. The gas phase reactions differ largely in rate compared to the aqueous reactions. In this work, the mechanism for the aqueous reactions have been studied computationally using ab initio quantum chemical calcultions. Reaction barriers have been quantified and compared with experimental data in the litterature. Also, the absorbtion spectrum of one of the postulated intermediates is verified using TDDFT. Furthermore, γ-radiolysis experiments have been conducted to test a model for aqueous radical oxidation of ammonia by investigating the yield of one of the final products, peroxynitrite. The model consists of a large part of reported reactions relevant to the aqueous radiolysis of nitrogen containing solutions (with an emphasis on ammonia). The dependence of the yield on the chosen experimental conditions is compared to that calculated using the model.</p><p>The results of the quantum chemical computations for the reaction with oxygen is in agreement with earlier experimentally reported absorption spectrum and rate of decomposition of a key intermediate, the aminyl peroxide radical. Both properties, however, only agree when a large enough number of explicit water molecules are included in the computations. The dependence of the results on the chosen number of water molecules as well as the level of theory is discussed. The evaluation of the model for radiolysis of aqueous ammonia shows that the current understanding of oxygenated systems where superoxide is in excess with respect to hydroxyl radical is good. For systems deficient in superoxide, the model fails to accurately predict the yield of the final product under investigation, peroxynitrite, and it raises the question whether some unknown process has been overlooked in earlier studies.</p>
----------------------------------------------------------------------
In diva2:778863 abstract is: <p>Tar in process gas is one of the key issues of the biomass gasification process. In this diploma work, an instrument for online analysis of tar in process gas has been  designed  and  constructed. The instrument  utilizes  a  photo  ionization  detector  (PID)  that  selectively  detects tar components and makes it possible to follow the changes within the process in real time. The issues of temperature limitations in the detector and tar deposit on the UV-­‐ lamp window has been investigated and to some extent addressed. The  temperature  issue origins from that the maximum temperature allowed in the detector usually is bellow 300 °C according to the manufacturers. As such low temperatures, some of the heavy tars are condensed which will give misleading signals from the detector. The limiting factor has been examined  to  be  the  gasket  between  the  ionization  chamber  and  the  UV-­‐lamp.  The  gasket has to be non-­‐electron  conducting  which  limits the  choice  of gasket materials. No  solution has been proposed regarding this matter. The tar deposition issue origins from that heavy tars   polymerizes   by   the   UV-­‐light.   The   polymerized   tar   will   deposit   on   the   lamp   and continuously decrease the signal from the detector. This problem  has  been  attended  by  heavily dilute the sample gas with nitrogen to minimize  the  amount  of  tar  that  passes  through the detector. The prototype instrument is built as a  stand  alone  unit  with  all  necessary auxiliary equipment required for the operation.</p>

w='UV-\xad‐lamp' val={'c': 'UV-lamp', 's': 'diva2:778863'}
w='UV-\xad‐light' val={'c': 'UV-light', 's': 'diva2:778863'}
w='UV-\xad‐' val={'c': 'UV-lamp', 's': 'diva2:778863', 'n': 'no full text'}
w='UV-\xad‐ lamp' val={'c': 'UV-lamp', 's': 'diva2:778863', 'n': 'no full text'}

corrected abstract:
<p>Tar in process gas is one of the key issues of the biomass gasification process. In this diploma work, an instrument for online analysis of tar in process gas has been designed and constructed. The instrument utilizes a photo ionization detector (PID) that selectively detects tar components and makes it possible to follow the changes within the process in real time. The issues of temperature limitations in the detector and tar deposit on the UV-lamp window has been investigated and to some extent addressed. The temperature issue origins from that the maximum temperature allowed in the detector usually is below 300 °C according to the manufacturers. As such low temperatures, some of the heavy tars are condensed which will give misleading signals from the detector. The limiting factor has been examined to be the gasket between the ionization chamber and the UV-lamp. The gasket has to be non-electron conducting which limits the choice of gasket materials. No solution has been proposed regarding this matter. The tar deposition issue origins from that heavy tars polymerizes by the UV-light. The polymerized tar will deposit on the lamp and continuously decrease the signal from the detector. This problem has been attended by heavily dilute the sample gas with nitrogen to minimize the amount of tar that passes through the detector. The prototype instrument is built as a stand alone unit with all necessary auxiliary equipment required for the operation.</p>
----------------------------------------------------------------------
In diva2:848249 abstract is: <p>Bone fracture fixation is one of the most important medical treatments commonly used in medical centers. One way to repair fractured bone fragments is to use screws and plates. But this method has some limitations and is not practical for small bones and the bones which are in sensitive places such as spinal bones. A promising treatment for fracture of such bones is bone adhesive, which can overcome some of the limitations such as the need of drilling and secondary surgery for removal of implants. </p><p>Fibre Reinforced Adhesive Patches (FRAPs) is a new generation of bone adhesive systems that was investigated in this thesis. In the FRAP system; a thiol-ene polymerization was photo-initiated by using campherquinone as photo-initiator and blue light (about 385-515 nm wavelength) from dental lamp as light source. In order to study the influence of different parementers on the performance of the FRAP, the samples with different allyl:thiol molar ratio were prepared and characterized by tensile testing. According to characterization results, the sample with 1:1 allyl:thiol molar ratio showed the highest modulus. The samples with thiol functionality in excess showed flexible and soft networks, while the samples with a low amount of allyl groups in excess were stronger and more rigid compare to samples with high amount of allyl group in excess. The samples with an excess of allyl functionality to a large extent compared to A1T1 (1:1 allyl :thiol molar ratio) that were weaker and less rigid. In the FRAP fabrication, the choice of which allyl: thiol ratio to use and   how they should be ordered on bone surface will determine the final bonding strength at the adhesive-bone interface. </p>

w='campherquinone' val={'c': 'camphorquinone', 's': 'diva2:848249'}
w='parementers' val={'c': 'parameters', 's': 'diva2:848249', 'n': 'no full text'}

corrected abstract:
<p>Bone fracture fixation is one of the most important medical treatments commonly used in medical centers. One way to repair fractured bone fragments is to use screws and plates. But this method has some limitations and is not practical for small bones and the bones which are in sensitive places such as spinal bones. A promising treatment for fracture of such bones is bone adhesive, which can overcome some of the limitations such as the need of drilling and secondary surgery for removal of implants. </p><p>Fibre Reinforced Adhesive Patches (FRAPs) is a new generation of bone adhesive systems that was investigated in this thesis. In the FRAP system; a thiol-ene polymerization was photo-initiated by using camphorquinone as photo-initiator and blue light (about 385-515 nm wavelength) from dental lamp as light source. In order to study the influence of different parameters on the performance of the FRAP, the samples with different allyl:thiol molar ratio were prepared and characterized by tensile testing. According to characterization results, the sample with 1:1 allyl:thiol molar ratio showed the highest modulus. The samples with thiol functionality in excess showed flexible and soft networks, while the samples with a low amount of allyl groups in excess were stronger and more rigid compare to samples with high amount of allyl group in excess. The samples with an excess of allyl functionality to a large extent compared to A1T1 (1:1 allyl :thiol molar ratio) that were weaker and less rigid. In the FRAP fabrication, the choice of which allyl: thiol ratio to use and   how they should be ordered on bone surface will determine the final bonding strength at the adhesive-bone interface. </p>
----------------------------------------------------------------------
In diva2:1873267 abstract is: <p>Enzyme-X is an industrially relevant enzyme utilized in a biocatalytic process for production of compound A4 by Cysbio. Currently, the production capacity of this process is limited by an issue commonly encountered when overexpressing recombinant proteins in <em>E. coli</em> (expression host of enzyme-X), namely the formation of inclusion bodies, i.e. aggregations of misfolded proteins. In an effort to improve production capacity by increasing enzyme-X solubility and/or yield, this project investigated the following: 1) effects of growth media composition (utilizing a DoE setup), 2) co-expression of anaplerotic enzyme pyruvate carboxylase (PYC), 3) co-expression of chaperones 4) expression of enzyme-X by different promoters.</p><p>The constructed strains were grown in shake flasks using a growth medium designed in this project.  Despite none of them outperforming the current production strain in this context, some interesting strains were identified demonstrating improved biomass yield upon certain level of PYC co-expression, involvement of GroELS chaperone system in folding of enzyme-X and the potential of the tighly-regulated tetracycline promoter in replacing the currently used, leaky and costly T7 promoter. Due to shake flask based screening being unreliable for differentiating better-performing strains from control, complementary assessment in a fed-batch fermentation setup is recommended, as any positive hits could be translated to reduction of production cost of compound A4. Lastly, further investigation is recommended to fine-tune induction of the tested promoters and co-expression of PYC and GroELS.</p>

partal corrected: diva2:1873267: <p>Enzyme-X is an industrially relevant enzyme utilized in a biocatalytic process for production of compound A4 by Cysbio. Currently, the production capacity of this process is limited by an issue commonly encountered when overexpress ing recombin ant proteins in <em>E. coli</em> (expression host of enzyme-X), namely the formation of inclusion bodies, i.e. aggregations of misfolded proteins. In an effort to improve production capacity by increasing enzyme-X solubility and/or yield, this project investigated the following: 1) effects of growth media composition (utilizing a DoE setup), 2) co-expression of anaplerotic enzyme pyruvate carboxylase (PYC), 3) co-expression of chaperones 4) expression of enzyme-X by different promoters.</p><p>The constructed strains were grown in shake flasks using a growth medium designed in this project.  Despite none of them outperforming the current production strain in this context, some interesting strains were identified demonstrating improved biomass yield upon certain level of PYC co-expression, involvement of GroELS chaperone system in folding of enzyme-X and the potential of the tighly-regulated tetracycline promoter in replacing the currently used, leaky and costly T7 promoter. Due to shake flask based screening be ing unreliable for differentiating better-performing strains from control, complementary assessment in a fed-batch fermentation setup is recommended, as any positive hits could be translated to reduction of production cost of compound A4. Lastly, further investigation is recommended to fine-tune induction of the tested promoters and co-expression of PYC and GroELS.</p>
w='tighly-regulated' val={'c': 'tightly-regulated', 's': 'diva2:1873267', 'n': 'no full text'}

corrected abstract:
<p>Enzyme-X is an industrially relevant enzyme utilized in a biocatalytic process for production of compound A4 by Cysbio. Currently, the production capacity of this process is limited by an issue commonly encountered when overexpressing recombinant proteins in <em>E. coli</em> (expression host of enzyme-X), namely the formation of inclusion bodies, i.e. aggregations of misfolded proteins. In an effort to improve production capacity by increasing enzyme-X solubility and/or yield, this project investigated the following: 1) effects of growth media composition (utilizing a DoE setup), 2) co-expression of anaplerotic enzyme pyruvate carboxylase (PYC), 3) co-expression of chaperones 4) expression of enzyme-X by different promoters.</p><p>The constructed strains were grown in shake flasks using a growth medium designed in this project.  Despite none of them outperforming the current production strain in this context, some interesting strains were identified demonstrating improved biomass yield upon certain level of PYC co-expression, involvement of GroELS chaperone system in folding of enzyme-X and the potential of the tightly-regulated tetracycline promoter in replacing the currently used, leaky and costly T7 promoter. Due to shake flask based screening being unreliable for differentiating better-performing strains from control, complementary assessment in a fed-batch fermentation setup is recommended, as any positive hits could be translated to reduction of production cost of compound A4. Lastly, further investigation is recommended to fine-tune induction of the tested promoters and co-expression of PYC and GroELS.</p>
----------------------------------------------------------------------
In diva2:1384216 abstract is: <p>Cepheid provides instruments and tests for molecular diagnostic testing of bacterial and viral diseases. Their goal is to deliver a better way to improve patient outcomes by enabling access to molecular diagnostic testing everywhere. These test kits are used in warm and humid countries which may challenge the stability of the kits. The test kits containly ophilized beads, which contain enzymes for the polymerase chain reaction, which usually ages first. The ageing of the beads is effected by the initial water content in them since it defines the quality. Therefore, it is of great importance to have a very precise and exact water measurement method. The degree project covers the optimization of the Standard Operating Procedure used today at Cepheid with the research question: Can optimization of the Karl Fischer method improve the precision when measuring the initial moisture content of freeze-dried enzyme beads? The method for measuring the water content was the Karl Fischer titration method and the instrument used was the Mettler Toledo C30S Compact Karl Fischer-coulometer. The different optimized parameters were the sample preparation, extraction process and how the measurement was proceeded. It can be concluded that optimized method provides significantly different results compared with the old method and therefore the new method is better.</p>

w='containly' val={'c': 'certain', 's': 'diva2:1384216'}
w='ophilized' val={'c': 'lyophilized', 's': 'diva2:1384216'}
Replaced many ligatures.

corrected abstract:
<p>Cepheid provides instruments and tests for molecular diagnostic testing of bacterial and viral diseases. Their goal is to deliver a better way to improve patient outcomes by enabling access to molecular diagnostic testing everywhere. These test kits are used in warm and humid countries which may challenge the stability of the kits. The test kits contain lyophilized beads, which contain enzymes for the polymerase chain reaction, which usually ages first. The ageing of the beads is effected by the initial water content in them since it defines the quality. Therefore, it is of great importance to have a very precise and exact water measurement method. The degree project covers the optimization of the Standard Operating Procedure used today at Cepheid with the research question: <em>Can optimization of the Karl Fischer method improve the precision when measuring the initial moisture content of freeze-dried enzyme beads?</em></p><p>The method for measuring the water content was the Karl Fischer titration method and the instrument used was the Mettler Toledo C30S Compact Karl Fischer-coulometer. The different optimized parameters were the sample preparation, extraction process and how the measurement was proceeded.</p><p>It can be concluded that optimized method provides significantly different results compared with the old method and therefore the new method is better.</p>
----------------------------------------------------------------------
In diva2:1621933 abstract is: <p>Gene therapy is the treatment of a genetic disease through the transfer of genetic material to a pa tient, aiming to obtain long-lasting expression of the transferred gene, with enough intensity to cause therapeutic effects. Adeno-associated viruses (AAV) are one of the most distinguishable vectors for gene therapy, but despite of its eminent clinical success, vector production is still a bottleneck.</p><p>In this work, several amino acids were tested as supplements for transient transfection media. The experiments were done at small scale, with volumes of 5 mL, to screen multiple parameters in parallel. We evaluated the variation on cultures’ viability and cell-density throughout 72 hours post transfection (hpT) in different media compositions. Moreover, transfection efficiency is analysed via GFP production by flow cytometry and fluorescence microscope. Finally, rAAV production titres were quantified using ELISA.</p><p>The results endorse the strategy of optimization of transient transfection and cultivation media as a way to enhance the yield of AVV particles production in HEK293 cells. We discuss important limitations on the method’s reproducibility and propose new approaches to improve even more the study in fu ture experiments.</p>

w='tient' val={'c': 'patient', 's': 'diva2:1621933', 'n': 'no full text'}
w='AVV' val={'c': 'AAV', 's': 'diva2:1621933', 'n': 'no full text - seems to be spelled correct in Swedish abstract'}

corrected abstract:
<p>Gene therapy is the treatment of a genetic disease through the transfer of genetic material to a patient, aiming to obtain long-lasting expression of the transferred gene, with enough intensity to cause therapeutic effects. Adeno-associated viruses (AAV) are one of the most distinguishable vectors for gene therapy, but despite of its eminent clinical success, vector production is still a bottleneck.</p><p>In this work, several amino acids were tested as supplements for transient transfection media. The experiments were done at small scale, with volumes of 5 mL, to screen multiple parameters in parallel. We evaluated the variation on cultures’ viability and cell-density throughout 72 hours post transfection (hpT) in different media compositions. Moreover, transfection efficiency is analysed via GFP production by flow cytometry and fluorescence microscope. Finally, rAAV production titres were quantified using ELISA.</p><p>The results endorse the strategy of optimization of transient transfection and cultivation media as a way to enhance the yield of AAV particles production in HEK293 cells. We discuss important limitations on the method’s reproducibility and propose new approaches to improve even more the study in fu ture experiments.</p>
----------------------------------------------------------------------
In diva2:1595031 abstract is: <p>During the last decade, T lymphocytes have been used for cancer immunotherapies due to their highly cytotoxic capacity and ability to prolong survival in cancer patients. The enhancing of their functionality has been challenging due to the lack of a system that can produce gene editing with high accuracy and efficiency. With the intention of developing a system that provides accurate spatiotemporal control, a novel designed retroviral system called Blue VIPR that uses optogenetics and clustered regularly interspaced short palindromic repeats CRISPR/Cas-system, is here investigated.</p><p>For this purpose, a total of three viral architectures based on the Blue VIPR system, were used for transduction of murine Cas9<sup>+</sup> T lymphocytes with the aim of producing knockouts. Optimization of the transduction efficiency was performed in Murine Embryonic Fibroblasts, as well as a comparison and analysis of the effects that these architectures provoke when used at different viral titters. Results shows that the novel BluVIPR is a powerful retroviral system that is able to produce knockouts in &gt; 90% of the population of T lymphocytes that is both transduced and activated (Thy1.1<sup>+</sup> and mCherry<sup>+</sup>).</p>

w='BluVIPR' val={'c': 'Blue VIPR', 's': 'diva2:1595031', 'n': 'no full text'}
w='CRISPR/Cas' val={'c': 'CRISPR/Cas9', 's': 'diva2:1595031', 'n': 'no full text'}

corrected abstract:
<p>During the last decade, T lymphocytes have been used for cancer immunotherapies due to their highly cytotoxic capacity and ability to prolong survival in cancer patients. The enhancing of their functionality has been challenging due to the lack of a system that can produce gene editing with high accuracy and efficiency. With the intention of developing a system that provides accurate spatiotemporal control, a novel designed retroviral system called Blue VIPR that uses optogenetics and clustered regularly interspaced short palindromic repeats CRISPR/Cas9-system, is here investigated.</p><p>For this purpose, a total of three viral architectures based on the Blue VIPR system, were used for transduction of murine Cas9<sup>+</sup> T lymphocytes with the aim of producing knockouts. Optimization of the transduction efficiency was performed in Murine Embryonic Fibroblasts, as well as a comparison and analysis of the effects that these architectures provoke when used at different viral titters. Results shows that the novel Blue VIPR is a powerful retroviral system that is able to produce knockouts in &gt; 90% of the population of T lymphocytes that is both transduced and activated (Thy1.1<sup>+</sup> and mCherry<sup>+</sup>).</p>
----------------------------------------------------------------------
In diva2:855915 abstract is: <p>The blood-brain barrier plays a vital role in protecting the central nervous system from variations in blood composition and toxins, it is also known to play a key role in diseases of the central nervous system. Most of our current understanding about biological mechanisms and disease comes from comprehensive studies of tissues or studies in live animals. An in vitro model of the blood-brain barrier can provide a platform which is more relatable to human physiology, is cost effective and ethically responsible. It has been previously described that the cells microenvironment coupled with other factors such as mechanical shear stress can lead to improved cell differerentiation and thus a more realistic in vitro model. Despite this very little has been achieved in the creation of a truly three-dimensional in vitro model of blood-brain-barrier.</p><p>This resport describes a microfluidic chip with a three-dimensional in vitro co-culture of human neurovascular cell types that closely resembles the organiszation of the blood-brain barrier - a '3D BBB Chip'. Said model was created by seeding cells in and around a circular lumen embedded in a three dimensional collagen hydrogel. The circular lumen was created by establishing a Saffman-Taylor instability with culture media which rather elegantly fingered through the more viscous collagen solution thus forming a long lumen like structure. Engineering the circular lumen required numerous studies to explore the chips surface chemistry and the composition of the collagen hydrogel.</p><p>It was found that addition of transglutaminase, a crosslinking enzyme improved the long-term viability of the hydrogel from six days to more than eight days when subjected to a fluidic flow rate of 100 µl/hr. Additionally it was observed that altering the hydrostatic pressure during the lumen formation allows the user to create circular lumens of varying diameter. Confocal imaging shows that human astrocytes and human brain pericytes were successfully embedded in the collage gel prior to the three-dimensopnal patterning and that human cerebral cortex microvascular endothelial cells were used to cover the entire lumen with a monolayer. Finally, initial permeability experiments show that the permeability of the cocultures for 3 kDa dextrand was recorded to be as low as 2x10<sup>-6</sup> cm/s.</p>

w='differerentiation' val={'c': 'differentiation', 's': 'diva2:855915', 'n': 'no full text'}
w='organiszation' val={'c': 'organization', 's': 'diva2:855915', 'n': 'no full text'}
w='resport' val={'c': 'report', 's': 'diva2:855915', 'n': 'no full text'}
w='three-dimensopnal' val={'c': 'three-dimensional', 's': 'diva2:855915', 'n': 'no full text'}
I assumed the usual italics for Latin words.

corrected abstract:
<p>The blood-brain barrier plays a vital role in protecting the central nervous system from variations in blood composition and toxins, it is also known to play a key role in diseases of the central nervous system. Most of our current understanding about biological mechanisms and disease comes from comprehensive studies of tissues or studies in live animals. An <em>in vitro</em> model of the blood-brain barrier can provide a platform which is more relatable to human physiology, is cost effective and ethically responsible. It has been previously described that the cells microenvironment coupled with other factors such as mechanical shear stress can lead to improved cell differentiation and thus a more realistic <em>in vitro</em> model. Despite this very little has been achieved in the creation of a truly three-dimensional <em>in vitro</em> model of blood-brain-barrier.</p><p>This report describes a microfluidic chip with a three-dimensional <em>in vitro</em> co-culture of human neurovascular cell types that closely resembles the organization of the blood-brain barrier - a '3D BBB Chip'. Said model was created by seeding cells in and around a circular lumen embedded in a three dimensional collagen hydrogel. The circular lumen was created by establishing a Saffman-Taylor instability with culture media which rather elegantly fingered through the more viscous collagen solution thus forming a long lumen like structure. Engineering the circular lumen required numerous studies to explore the chips surface chemistry and the composition of the collagen hydrogel.</p><p>It was found that addition of transglutaminase, a crosslinking enzyme improved the long-term viability of the hydrogel from six days to more than eight days when subjected to a fluidic flow rate of 100 µl/hr. Additionally it was observed that altering the hydrostatic pressure during the lumen formation allows the user to create circular lumens of varying diameter. Confocal imaging shows that human astrocytes and human brain pericytes were successfully embedded in the collage gel prior to the three-dimensional patterning and that human cerebral cortex microvascular endothelial cells were used to cover the entire lumen with a monolayer. Finally, initial permeability experiments show that the permeability of the cocultures for 3 kDa dextrand was recorded to be as low as 2x10<sup>-6</sup> cm/s.</p>
----------------------------------------------------------------------
Note that this is a scanned document and there are no fonts, just images of pages.

In diva2:1741440 abstract is: <p>In humans, female fertility is established before birth and is characterized by a limited follicular reserve, which decreases naturally with age. Women’s fertility depends on the steroid and oocyte production, which can be negatively influenced by pathological conditions, such as polycystic ovary syndrome (PCOS), premature ovarian insufficiency (POI) and cancer, as well as several lifestyle and environmental factors. Since all of these factors could lead to a decrease in fertility, fertility preservation techniques and assisted reproductive technologies (ARTs) have become increasingly important. Current research for gonad regeneration include organoid and spheroid formation, as a novel type of in vitro 3D culturing, which enables the establishment of an organ model systems. During this thesis, two different 3D-culture systems: three-layer gradient system (3-LGS) and Biosilk™, were compared for spheroid formation from adult human ovarian cells, stemming from both cortex and medulla. In the 3-LGS, some unorganized aggregates, based on medulla, could be found up to 25 days of culture, but did not yield in any final 3D-cultures after 6 weeks of culture. Consequently, further investigations are needed to conclude whether 3-LGS can be used for human adult ovarian cells. In contrast to this, ovarian 3D-cultures could be achieved with Biosilk™, using cells from both cortex and medulla. These were either fixed or stored in -80°C for later bulk RNA-sequencing and RNA fluorescence in situ hybridization (RNA-FISH). An RNA-FISH protocol (RNAscope®) was refined for this purpose. Tested adjustments included varying glass-type for sectioning, and tissue bleaching time. Moreover, different fluorophores were tested for clearer visualization of ovarian tissue material. Tissue sectioned on coated coverslips did not result in a sharper signal as first anticipated. Instead, the tissue was both autofluorescent, and folding or falling off of the coated coverslips. Therefore, sectioning on coverslips was not continued. There is, however, a possibility that it could be used, if the RNAscope® protocol was adjusted accordingly. Since both ovarian tissue and Biosilk™ are autofluorescent, it was concluded that a longer bleaching time than in the original RNAscope® protocol, resulted in less autofluorescense and a more clear signal. Thereby, making the protocol more suitable for application on ovary 3D-cultures based on Biosilk™, than it was before. Lastly, the tested fluorophores TSA 520, TSA 570 and TSA 650 resulted in a sharper signal than Fluorescein Tyramide, Cy 3 and Cy 5.</p>

w='autofluorescense' val={'c': 'autofluorescence', 's': ['diva2:1788558', 'diva2:1741440']}

corrected abstract:
<p>In humans, female fertility is established before birth and is characterized by a limited follicular reserve, which decreases naturally with age. Women’s fertility depends on the steroid and oocyte production, which can be negatively influenced by pathological conditions, such as polycystic ovary syndrome (PCOS), premature ovarian insufficiency (POI) and cancer, as well as several lifestyle and environmental factors. Since all of these factors could lead to a decrease in fertility, fertility preservation techniques and assisted reproductive technologies (ARTs) have become increasingly important. Current research for gonad regeneration include organoid and spheroid formation, as a novel type of <em>in vitro</em> 3D culturing, which enables the establishment of an organ model systems. During this thesis, two different 3D-culture systems: three-layer gradient system (3-LGS) and Biosilk™, were compared for spheroid formation from adult human ovarian cells, stemming from both cortex and medulla. In the 3-LGS, some unorganized aggregates, based on medulla, could be found up to 25 days of culture, but did not yield in any final 3D-cultures after 6 weeks of culture. Consequently, further investigations are needed to conclude whether 3-LGS can be used for human adult ovarian cells. In contrast to this, ovarian 3D-cultures could be achieved with Biosilk™, using cells from both cortex and medulla. These were either fixed or stored in -80°C for later bulk RNA-sequencing and RNA fluorescence <em>in situ</em> hybridization (RNA-FISH). An RNA-FISH protocol (RNAscope®) was refined for this purpose. Tested adjustments included varying glass-type for sectioning, and tissue bleaching time. Moreover, different fluorophores were tested for clearer visualization of ovarian tissue material. Tissue sectioned on coated coverslips did not result in a sharper signal as first anticipated. Instead, the tissue was both autofluorescent, and folding or falling off of the coated coverslips. Therefore, sectioning on coverslips was not continued. There is, however, a possibility that it could be used, if the RNAscope® protocol was adjusted accordingly. Since both ovarian tissue and Biosilk™ are autofluorescent, it was concluded that a longer bleaching time than in the origin al RNAscope® protocol, resulted in less autofluorescence and a more clear signal. Thereby, making the protocol more suitable for application on ovary 3D-cultures based on Biosilk™, than it was before. Lastly, the tested fluorophores TSA 520, TSA 570 and TSA 650 resulted in a sharper signal than Fluorescein Tyramide, Cy 3 and Cy 5.</p>
----------------------------------------------------------------------
In diva2:739841 abstract is: <p><strong>Background</strong></p><p>Because of extreme population and a lack of resources the risk of beeing harmed while admitted to a hospital in Bangladesh is big. Mistakes made at operating theatres can result in devastating consequences, but by evaluating the patient safety that risk can be minimized. Right now Bangladesh is in the middle of an industrialisation that is contributing to the growing need for an expanding health care. The country is regularly suffering from cyclones, tsunamis and monsoon rains and there is an urgent demand for safe health care.</p><p><strong>Method</strong></p><p>The aim of this thesis was to study the physical structure, organisation and practice at operating theatres in Bangladesh. At three private and two public hospitals 14 operating rooms in total were visited and the basic equipment was examined. Managers, physicians, nurses and technicians were interviewed at all hospitals, 41 people in total participated in the study.</p><p><strong>Results</strong></p><p>The temperature control was not up to standard, bigger storages were needed and none of the public hospitals had enough washing equipment for proper scrubbing. Only one hospital could monitor the patient’s body temperature during surgery and proper resuscitation equipment was missing in half of the operating rooms. The autoclave process could not keep up with the surgeries and delays were not unusual. The cleaning staff had no training in patient safety and the staff found that the nurse’s education was not enough. The reporting of mistakes rarely reached the management and a written report was unusual.</p><p><strong>Discussion</strong></p><p>Most of the staff did not know what calibration meant and there were only biomedical departments at two of the hospitals. Even though training was re- quested by the staff the management did not plan for any changes. This shows that it is the organisation, not the human errors, that is the source to the unstable situation of health care. The lacking of the reporting system is an- other reason for the slow development. Staff with technical knowledge must be available at the hospitals in order to help prevent risks and all hospitals should establish a biomedical department. Patients had to lie on the floor, due to the shortness of space. This is not good for patient safety, but the alternative would be that they would end up with no help at all. The outcome of patient safety should always result in better health for the people. The personal had this view of thinking and they showed great engagement to their work.</p><p><strong>Key words:</strong> Patient safety, Bangladesh, operating theatres, operating rooms</p>

w='beeing' val={'c': 'being', 's': ['diva2:862341', 'diva2:739841', 'diva2:1455922'], 'n': 'error in original'}

corrected abstract:
<p><strong><em>Background</em></strong></p><p>Because of extreme population and a lack of resources the risk of beeing harmed while admitted to a hospital in Bangladesh is big. Mistakes made at operating theatres can result in devastating consequences, but by evaluating the patient safety that risk can be minimized. Right now Bangladesh is in the middle of an industrialisation that is contributing to the growing need for an expanding health care. The country is regularly suffering from cyclones, tsunamis and monsoon rains and there is an urgent demand for safe health care.</p><p><strong><em>Method</em></strong></p><p>The aim of this thesis was to study the physical structure, organisation and practice at operating theatres in Bangladesh. At three private and two public hospitals 14 operating rooms in total were visited and the basic equipment was examined. Managers, physicians, nurses and technicians were interviewed at all hospitals, 41 people in total participated in the study.</p><p><strong><em>Results</em></strong></p><p>The temperature control was not up to standard, bigger storages were needed and none of the public hospitals had enough washing equipment for proper scrubbing. Only one hospital could monitor the patient’s body temperature during surgery and proper resuscitation equipment was missing in half of the operating rooms. The autoclave process could not keep up with the surgeries and delays were not unusual. The cleaning staff had no training in patient safety and the staff found that the nurse’s education was not enough. The reporting of mistakes rarely reached the management and a written report was unusual.</p><p><strong><em>Discussion</em></strong></p><p>Most of the staff did not know what calibration meant and there were only biomedical departments at two of the hospitals. Even though training was requested by the staff the management did not plan for any changes. This shows that it is the organisation, not the human errors, that is the source to the unstable situation of health care. The lacking of the reporting system is another reason for the slow development. Staff with technical knowledge must be available at the hospitals in order to help prevent risks and all hospitals should establish a biomedical department. Patients had to lie on the floor, due to the shortness of space. This is not good for patient safety, but the alternative would be that they would end up with no help at all. The outcome of patient safety should always result in better health for the people. The personal had this view of thinking and they showed great engagement to their work.</p>
----------------------------------------------------------------------
In diva2:1296147 abstract is: <p>Wastewater treatment plants (WWT P ) are often the cause of malodor. The com-pounds which are the main causes of the odor is hydrogen sulﬁde (H2S), ammonia (NH3), mercaptans (RSH) and volatile organic compounds (V OC) [1]. The odorous air can be analyzed to determine the concentration of the odorants. The odorous can also be analyzed by measuring the odor. The odor is measured, a test panel of people smells the odorous air and determines how many times greater the concentration of the odorants is compared to the odor threshold which is the concentration at which a compound or a mixture is detectable by smell [3].Measurements were done at three systems for air puriﬁcation at three diﬀerent locations, the Vimmerby WWT P , the Alvim WWT P and Renova’s biological waste treatment facility in Gothenburg. The odor was measured at the inlet and the outlet and the concentration of H2S and ozone (O3) were measured at all the sampling points of the systems. The system at Vimmerby consisted of three CIF s followed by an UV-reactor and an AC-reactor. In Alvim there were two system which used UV and AC. The system at Renova consisted of a barrier ﬁlter followed by UV and AC.The system at the Vimmerby WWT P had a conversion rate between 87-97% of H2S. The CIF s had conversions between 50-64% of the H2S. H2S was not detected at any of the other systems. O3 was only detected at Renova where ground level O3 was present at the inlet, 0.16 ppm. The concentration increased to 0.20 ppm after the UV-reactor. The activated carbon could adsorb all the incoming O3.The odor at the Vimmerby WWT P was determined to 27500 Ou/m3 at the inlet and 19071 Ou/m3 at the outlet. The odor conversion over the system was 31%. The odor conversion at the Alvim WWTP was 99.8%. With an odor of 5490 Ou/m3 at the inlet and 11 Ou/m3 at the outlet. The ingoing air at the system at Renova had an odor of 434 Ou/m3 and was reduced to 36 Ou/m3 at outlet. The odor conversion at Renova was 92%.To increase the accuracy of the measurements he time between the sampling and measurements should have been minimized. The test panels should also have been larger and the panelists should have been screened in advance, so results from panelists which were over and under sensitive to odors were not included in the ﬁnal results.</p>

w='WWT' val={'c': 'WWTP', 's': 'diva2:1296147', 'n': 'correct in original - but the character spacing makes it look as if there is a space'}
w='WWT P' val={'c': 'WWTP', 's': 'diva2:1296147', 'n': 'correct in original - but the character spacing makes it look as if there is a space'}
Replaced several different ligatures.


corrected abstract:
<p>Wastewater treatment plants (WWTP) are often the cause of malodor. The compounds which are the main causes of the odor is hydrogen sulfide (H<sub>2</sub>S), ammonia (NH3), mercaptans (RSH) and volatile organic compounds (VOC) [1]. The odorous air can be analyzed to determine the concentration of the odorants. The odorous can also be analyzed by measuring the odor. The odor is measured, a test panel of people smells the odorous air and determines how many times greater the concentration of the odorants is compared to the odor threshold which is the concentration at which a compound or a mixture is detectable by smell [3].</p><p>Measurements were done at three systems for air purification at three different locations, the Vimmerby WWTP, the Alvim WWTP and Renova’s biological waste treatment facility in Gothenburg. The odor was measured at the inlet and the outlet and the concentration of H<sub>2</sub>S and ozone (O<sub>3</sub>) were measured at all the sampling points of the systems. The system at Vimmerby consisted of three CIFs followed by an UV-reactor and an AC-reactor. In Alvim there were two system which used UV and AC. The system at Renova consisted of a barrier filter followed by UV and AC.</p><p>The system at the Vimmerby WWTP had a conversion rate between 87-97% of H<sub>2</sub>S. The CIFs had conversions between 50-64% of the H<sub>2</sub>S. H<sub>2</sub>S was not detected at any of the other systems. O<sub>3</sub> was only detected at Renova where ground level O<sub>3</sub> was present at the inlet, 0.16 <em>ppm</em>. The concentration increased to 0.20 <em>ppm</em> after the UV-reactor. The activated carbon could adsorb all the incoming O<sub>3</sub>.</p><p>The odor at the Vimmerby WWTP was determined to 27500 Ou/m<sup>3</sup> at the inlet and 19071 Ou/m<sup>3</sup> at the outlet. The odor conversion over the system was 31%. The odor conversion at the Alvim WWTP was 99.8%. With an odor of 5490 Ou/m<sup>3</sup> at the inlet and 11 Ou/m<sup>3</sup> at the outlet. The ingoing air at the system at Renova had an odor of 434 Ou/m<sup>3</sup> and was reduced to 36 Ou/m<sup>3</sup> at outlet. The odor conversion at Renova was 92%.</p><p>To increase the accuracy of the measurements he time between the sampling and measurements should have been minimized. The test panels should also have been larger and the panelists should have been screened in advance, so results from panelists which were over and under sensitive to odors were not included in the final results.</p>
----------------------------------------------------------------------
'diva2:1352888' seems to be a duplicate of diva2:1296147
----------------------------------------------------------------------
In diva2:1430868 abstract is: <p>Polyhydroxyalkanoates (PHAs) are a group of bioplastics, which are produced through microorganisms. They are accumulated in granules inside bacteria’s cell cytoplasm and serve as an energy reserve. Moreover, PHAs are completely biodegradable and biocompatible biopolyesters, which make them to suitable materials for medical applications and replace conventional petrochemical plastics. However, it is not economically feasible to produce PHAs yet, as it is four to nine times as expensive as to produce fossil fuel-based plastics. In order to reduce the price, it is possible to use waste streams rich in carbon and mixed cultures as microorganisms, which was applied in this thesis work.</p><p>In this study, PHAs were synthesized from a volatile fatty acid (VFA) mixture rich in hexanoic acid, which was produced by anaerobic digestion of waste streams. To be able to obtain the maximum PHA content, the experimental work was separated into a selection phase and a production phase respectively. During the selection step, enrichment of the mixed culture took place during 50 days altering feast/famine cycles. The production phase was then conducted in a fed-batch cultivation to accumulate as much PHAs as possible, while utilizing the enriched mixed culture. </p><p>The selection phase was seen as successful since the quantity of synthesized PHA increased with time. Solely polyhydroxybutyrate (PHB) was formed during this period. The specific consumption rates for the hexanoic acid and acetic acid were almost the same in this phase (0.10 g hexanoic acid/(g volatile suspended soilds (VSS),h) and 0.11 g acetic acid/(g VSS,h)), which suggests that the consumption of these majoritarian fatty acids was simultaneous. However, the determined consumption rate for butyric acid was approximately solely half of the values for hexanoic acid and acetic acid. The highest PHA yield obtained in the enrichment phase was 0.26 g PHB/g VFA.  In the production phase, the highest achieved PHA content was 31.4 % of VSS, which was obtained after five hours. Both PHB and polyhydroxyvalerate (PHV) were formed in this phase, even though the quantity of accumulated PHB dominated with its approximately 97 weight-%.</p>

w='soilds' val={'c': 'solids', 's': 'diva2:1430868', 'n': 'error in original'}

corrected abstract:
<p>Polyhydroxyalkanoates (PHAs) are a group of bioplastics, which are produced through microorganisms. They are accumulated in granules inside bacteria’s cell cytoplasm and serve as an energy reserve. Moreover, PHAs are completely biodegradable and biocompatible biopolyesters, which make them to suitable materials for medical applications and replace conventional petrochemical plastics. However, it is not economically feasible to produce PHAs yet, as it is four to nine times as expensive as to produce fossil fuel-based plastics. In order to reduce the price, it is possible to use waste streams rich in carbon and mixed cultures as microorganisms, which was applied in this thesis work.</p><p>In this study, PHAs were synthesized from a volatile fatty acid (VFA) mixture rich in hexanoic acid, which was produced by anaerobic digestion of waste streams. To be able to obtain the maximum PHA content, the experimental work was separated into a selection phase and a production phase respectively. During the selection step, enrichment of the mixed culture took place during 50 days altering feast/famine cycles. The production phase was then conducted in a fed-batch cultivation to accumulate as much PHAs as possible, while utilizing the enriched mixed culture.</p><p>The selection phase was seen as successful since the quantity of synthesized PHA increased with time. Solely polyhydroxybutyrate (PHB) was formed during this period. The specific consumption rates for the hexanoic acid and acetic acid were almost the same in this phase (0.10 g hexanoic acid/(g volatile suspended soilds (VSS),h) and 0.11 g acetic acid/(g VSS,h)), which suggests that the consumption of these majoritarian fatty acids was simultaneous. However, the determined consumption rate for butyric acid was approximately solely half of the values for hexanoic acid and acetic acid. The highest PHA yield obtained in the enrichment phase was 0.26 g PHB/g VFA. In the production phase, the highest achieved PHA content was 31.4 % of VSS, which was obtained after five hours. Both PHB and polyhydroxyvalerate (PHV) were formed in this phase, even though the quantity of accumulated PHB dominated with its approximately 97 weight-%.</p>
----------------------------------------------------------------------
In diva2:744697 abstract is: <p>In this study we deliver genes coding for chimeric proteins consisting of an affinity ligand (Affibody molecule; binding to Ras or Raf) attached to a degrading signal through a lentiviral delivery system. The goal is to degrade Ras and Raf inside the cell to create a system that allows for control of proliferation through inhibition of signaling through the MAPK signal pathway. Three different constructs were analyzed utilizing slightly different ways to degrade Ras or Raf proteins through the endogenous proteasomal degradation system. The first construct emphasize on using the proteasomes own degradation tag ubiquitin attached to the Affibody molecule. The second uses Protax technology destilled from HIF1 to recruit the Affibody molecule. The second uses Protac technology destilled from HIF1 to recruit the Afribody molecules to the von Hippel Lindau (VHL) part of the VBC-Cul2 E3 ubiquitin ligase that actively marks HIF1 for proteasomal destruction under normoxic contitions. Last but not least a construct utilizing the TNFα inducible IKBα protein that is degraded as a stress response to TNFα stimulation in cells. As a control we utilized the Affibody molecule ZTaq which was not expected to interact with any protein inside the cell. Within the IKBα track an additional construct was also assembled that contained an affinity matured variant of the Affibody binding to Raf, henceforth called Raf<sub>3</sub>. Delivery of genes were successful and the cells were monitored closely. Proliferation stalled slightly but, after extensive testing, no solid conclusions could be made if the proposed effect was achieved or simply toxic for the cells. Extended testing is needed to make any decisions on the future of the concept.</p>

w='destilled' val={'c': 'distilled', 's': 'diva2:744697', 'n': 'no full text'}
w='contitions' val={'c': 'conditions', 's': 'diva2:744697', 'n': 'no full text'}
w='Afribody' val={'c': 'Affibody', 's': 'diva2:744697', 'n': 'no full text'}
w='ZTaq' val={'c': 'Z<sub>Taq</sub>', 's': 'diva2:744697', 'n': 'no full text'}

corrected abstract:
<p>In this study we deliver genes coding for chimeric proteins consisting of an affinity ligand (Affibody molecule; binding to Ras or Raf) attached to a degrading signal through a lentiviral delivery system. The goal is to degrade Ras and Raf inside the cell to create a system that allows for control of proliferation through inhibition of signaling through the MAPK signal pathway. Three different constructs were analyzed utilizing slightly different ways to degrade Ras or Raf proteins through the endogenous proteasomal degradation system. The first construct emphasize on using the proteasomes own degradation tag ubiquitin attached to the Affibody molecule. The second uses Protax technology distilled from HIF1 to recruit the Affibody molecule. The second uses Protac technology distilled from HIF1 to recruit the Affibody molecules to the von Hippel Lindau (VHL) part of the VBC-Cul2 E3 ubiquitin ligase that actively marks HIF1 for proteasomal destruction under normoxic conditions. Last but not least a construct utilizing the TNFα inducible IKBα protein that is degraded as a stress response to TNFα stimulation in cells. As a control we utilized the Affibody molecule Z<sub>Taq</sub> which was not expected to interact with any protein inside the cell. Within the IKBα track an additional construct was also assembled that contained an affinity matured variant of the Affibody binding to Raf, henceforth called Raf<sub>3</sub>. Delivery of genes were successful and the cells were monitored closely. Proliferation stalled slightly but, after extensive testing, no solid conclusions could be made if the proposed effect was achieved or simply toxic for the cells. Extended testing is needed to make any decisions on the future of the concept.</p>
----------------------------------------------------------------------
In diva2:852274 abstract is: <p>Allogeneic hematopoietic stem cell transplantation (HSCT) is a widely used medical procedure for treatment of various malignant and non-malignant diseases. However, there are still great risks associated with this treatment. A major complication of allogeneic HSCT is acute graft-versus-host disease (aGvHD). Due to difficulty in treating severe forms of acute GvHD it is desirable to identify predictive markers to assess the risk of patients developing aGvHD. Unfortunately, good biomarkers indicative of acute GvHD remain few. Therefore, the aim of this project is to shed further light on the topic by studying the graft in vitro in hope of finding correlations between expression or secretion of biomarkers and disease progression.</p><p>This study was conducted on sixteen patient-donor couples for allo-HSCT. Post HSCT, the patients were graded with no acute GvHD, grade I or grade II acute GvHD. We looked at markers for T lymphocytes using a mixed lymphocyte reaction (MLR) to analyze how graft cells responded to  irradiated cells from the graft recipient. To this end, we used flow cytometry and Luminex to analyze the reactivity of the donor cells and the correlation of biomarkers with later in vivo development of grade 0 to II of acute GVHD. Ou main finding was a significant decrease (p=0.004) in the cell viability (7-AAD cells) of graft cells for patiens who later developed grade II acute GvHD, compared to patients with no acute GvHD. Overall there were indications of higher frquencies of donor effector T cells for grafts given to patiens who later developed grade II aGvHD. To better assess the correlation between dvelopment of higher stages of acute GvHD and the frequency of different biomarkers more in depth analysis and larger patient groups are needed.</p>

w='dvelopment' val={'c': 'development', 's': 'diva2:852274', 'n': 'no full text'}
w='frquencies' val={'c': 'frequencies', 's': 'diva2:852274', 'n': 'no full text'}
w='patiens' val={'c': 'patients', 's': ['diva2:801778', 'diva2:852274'], 'n': 'no full text'}

corrected abstract:
<p>Allogeneic hematopoietic stem cell transplantation (HSCT) is a widely used medical procedure for treatment of various malignant and non-malignant diseases. However, there are still great risks associated with this treatment. A major complication of allogeneic HSCT is acute graft-versus-host disease (aGvHD). Due to difficulty in treating severe forms of acute GvHD it is desirable to identify predictive markers to assess the risk of patients developing aGvHD. Unfortunately, good biomarkers indicative of acute GvHD remain few. Therefore, the aim of this project is to shed further light on the topic by studying the graft in vitro in hope of finding correlations between expression or secretion of biomarkers and disease progression.</p><p>This study was conducted on sixteen patient-donor couples for allo-HSCT. Post HSCT, the patients were graded with no acute GvHD, grade I or grade II acute GvHD. We looked at markers for T lymphocytes using a mixed lymphocyte reaction (MLR) to analyze how graft cells responded to  irradiated cells from the graft recipient. To this end, we used flow cytometry and Luminex to analyze the reactivity of the donor cells and the correlation of biomarkers with later in vivo development of grade 0 to II of acute GVHD. Ou main finding was a significant decrease (p=0.004) in the cell viability (7-AAD cells) of graft cells for patients who later developed grade II acute GvHD, compared to patients with no acute GvHD. Overall there were indications of higher frequencies of donor effector T cells for grafts given to patients who later developed grade II aGvHD. To better assess the correlation between development of higher stages of acute GvHD and the frequency of different biomarkers more in depth analysis and larger patient groups are needed.</p>
----------------------------------------------------------------------
title: "Preparation and evaluation of sulfided NiMo/γ-Al2O3 hydrotreating catalysts"
==>    "Preparation and evaluation of sulfided NiMo/γ-Al<sub>2</sub>O<sub>3</sub> hydrotreating catalysts"

In diva2:953658 abstract is: <p><strong> </strong></p><p>Four nickel-molybdenum catalysts were synthesized on gamma alumina with higher surface area and on NiMo catalyst was prepared using gamma alumina with lower surface area. Catalysts with higher-surface-area support were prepared by co-impregnation, sequential impregnation and adding phosphorous. Theses catalysts were calcined at 500  ͦC. Effect of higher calcination temperature was investigated by preparation of one catalyst calcined at 700 ͦC. Catalysts were thoroughly characterized via four characterization techniques. The hydrotreating activity of three catalysts was carried out in a micro reactor at high pressure and three different temperatures with Nynas vacuum middle distillate. Prior to the test, sulfiding was carried out to activate the catalysts. Hydrotreated-oil samples as products were analyzed to evaluate the activity and conversion of the catalyst. Also, the spent catalysts were characterized to evaluate the surface area characteristics and deactivation of catalysts. Addition of phosphorous to NiMo/gamma-Al2O3 improved the interaction between the metals and the support as well as reduced the coke formation as observed in scanning electron microscopy micrographs.</p>

w='ͦC' val={'c': '℃', 's': 'diva2:953658', 'n': 'correct in original'}

corrected abstract:
<p>Four nickel-molybdenum catalysts were synthesized on γ-alumina with higher surface area and on NiMo catalyst was prepared using γ-alumina with lower surface area. Catalysts with higher-surface-area support were prepared by coimpregnation, sequential impregnation and adding phosphorous. Theses catalysts were calcined at 500 ℃. Effect of higher calcination temperature was investigated by preparation of one catalyst calcined at 700 ℃. Catalysts were thoroughly characterized via four characterization techniques. The hydrotreating activity of three catalysts was carried out in a micro reactor at high pressure and three different temperature with Nynas vacuum middle distillate. Prior to the test, sulfiding was carried out to activate the catalysts. Hydrotreated-oil samples as products were analyzed to evaluate the activity and conversion of the catalyst. Also, the spent catalysts were characterized to evaluate the surface area characteristics and deactivation of catalysts. Addition of phosphorous to NiMo/γ-Al<sub>2</sub>O<sub>3</sub> improved the interaction between the metals and the support as well as reduced the coke formation as observed in scanning electron microscopy micrographs.</p>
----------------------------------------------------------------------
In diva2:1640551 abstract is: <p>B-SPORT+ is a project that was interested in developing an application for advice and guidance in regards to physical exercise adapted for people with disabilities. B-SPORT+ identified the need for a decentralized ledger in the application. A decentralized ledger is a register that stores data on transactions performed in an application. In previous work on the application, a Blockchain was highlighted as a possible solution. However, B-SPORT+ experienced that this technology contained disadvantages such as high energy consumption and expensive implementation. Therefore, this work investigated, developed and evaluated an alternative to Blockchain using relational databases.</p><p>The result was two prototypes. The first prototype mimicked Blockchain technology by horizontally fragmenting a relational database that stored a table of performed transactions. Then, cryptographic hashing was used to validate transactions between database fragments. A prototype was also developed using Blockchain technology, this prototype was used to evaluate the first prototype. The evaluation showed that the structure of the SQL prototype reduced memory utilization in user computers it also reduced energy consumption and time when performing transactions. This structure also allowed moderation of data in the ledger, which was vital for the application BSPORT + wanted to develop. </p>

partal corrected: diva2:1640551: <p>B-SPORT+ is a project that was interested in developing an application for advice and guidance in regards to physical exercise adapted for people with dis abilities. B-SPORT+ identified the need for a decentralized ledger in the application. A decentralized ledger is a register that stores data on transactions performed in an application. In previous work on the application, a Blockchain was highlighted as a possible solution. However, B-SPORT+ experienced that this technology contained dis advantages such as high energy consumption and expensive implementation. Therefore, this work investigated, developed and evaluated an alternative to Blockchain using relational databases.</p><p>The result was two prototypes. The first prototype mimicked Blockchain technology by horizontally fragmenting a relational database that stored a table of performed transactions. Then, cryptographic hashing was used to validate transactions between database fragments. A prototype was also developed using Blockchain technology, this prototype was used to evaluate the first prototype. The evaluation showed that the structure of the SQL prototype reduced memory utilization in user computers it also reduced energy consumption and time when performing transactions. This structure also allowed moderation of data in the ledger, which was vital for the application BSPORT + wanted to develop. </p>
w='BSPORT' val={'c': 'B-SPORT+', 's': 'diva2:1640551', 'n': 'hyphen split over linebreak'}
The last "B-SPORT +" is set with a space in the final sentence of the original abstract.


corrected abstract:
<p>B-SPORT+ is a project that was interested in developing an application for advice and guidance in regards to physical exercise adapted for people with disabilities. B-SPORT+ identified the need for a decentralized ledger in the application. A decentralized ledger is a register that stores data on transactions performed in an application. In previous work on the application, a Blockchain was highlighted as a possible solution. However, B-SPORT+ experienced that this technology contained disadvantages such as high energy consumption and expensive implementation. Therefore, this work investigated, developed and evaluated an alternative to Blockchain using relational databases.</p><p>The result was two prototypes. The first prototype mimicked Blockchain technology by horizontally fragmenting a relational database that stored a table of performed transactions. Then, cryptographic hashing was used to validate transactions between database fragments. A prototype was also developed using Blockchain technology, this prototype was used to evaluate the first prototype. The evaluation showed that the structure of the SQL prototype reduced memory utilization in user computers it also reduced energy consumption and time when performing transactions. This structure also allowed moderation of data in the ledger, which was vital for the application B-SPORT + wanted to develop.</p>
----------------------------------------------------------------------
In diva2:725219 abstract is: <p>Dramatify is providing TV and film production companies with a software as a service for project management. The web application is accessible from any device with a modern web browser from anywhere in the world. Dramatify were having performance issues with high latency and needed help to implement cache for maximum performance gain. To solve the problem, a research was conducted where information about suitable ap-plications to implement, test and analyze prototypes for storing data in cache.</p><p>The result was two prototypes, one for the client and one for the server, for managing cache. Performance testing was done with automatic tests on multiple devices in different web browsers. The tests was collecting rele-vant data to measure the performance in conjunction to the original imple-mentation. When analyzing the collected test data, it showed that the client prototype was 32 percent faster and that the server prototype was 21 per-cent faster than the original implementation.</p>

w='rele-vant' val={'c': 'relevant', 's': 'diva2:725219'}

corrected abstract:
<p>Dramatify is providing TV and film production companies with a software as a service for project management. The web application is accessible from any device with a modern web browser from anywhere in the world. Dramatify were having performance issues with high latency and needed help to implement cache for maximum performance gain. To solve the problem, a research was conducted where information about suitable applications to implement, test and analyze prototypes for storing data in cache.</p><p>The result was two prototypes, one for the client and one for the server, for managing cache. Performance testing was done with automatic tests on multiple devices in different web browsers. The tests was collecting relevant data to measure the performance in conjunction to the original implementation. When analyzing the collected test data, it showed that the client prototype was 32 percent faster and that the server prototype was 21 percent faster than the original implementation.</p>
----------------------------------------------------------------------
In diva2:1865596 abstract is: <p>Sensitive data leaking from a system can have tremendous negative consequences, such as discrimination, social stigma, and fraudulent economic consequences for those whose data has been leaked. Therefore, it’s of utmost importance that sensitive data is not leaked from a system. This thesis investigated different methods to prevent sensitive patient data from leaking in a machine learning system. Various methods have been investigated and evaluated based on previous research; the methods used in this thesis are a large language model (LLM) for code analysis and a membership inference attack on models to test their privacy level. The LLM code analysis results show that the Llama 3 (an LLM) model had an accuracy of 90% in identifying malicious code that attempts to steal sensitive patient data. The model analysis can evaluate and determine membership inference of sensitive patient data used for training in machine learning models, which is essential for determining data leakage a machine learning model can pose in machine learning systems. Further studies in increasing the deterministic and formatting of the LLM‘s responses must be investigated to ensure the robustness of the security system that utilizes LLMs before it can be deployed in a production environment. Further studies of the model analysis can apply a wider variety of evaluations, such as increased size of machine learning model types and increased range of attack testing types of machine learning models, which can be implemented into machine learning systems.</p>

w='LLM‘s' val={'c': "LLM's", 's': 'diva2:1865596', 'n': 'original uses 0x2018 Left Single Quotation Mark as an apostrophe'}

no changes
----------------------------------------------------------------------
In diva2:1595504 abstract is: <p>Prostate cancer is one of the most common types of cancer worldwide and claims hundreds of thousands of lives annually. Currently the most common treatment for prostate cancer is external beam radiotherapy, however, this treatment comes with serious side effects since it lacks selectivity for the cancer cells. Therefore, less harmful treatments are needed and sought for, such as targeted treatments that are intended to only affect cancer cells and thereby reduce the side effects. Targeted treatments require a target that differentiates the cancer cells from healthy cells. A promising target candidate that has gained attention in recent years is gastrin releasing peptide receptor (GRPR), a protein commonly overexpressed in prostate cancer cells. Furthermore, a targeting molecule intended to bind to the target is also required. For this purpose, the bombesin analogue RM26, a high affinity GRPR binder, shows promise.</p><p>Previous studies have led to the development of RM26-conjugates for the purpose of targeted prostate cancer radiotherapy. In these conjugates RM26 has been linked to a DOTA-chelator for radiolabeling, and an albumin binding domain (ABD) to prolong the conjugate’s half-life in vivo by binding to human serum albumin (HSA). The idea is that the RM26-conjugate will bind to both HSA in the blood and to GRPR on the prostate cancer cells and eliminate the cancer cells with the radiation from the radionuclide attached to the DOTA-chelator. Although these earlier studied conjugates have been very promising some improvements of certain aspects need to be achieved, mainly to improve the biodistribution with retained GRPR binding affinity. Therefor the purpose of this project was to produce three new versions of previous RM26- conjugates and evaluate if they are suitable for further prostate cancer therapy studies.</p><p>The three RM26-conjugates were developed with primarily recombinant expression in E. coli cells and solid phase peptide synthesis (SPPS). The characterization phase in this project was carried out with mainly five different methods: matrix-assisted laser desorption ionization time- of-flight mass spectrometry (MALDI-TOF-MS), electrospray ionization- mass spectrometry (ESI-MS), circular dichroism (CD), surface plasmon resonance (SPR) and flow cytometry. The results showed that all three new RM26-conjugates were possible to produce and yielded final products corresponding to the expected molecular weights. Furthermore, the results indicate that all three RM26-conjuagtes are stable and maintain their structural properties under in vivo- temperatures and that they have high binding affinity for HSA. Further studies need to be conducted before drawing any certain conclusions regarding GRPR binding affinity.</p>

w='RM26-conjuagtes' val={'c': 'RM26-conjugates', 's': 'diva2:1595504', 'n': 'error in original'}
Added the italics.

corrected abstract:
<p>Prostate cancer is one of the most common types of cancer worldwide and claims hundreds of thousands of lives annually. Currently the most common treatment for prostate cancer is external beam radiotherapy, however, this treatment comes with serious side effects since it lacks selectivity for the cancer cells. Therefore, less harmful treatments are needed and sought for, such as targeted treatments that are intended to only affect cancer cells and thereby reduce the side effects. Targeted treatments require a target that differentiates the cancer cells from healthy cells. A promising target candidate that has gained attention in recent years is gastrin releasing peptide receptor (GRPR), a protein commonly overexpressed in prostate cancer cells. Furthermore, a targeting molecule intended to bind to the target is also required. For this purpose, the bombesin analogue RM26, a high affinity GRPR binder, shows promise.</p><p>Previous studies have led to the development of RM26-conjugates for the purpose of targeted prostate cancer radiotherapy. In these conjugates RM26 has been linked to a DOTA-chelator for radiolabeling, and an albumin binding domain (ABD) to prolong the conjugate’s half-life <em>in vivo</em> by binding to human serum albumin (HSA). The idea is that the RM26-conjugate will bind to both HSA in the blood and to GRPR on the prostate cancer cells and eliminate the cancer cells with the radiation from the radionuclide attached to the DOTA-chelator. Although these earlier studied conjugates have been very promising some improvements of certain aspects need to be achieved, mainly to improve the biodistribution with retained GRPR binding affinity. Therefor the purpose of this project was to produce three new versions of previous RM26- conjugates and evaluate if they are suitable for further prostate cancer therapy studies.</p><p>The three RM26-conjugates were developed with primarily recombinant expression in E. coli cells and solid phase peptide synthesis (SPPS). The characterization phase in this project was carried out with mainly five different methods: matrix-assisted laser desorption ionization time-of-flight mass spectrometry (MALDI-TOF-MS), electrospray ionization- mass spectrometry (ESI-MS), circular dichroism (CD), surface plasmon resonance (SPR) and flow cytometry. The results showed that all three new RM26-conjugates were possible to produce and yielded final products corresponding to the expected molecular weights. Furthermore, the results indicate that all three RM26-conjuagtes are stable and maintain their structural properties under <em>in vivo</em>-temperatures and that they have high binding affinity for HSA. Further studies need to be conducted before drawing any certain conclusions regarding GRPR binding affinity.</p>
----------------------------------------------------------------------
In diva2:854287 abstract is: <p>Quantitative analysis of protein can give a better understanding of protein function as well as finding new biomarkers that can help in diagnostics. Mass spectrometry (MS) is today the main technology for high-throughput protein analysis. To get an absolute concentration of a protein with MS analysis, a good reference is of essence. By uing heavy isotope-labeled proteins or peptides as reference a mass shift is created that enables a quantitative analysis with MS where the intensity fo the exact same peptides can be cmpared. Protein Epitope Signature Tag (PrEST) is a unique protein fragment produced for all human proteins in the course of the Human Protein Atlas (HPA) project to function as antigens in the antibody production. In this project the production of heavy isotope-labeled PrESTs was optimized and their suitability as references examined. By cultivating Escherichia coli cells auxotrophic for arginine and lysine in a minimal medium with heavy isotope-labeled versions of these amino acids, heavy isotope-labeled PrESTs wer produced in a 24-well plate with an average total amount of 0.8 mg that could be used in future reference studies. Only a few different modifications were seen but as many as 11 out of 24 PrESTs had an acetylation that could depend on pore oxygenation in the culture format but need to be verified with more studies with shorter culture time.</p>

w='cmpared' val={'c': 'compared', 's': 'diva2:854287', 'n': 'no full text'}
w='uing' val={'c': 'using', 's': 'diva2:854287', 'n': 'no full text'}

corrected abstract:
<p>Quantitative analysis of protein can give a better understanding of protein function as well as finding new biomarkers that can help in diagnostics. Mass spectrometry (MS) is today the main technology for high-throughput protein analysis. To get an absolute concentration of a protein with MS analysis, a good reference is of essence. By using heavy isotope-labeled proteins or peptides as reference a mass shift is created that enables a quantitative analysis with MS where the intensity fo the exact same peptides can be compared. Protein Epitope Signature Tag (PrEST) is a unique protein fragment produced for all human proteins in the course of the Human Protein Atlas (HPA) project to function as antigens in the antibody production. In this project the production of heavy isotope-labeled PrESTs was optimized and their suitability as references examined. By cultivating Escherichia coli cells auxotrophic for arginine and lysine in a minimal medium with heavy isotope-labeled versions of these amino acids, heavy isotope-labeled PrESTs wer produced in a 24-well plate with an average total amount of 0.8 mg that could be used in future reference studies. Only a few different modifications were seen but as many as 11 out of 24 PrESTs had an acetylation that could depend on pore oxygenation in the culture format but need to be verified with more studies with shorter culture time.</p>
----------------------------------------------------------------------
In diva2:1642804 abstract is: <p>In this project, oligomers from the linear β-1,6-glucan pustulan were produced using the enzyme β-1,6-glucanase. These oligosaccharides have potential commercial value as analytical standards in biochemical research, or as bioactive molecules in biomedical products. Before such oligosaccharides can be tested or used for these purposes, an efficient and time-/cost-effective bioproduction process must be developed, using sustainable chemical-free techniques wherever possible. In order to accomplish this, the starting material first had to be characterised in regards to molecular weight, linkage, degree of acetylation, monosaccharide composition, and water content, in order for us to fully understand the profile and yields of oligomers generated. The enzymatic reaction was optimised and scaled up from 1ml to 50ml. The oligomers were then purified using size exclusion chromatography and characterised. The results show a multitude of oligomers from mono- to octamers with varying degrees of acetylation can be produced, with the relative majority being tetramers. The ability to separate these oligomers with the methods used was proven successful although a longer column or multiple passes would be required for a complete separation. This project confirms the feasibility of generating β-1,6-glucan oligomers using the enzyme under investigation, and gives recommendations for further optimisations of the process.</p>

w='monoto' val={'c': 'mono- to', 's': 'diva2:1642804', 'n': 'part of " mono- to octamers"'}

corrected abstract:
<p>In this project, oligomers from the linear β-1,6-glucan pustulan were produced using the enzyme β-1,6-glucanase. These oligosaccharides have potential commercial value as analytical standards in biochemical research, or as bioactive molecules in biomedical products. Before such oligosaccharides can be tested or used for these purposes, an efficient and time-/cost-effective bioproduction process must be developed, using sustainable chemical-free techniques wherever possible. In order to accomplish this, the starting material first had to be characterised in regards to molecular weight, linkage, degree of acetylation, monosaccharide composition, and water content, in order for us to fully understand the profile and yields of oligomers generated. The enzymatic reaction was optimised and scaled up from 1ml to 50ml. The oligomers were then purified using size exclusion chromatography and characterised. The results show a multitude of oligomers from mono- to octamers with varying degrees of acetylation can be produced, with the relative majority being tetramers. The ability to separate these oligomers with the methods used was proven successful although a longer column or multiple passes would be required for a complete separation. This project confirms the feasibility of generating β-1,6-glucan oligomers using the enzyme under investigation, and gives recommendations for further optimisations of the process.</p>
----------------------------------------------------------------------
In diva2:801712 abstract is: <p>Overexpressin of HER2 is seen in a number of cancer types and is characterized by aggressive disease with poor prognosis. HER2 targeted therapeutics, such as monoclonal antibodies, are today established for the treatment of HER2 positive cancer types but resistance to these therapeutics are common leading to relapse of disease. This could be overcome by creating bispecific affinity proteins with the ability to simultaneously bind two tumour-associated antigens. In HER2 positive cancer cells coexpression of HER3 is seen. Moreover, HER2 and HER3 form a very potent oncogenic unit with overactive signalling which results in increased proliferation and survival of cells. This receptor pair is therefore an interesting target for the development of bispecific affinity proteins. In this project, bispecific and bivalent Affibody mnolecules targeting this heterodimer have been produced. The molecules, which have high affinity for HER2 and HER3, are fused to an albuminbinding domain (ABD). This domain was used as a purification tag for affinity purification after protein production in <em>E.coli</em> and will also prolong the in vivo halflife of the constructs. Successful protein production and purification of the constructs were followed by <em>in vitro</em> cell studies. In these, the antiproliferative effects of the constructs on the pancreatic cancer cell line BxPc3, which coexpresses HER2 and HER3, were studied. The results from these studies indicate that the Affibody molecules have antiproliferative effects on the cancer cells. By this, we believe that the constructs are interesting for <em>in vivo</em> studies.</p>

w='mnolecules' val={'c': 'molecules', 's': 'diva2:801712', 'n': 'no full text'}
w='Overexpressin' val={'c': 'Overexpression', 's': 'diva2:801712'}

corrected abstract:
<p>Overexpress in of HER2 is seen in a number of cancer types and is characterized by aggressive disease with poor prognosis. HER2 targeted therapeutics, such as monoclonal antibodies, are today established for the treatment of HER2 positive cancer types but resistance to these therapeutics are common leading to relapse of disease. This could be overcome by creating bispecific affinity proteins with the ability to simultaneously bind two tumour-associated antigens. In HER2 positive cancer cells coexpression of HER3 is seen. Moreover, HER2 and HER3 form a very potent oncogenic unit with overactive signalling which results in increased proliferation and survival of cells. This receptor pair is therefore an interesting target for the development of bispecific affinity proteins. In this project, bispecific and bivalent Affibody molecules targeting this heterodimer have been produced. The molecules, which have high affinity for HER2 and HER3, are fused to an albuminbinding domain (ABD). This domain was used as a purification tag for affinity purification after protein production in <em>E.coli</em> and will also prolong the in vivo halflife of the constructs. Successful protein production and purification of the constructs were followed by <em>in vitro</em> cell studies. In these, the antiproliferative effects of the constructs on the pancreatic cancer cell line BxPc3, which coexpresses HER2 and HER3, were studied. The results from these studies indicate that the Affibody molecules have antiproliferative effects on the cancer cells. By this, we believe that the constructs are interesting for <em>in vivo</em> studies.</p>
----------------------------------------------------------------------
In diva2:1774737 abstract is: <p>Autoimmune diseases are complex, chronic, inflammatory conditions characterized by dysregulation of the immune system, resulting in inflammation and damage to various tissues, cells and organs. These diseases significantly impact individuals’ quality of life and often contribute to increased mortality risk in the presence of comorbidities. However, due to the diverse array of symptoms associated with different autoimmune diseases, accurate diagnosis, prognosis, and treatment evaluation pose significant challenges. Thus, there is a pressing need for the discovery of novel biomarkers. </p><p>In this study, a comprehensive analysis of 944 plasma samples using the OlinkR Explore platform was conducted, generating data on 1463 unique proteins. Based on the expression data, associated proteins were identified for six selected autoimmune diseases, namely multiple sclerosis, myositis, rheumatoid arthritis, systemic sclerosis, Sjögren’s syndrome, and systemic lupus erythematosus, as well as some of their defined subgroups. These are prospective biomarkers and have the potential to aid in early diagnosis, therapeutic intervention, subgroup identification, disease differentiation, and disease prognosis. Notably, some of these proteins have not been previously associated with the specific diseases in the existing literature, especially not in plasma samples, thereby offering intriguing new perspectives for biomarker development. However, it is of great importance to conduct robust validation studies in independent cohorts to confirm the outcomes of this study. </p><p>In summary, our findings highlight the potential utility of these proteomic plasma biomarkers in improving the early detection, subgroup characterization, and disease differentiation of autoimmune diseases. The identification of these proteins will hopefully stimulate further investigation in the field of biomarker research and potential advancements in personalized medicine. </p>

w='OlinkR' val={'c': 'Olink®', 's': 'diva2:1774737', 'n': 'correct in original'}

corrected abstract:
<p>Autoimmune diseases are complex, chronic, inflammatory conditions characterized by dysregulation of the immune system, resulting in inflammation and damage to various tissues, cells and organs. These diseases significantly impact individuals’ quality of life and often contribute to increased mortality risk in the presence of comorbidities. However, due to the diverse array of symptoms associated with different autoimmune diseases, accurate diagnosis, prognosis, and treatment evaluation pose significant challenges. Thus, there is a pressing need for the discovery of novel biomarkers.</p><p>In this study, a comprehensive analysis of 944 plasma samples using the Olink® Explore platform was conducted, generating data on 1463 unique proteins. Based on the expression data, associated proteins were identified for six selected autoimmune diseases, namely multiple sclerosis, myositis, rheumatoid arthritis, systemic sclerosis, Sjögren’s syndrome, and systemic lupus erythematosus, as well as some of their defined subgroups. These are prospective biomarkers and have the potential to aid in early diagnosis, therapeutic intervention, subgroup identification, disease differentiation, and disease prognosis. Notably, some of these proteins have not been previously associated with the specific diseases in the existing literature, especially not in plasma samples, thereby offering intriguing new perspectives for biomarker development. However, it is of great importance to conduct robust validation studies in independent cohorts to confirm the outcomes of this study.</p><p>In summary, our findings highlight the potential utility of these proteomic plasma biomarkers in improving the early detection, subgroup characterization, and disease differentiation of autoimmune diseases. The identification of these proteins will hopefully stimulate further investigation in the field of biomarker research and potential advancements in personalized medicine.</p>
----------------------------------------------------------------------
In diva2:801785 abstract is: <p>Genetic engineering has ushered a new paradigm in neuroscience research. The possibilities that this confers to research methodologies is in large limited only by the supply of innovative ideas. The hope is that some of these new approaches will aid in the unfolding of the mechanistics behind brain function. The advantages include the possibility to utilize the difference in the genomic structure of specific cell-types to exclusively examine defined neuronal populations. This master's thesis implements a new approach to genetically based connectivity mapping that includes transgenic mice and viral vectors. The presented "projection specific" approach maps the origins of the monosynaptic inputs to the serotenergic neurons of the dorsal raphe (DR) mucleus that have axonal projections to either the prefrontal cortex (PFC) or the striatum (STR). The methodology is explained and expermental data ispresented. Conclusions include the validity of the methodology as a potential apprach to connectivity mapping that coould provide insights specific from other methods. However, variability in results and used reagents, as well as failed experimental trials, led to data of insufficient statistical power to confer conclusions regarding connectivity patterns. Bottlenecks and future improvements of the presented protocol are further discussed.</p>

w='coould' val={'c': 'could', 's': 'diva2:801785', 'n': 'no full text'}
w='expermental' val={'c': 'experimental', 's': 'diva2:801785', 'n': 'no full text'}
w='mucleus' val={'c': 'nucleus', 's': 'diva2:801785', 'n': 'no full text'}
w='serotenergic' val={'c': 'serotonergic', 's': 'diva2:801785', 'n': 'no full text'}
w='apprach' val={'c': 'approach', 's': 'diva2:801785'}

corrected abstract:
<p>Genetic engineering has ushered a new paradigm in neuroscience research. The possibilities that this confers to research methodologies is in large limited only by the supply of innovative ideas. The hope is that some of these new approaches will aid in the unfolding of the mechanistics behind brain function. The advantages include the possibility to utilize the difference in the genomic structure of specific cell-types to exclusively examine defined neuronal populations. This master's thesis implements a new approach to genetically based connectivity mapping that includes transgenic mice and viral vectors. The presented "projection specific" approach maps the origins of the monosynaptic inputs to the serotonergic neurons of the dorsal raphe (DR) nucleus that have axonal projections to either the prefrontal cortex (PFC) or the striatum (STR). The methodology is explained and experimental data is presented. Conclusions include the validity of the methodology as a potential approach to connectivity mapping that could provide insights specific from other methods. However, variability in results and used reagents, as well as failed experimental trials, led to data of insufficient statistical power to confer conclusions regarding connectivity patterns. Bottlenecks and future improvements of the presented protocol are further discussed.</p>
----------------------------------------------------------------------
In diva2:744736 abstract is: <p>Biomarkers in cancer are present in both disease and normal tissue, which narrows the therapeutic window in order to avoid severe side effects. In this project, the concept of using protease activated affibody molecules to target the biomarker Human Epidermal Growth Factor Receptor 2 /HER2), was studied. HER2 is over expressed in certain types of breast cancer.</p><p>Tumors have an up regulation in expression of specific proteases in their immediate surroundings. These proteases are quite specific to different tumors, making them ideal to activate prodrugs at the site of the tumor. This allows for a muck more specific targeting of the biomarkers present in disease tissue and thus increases the therapeutic window. One of the causes of breast cancer is through an over expression of HER2, which is responsible for 15.20% of all cases. This type of tumors produces certain types of proteases that are active in the area surrounding the tumor. One of these proteases is matrix mealloprotease 1 (MMP1).</p><p>By linking two affibodies together, one that targets HER2 (Z<sub>HER2:342</sub>) and one antiidiotypic for the HER2-binding affiboday (Z<sub>E01</sub>), they will interact with each other and binding to HER2 can be blocked. By inserting a protease cleavage site into the linker, the dimer can be activated at the tumor site through cleavage, which releases the HER2-binding affibody and allows it to bind to its target and prevent dimerization. The protease cleavage site is a MMP1 stie with the amino acid sequence GPQAIAGQ.</p><p>The constructs were cloned and analyzed using staphylococcal cell display and flow sytometry. It was shown that Z<sub>E01</sub> effectively blocked binding of Z<sub>HER2:342</sub> to HER2. Analysis of the constructs showed that incubation with MMP1 successfully cleaved the linker and allowed Z<sub>HER2:342</sub> to bind HER2.</p><p>The method is very promising, and can be used to specifically target other biomarkers as well in the future. Especially interesting targets are the biomarkers that are quite unspecific for a disease. This usually associates them with more severe side effects and less effective tretments.</p>

w='affiboday' val={'c': 'affibody', 's': 'diva2:744736'}
w='mealloprotease' val={'c': 'metalloprotease', 's': 'diva2:744736', 'n': 'no full text'}
w='sytometry' val={'c': 'cytometry', 's': 'diva2:744736', 'n': 'no full text'}
w='stie' val={'c': 'site', 's': 'diva2:744736', 'n': 'no full text'}
w='tretments' val={'c': 'treatments', 's': 'diva2:744736', 'n': 'no full text'}

corrected abstract:
<p>Biomarkers in cancer are present in both disease and normal tissue, which narrows the therapeutic window in order to avoid severe side effects. In this project, the concept of using protease activated affibody molecules to target the biomarker Human Epidermal Growth Factor Receptor 2 /HER2), was studied. HER2 is over expressed in certain types of breast cancer.</p><p>Tumors have an up regulation in expression of specific proteases in their immediate surroundings. These proteases are quite specific to different tumors, making them ideal to activate prodrugs at the site of the tumor. This allows for a muck more specific targeting of the biomarkers present in disease tissue and thus increases the therapeutic window. One of the causes of breast cancer is through an over expression of HER2, which is responsible for 15.20% of all cases. This type of tumors produces certain types of proteases that are active in the area surrounding the tumor. One of these proteases is matrix metalloprotease 1 (MMP1).</p><p>By linking two affibodies together, one that targets HER2 (Z<sub>HER2:342</sub>) and one antiidiotypic for the HER2-binding affibody (Z<sub>E01</sub>), they will interact with each other and binding to HER2 can be blocked. By inserting a protease cleavage site into the linker, the dimer can be activated at the tumor site through cleavage, which releases the HER2-binding affibody and allows it to bind to its target and prevent dimerization. The protease cleavage site is a MMP1 site with the amino acid sequence GPQAIAGQ.</p><p>The constructs were cloned and analyzed using staphylococcal cell display and flow cytometry. It was shown that Z<sub>E01</sub> effectively blocked binding of Z<sub>HER2:342</sub> to HER2. Analysis of the constructs showed that incubation with MMP1 successfully cleaved the linker and allowed Z<sub>HER2:342</sub> to bind HER2.</p><p>The method is very promising, and can be used to specifically target other biomarkers as well in the future. Especially interesting targets are the biomarkers that are quite unspecific for a disease. This usually associates them with more severe side effects and less effective treatments.</p>
----------------------------------------------------------------------
In diva2:881652 abstract is: <p>In this master thesis project, the performance of Subtilisin Carlsberg (SC) in the kinetic resolution of alcohols was explored. Methods for the heterologous expression of SC in <em>B. subtilis</em> WB600 were established and optimised. An EziG<sup>TM</sup> carrier material was applied receiving a combined purification and immobilisation protocol. To increase the performance of SC in the kinetic resolution of secondary alcohols <em>via</em> a transacylation reaction in organic solvent, protein engineering by rational design was applied. Amino acid substitutions for site-directed mutagenesis were selected by molecular modeling and literature research to improve enzyme performance in terms of both enantioselectivity (<em>E</em>-value, <em>E</em>) and general activity (product yield, y [%]). A model transacylation reaction using vinyl butyrate as acyl-donor and <em>rac</em>-1-phenylethanol as acyl-acceptor was used to characterize SC (<em>E</em> = 40 ± 10, y = 3%) and eleven SC variants. Furthermore, the model reaction was used to characterise SC and four SC variants by Michaelis-Menten kinetics using  <em>(R)</em>- and <em>(S)</em>-1-phenylethanol separately. The highest <em>E</em>-value was achieved when applying SC G166L/M222F double variant in the model reaction (<em>E</em> = 165±17). Also this double variant showed a good performance in the kinetic resolution using <em>rac</em>-1-phenylethanol (<em>E</em> = 122±26, y = 20%). Another double variant of SC, G166L/M222C, performed equally well in the kinetic resolution of the model reaction (<em>E</em> = 125 ± 10, y = 16%). In addition, the SC G166L/M222C variant was shown to convert a larger substrate <em>rac</em>-1-phenylpropanol with excellent enatnioselectivity (<em>E</em> = 335 ± 33, y = 6%) compared to wild type SC (<em>E</em> = 23, y = 0,3%). To summarize, two variants of SC (G166L/M222C and G166L/M222F) were created and showed improved performance when applied in the kinetic resolution of <em>rac</em>-1-phenylethanol, compared to the wild-type enzyme.</p>

w='enatnioselectivity' val={'c': 'enantioselectivity', 's': 'diva2:881652', 'n': 'no full text'}

corrected abstract:
<p>In this master thesis project, the performance of Subtilisin Carlsberg (SC) in the kinetic resolution of alcohols was explored. Methods for the heterologous expression of SC in <em>B. subtilis</em> WB600 were established and optimised. An EziG<sup>TM</sup> carrier material was applied receiving a combined purification and immobilisation protocol. To increase the performance of SC in the kinetic resolution of secondary alcohols <em>via</em> a transacylation reaction in organic solvent, protein engineering by rational design was applied. Amino acid substitutions for site-directed mutagenesis were selected by molecular modeling and literature research to improve enzyme performance in terms of both enantioselectivity (<em>E</em>-value, <em>E</em>) and general activity (product yield, y [%]). A model transacylation reaction using vinyl butyrate as acyl-donor and <em>rac</em>-1-phenylethanol as acyl-acceptor was used to characterize SC (<em>E</em> = 40 ± 10, y = 3%) and eleven SC variants. Furthermore, the model reaction was used to characterise SC and four SC variants by Michaelis-Menten kinetics using  <em>(R)</em>- and <em>(S)</em>-1-phenylethanol separately. The highest <em>E</em>-value was achieved when applying SC G166L/M222F double variant in the model reaction (<em>E</em> = 165±17). Also this double variant showed a good performance in the kinetic resolution using <em>rac</em>-1-phenylethanol (<em>E</em> = 122±26, y = 20%). Another double variant of SC, G166L/M222C, performed equally well in the kinetic resolution of the model reaction (<em>E</em> = 125 ± 10, y = 16%). In addition, the SC G166L/M222C variant was shown to convert a larger substrate <em>rac</em>-1-phenylpropanol with excellent enantioselectivity (<em>E</em> = 335 ± 33, y = 6%) compared to wild type SC (<em>E</em> = 23, y = 0,3%). To summarize, two variants of SC (G166L/M222C and G166L/M222F) were created and showed improved performance when applied in the kinetic resolution of <em>rac</em>-1-phenylethanol, compared to the wild-type enzyme.</p>
----------------------------------------------------------------------
In diva2:893748 abstract is: <p>The use of enzymes in the production of enantiomerically pure compounds is constantly increasing due to the many benefits and sustainabillity of the approach. Optically pure molecules are important building blocks in the pharmaceutical industry, as well as in other industrial biotechnological fields. The use of enzymes in kinetic resolution to obtain deracemization of these building blocks puts high demand on the stability, activity and selectivity of the catalysts. Enzyme engineering is commonly applied to improve these properties in order to reach a successful resolution system, and when structural information about an enzyme is available, it can be utilized to predict the outcome of certain point-mutations by molecular modeling.</p><p>The aim of this Master Thesis project was to improve the natural (∫)-selective nature of a serine protease denoted subtilisin Carlsberg towards secondary alcohols in a model transacylation reaction. To achieve this, a working expression system was set up. A his<sub>6</sub>-tag was added to the C-terminal of the enzyme gene construct, and the enzyme was expressed in <em>Bacillus subtilis (B. subtilis)</em>. The wild type enzyme as well as all enzyme variants tested were immobilized on EziG™ beads and treated with two surfactants previously reported to have a positive impact on the selectivity of subtilisin Carlsberg in the literature.</p><p>The second tetrahedral intermediate of the reaction mechanism was built into an existing crystal structure using the YASARA software in order to identify suitable mutations, plausible to improve the selectivity. The assumption was made that the fit of methyl substituent of the alcohol substrate, in a medium-sized pocket of the active site, would have the greatest impact on the selectivity, and the mutants were selected based on this property alone. However, when the 7 enzyme variants selected and created was tested, it was clear that the fit in this pocket was not enough to explain the improved, unaltered or decreased selectivity of the variants. The results were evaluated further, focusing on steric hindrance between the residues surrounding the active site, and the phenyl group of the substrate, as well as the hydrogen bonds that are essential for the reaction mechanism to work. These results combined made it possible to draw valid conclustion about the obtained enantioselectivity of all variants.</p><p>In the end, two enzyme variants of improved enantioselectiviy was obtained; Met222Gly, and Met222Gly/Leu217Met. It was also shown that conducting the reaction in Tetrahydrofuran (THF) increased the selectivity, as well as the addition of the surfactants. However, the results need to be repeated and the variants should be tested using other substrates as well, but the results are promising.</p>

w='conclustion' val={'c': 'conclusion', 's': 'diva2:893748'}
w='enantioselectiviy' val={'c': 'enantioselectivity', 's': 'diva2:893748', 'n': 'no full text'}
w='sustainabillity' val={'c': 'sustainability', 's': 'diva2:893748'}

corrected abstract:
<p>The use of enzymes in the production of enantiomerically pure compounds is constantly increasing due to the many benefits and sustainabillity of the approach. Optically pure molecules are important building blocks in the pharmaceutical industry, as well as in other industrial biotechnological fields. The use of enzymes in kinetic resolution to obtain deracemization of these building blocks puts high demand on the stability, activity and selectivity of the catalysts. Enzyme engineering is commonly applied to improve these properties in order to reach a successful resolution system, and when structural information about an enzyme is available, it can be utilized to predict the outcome of certain point-mutations by molecular modeling.</p><p>The aim of this Master Thesis project was to improve the natural (∫)-selective nature of a serine protease denoted subtilisin Carlsberg towards secondary alcohols in a model transacylation reaction. To achieve this, a working expression system was set up. A his<sub>6</sub>-tag was added to the C-terminal of the enzyme gene construct, and the enzyme was expressed in <em>Bacillus subtilis (B. subtilis)</em>. The wild type enzyme as well as all enzyme variants tested were immobilized on EziG™ beads and treated with two surfactants previously reported to have a positive impact on the selectivity of subtilisin Carlsberg in the literature.</p><p>The second tetrahedral intermediate of the reaction mechanism was built into an existing crystal structure using the YASARA software in order to identify suitable mutations, plausible to improve the selectivity. The assumption was made that the fit of methyl substituent of the alcohol substrate, in a medium-sized pocket of the active site, would have the greatest impact on the selectivity, and the mutants were selected based on this property alone. However, when the 7 enzyme variants selected and created was tested, it was clear that the fit in this pocket was not enough to explain the improved, unaltered or decreased selectivity of the variants. The results were evaluated further, focusing on steric hindrance between the residues surrounding the active site, and the phenyl group of the substrate, as well as the hydrogen bonds that are essential for the reaction mechanism to work. These results combined made it possible to draw valid conclusion about the obtained enantioselectivity of all variants.</p><p>In the end, two enzyme variants of improved enantioselectivity was obtained; Met222Gly, and Met222Gly/Leu217Met. It was also shown that conducting the reaction in Tetrahydrofuran (THF) increased the selectivity, as well as the addition of the surfactants. However, the results need to be repeated and the variants should be tested using other substrates as well, but the results are promising.</p>
----------------------------------------------------------------------
In diva2:1528487 abstract is: <p>Poly(vinyl acetate) (PVAc) has a major application as an indoor wood adhesive. Low water stability is however, one of the greatest drawbacks of PVAc. By grafting PVAc from a chitosan (CS) backbone (CS-graft- PVAc) water stability of adhesive is increased while good mechanical and adhesive properties are retained. Simultaneously the percentage of bio-based content is increased. This work investigates the proposed re- action mechanisms between chitosan and cerium(IV) ammonium nitrate (CAN) which is used as an initiator for the grafting reaction. Litera- ture studies showed one dominating reaction mechanism and some not as common. The reaction mechanisms and their shortcomings are pre- sented and discussed in the report. </p>

w='CS-graftPVAc' val={'c': 'CS-graft-PVAc', 's': 'diva2:1528487'}

corrected abstract:
<p>Poly(vinyl acetate) (PVAc) has a major application as an indoor wood adhesive. Low water stability is however, one of the greatest drawbacks of PVAc. By grafting PVAc from a chitosan (CS) backbone (CS-graft-PVAc) water stability of adhesive is increased while good mechanical and adhesive properties are retained. Simultaneously the percentage of bio-based content is increased. This work investigates the proposed reaction mechanisms between chitosan and cerium(IV) ammonium nitrate (CAN) which is used as an initiator for the grafting reaction. Literature studies showed one dominating reaction mechanism and some not as common. The reaction mechanisms and their shortcomings are presented and discussed in the report.</p>
----------------------------------------------------------------------
In diva2:1770888 abstract is: <p>One problem with building new nuclear reactors for electricity production is the large investment costs and the long time needed for permissions and construction. Most recently is the Olkiluoto nuclear power plant in Finland where a third reactor was built, it took 18 years to finalize the reactor, and this is not including the time of handling the licencing application. Small modular nuclear reactors, SMR which is a smaller reactor with a reduced effect has been proposed to reduce the cost of investment and the time it takes from license application to finalized reactor. The SMR reactors have many advantages. It will be fabricated in factories reducing the time of construction, since reactor units are the same, the same reactor design only needs one licencing for all reactors. Compared to large reactors built today, where every single reactor needs a new licencing. Smaller reactors lead to shorter construction times and lower investment. It is also less expensive to produce reactors in series where the cost per unit decreases for every unit produced.</p><p>Most reactors in operation today use light water as a moderator and coolant and are called light water reactors, LWR. Two designs are common, one where the water is boiled in the reactor and goes directly to the turbine, this reactor is called a boiling water reactor, BWR. The other common reactor is a pressurized water reactor, PWR where a higher pressure does not allow the water to boil. Instead, it is heated and boils water in a steam generator that then turns the turbine blades. There are other reactor concepts which do not use light water as a moderator and coolant, The SMR concepts that are the closest to construction are all LWR because it is where most experience from previous reactor designs is.</p><p>By investigating seven SMR reactor concepts, six PWR and one BWR, it was concluded that they are similar to reactors operating today, but with some simplification and passive safety systems like natural circulation for many of them. The materials that will be used are also materials that have previously been used. One of the simplifications for several of the reactor concepts is the removal of a soluble neutron absorber in PWR reactors and instead, the use of control rods and burnable neutron poison to a larger extent. The soluble neutron absorber is usually boric acid which has a high neutron cross-section. The use of boric acid will reduce the pH in the reactor which increases corrosion, and it also affects the radiolysis of water.</p><p>This work has investigated the rate constant for the reaction of boric acid with hydroxyl radicals formed in the radiolysis of water. In the experiments, coumarin-3-carboxylic acid was used as a probe to study the reaction rate using competition kinetics. The hydroxylated product formed has been detected using fluorescence. The rate constant measured was 1.25 ∙ 106 M-1 s-1 which is higher than previous literature values. Some of the discrepancies could be explained by the presence of the counter base, borate and the tetraborate that is formed at high concentrations.</p>

w='M-1' val={'c': 'M<sup>-1</sup>', 's': 'diva2:1770888', 'n': 'the URL to the full text points to another thesis'}
w='s-1' val={'c': 's<sup>-1</sup>', 's': 'diva2:1770888', 'n': 'the URL to the full text points to another thesis'}

corrected abstract:
<p>One problem with building new nuclear reactors for electricity production is the large investment costs and the long time needed for permissions and construction. Most recently is the Olkiluoto nuclear power plant in Finland where a third reactor was built, it took 18 years to finalize the reactor, and this is not including the time of handling the licencing application. Small modular nuclear reactors, SMR which is a smaller reactor with a reduced effect has been proposed to reduce the cost of investment and the time it takes from license application to finalized reactor. The SMR reactors have many advantages. It will be fabricated in factories reducing the time of construction, since reactor units are the same, the same reactor design only needs one licencing for all reactors. Compared to large reactors built today where every single reactor needs a new licencing. Smaller reactors lead to shorter construction times and lower investment. It is also less expensive to produce reactors in series where the cost per unit decreases for every unit produced.</p><p>Most reactors in operation today use light water as a moderator and coolant and are called light water reactors, LWR. Two designs are common, one where the water is boiled in the reactor and goes directly to the turbine, this reactor is called a boiling water reactor, BWR. The other common reactor is a pressurized water reactor, PWR where a higher pressure does not allow the water to boil. Instead, it is heated and boils water in a steam generator that then turns the turbine blades. There are other reactor concepts which do not use light water as a moderator and coolant, The SMR concepts that are the closest to construction are all LWR because it is where most experience from previous reactor designs is.</p><p>By investigating seven SMR reactor concepts, six PWR and one BWR, it was concluded that they are similar to reactors operating today, but with some simplification and passive safety systems like natural circulation for many of them. The materials that will be used are also materials that have previously been used. One of the simplifications for several of the reactor concepts is the removal of a soluble neutron absorber in PWR reactors and instead, the use of control rods and burnable neutron poison to a larger extent. The soluble neutron absorber is usually boric acid which has a high neutron cross-section. The use of boric acid will reduce the pH in the reactor which increases corrosion, and it also affects the radiolysis of water.</p><p>This work has investigated the rate constant for the reaction of boric acid with hydroxyl radicals formed in the radiolysis of water. In the experiments, coumarin-3-carboxylic acid was used as a probe to study the reaction rate using competition kinetics. The hydroxylated product formed has been detected using fluorescence. The rate constant measured was 1.25 ∙ 10<sup>6</sup> M<sup>-1</sup> s<sup>-1</sup> which is higher than previous literature values. Some of the discrepancies could be explained by the presence of the counter base, borate and the tetraborate that is formed at high concentrations.</p>
----------------------------------------------------------------------
In diva2:895964 abstract is: <p>Recent advancements in studies on gene fusions of recombinant spider silk and a functional protein domain puts spider silk materials as a promising candidate in the research for more efficient delivery systems of growth factors (GFs) in the field of tissue engineering. This master thesis study investigates the functional properties of recombinant spider silk functionalized with growth factors. The retained functionalities of a gene fusion of the miniature decombinant silk protein 4RepCT and a fibroblast growth factor (FGF), as well as three different constructs of 4RepCT and epidermal growth factor (EGF), was investigated. Retained functionality of the silk domain was studied by investigating the formation of multilayers of the silk-FGF fusion protein using SPR and QCM-D technology, in addition to the ability of the gene fusions to form water stable fibers. Exposure of the FGF receptor (FGFR) binding site to FGF immobilized via silk was investigated by binding analysis of FGFR to a layer of 4RepCT-FGF fusion protein using SPR technology. Multilayer formation of the 4RecpCT-FGF fusion protein was demonstrated, as well as fiber formation of the different silk-GF fusions. The exposure of FGFR bindin sit to immobilized FGF via silk gave inconclusive results, as unspecific binding of FGFR to a layer of wild type 4RepCT was prominent. While the retained functionality of the sild domain was shown through multilayer formation and fiber formation, further studies need to be conducted on the retained funcionality of the growth factor domain.</p>

w='funcionality' val={'c': 'functionality', 's': 'diva2:895964', 'n': 'no full text'}
w='sild' val={'c': 'silk', 's': 'diva2:895964', 'n': 'no full text'}

corrected abstract:
<p>Recent advancements in studies on gene fusions of recombinant spider silk and a functional protein domain puts spider silk materials as a promising candidate in the research for more efficient delivery systems of growth factors (GFs) in the field of tissue engineering. This master thesis study investigates the functional properties of recombinant spider silk functionalized with growth factors. The retained functionalities of a gene fusion of the miniature decombinant silk protein 4RepCT and a fibroblast growth factor (FGF), as well as three different constructs of 4RepCT and epidermal growth factor (EGF), was investigated. Retained functionality of the silk domain was studied by investigating the formation of multilayers of the silk-FGF fusion protein using SPR and QCM-D technology, in addition to the ability of the gene fusions to form water stable fibers. Exposure of the FGF receptor (FGFR) binding site to FGF immobilized via silk was investigated by binding analysis of FGFR to a layer of 4RepCT-FGF fusion protein using SPR technology. Multilayer formation of the 4RecpCT-FGF fusion protein was demonstrated, as well as fiber formation of the different silk-GF fusions. The exposure of FGFR binding site to immobilized FGF via silk gave inconclusive results, as unspecific binding of FGFR to a layer of wild type 4RepCT was prominent. While the retained functionality of the silk domain was shown through multilayer formation and fiber formation, further studies need to be conducted on the retained functionality of the growth factor domain.</p>
----------------------------------------------------------------------
In diva2:1094051 abstract is: <p>Volatile sulfur compounds are cause of concern because, when present in high concentrations, they constitute a danger for health because of their strong toxicity. Furthermore, for low concentrations, they are often a cause of complaint, because of their low odor threshold. In this context, the purpose of this Thesis is to evaluate a new technology for the abatement of sulfur-based malodorous compounds. The investigated technology consists in the use of ozone generating low-pressure UV mercury lamps, operating at room temperature. Hydrogen sulfide is often found in industrial processes, (e.g. WWTPs (Wastewater Treatment Plants), leather production, sewage treatment, garbage disposal, etc). Moreover, it presents both a very high toxicity a low odor threshold. Thus, due to its high representativeness of the case, hydrogen sulfide was chosen as reference compound for the purposes of this project. In order to evaluate a wide range of cases, several experiments using different residence times, humidity contents and inlet concentrations of the pollutant were conducted. The obtained results show that this technology generally presents discrete conversion efficiencies, although not suffcient to be used as freestanding process. For this reason, a pretreatment is revealed to be necessary. The best conversion efficiency was obtained for low flow rates and high moisture content. At the end of the project, as side-study, a possible pretreatment using an adsorbent bed constituted by granular ferric oxide was evaluated.</p><p> </p>

w='suffcient' val={'c': 'sufficient', 's': 'diva2:1094051', 'n': 'missing ligature'}

corrected abstract:
<p>Volatile sulfur compounds are cause of concern because, when present in high concentrations, they constitute a danger for health because of their strong toxicity. Furthermore, for low concentrations, they are often cause of complaint, because of their low odor threshold. In this context, the purpose of this Thesis is to evaluate a new technology for the abatement of Sulfur-based malodorous compounds. The investigated technology consists in the use of Ozone generating low-pressure UV mercury lamps, operating at room temperature. Hydrogen sulfide is often found in industrial processes, (e.g. WWTPs (Wastewater Treatment Plants), leather production, sewage treatment, garbage disposal, &hellip;). Moreover, it presents both a very high toxicity a low odor threshold. Thus, due to its high representativeness of the case, hydrogen sulfide was chosen as reference compound for the purposes of this project. In order to evaluate a wide range of cases, several experiments using different residence times, humidity contents and inlet concentrations of the pollutant were conducted. The obtained results show that this technology generally presents discrete conversion efficiencies, although not sufficient to be used as freestanding process. For this reason, a pretreatment is revealed to be necessary. The best conversion efficiency was obtained for low flow rates and high moisture content. At the end of the project, as side-study, a possible pre-treatment using an adsorbent bed constituted by granular ferric oxide was evaluated.</p>
----------------------------------------------------------------------
In diva2:744729 abstract is: <p>Swedish wastewater treatment plants are exposed to many active pharmaceutical ingredients (APIs) that are released into our waters. On the Swedish market today there are about 1200 APIs. There is a risk that aquatic organisms are negatively affected by the exposure of APIs since there are similarities in the metabolic processes of humans and animals. It has been shown that fishes have become hermaphrodite as infusion of natural human synthetic estrogens which have passed through the WWTPs.</p><p>This master thesis is part of a project that is included in MistraPharma phase 2 through KTH who have been given the subject removal of prioritized APIs in wastewater treatment. The aim is to design, operate and evaluate wastewater treatment techniques in pilot and semi-large scale. Earlier the project have been aiming to reduce the infusion of pharmaceuticals to the WWWTP, map out the quantity that is passing through the WWTPs, which APIs that are most harmful and what potential techniques that can be applied.</p><p>My master thesis has focused on the potential techniques for removal of APIs from the wastewater. There are many techniques available today, the intention is to evaluate if any new methods have been discovered since the pharaceutical project was finished and the best way of using the most promising techniques. The aim is to remove 0+% of the APIs from the wastewater. The technique used has to be effective in order to reduce the costs. The two most promising techniques, activated carbon and ozone have been evaluated regarding design and choice of starting material. In the evaluation of ozone two different contactors, "bubble diffuser" and "multistage", were compared to reveal which one who gave best results. The trials with activated carbon had the same kind of contactor but activated carbons of different origins were used these were Swedish barbecue coal from deciduous tree and pyrolysis of sludge, asa reference Filtrasorb 400 was used. In lab scale seven different activated carbons were graded depending on their adsorption capacity, the two best was selected for further trials and compared to Filtrasorb 400.</p><p>The results from the experiments with ozone reveled that the multistage was better than thebubble diffuser. The highest ozone dose with the multistage gave best results of all experiments with ozone. Comparison between the three activated carbon types showed that F400 had best adsorption capacity while SWE and sludge showed less promising results. F400 removed 90% of the APIs one week and had consistently high removal percentage over the weeks. The multistage could probably achieve a higher removal percentage with a higher ozone dosage and be a fair competitor to F400. The most promising approach for the industry would be a combination of multistage and F400.</p><p></p>

w='pharaceutical' val={'c': 'pharmaceutical', 's': 'diva2:744729', 'n': 'no full text'}
w='WWWTP' val={'c': 'WWTP', 's': 'diva2:744729', 'n': 'no full text'}

corrected abstract:
<p>Swedish wastewater treatment plants are exposed to many active pharmaceutical ingredients (APIs) that are released into our waters. On the Swedish market today there are about 1200 APIs. There is a risk that aquatic organisms are negatively affected by the exposure of APIs since there are similarities in the metabolic processes of humans and animals. It has been shown that fishes have become hermaphrodite as infusion of natural human synthetic estrogens which have passed through the WWTPs.</p><p>This master thesis is part of a project that is included in MistraPharma phase 2 through KTH who have been given the subject removal of prioritized APIs in wastewater treatment. The aim is to design, operate and evaluate wastewater treatment techniques in pilot and semi-large scale. Earlier the project have been aiming to reduce the infusion of pharmaceuticals to the WWTP, map out the quantity that is passing through the WWTPs, which APIs that are most harmful and what potential techniques that can be applied.</p><p>My master thesis has focused on the potential techniques for removal of APIs from the wastewater. There are many techniques available today, the intention is to evaluate if any new methods have been discovered since the pharmaceutical project was finished and the best way of using the most promising techniques. The aim is to remove 0+% of the APIs from the wastewater. The technique used has to be effective in order to reduce the costs. The two most promising techniques, activated carbon and ozone have been evaluated regarding design and choice of starting material. In the evaluation of ozone two different contactors, "bubble diffuser" and "multistage", were compared to reveal which one who gave best results. The trials with activated carbon had the same kind of contactor but activated carbons of different origins were used these were Swedish barbecue coal from deciduous tree and pyrolysis of sludge, asa reference Filtrasorb 400 was used. In lab scale seven different activated carbons were graded depending on their adsorption capacity, the two best was selected for further trials and compared to Filtrasorb 400.</p><p>The results from the experiments with ozone reveled that the multistage was better than the bubble diffuser. The highest ozone dose with the multistage gave best results of all experiments with ozone. Comparison between the three activated carbon types showed that F400 had best adsorption capacity while SWE and sludge showed less promising results. F400 removed 90% of the APIs one week and had consistently high removal percentage over the weeks. The multistage could probably achieve a higher removal percentage with a higher ozone dosage and be a fair competitor to F400. The most promising approach for the industry would be a combination of multistage and F400.</p>
----------------------------------------------------------------------
In diva2:745010 abstract is: <p></p><p>The aim of this project was to synthesize highly swelling hydrogels from wood hydrolysates, which are hemicellulose rich side products from industrial processes using wood as raw material. Hydrogels were successfully synthesized from one pure hemicellulose, O-acetyl-galactoglucomannan and two wood hydrolysates originating from spruce and birch wood. The hydrogels were synthesized in a three step procedure: synthesis of a coupling agent, grafting of vinyl functionality onto the hemicelluloses and finally crosslinking with acrylic acid (AA) or AA and 2-hydroxyethyl methacrylate (HEMA) as co-monomers. The degree of substitution (DSvinyl) was determined with 1H-NMR analysis and the inclusion of the co-monomers into the hydrogels was confirmed using FT-IR analysis. High equilibrium swellings (Qeq) in the region of 500 grams of water per gram of dry material were achieved. The DSvinyl of the hydrolysates was discovered to have a high impact on the Qeq while the effect of adding small amounts of HEMA, instead of only adding AA, was ambiguous. The inclusion of AA was found to have a large effect on the Qeq both from comparing with samples with high HEMA content in this study and compared to earlier published results from our division where no co-monomer or only HEMA was added. Using water as solvent in the crosslinking step was found to give higher Qeq than using DMSO. From the results it is concluded that hydrolysates can be used for creating highly swelling hydrogels and that further purification to increase the hemicellulose content may be unnecessary and perhaps even detrimental to the Qeq.</p>

w='DSvinyl' val={'c': 'DS<sub>vinyl</sub>', 's': 'diva2:745010', 'n': 'possibly - as degree of substitution (DS) the subscrript would indicte which polymer is being consider - no full text'}

corrected abstract:
<p>The aim of this project was to synthesize highly swelling hydrogels from wood hydrolysates, which are hemicellulose rich side products from industrial processes using wood as raw material. Hydrogels were successfully synthesized from one pure hemicellulose, O-acetyl-galactoglucomannan and two wood hydrolysates originating from spruce and birch wood. The hydrogels were synthesized in a three step procedure: synthesis of a coupling agent, grafting of vinyl functionality onto the hemicelluloses and finally crosslinking with acrylic acid (AA) or AA and 2-hydroxyethyl methacrylate (HEMA) as co-monomers. The degree of substitution (DS<sub>vinyl</sub>) was determined with 1H-NMR analysis and the inclusion of the co-monomers into the hydrogels was confirmed using FT-IR analysis. High equilibrium swellings (Qeq) in the region of 500 grams of water per gram of dry material were achieved. The DS<sub>vinyl</sub> of the hydrolysates was discovered to have a high impact on the Qeq while the effect of adding small amounts of HEMA, instead of only adding AA, was ambiguous. The inclusion of AA was found to have a large effect on the Qeq both from comparing with samples with high HEMA content in this study and compared to earlier published results from our division where no co-monomer or only HEMA was added. Using water as solvent in the crosslinking step was found to give higher Qeq than using DMSO. From the results it is concluded that hydrolysates can be used for creating highly swelling hydrogels and that further purification to increase the hemicellulose content may be unnecessary and perhaps even detrimental to the Qeq.</p>
----------------------------------------------------------------------
In diva2:1270983 abstract is: <p>The waste treatment plant of Telge Återvinning in Tveta have old landfills which produces leachate and processwater. The leachate is transported with pumps to the water treatment plant Himmerfärdsverket, Syvab. The water treatment plant has a REVAQ-certificate since 2009 and the requirements to keep the certificate have become tougher. Perfluorooctanesulfonate (PFOS) is one of the substances in the produced leachate. The content limit for that substance has now been decreased to 15 ng/l which means that Telge Återvinning must lower their PFOS content in their leachate. To do so they’ll need a PFOS removal efficiency of 90% based on their current content value. The leachate also contains lots of particles which needs to be removed if the desired removal efficiency is to be reached.</p><p>PFOS is a none-reactive, bio-accumulative, substance which is also toxic. There’s only a few treatment methods that can be used for the removal of this substance in liquid matrixes because of its none-reactive nature. The point of this report has been to highlight these methods and evaluate them based on their possible usage as a treatment method for the leachate at the waste treatment plant. The information for these methods have been gathered through literature studies and interviews.</p><p>The available methods which were able to achieve the desired removal efficiency were the reverse osmosis and the nanofiltration, the ion exchanger, the granular active carbon filter and powdered active carbon. From these available methods the granular active carbon filter was thought to be the most suited method for treating the leachate. The most suited method was decided based on its operation cost and familiarity with prior PFOS treatment plant. The treatment process for the particle removal couldn’t be fully decided without field studies. Although flocculation was decided to be the primary step in the treatment process.</p>

w='Himmerfärdsverket' val={'c': 'Himmerfjärdsverket', 's': 'diva2:1270983'}

corrected abstract:
<p>The waste treatment plant of Telge Återvinning in Tveta have old landfills which produces leachate and process water. The leachate is transported with pumps to the water treatment plant Himmerfjärdsverket, Syvab. The water treatment plant has a REVAQ-certificate since 2009 and the requirements to keep the certificate have become tougher. Perfluorooctanesulfonate (PFOS) is one of the substances in the produced leachate. The content limit for that substance has now been decreased to 15 ng/l which means that Telge Återvinning must lower their PFOS content in their leachate. To do so they’ll need a PFOS removal efficiency of 90% based on their current content value. The leachate also contains lots of particles which needs to be removed if the desired removal efficiency is to be reached.</p><p>PFOS is a none-reactive, bio-accumulative, substance which is also toxic. There’s only a few treatment methods that can be used for the removal of this substance in liquid matrixes because of its none-reactive nature. The point of this report has been to highlight these methods and evaluate them based on their possible usage as a treatment method for the leachate at the waste treatment plant. The information for these methods have been gathered through literature studies and interviews.</p><p>The available methods which were able to achieve the desired removal efficiency were the reverse osmosis and the nanofiltration, the ion exchanger, the granular active carbon filter and powdered active carbon. From these available methods the granular active carbon filter was thought to be the most suited method for treating the leachate. The most suited method was decided based on its operation cost and familiarity with prior PFOS treatment plant. The treatment process for the particle removal couldn’t be fully decided without field studies. Although flocculation was decided to be the primary step in the treatment process.</p>
----------------------------------------------------------------------
In diva2:1674381 abstract is: <p>Interactive Ątness apps can motivate people to lead an active lifestyle. However, it is not yet fully understood how smartphone data collected during exercise is related to the user's actual physical condition. The aim of this work is to investigate in what way and how accurately fitness-related parameters can be estimated using a smartphone. To achieve this, three separate tests were conducted:</p><p>The first test examined how acceleration data (external intensity) and % of heart rate reserve (internal intensity) correlate with each other during three different exercises (bodyweight squats, jumping jacks, and standing mountain climbers). Results (n=6) show that regardless of accelerometer position, there is a moderate to strong correlation between internal and external intensity for squats (r=0.656-0.789) and mountain climbers (r=0.592-0.782), and a weak to moderate correlation for jumping jacks (r=0.291-0.587).</p><p>The second test determined how accurately heart rate can be estimated with a smartphone camera (contactless and contact-based). Heart rate was determined simultaneously with a heart rate monitor and a smartphone. The mean absolute error of the contact-based method (n=120) was 5.42 ± 1.44 beats per minute. The mean absolute error of the contactless method (n=30) was 3.60 ± 0.81. However, the contactless method did not provide reliable readings for heart rates above the resting level.</p><p>The third test evaluated how accurately a VO<sub>2</sub>max test, in this case, the 3-minute step-in-place test, can be monitored with a smartphone. The results show that step count could be computed from a video recording with a mean absolute error of 2.69 ± 1.89 steps per minute. The knee height could be distinguished as high or low in 98% of cases (n=4). The results of this work can help developers create highly personalized and meaningful fitness apps.</p>

w='Ątness' val={'c': 'fitness', 's': 'diva2:1674381', 'n': 'correct in original'}

corrected abstract:
<p>Interactive fitness apps can motivate people to lead an active lifestyle. However, it is not yet fully understood how smartphone data collected during exercise is related to the user's actual physical condition. The aim of this work is to investigate in what way and how accurately fitness-related parameters can be estimated using a smartphone. To achieve this, three separated tests were conducted:</p><p>The first test examined how acceleration data (external intensity) and % of heart rate reserve (internal intensity) correlate with each other during three different exercises (bodyweight squats, jumping jacks, and standing mountain climbing). Results (n=6) show that regardless of accelerometer position, there is a moderate to strong correlation between internal and external intensity for squats (r=0.656-0.789) and mountain climbers (r=0.592-0.782), and a weak to moderate correlation for jumping jacks (r=0.291-0.587).</p><p>The second test determined how accurately heart rate can be estimated with a smartphone camera (contactless and contact-based). Heart rate was determined simultaneously with a heart rate monitor and a smartphone. The mean absolute error of the contact-based method (n=120) was 5.42 ± 1.44 beats per minute. The mean absolute error of the contactless method (n=30) was 3.60 ± 0.81. However, the contactless method did not provide reliable readings for heart rates above the resting level.</p><p>The third test evaluated how accurately a VO<sub>2max</sub> test, in this case, the 3-minute step-in-place test, can be monitored with a smartphone. The results show that step count could be computed from a video recording with a mean absolute error of 2.69 ± 1.89 steps per minute. The knee height could be distinguished as high or low in 98% of cases (n=4).</p><p>The results of this work can help developers create highly personalized and meaningful fitness apps.</p>
----------------------------------------------------------------------
In diva2:801773 abstract is: <p>Affinity proteomics is often used un biomarker discovery and validation due to the high sample throughput, as compared to for example mass spectrometry approaches. Reverse phase protein arrays (RPPAs) are especially suitable in this regard, since thousands of samples can be analysed simultaneously under identical conditions. RPPA can theoretically be applied to any biological sample, however, tissue and cell lysates are most commonly used, while there are a limited number of studies addressing serum and plasma. This is often explained by the higher sample coplaxity of the latter two, which greatly increases the demand on sensitivity and specificity. The aim of this master thesis project was therefore to investigate the possibilities of applying RPPA to serum and plasma samples, and to optimise the method for the same. Significant differences in the performances of 12 different slide types were observed when comparing parameters such as background intensity, signal-to-noise ratio, binding capacity, spot morphology and positioning, the effect of blocking buffers and washing procedure, dilution curve representation, and sample separation. The different slide types represented various 3-dimensional substrata, e.g. nitrocellulose and hydrogels, and a 2-dimensional epoxide coating. Significant and expected sample separations were seen for various targets, supporting indications of previously suggested limits of detection were seen, and promising reproducibility was observed. UniSart 3D Nitro slides (nitrocellulose substratum) from Sartorius Stedim Biotech demonstrated the most promising results overall and were therefore selected for further method development.</p>

w='coplaxity' val={'c': 'complexity', 's': 'diva2:801773', 'n': 'no full text'}

corrected abstract:
<p>Affinity proteomics is often used un biomarker discovery and validation due to the high sample throughput, as compared to for example mass spectrometry approaches. Reverse phase protein arrays (RPPAs) are especially suitable in this regard, since thousands of samples can be analysed simultaneously under identical conditions. RPPA can theoretically be applied to any biological sample, however, tissue and cell lysates are most commonly used, while there are a limited number of studies addressing serum and plasma. This is often explained by the higher sample complexity of the latter two, which greatly increases the demand on sensitivity and specificity. The aim of this master thesis project was therefore to investigate the possibilities of applying RPPA to serum and plasma samples, and to optimise the method for the same. Significant differences in the performances of 12 different slide types were observed when comparing parameters such as background intensity, signal-to-noise ratio, binding capacity, spot morphology and positioning, the effect of blocking buffers and washing procedure, dilution curve representation, and sample separation. The different slide types represented various 3-dimensional substrata, e.g. nitrocellulose and hydrogels, and a 2-dimensional epoxide coating. Significant and expected sample separations were seen for various targets, supporting indications of previously suggested limits of detection were seen, and promising reproducibility was observed. UniSart 3D Nitro slides (nitrocellulose substratum) from Sartorius Stedim Biotech demonstrated the most promising results overall and were therefore selected for further method development.</p>
----------------------------------------------------------------------
In diva2:1786734 abstract is: <p>Polyester is a synthetic material made from polyethylene terephthalate (PET), which is synthesised from fossil raw materials. Many clothing manufacturers are using polyester in their clothing, which, from an environmental perspective, creates a non-sustainable cycle. However, manufacturing can be made more sustainable by using recycled PET bottles as the raw material for polyester yarn. A clothing company that has taken a stand against materials made from fossil fibres and instead invests in only selling clothes made from sustainable materials is the Swedish children's clothing brand Polarn o. Pyret. Being able to use polyester made from PET bottles is important for Polarn o. Pyret to ensure the use of sustainable fibres in their clothing. In the past, the recycled polyester was used in combination with polyester from virgin fibre, but now the goal is instead to switch completely to the recycled polyester. However, the increased quantity of recycled polyester can involve new risks. The need to map potential contaminants is important because Polarn o. Pyret makes clothes for children, which entails strict requirements on the chemical content of the clothes. The purpose of this work is to get an overview of which contaminants that can accumulate in the polyester fibre and thus pose a risk when using recycled polyester made from PET bottles. The goal is to be able to shed light on these contaminants and write a proposal for a risk assessment guide that can establish a foundation for how Polarn o. Pyret should be able to act when using recycled polyester. The work was carried out through an extensive literature study, where research articles and review articles published on scientific databases were the main source of information. Focus was on the risk of non-intentionally added substances that can be traced from recycled PET bottles. The risk assessment was done in consideration of the regulations that is established within the European Union (EU), using the EU chemicals regulation REACH and the European Chemicals Agency (ECHA) database. </p><p>After the mapping of NIAS in r-PET, the literature showed 42 potential contaminants to be present. These substances originated from the degradation of PET or the degradation of additives, and impurities from the recycling process and post-consumption use. Adhesives, labels, and caps are a contributing factor to the formation of NIAS originating from the recycling process. Since PVC was found to be the most common plastic that could contaminate PET via recycling, the additives used in PVC could also migrate to PET and create impurities. The most common phthalate found in this study was DEHP, which was the main plasticiser in PVC. </p><p>15 substances of the NIAS found in the literature was CMR substances, which means that they either were, or were suspected to be, carcinogenic, mutagenic and/or toxic to reproduction. Among these were substances such as benzene, antimony, cadmium, lead, acetaldehyde, and formaldehyde. Certain substances found through the literature were classified to be endocrine disruptors. These NIAS were p-nonylphenol, the organophosphite TNPP, the phthalates DEHP, DBP, BBP and, DIBP, nickel and BPA. Some of the NIAS found were under assessment of being classified as persistent, bioaccumulativ and toxic (PBT). The PBT substances found were the organophosphite compound Irgafos 168 and the UV-stabilisers Tinuvin P, UV-234 and UV- 328. The risk assessment showed 20 contaminants that may cause irritation to the skin/eyes and/or throat and 24 contaminants which may be toxic to the aquatic life. </p><p>This risk assessment showed that the recycling process of PET bottles is not completely safe, because potential contaminants could be transferred into the polyester yarn, and then carried over into children's garment and pose health risks to children. </p>


w='UV- 328' val={'c': 'UV-328', 's': 'diva2:1786734', 'n': 'correct in original - the dash is at the end of a line'}

corrected abstract:
<p>Polyester is a synthetic material made from polyethylene terephthalate (PET), which is synthesised from fossil raw materials. Many clothing manufacturers are using polyester in their clothing, which, from an environmental perspective, creates a non-sustainable cycle. However, manufacturing can be made more sustainable by using recycled PET bottles as the raw material for polyester yarn. A clothing company that has taken a stand against materials made from fossil fibres and instead invests in only selling clothes made from sustainable materials is the Swedish children's clothing brand Polarn o. Pyret. Being able to use polyester made from PET bottles is important for Polarn o. Pyret to ensure the use of sustainable fibres in their clothing. In the past, the recycled polyester was used in combination with polyester from virgin fibre, but now the goal is instead to switch completely to the recycled polyester. However, the increased quantity of recycled polyester can involve new risks. The need to map potential contaminants is important because Polarn o. Pyret makes clothes for children, which entails strict requirements on the chemical content of the clothes. The purpose of this work is to get an overview of which contaminants that can accumulate in the polyester fibre and thus pose a risk when using recycled polyester made from PET bottles. The goal is to be able to shed light on these contaminants and write a proposal for a risk assessment guide that can establish a foundation for how Polarn o. Pyret should be able to act when using recycled polyester. The work was carried out through an extensive literature study, where research articles and review articles published on scientific databases were the main source of information. Focus was on the risk of non-intentionally added substances that can be traced from recycled PET bottles. The risk assessment was done in consideration of the regulations that is established within the European Union (EU), using the EU chemicals regulation REACH and the European Chemicals Agency (ECHA) database.</p><p>After the mapping of NIAS in r-PET, the literature showed 42 potential contaminants to be present. These substances originated from the degradation of PET or the degradation of additives, and impurities from the recycling process and post-consumption use. Adhesives, labels, and caps are a contributing factor to the formation of NIAS originating from the recycling process. Since PVC was found to be the most common plastic that could contaminate PET via recycling, the additives used in PVC could also migrate to PET and create impurities. The most common phthalate found in this study was DEHP, which was the main plasticiser in PVC.</p><p>15 substances of the NIAS found in the literature was CMR substances, which means that they either were, or were suspected to be, carcinogenic, mutagenic and/or toxic to reproduction. Among these were substances such as benzene, antimony, cadmium, lead, acetaldehyde, and formaldehyde. Certain substances found through the literature were classified to be endocrine disruptors. These NIAS were p-nonylphenol, the organophosphite TNPP, the phthalates DEHP, DBP, BBP and, DIBP, nickel and BPA. Some of the NIAS found were under assessment of being classified as persistent, bioaccumulativ and toxic (PBT). The PBT substances found were the organophosphite compound Irgafos 168 and the UV-stabilisers Tinuvin P, UV-234 and UV-328. The risk assessment showed 20 contaminants that may cause irritation to the skin/eyes and/or throat and 24 contaminants which may be toxic to the aquatic life.</p><p>This risk assessment showed that the recycling process of PET bottles is not completely safe, because potential contaminants could be transferred into the polyester yarn, and then carried over into children's garment and pose health risks to children.</p>
----------------------------------------------------------------------
In diva2:1787126 abstract is: <p>A new way of activating cellulose for chemical modification using green processing conditions has been developed before the start of this thesis. Two versions of the novel betaine-based ionic liquid, BBIL has been synthesised, one using mesylate as a counter-ion, M-BBIL and one using acetate, A-BBIL. Before surface initiated-Ring Opening Polymerisation (SI-ROP) onto the activated cellulose can proceed, it is of interest to investigate the effects that the ionic liquids will have on the polymerization behaviour.</p><p>The catalytic behaviour of both M-BBIL and A-BBIL were tested on two different polymerisation reactions. Firstly, ROP of ε-CL was tested. The results showed that neither ionic liquid could show any co-catalytic behaviour when used in combination with the catalysts MSA and DPP. When used on their own, A-BBIL was concluded to be ineffective since thermal degradation inhibited polymerisation at temperatures above 85<sup>o</sup>C. M-BBIL could however achieve good kinetics when used at 160<sup>o</sup>C. The second reaction that was tested was the coROP of phthahlic anhydridecyclohexene oxide (PACHO) and phthalic anhydride-limonene oxide (PALO). Both copolymerisations could successfully be polymerised using M-BBIL, with only minor amounts of homopolymerisation of the epoxide if the right conditions were used. The reactions were however shown to be highly sensitive to water and more optimization is needed to reach higher molecular weights. Tests of trying to graft the PACHO polymers onto cellulose that had first been activated and then modified with phthalic anhydride moieties was also shown to be highly sensitive to water. Successful grafting could however be achieved by performing reaction in partially dried hydrophobic solvents.</p>

w='initiated-Ring' val={'c': 'initiated - Ring', 's': 'diva2:1787126', 'n': 'part of " surface initiated-Ring Opening Polymerisation (SI-ROP)"'}
w='phthahlic' val={'c': 'phthalic', 's': 'diva2:1787126', 'n': 'error in original'}
I fixed the use of "polymerisation" to be consistent in the text.


corrected abstract:
<p>A new way of activating cellulose for chemical modification using green processing conditions has been developed before the start of this thesis. Two versions of the novel betaine-based ionic liquid, BBIL has been synthesised, one using mesylate as a counter-ion, M-BBIL and one using acetate, A-BBIL. Before surface initiated - Ring Opening Polymerisation (SI-ROP) onto the activated cellulose can proceed, it is of interest to investigate the effects that the ionic liquids will have on the polymerisation behaviour.</p><p>The catalytic behaviour of both M-BBIL and A-BBIL were tested on two different polymerisation reactions. Firstly, ROP of ε-CL was tested. The results showed that neither ionic liquid could show any co-catalytic behaviour when used in combination with the catalysts MSA and DPP. When used on their own, A-BBIL was concluded to be ineffective since thermal degradation inhibited polymerisation at temperatures above 85°C. M-BBIL could however achieve good kinetics when used at 160°C. The second reaction that was tested was the coROP of phthahlic anhydridecyclohexene oxide (PACHO) and phthalic anhydride-limonene oxide (PALO). Both copolymerisations could successfully be polymerised using M-BBIL, with only minor amounts of homopolymerisation of the epoxide if the right conditions were used. The reactions were however shown to be highly sensitive to water and more optimisation is needed to reach higher molecular weights. Tests of trying to graft the PACHO polymers onto cellulose that had first been activated and then modified with phthalic anhydride moieties was also shown to be highly sensitive to water. Successful grafting could however be achieved by performing reaction in partially dried hydrophobic solvents.</p>
----------------------------------------------------------------------
In diva2:854692 abstract is: <p><em>Phytophthora infestans </em>is plant pathogenic Oomycete that is infamous of the Irish famine in 19th century. Late blight in potato <em>(Solanum tuberosum) </em>caused by P.<em>infestans </em>is still a threat to potato cultivation all around the globe. A lot of research had been done on this organism, yet there is no effective way of ceasing the spread of P.<em>infestans</em>. Conventional fungicides seem to be ineffective and only way to control the disease seems to produce disease resistant crops. In the interest of addressing the issue, a chemogenomic approach is adopted to identify chemicals that inhibit the growth and germination of the organism. A chemical library derived from a mother compound, which was previously identified to inhibit the P.<em>infestans</em>, is screened to evaluate the activity on the zoospore germination and the mycerlial growth of P.<em>infestans</em>. In doing so, some chemicals had been identified to have remarkably effective inhibitoryaction on the zoospores and the hyphae. The concentrations of the chemicals at which they are identified were determined. However, because of time constraint, a completed screening wasnot performed and thus further experiments are required to compare and evaluate all the chemicals of the library.</p>

w='mycerlial' val={'c': 'mycelial', 's': 'diva2:854692', 'n': 'no full text'}

corrected abstract:
<p><em>Phytophthora infestans</em> is plant pathogenic Oomycete that is infamous of the Irish famine in 19th century. Late blight in potato <em>(Solanum tuberosum) </em>caused by <em>P. infestans</em> is still a threat to potato cultivation all around the globe. A lot of research had been done on this organism, yet there is no effective way of ceasing the spread of <em>P. infestans</em>. Conventional fungicides seem to be ineffective and only way to control the disease seems to produce disease resistant crops. In the interest of addressing the issue, a chemogenomic approach is adopted to identify chemicals that inhibit the growth and germination of the organism. A chemical library derived from a mother compound, which was previously identified to inhibit the <em>P. infestans</em>, is screened to evaluate the activity on the zoospore germination and the mycelial growth of <em>P. infestans</em>. In doing so, some chemicals had been identified to have remarkably effective inhibitory action on the zoospores and the hyphae. The concentrations of the chemicals at which they are identified were determined. However, because of time constraint, a completed screening was not performed and thus further experiments are required to compare and evaluate all the chemicals of the library.</p>
----------------------------------------------------------------------
In diva2:1607300 abstract is: <p>The processing industry is the largest source of sulfur dioxide (SO2) emissions in Sweden, which includes the non­ferrous metals industry. The copper smelter Boliden Rönnskär has an environmental permit to emit a maximum of 3500 tonnes of SO2/year, a limit that the smelter has been close to in recent years. To reduce the SO2 emissions at Rönnskär, wet scrubbing (together with a bag filter for dust cleaning) has been proposed as a method for cleaning the SO2­-bearing, intermittent tapping gases from the flash furnace. To find the optimal wet scrubbing technique for the purpose, wet scrubbing techniques based on the following reagents were investigated, evaluated and compared in this report: caustic soda, soda ash, peroxide, lime and zinc oxide. Measurements were also done on the secondary hood gases from the converters, which could make use of the remaining capacity in the scrubber. Further, tests were conducted on various process waters from other processes at Rönnskär, waters that could be reused in the scrubber. The scrubber techniques were then evaluated based on the input data and system requirements using simulation and design software as well as theoretical calculations. The results suggested that it is reasonable to clean secondary hood gases in the scrubber, as they contained approximately 280 tonnes of SO2/year. This could be compared to the flash tapping gases that contained approximately 445 tonnes of SO2/year. Among the scrubbers, the peroxide scrubber evolved as the most attractive technique due to its relatively low life cycle cost and due to its suitability with the leaching plant and the flash cooling tower process water. The other packed tower techniques, caustic soda and soda ash had the highest life cycle costs, mainly due to their high reagent costs. The soda ash scrubber, which was the cheaper of the two sodium­-based scrubbers, could still be a suitable alternative due to its simplicity. The open spray towers had lower life cycle costs than the packed towers. However, the lime scrubber had several disadvantages that makes it an unsuitable alternative. In turn, the zinc oxide scrubber is a relatively under­researched and unproven technique, but should still be studied further as it could be integrated with the zinc smelting process at Rönnskär. The use of process waters in the scrubber would lead to a net reduction of process water to the process water treatment plant and would lead to reduced reagent costs if a stripper is installed to remove the SO2 from the process waters before entering the scrubber.</p>

w='sodium\xad-based' val={'c': 'sodium-based', 's': 'diva2:1607300'}
w='SO2\xad-bearing' val={'c': 'SO2-bearing', 's': 'diva2:1607300'}

corrected abstract:
<p>The processing industry is the largest source of sulfur dioxide (SO<sub>2</sub>) emissions in Sweden, which includes the non­ferrous metals industry. The copper smelter Boliden Rönnskär has an environmental permit to emit a maximum of 3500 tonnes of SO<sub>2</sub>/year, a limit that the smelter has been close to in recent years. To reduce the SO<sub>2</sub> emissions at Rönnskär, <em>wet scrubbing</em> (together with a bag filter for dust cleaning) has been proposed as a method for cleaning the SO<sub>2</sub>­bearing, intermittent <em>tapping gases</em> from the flash furnace. To find the optimal wet scrubbing technique for the purpose, wet scrubbing techniques based on the following reagents were investigated, evaluated and compared in this report: <em>caustic soda</em>, <em>soda ash</em>, <em>peroxide</em>, <em>lime</em> and <em>zinc oxide</em>. Measurements were also done on the <em>secondary hood</em> gases from the converters, which could make use of the remaining capacity in the scrubber. Further, tests were conducted on various process waters from other processes at Rönnskär, waters that could be reused in the scrubber. The scrubber techniques were then evaluated based on the input data and system requirements using simulation and design software as well as theoretical calculations.</p><p>The results suggested that it is reasonable to clean secondary hood gases in the scrubber, as they contained approximately 280 tonnes of SO<sub>2</sub>/year. This could be compared to the flash tapping gases that contained approximately 445 tonnes of SO<sub>2</sub>/year. Among the scrubbers, the peroxide scrubber evolved as the most attractive technique due to its relatively low life cycle cost and due to its suitability with the leaching plant and the flash cooling tower process water. The other packed tower techniques, caustic soda and soda ash had the highest life cycle costs, mainly due to their high reagent costs. The soda ash scrubber, which was the cheaper of the two sodium­based scrubbers, could still be a suitable alternative due to its simplicity. The open spray towers had lower life cycle costs than the packed towers. However, the lime scrubber had several disadvantages that makes it an unsuitable alternative. In turn, the zinc oxide scrubber is a relatively under­researched and unproven technique, but should still be studied further as it could be integrated with the zinc smelting process at Rönnskär. The use of process waters in the scrubber would lead to a net reduction of process water to the process water treatment plant and would lead to reduced reagent costs if a stripper is installed to remove the SO<sub>2</sub> from the process waters before entering the scrubber.</p>
----------------------------------------------------------------------
In diva2:1521389 abstract is: <p>With the development of information and communication technology (ICT), car sharing becomes more and more popular. It is a short-term car rental service and Volvo entered this market in 2019. The purpose of this thesis is to design for a seamless and personalized experience for car sharing users with the smartphone.</p><p>A literature overview and user interviews were performed to get a general understanding of the user groups, usage patterns of the car sharing service and their experience of connecting the smartphones to the car. Two user tests were made to iterate the design and get to know users’ opinions about the design concepts. Personas and scenarios were built up and that made the basis for the design of user interfaces on the car-sharing apps and the \ac{IVI}. As a result, a concept solution was brought up. The general process of the car sharing service was: booking a car, finding a car, getting onboard, picking up friends, and getting offboard. For drivers, they can find and unlock the car seamlessly with the phone, get onboard with all preferred settings applied, safely log into the IVI system, easily pick up friends by seeing their position, and clear personal data when they return the car. For passengers, they can see the position of the car and estimated arrival time, scan a QR code to turn their phones to a remoter of the IVI system, directly send a destination to the IVI system, and share their favorite music. Nearly all the users liked the designed functions in the tests but the adoption of the service was mainly affected by two factors. One was privacy concerns and the other was function value. Users made a choice of how important more functions were compared with sequences regarding loss of privacy. More future research needs to be done to further validate the findings of this thesis and achieve the designed experience.</p>


w='ac{IVI' val={'c': 'In-vehicle Infotainment System (IVI)', 's': 'diva2:1521389', 'n': 'correct in original - looks  like a LaTeX macro that was not handled'}

corrected abstract:
<p>With the development of information and communication technology (ICT), car sharing becomes more and more popular. It is a short-term car rental service and Volvo entered this market in 2019. The purpose of this thesis is to design for a seamless and personalized experience for car sharing users with the smartphone.</p><p>A literature overview and user interviews were performed to get a general understanding of the user groups, usage patterns of the car sharing service and their experience of connecting the smartphones to the car. Two user tests were made to iterate the design and get to know users’ opinions about the design concepts. Personas and scenarios were built up and that made the basis for the design of user interfaces on the car-sharing apps and the In­vehicle Infotainment System (IVI). As a result, a concept solution was brought up. The general process of the car sharing service was: booking a car, finding a car, getting onboard, picking up friends, and getting offboard. For drivers, they can find and unlock the car seamlessly with the phone, get onboard with all preferred settings applied, safely log into the IVI system, easily pick up friends by seeing their position, and clear personal data when they return the car. For passengers, they can see the position of the car and estimated arrival time, scan a QR code to turn their phones to a remoter of the IVI system, directly send a destination to the IVI system, and share their favorite music. Nearly all the users liked the designed functions in the tests but the adoption of the service was mainly affected by two factors. One was privacy concerns and the other was function value. Users made a choice of how important more functions were compared with sequences regarding loss of privacy. More future research needs to be done to further validate the findings of this thesis and achieve the designed experience.</p>
----------------------------------------------------------------------
In diva2:787188 abstract is: <p>The project in question relates to reproductive proteins from Pieris Napi, a common Swedish butterfly species. The analysis was performed by capillary electrophoresis and mass spectroscopy, MALDI-TOF.</p><p>To ensure good results from the analysis of the fresh butterflies, a method development was performed on old butterfly samples. Several solvents were used to determine which extraction medium was best suited for protein extraction. MilliQ-water, acetonitrile, hexafluoroisopropanol and methanol were included in the examination. All solvents were well suited as extraction medium, except acetonitrile which lacked clear signals in the electropherograms.</p><p>When the method development was finished different types of proteins could be studied by the aid of the chosen extraction media. After electrophoresis on the fresh samples, it could be presured that both hydrophobic and hydrophilic proteins were present in the spermatophores. Multiple electropherograms with characteristic profiles for each solvent were obtained which indicated that different types of proteins were detected. Some similarities could be observed between the extraction from the MilliQ-water and methanol.</p><p>Protein stability during a three week period was assured by regular controls with electrophoresis. Stability could be shown through similar profiles in the electropherograms from the various weeks. Since the proteins shown such stability, MALDI-TOF analysis was done on the samples after all the electrophoresis was performed. Proteins were examined in both high and low molecular fields. This ensured results in the form of spectra. The spectrum obtained from HFIP-extraction differed from the spectrum from methanol- and water extractions, indicating that hydrophobic proteins are present in the spermatophores.</p>

w='presured' val={'c': 'pressured', 's': ['diva2:787218', 'diva2:787188']}

corrected abstract:
<p>The project in question relates to reproductive proteins from Pieris Napi, a common Swedish butterfly species. The analysis was performed by capillary electrophoresis and mass spectroscopy, MALDI-TOF.</p><p>To ensure good results from the analysis of the fresh butterflies, a method development was performed on old butterfly samples. Several solvents were used to determine which extraction medium was best suited for protein extraction. MilliQ-water, acetonitrile, hexafluoroisopropanol and methanol were included in the examination. All solvents were well suited as extraction medium, except acetonitrile which lacked clear signals in the electropherograms.</p><p>When the method development was finished different types of proteins could be studied by the aid of the chosen extraction media. After electrophoresis on the fresh samples, it could be pressured that both hydrophobic and hydrophilic proteins were present in the spermatophores. Multiple electropherograms with characteristic profiles for each solvent were obtained which indicated that different types of proteins were detected. Some similarities could be observed between the extraction from the MilliQ-water and methanol.</p><p>Protein stability during a three week period was assured by regular controls with electrophoresis. Stability could be shown through similar profiles in the electropherograms from the various weeks. Since the proteins shown such stability, MALDI-TOF analysis was done on the samples after all the electrophoresis was performed. Proteins were examined in both high and low molecular fields. This ensured results in the form of spectra. The spectrum obtained from HFIP-extraction differed from the spectrum from methanol- and water extractions, indicating that hydrophobic proteins are present in the spermatophores.</p>
----------------------------------------------------------------------
In diva2:804978 abstract is: <p>The ability of an organism to adapt to changes in its environment is an essential skill for survival in all kingdoms of life. This led to a vast diversity of pathways that employ intracellular second messengers molecules, whish transduce external signals by modulating intracellular processes. Here, we focus on possible modulations of second messenger signalling pathways for therapeutic and industrial applications.</p><p> </p><p>Project 1: Recent insights into the immune stimulatory effect of cyclic dinucleotides in mammals has widened the scope of their potential therapeutic applications within vaccination, cancer treatment, and autoimmune disease. By fusing the Caulobacter crescentus diguanylate cyclase A (DGCA) to theHIV accessory protein vpr with a pol-derived protease cleavage site linker, we have engineered a HIV-1-derived DGCA protein transducing nanoparticle. Viral protease-mediated release of native DGCA during viral maturation enabled targeted transient delivery of DGCA with the ability to activate the STING pathway via the bacterial second messenger c-di-GMP. DGCA protein transduction demonstrates dose-dependent modulation of STING-mediated activation of URF3 regulatory element containing promoters in transgenic mammalian cells and in immune cells.</p><p>Project 2: Heterologous gene regulation systems offer the possibility to increase productivity of biopharmaceuticals and fine-tune production of  potentially toxic and difficult-to-produce products. By functionally linking the signal transdcution of the bacterial photo-activated adenylate cyclase bPAC from Beggiatoa sp. PS to the PKA-dependent pathway, and the photo-activateddiguanylate cyclase (DGCL) domain of BphG1 from Rhodobacter sphaeroides to the STING-dependent pathway in mammalian cells, we have engineered a dual wavelength controlled optogenetic expression system for biopharmaceutical production. Activated by blue and near infrared light respectively, transient gene expression from both synthetic pathways can be extensively fine-tuned in transgenic mammalian cells. When scaled for production the dual wavelength controlled expression system displays strong transgene expression with high spatiotemporal resolution when induced by light.</p><p> </p><p>The biological diversity of second messenger signalling pathways provide researchers with the ability to modulate specific cellular functions and construct novel signalling pathways. By employing cross-species and engineered second messenger effector proteins and downstream targets, this work demonstrates two interesting applications for repurposing second messenger molecules and pathways.</p>

w='transdcution' val={'c': 'transduction', 's': 'diva2:804978'}
w='vpr' val={'C': 'Vpr', 's': 'diva2:804978', 'n': 'no full text'}

corrected abstract:
<p>The ability of an organism to adapt to changes in its environment is an essential skill for survival in all kingdoms of life. This led to a vast diversity of pathways that employ intracellular second messengers molecules, whish transduce external signals by modulating intracellular processes. Here, we focus on possible modulations of second messenger signalling pathways for therapeutic and industrial applications.</p><p> </p><p>Project 1: Recent insights into the immune stimulatory effect of cyclic dinucleotides in mammals has widened the scope of their potential therapeutic applications within vaccination, cancer treatment, and autoimmune disease. By fusing the Caulobacter crescentus diguanylate cyclase A (DGCA) to the HIV accessory protein vpr with a pol-derived protease cleavage site linker, we have engineered a HIV-1-derived DGCA protein transducing nanoparticle. Viral protease-mediated release of native DGCA during viral maturation enabled targeted transient delivery of DGCA with the ability to activate the STING pathway via the bacterial second messenger c-di-GMP. DGCA protein transduction demonstrates dose-dependent modulation of STING-mediated activation of URF3 regulatory element containing promoters in transgenic mammalian cells and in immune cells.</p><p>Project 2: Heterologous gene regulation systems offer the possibility to increase productivity of biopharmaceuticals and fine-tune production of  potentially toxic and difficult-to-produce products. By functionally linking the signal transduction of the bacterial photo-activated adenylate cyclase bPAC from Beggiatoa sp. PS to the PKA-dependent pathway, and the photo-activated diguanylate cyclase (DGCL) domain of BphG1 from Rhodobacter sphaeroides to the STING-dependent pathway in mammalian cells, we have engineered a dual wavelength controlled optogenetic expression system for biopharmaceutical production. Activated by blue and near infrared light respectively, transient gene expression from both synthetic pathways can be extensively fine-tuned in transgenic mammalian cells. When scaled for production the dual wavelength controlled expression system displays strong transgene expression with high spatiotemporal resolution when induced by light.</p><p> </p><p>The biological diversity of second messenger signalling pathways provide researchers with the ability to modulate specific cellular functions and construct novel signalling pathways. By employing cross-species and engineered second messenger effector proteins and downstream targets, this work demonstrates two interesting applications for repurposing second messenger molecules and pathways.</p>
----------------------------------------------------------------------
In diva2:1673943 abstract is: <p>Affibody molecules are small affinity proteins (6.5 kDa) suggested to substitute monoclonal antibodies in therapeutic applications, e.g., antibody-drug conjugates (ADCs) targeting biomarker proteins expressed on cancer cells. An affibody-drug conjugate (AffiDC) could be used to target these types of overexpressed proteins on cancer cells while offering attractive properties, such as rapid transportation and distribution in the body, as well as efficient tumour penetration. These AffiDCs could be used as a targeted cancer therapy for cancers that are yet to be treatable and curable, like urothelial cancers. </p><p>This study suggested the use of ABD-fused affibodies to target a novel cancer protein that has been shown to be overexpressed on cancer cells, including breast, pancreatic and urothelial cancer. Affibody candidates toward this novel target were selected from a recombinant library, of 1×10<sup>11</sup> transformants, that is expressed using <em>E. coli</em> cell display system. The final candidates were subsequently biochemically characterized and assessed for affinity for the target. Three affibodies were finally identified and assessed in <em>in vitro</em> studies on mammalian cells, revealing two affibodies that appear to bind to the cell lines BT-474 and MCF-7 with KD ranging 10 to 100 nM.</p>

w='KD' val={'c': 'K<sub>D</sub>', 's': 'diva2:1673943'}
Note that there were some changes in the wording in the DIVA version that differ from the thesis. For example,
"transformants" was used rather than "variants" which is used in the thesis. Similarly, "cells" versus "cancer cells".


corrected abstract:
<p>Affibody molecules are small affinity proteins (6.5 kDa) suggested to substitute monoclonal antibodies in therapeutic applications, e.g., antibody-drug conjugates (ADCs) targeting biomarker proteins expressed on cancer cells. An affibody-drug conjugate (AffiDC) could be used to target these types of overexpressed proteins on cancer cells while offering attractive properties, such as rapid transportation and distribution in the body, as well as efficient tumour penetration. These AffiDCs could be used as a targeted cancer therapy for cancers that are yet to be treatable and curable, like urothelial cancers.</p><p>This study suggested the use of ABD-fused affibodies to target a novel cancer protein that has been shown to be overexpressed on cancer cells, including breast, pancreatic and urothelial cancer. Affibody candidates toward this novel target were selected from a recombinant library, of 1×10<sup>11</sup> variants, that is expressed using <em>E. coli</em> cell display system. The final candidates were subsequently biochemically characterized and assessed for affinity for the target. Three affibodies were finally identified and assessed in <em>in vitro</em> studies on mammalian cancer cells, revealing two affibodies that appear to bind to the cell lines BT-474 and MCF-7 with K<sub>D</sub> ranging 10 to 100 nM.</p>
----------------------------------------------------------------------
In diva2:851740 abstract is: <p>About 25% of the population in the industrialized countries suffers from allergic diseases. In Sweden, about 1/3 of the population in the age 25-45 suffer from allergy of which most common allergic are caused by pollen and fur. Today, there are several ways to treat allergens. The only treatment to obtain a long lasting effect is called allergen-specific immunotherapy. However, there are still difficulties to overcome with these therapies, including the risk of side effects and the risk to induce novel sensitization to proteins from the therapy. </p><p>The main wim with this project was to improve allergen specific immunotherapy by targeting effector cells, whoch can cause the symptoms, and the goal was to find binders for inhibitory receptors on mast cells and basophils tp down-regulate degranulation.</p><p>In this project I have therefore selected four affibodies using phage display that binds to two different inhibitory receptors expressed on mast cells and basophils. I have produced different constructs with these affibodies, and the original affibody, using a novel cloning system that has been developed during this time. The products contained a His6 tag for convenient purification using liquid chromotography (ÄKTA®) or magnetic beads. Finally, tje bomdomg am affinity of the produced constructs were analyzed ex vivo and in vitro using Flow cytometry (FACS), and Biacore®, respectively.</p><p>Preliminary results indicate that one of the selected affibodies bound the target, however more experiments are required to validate this finding.</p>

w='bomdomg' val={'c': 'binding', 's': 'diva2:851740', 'n': 'no full text, in tyhis and other words the typist was off their key positions'}
w='chromotography' val={'c': 'chromatography', 's': 'diva2:851740'}
w='whoch' val={'c': 'which', 's': 'diva2:851740', 'n': 'no full text'}
w='tje' val={'c': 'the', 's': 'diva2:851740', 'n': 'no full text'}
There were also some other error such as "tp" which should be "top" and "am" which should be "of".


corrected abstract:
<p>About 25% of the population in the industrialized countries suffers from allergic diseases. In Sweden, about 1/3 of the population in the age 25-45 suffer from allergy of which most common allergic are caused by pollen and fur. Today, there are several ways to treat allergens. The only treatment to obtain a long lasting effect is called allergen-specific immunotherapy. However, there are still difficulties to overcome with these therapies, including the risk of side effects and the risk to induce novel sensitization to proteins from the therapy. </p><p>The main wim with this project was to improve allergen specific immunotherapy by targeting effector cells, which can cause the symptoms, and the goal was to find binders for inhibitory receptors on mast cells and basophils top down-regulate degranulation.</p><p>In this project I have therefore selected four affibodies using phage display that binds to two different inhibitory receptors expressed on mast cells and basophils. I have produced different constructs with these affibodies, and the original affibody, using a novel cloning system that has been developed during this time. The products contained a His6 tag for convenient purification using liquid chromatography (ÄKTA®) or magnetic beads. Finally, the binding of affinity of the produced constructs were analyzed ex vivo and in vitro using Flow cytometry (FACS), and Biacore®, respectively.</p><p>Preliminary results indicate that one of the selected affibodies bound the target, however more experiments are required to validate this finding.</p>
----------------------------------------------------------------------
In diva2:1595231 abstract is: <p>The development of new high affinity proteins and peptides rely heavily on surface display of large libraries on various organisms. The most common method for surface display and protein/peptide selection is the phage display system. This system allows for creation of large-scale libraries but lacks the ability to continuously monitor the progression of the library between selection rounds. Bacterial display is beginning to emerge as an alternative to phage display with the ability to monitor selection rounds through fluorescent activated single cell sorting and flow cytometry. Previously, large scale libraries have been hard to express on bacteria due to a lack of developed transporter proteins for membrane expression. However, recent advances have enabled protein/peptide display on the surface of gram-negative bacteria such as <em>Escherichia Coli</em> allowing for display of larger libraries due to the ease of transforming these cells (1).</p><p>This study has evaluated a naïve affibody library displayed on the surface of <em>E. coli</em> through repeated rounds of selections towards two different targets: an overexpressed cancer cell receptor and the binding domain of another affibody. Selected affibodies were also recombinantly produced and analysed for their target affinity and stability. Results indicate that discovered affibodies exhibit binding affinity towards their desired target with KD- values in the nanomolar range thus proving the effectiveness of the naïve library in its ability to produce affibody-target binders.</p>

w='KDvalues' val={'c': 'KD-values', 's': 'diva2:1595231', 'n': 'no full text'}
w='KD- values' val={'c': 'KD-values', 's': 'diva2:1595231', 'n': 'no full text'}

corrected abstract:
<p>The development of new high affinity proteins and peptides rely heavily on surface display of large libraries on various organisms. The most common method for surface display and protein/peptide selection is the phage display system. This system allows for creation of large-scale libraries but lacks the ability to continuously monitor the progression of the library between selection rounds. Bacterial display is beginning to emerge as an alternative to phage display with the ability to monitor selection rounds through fluorescent activated single cell sorting and flow cytometry. Previously, large scale libraries have been hard to express on bacteria due to a lack of developed transporter proteins for membrane expression. However, recent advances have enabled protein/peptide display on the surface of gram-negative bacteria such as <em>Escherichia Coli</em> allowing for display of larger libraries due to the ease of transforming these cells (1).</p><p>This study has evaluated a naïve affibody library displayed on the surface of <em>E. coli</em> through repeated rounds of selections towards two different targets: an overexpressed cancer cell receptor and the binding domain of another affibody. Selected affibodies were also recombinantly produced and analysed for their target affinity and stability. Results indicate that discovered affibodies exhibit binding affinity towards their desired target with KD-values in the nanomolar range thus proving the effectiveness of the naïve library in its ability to produce affibody-target binders.</p>
----------------------------------------------------------------------
In diva2:1574715 abstract is: <p>Tissue engineering is an interdisciplinary field aiming to create functional tissue with applications in regenerative medicine and <em>in vitro</em> human tissue modeling. To mimic tissue, cells can be seeded into engineered three-dimensional (3D) scaffolds that mimic the natural extracellular matrix (ECM). Aiming for higher reproducibility and complexity, an alternative for creating such scaffolds is bioprinting using a hydrogel-based bioink. Silk formed by the recombinantly produced spider silk protein 4RepCT, functionalized with a binding site from fibronectin, naturally found in the ECM, is called FN-silk. FN-silk can be used to produce nanowires with defined shapes that present dimensions in the micro- to nanoscale, similar to fibers found in the natural ECM. In this thesis, the hypothesis that the addition of FN-silk and nanowires would provide cells with an ECM-like support, and thereby improve cellmatrix adhesion and viability of cells within a printable hydrogel, is tested. To investigate this, human mesenchymal stem cells (hMSCs) were cultivated within a two-component hyaluronic acid (HA) hydrogel with and without FN-silk and/or nanowires. The cells were analyzed for viability and cell morphology within the hydrogel over time. Live/dead viability assays were used to determine an optimal cell seeding density to ~300 cells/μl in hydrogels with a total volume of 20 μl. hMSCs were found to be viable in the HA hydrogel with and without the FN-silk formats. To investigate cell morphology, fluorescence staining for filamentous actin (f-actin) and vinculin, proteins involved in cell-matrix adhesion, was performed at different time points to observe any time dependent changes. As expected, hMSCs cultured without FN-silk formats presented a rounded morphology for up to 8 days in culture. When analyzing hMSCs cultured in HA hydrogels with 1 μg/μl FN-silk as an additive, cells presented a clear elongated morphology after 7-8 days in culture. This effect started to be visible around day 4 after cell seeding. When analyzing cells in HA hydrogels with silk nanowires alone, this effect was not seen. However, the elongated morphology was again observed in cultures with both nanowires and FN-silk. Together this shows that FN-silk could be used as an additive for 3D cell culture, however, at this point it is unclear whether nanowires alone would improve the quality of the 3D support. As the concentration of nanowires tested here was low in respect to cell density (a ratio of approximately 1:1, with ~300 nanowires/μl hydrogel) it was clear that the majority of cells were not in close vicinity to the nanowires. Future studies with higher nanowire concentrations would be required to investigate this further.</p>

Old change is a merged word "cellmatrix" should be "cell matrix".

corrected abstract:
<p>Tissue engineering is an interdisciplinary field aiming to create functional tissue with applications in regenerative medicine and <em>in vitro</em> human tissue modeling. To mimic tissue, cells can be seeded into engineered three-dimensional (3D) scaffolds that mimic the natural extracellular matrix (ECM). Aiming for higher reproducibility and complexity, an alternative for creating such scaffolds is bioprinting using a hydrogel-based bioink. Silk formed by the recombinantly produced spider silk protein 4RepCT, functionalized with a binding site from fibronectin, naturally found in the ECM, is called FN-silk. FN-silk can be used to produce nanowires with defined shapes that present dimensions in the micro- to nanoscale, similar to fibers found in the natural ECM. In this thesis, the hypothesis that the addition of FN-silk and nanowires would provide cells with an ECM-like support, and thereby improve cell matrix adhesion and viability of cells within a printable hydrogel, is tested. To investigate this, human mesenchymal stem cells (hMSCs) were cultivated within a two-component hyaluronic acid (HA) hydrogel with and without FN-silk and/or nanowires. The cells were analyzed for viability and cell morphology within the hydrogel over time. Live/dead viability assays were used to determine an optimal cell seeding density to ~300 cells/μl in hydrogels with a total volume of 20 μl. hMSCs were found to be viable in the HA hydrogel with and without the FN-silk formats. To investigate cell morphology, fluorescence staining for filamentous actin (f-actin) and vinculin, proteins involved in cell-matrix adhesion, was performed at different time points to observe any time dependent changes. As expected, hMSCs cultured without FN-silk formats presented a rounded morphology for up to 8 days in culture. When analyzing hMSCs cultured in HA hydrogels with 1 μg/μl FN-silk as an additive, cells presented a clear elongated morphology after 7-8 days in culture. This effect started to be visible around day 4 after cell seeding. When analyzing cells in HA hydrogels with silk nanowires alone, this effect was not seen. However, the elongated morphology was again observed in cultures with both nanowires and FN-silk. Together this shows that FN-silk could be used as an additive for 3D cell culture, however, at this point it is unclear whether nanowires alone would improve the quality of the 3D support. As the concentration of nanowires tested here was low in respect to cell density (a ratio of approximately 1:1, with ~300 nanowires/μl hydrogel) it was clear that the majority of cells were not in close vicinity to the nanowires. Future studies with higher nanowire concentrations would be required to investigate this further.</p>
----------------------------------------------------------------------
In diva2:635950 abstract is: <p>The present study utilizes finite element analysis in order to simulate a surgical operation in the treatment of a hallux rigidus case, as designed and developed by Episurf Medical AB (Stockholm, Sweden). The surgical intervention includes an initial cheilectomy as well as an insertion of an orthopedic implant.</p><p>The goal of the study was to evaluate the current concept of the medical intervention as it is manifested today, as well as to give design suggestions as how to further improve the pre-planning of the surgery. MRI-images of the first metatarsophalangeal joint in the hallux was collected from a patient suffering from hallux rigidus, and used in order to build case-specific geometrical images to be used in the FE analysis. The simulation was setup as to simulate a normal motion in the first metatarsophalangeal joint during a normal gait pattern.</p><p>The first simulation was conducted without any intervention, while the second was conducted after a pre-determined operation plan in accordance with the surgical operation that Episurf Medical AB wants to perform. The results was then compared and analyzed in order to determine the post-surgical effects that such an operation could have on the patient. A third and final simulation was then performed, by using optimization algorithms in order to make suggestions to the pre-planned cheilectomy shape, as well as orientation of the implant.</p><p>Two parameters were being investigated in order to assess the surgical intervention as designed by Episurf Medical AB; the contact stress on the articular side of the metatarsal head, and the strain on the implant shaft.</p><p>The current manifestation of the cheilectomy did not reduce the contact stress compared to the untreated condition, as the implant failed to be a load baring surface due to the two dimensional nature of which it is conceived. Instead, the contact surface area is reduced and positioned medial and lateral to the implant head. The optimization algorithm could reduce the maximum contact stress significantly, from 295MPa and 400MPa in the treated and untreated conditons respectively, to 160MPa after the optimization algorithm.</p><p>It became clear that the angle of the cheilectomy as well as the orientation of the implant angle has an incriminating effect on the post-operative results. However, the shape of the cheilectomy as well as the design of the implant would need to be revised in future embodiments, as the current concept failed to provide joint with a new articulating surface. Further development of the models formulated in this thesis is advised, as well as validating the findings with clinical data.</p>

w='conditons' val={'c': 'conditions', 's': 'diva2:635950', 'n': 'no full text'}
I assumed the usual italics for Latin words.

corrected abstract:
<p>The present study utilizes finite element analysis in order to simulate a surgical operation in the treatment of a <em>hallux rigidus</em> case, as designed and developed by Episurf Medical AB (Stockholm, Sweden). The surgical intervention includes an initial cheilectomy as well as an insertion of an orthopedic implant.</p><p>The goal of the study was to evaluate the current concept of the medical intervention as it is manifested today, as well as to give design suggestions as how to further improve the pre-planning of the surgery. MRI-images of the first metatarsophalangeal joint in the hallux was collected from a patient suffering from <em>hallux rigidus</em>, and used in order to build case-specific geometrical images to be used in the FE analysis. The simulation was setup as to simulate a normal motion in the first metatarsophalangeal joint during a normal gait pattern.</p><p>The first simulation was conducted without any intervention, while the second was conducted after a pre-determined operation plan in accordance with the surgical operation that Episurf Medical AB wants to perform. The results was then compared and analyzed in order to determine the post-surgical effects that such an operation could have on the patient. A third and final simulation was then performed, by using optimization algorithms in order to make suggestions to the pre-planned cheilectomy shape, as well as orientation of the implant.</p><p>Two parameters were being investigated in order to assess the surgical intervention as designed by Episurf Medical AB; the contact stress on the articular side of the metatarsal head, and the strain on the implant shaft.</p><p>The current manifestation of the cheilectomy did not reduce the contact stress compared to the untreated condition, as the implant failed to be a load baring surface due to the two dimensional nature of which it is conceived. Instead, the contact surface area is reduced and positioned medial and lateral to the implant head. The optimization algorithm could reduce the maximum contact stress significantly, from 295MPa and 400MPa in the treated and untreated conditions respectively, to 160MPa after the optimization algorithm.</p><p>It became clear that the angle of the cheilectomy as well as the orientation of the implant angle has an incriminating effect on the post-operative results. However, the shape of the cheilectomy as well as the design of the implant would need to be revised in future embodiments, as the current concept failed to provide joint with a new articulating surface. Further development of the models formulated in this thesis is advised, as well as validating the findings with clinical data.</p>
----------------------------------------------------------------------
diva2:646620 no changes
Note that "hallux rigidus" was _not_ set in italics in the thesis, even though it  is a latin term.
----------------------------------------------------------------------
In diva2:505342 abstract is: <p>By the everyday increasing enthusiasm for using renewable-sustainable sources in energy production area, focusing on one and optimizing it in the best possible way should be of much interest.</p>
<p>Biogas production from anaerobic digestion of wastes is a well known energy source which could be applied more efficiently if the CO2portion of it would be upgraded to CH4as well. There is a methanation reaction which could convert carbon dioxide to methane with the use of hydrogenation.</p>
<p>In this report, the effort is to simulate this methanation reactor which is a catalytic bed of ruthenium on alumina base. The temperature change and its’ effect on reaction kinetics and equilibrium, also deriving designing parameters for the catalyst bed are different tasks which was tried to be covered in this thesis work.</p>
<p>Based on calculations, the reactor can operate isothermally or adiabatically. The point is that each method has its own cons and pros. For the isothermal case finally the optimum temperature to run the reaction is decided to be 600 K in 10 bar total pressure. In adiabatic case then it is understood to work on interstage cooling strategy which in given conditions came to the number of 6 for reactors and 5 for interstage cooling devices.</p>
<p>Afterwards it is thought to apply some technical changes to conventional adiabatic method and recycle some part of the product to the entrance of the reactor and assist the conversion. In this method number of reactors would be reduced to 2 and one heat exchanger in the middle.</p>
<p>Selecting the best process in large scale treatment, needs lots of economical analysis and detail design while in small scale condition the most preferred method to run the reaction is isothermal.</p>

w='CO2portion' val={'c': 'CO<sub>2</sub> portion', 's': 'diva2:505342', 'n': 'correct in original'}
w='CH4' val={'c': 'CH<sub>4</sub>', 's': 'diva2:505342', 'n': 'correct in original'}

corrected abstract:
<p>By the everyday increasing enthusiasm for using renewable-sustainable sources in energy production area, focusing on one and optimizing it in the best possible way should be of much interest.</p><p>Biogas production from anaerobic digestion of wastes is a well known energy source which could be applied more efficiently if the CO<sub>2</sub> portion of it would be upgraded to CH<sub>4</sub> as well. There is a methanation reaction which could convert carbon dioxide to methane with the use of hydrogenation.</p><p>In this report, the effort is to simulate this methanation reactor which is a catalytic bed of ruthenium on alumina base. The temperature change and its’ effect on reaction kinetics and equilibrium, also deriving designing parameters for the catalyst bed are different tasks which was tried to be covered in this thesis work.</p><p>Based on calculations, the reactor can operate isothermally or adiabatically. The point is that each method has its own cons and pros. For the isothermal case finally the optimum temperature to run the reaction is decided to be 600 K in 10 bar total pressure. In adiabatic case then it is understood to work on interstage cooling strategy which in given conditions came to the number of 6 for reactors and 5 for interstage cooling devices.</p><p>Afterwards it is thought to apply some technical changes to conventional adiabatic method and recycle some part of the product to the entrance of the reactor and assist the conversion. In this method number of reactors would be reduced to 2 and one heat exchanger in the middle.</p><p>Selecting the best process in large scale treatment, needs lots of economical analysis and detail design while in small scale condition the most preferred method to run the reaction is isothermal.</p>
----------------------------------------------------------------------
In diva2:853465 abstract is: <p>Circulating tumor cells (CTCs) are cells that have broken away from a solid tumor and entered the blood stream. Some of these cells can have the potentiao to exit the blood stream at another site and form metastes. The number of CTCs has been shown to be a prognostic marker for both metastatic breast cancer as well as metastatic prostate cancer and metastatic colon cancer. The concentration of CRCs in peripheral blood is however usually low, down to less than one CTC per ml blood. This makes them challenging to detect. Several techniques have been developed for the isolation and detection of CTCs, but the only FDA approved method (CellSearch from Veridex) focuses on enumeration of CTCs, although much more information relevant to prognosis and treatment decisions could be obtained from molecular analysis of the CTCs.</p><p>This project aims at detectin and molecularly characterizing single tumor cells using reverse transcriptiase multiplexed ligation-dependent probe amplification (RT-MLPA). RT-MLPA is a PCR-based method where gene specific primers are used to convert transcripts into cDNA flowwoed by gene specific hybridization and ligation of DNA probes. The DNA probes are then amplified using only one primer pair. In this report results supporting the single cell sensitivity is presented, along with work to achieve a more reliable and robust gene expression profile for a seven markers panel perfomred on three different breast cancer cell lines. The tumor cell specificity in contamination mononuclear cell material of this panel is also tested. Finally a proof of concept using fixated input material in the form of whole blood spiked with tumor cells is used for an immunomagnetic isolation targeting EpCAM followed by an RT-MLPA.</p><p>Remaining work to be done on the seven markers breast cancer panel for RT-MLPA mainly include a redesign of the reverse transcirption step in order to achiee a more reliable gee expresseion profile before the method can be tested in real purified CTCs.</p>

w='transcirption' val={'c': 'transcription', 's': 'diva2:853465'}
w='transcriptiase' val={'c': 'transcriptase', 's': 'diva2:853465'}
w='expresseion' val={'c': 'expression', 's': 'diva2:853465', 'n': 'no full text'}
w='detectin' val={'c': 'detecting', 's': 'diva2:853465', 'n': 'no full text'}
w='flowwoed' val={'c': 'followed', 's': 'diva2:853465', 'n': 'no full text'}
w='metastes' val={'c': 'metastases', 's': 'diva2:853465', 'n': 'no full text'}
w='perfomred' val={'c': 'performed', 's': 'diva2:853465', 'n': 'no full text'}
w='potentiao' val={'c': 'potential', 's': 'diva2:853465', 'n': 'no full text'}
w='achiee' val={'c': 'achieve', 's': 'diva2:853465'}

corrected abstract:
<p>Circulating tumor cells (CTCs) are cells that have broken away from a solid tumor and entered the blood stream. Some of these cells can have the potential to exit the blood stream at another site and form metastases. The number of CTCs has been shown to be a prognostic marker for both metastatic breast cancer as well as metastatic prostate cancer and metastatic colon cancer. The concentration of CRCs in peripheral blood is however usually low, down to less than one CTC per ml blood. This makes them challenging to detect. Several techniques have been developed for the isolation and detection of CTCs, but the only FDA approved method (CellSearch from Veridex) focuses on enumeration of CTCs, although much more information relevant to prognosis and treatment decisions could be obtained from molecular analysis of the CTCs.</p><p>This project aims at detecting and molecularly characterizing single tumor cells using reverse transcriptase multiplexed ligation-dependent probe amplification (RT-MLPA). RT-MLPA is a PCR-based method where gene specific primers are used to convert transcripts into cDNA followed by gene specific hybridization and ligation of DNA probes. The DNA probes are then amplified using only one primer pair. In this report results supporting the single cell sensitivity is presented, along with work to achieve a more reliable and robust gene expression profile for a seven markers panel performed on three different breast cancer cell lines. The tumor cell specificity in contamination mononuclear cell material of this panel is also tested. Finally a proof of concept using fixated input material in the form of whole blood spiked with tumor cells is used for an immunomagnetic isolation targeting EpCAM followed by an RT-MLPA.</p><p>Remaining work to be done on the seven markers breast cancer panel for RT-MLPA mainly include a redesign of the reverse transcription step in order to achieve a more reliable gee expression profile before the method can be tested in real purified CTCs.</p>
----------------------------------------------------------------------
In diva2:744730 abstract is: <p>Massive parallel sequencing (MPS) techniques provide huge opportunities in the area of life sciences. During the last five years, high-throughput sequencing platforms have become readily accessible. However, these platforms are expensive and the race to develop platforms at a reduced cost is still open. Netiher researchers nor practitioners have been able to reach a point at which routine sequencing (not including bacterial genomes) of high numbers of genomes is possible, and consequently it is often desired to select genomic regions of interest and enrich these regions before sequencing. In order to save money, barcoding of DNA has become popular during the last years. A traditional approach for using barcodes is to synthesize one barcode per sample and run all reactions in parallel and pool them toether before amplicification or sequencing. This traditional approach is very costly and is often nos used for batches larger than 100 samples.</p><p>To create a library of barcodes that is cheap, one oligo can be synthesi&lt;ed with degenerated bases (N). If 20 random degenerated bases are used in this way, this means that 4<sup>20</sup> = 1.1 trillion combinations of barcodes are available. In practice, this implies that millions of barcodes are made for the same coast as that for one with the traditional approach. All the barcodes will be mixed in one tube and a physical separation of the barcodes can be accomplished by doing emPCR. We thereby perform compartmentalization and clonal amplification barcode on its surface (93%) by using a copy per bead (cpb) of 0.08. This result indicates that we successfully performed compartmentalization and monoclonal amplification of the barcodes on the bead surface.</p><p>Through the arrival of massive parallel short-read sequencing tools, the cost of sequencing DNA has been reduced significantly during the last years, thereby making it possible to sequence hundreds of bacertial genomes in one run. Still, the reduced length of the reads, compared with capillary-based methods, forms novel challenges in genome assembly. We have designed a method where a Nextera library preparation of genomic DNA fragments is coupled to one monoclonally amplified barcode on a bead, thereby achieving single fragment resolution of the sequencing data.</p>

w='synthesi&lt;ed' val={'c': 'synthesized', 's': 'diva2:744730', 'n': 'no full text'}
w='toether' val={'c': 'together', 's': 'diva2:744730', 'n': 'no full text'}
w='amplicification' val={'c': 'amplification', 's': 'diva2:744730', 'n': 'no full text'}
w='Netiher' val={'c': 'Neither', 's': 'diva2:744730', 'n': 'no full text'}

corrected abstract:
<p>Massive parallel sequencing (MPS) techniques provide huge opportunities in the area of life sciences. During the last five years, high-throughput sequencing platforms have become readily accessible. However, these platforms are expensive and the race to develop platforms at a reduced cost is still open. Neither researchers nor practitioners have been able to reach a point at which routine sequencing (not including bacterial genomes) of high numbers of genomes is possible, and consequently it is often desired to select genomic regions of interest and enrich these regions before sequencing. In order to save money, barcoding of DNA has become popular during the last years. A traditional approach for using barcodes is to synthesize one barcode per sample and run all reactions in parallel and pool them together before amplification or sequencing. This traditional approach is very costly and is often nos used for batches larger than 100 samples.</p><p>To create a library of barcodes that is cheap, one oligo can be synthesized with degenerated bases (N). If 20 random degenerated bases are used in this way, this means that 4<sup>20</sup> = 1.1 trillion combinations of barcodes are available. In practice, this implies that millions of barcodes are made for the same coast as that for one with the traditional approach. All the barcodes will be mixed in one tube and a physical separation of the barcodes can be accomplished by doing emPCR. We thereby perform compartmentalization and clonal amplification barcode on its surface (93%) by using a copy per bead (cpb) of 0.08. This result indicates that we successfully performed compartmentalization and monoclonal amplification of the barcodes on the bead surface.</p><p>Through the arrival of massive parallel short-read sequencing tools, the cost of sequencing DNA has been reduced significantly during the last years, thereby making it possible to sequence hundreds of bacertial genomes in one run. Still, the reduced length of the reads, compared with capillary-based methods, forms novel challenges in genome assembly. We have designed a method where a Nextera library preparation of genomic DNA fragments is coupled to one monoclonally amplified barcode on a bead, thereby achieving single fragment resolution of the sequencing data.</p>
----------------------------------------------------------------------
In diva2:744733 abstract is: <p>The aim of this project was to label human antibodies for PET-applications with <sup>18</sup>F via the IgG-binding Z-domain. The Z-domain was functionalized on its N-terminal in order to be able to attach a radioactive label by a cehmoselective reaction. For the labeling reaction, two strategies were used: oxime chemistry and click chemistry. The Z-domain could then be attached to the antibody by UV-conjugation, by inserting a benzoylphenylalanine (BPA) residue in the peptide ewquence. An aminoosy (AO)-functionalized Z-domain, AO-Z32BPA, was successfully synthesized and the aminooxy group was available for the oxime reaction, which is the method that will be used for labeling it. It was also shown that the aminooxy-functionalized Z-domain reained its ability to be covalently linke to an antibody by UV-conjugation. Z-domains were also prepared to take part in a click reaction., by coupling the dibenzocycllctyl (DBCO) group to the N-terminal of the peptide. No product was obtained from click reaction with an azido reagent, although several reaction conditions were tried. When introducing short-lived radiolabels to proteins for PET-imaging, time is crucial. An investigation of the UV-conjugation protocol was made, in order to decrease the time as much as possible. Results from the investigation of the UV-conjugation protocol show that the time of the protocol could be substantially reduced.</p>

w='aminoosy' val={'c': 'aminooxy', 's': 'diva2:744733'}
w='ewquence' val={'c': 'sequence', 's': 'diva2:744733', 'n': 'no full text'}
w='dibenzocycllctyl' val={'c': 'dibenzocyclooctyne', 's': 'diva2:744733', 'n': 'no full text'}
w='cehmoselective' val={'c': 'chemoselective', 's': 'diva2:744733'}
w='reained' val={'c': 'retained', 's': 'diva2:744733', 'n': 'no full text'}

corrected abstract:
<p>The aim of this project was to label human antibodies for PET-applications with <sup>18</sup>F via the IgG-binding Z-domain. The Z-domain was functionalized on its N-terminal in order to be able to attach a radioactive label by a chemoselective reaction. For the labeling reaction, two strategies were used: oxime chemistry and click chemistry. The Z-domain could then be attached to the antibody by UV-conjugation, by inserting a benzoylphenylalanine (BPA) residue in the peptide sequence. An aminooxy (AO)-functionalized Z-domain, AO-Z32BPA, was successfully synthesized and the aminooxy group was available for the oxime reaction, which is the method that will be used for labeling it. It was also shown that the aminooxy-functionalized Z-domain retained its ability to be covalently linke to an antibody by UV-conjugation. Z-domains were also prepared to take part in a click reaction., by coupling the dibenzocyclooctyne (DBCO) group to the N-terminal of the peptide. No product was obtained from click reaction with an azido reagent, although several reaction conditions were tried. When introducing short-lived radiolabels to proteins for PET-imaging, time is crucial. An investigation of the UV-conjugation protocol was made, in order to decrease the time as much as possible. Results from the investigation of the UV-conjugation protocol show that the time of the protocol could be substantially reduced.</p>
----------------------------------------------------------------------
In diva2:1774447 abstract is: <p>Dragline silk is a fiber naturally produced by spiders that demonstrate characteristics desirable in a biomaterial. Limitations of extraction from spiders has led to the development of recombinantly produced versions of dragline silk proteins for various biomedical applications. A smaller version of the native spider silk protein, 4RepCT, has been engineered and expressed in Escherichia coli to produce fibers similar to that of native spider silk. Structural characterization of the 4RepCT structure is crucial to assess functional properties of the silk and to further develop materials that better mimics the native fibers. As of now, there is a lack of information regarding the structure of the C-terminal (CT) domain and its role in fiber formation. </p><p>The CT-domain structure is herein investigated using Circular Dichroism (CD) and solid state Nuclear Magnetic Resonance (ssNMR). The latter is limited in its ability to distinguish the CT-domain from the 4Rep part of the protein, resulting in a spectrum lacking in structural information for the CT-domain. This report presents a partial labelling strategy using Sortase A coupling to overcome this problem. The results show that production of 4Rep coupled with a Sortase signal (4Rep-SRT) and uniformly 13C and 15N labled CT-domain with an N-terminal glycine (G-CT) covalently coupled using Sortase A resulted in a fiber, although too weak to be removed and analyzed by ssNMR. To successfully produce fibers to send for ssNMR, higher protein concentrations are required and the purification protocol for 4Rep-SRT needs to be optimized to make the process more efficient. Despite the lack of final product sufficient for ssNMR analysis, the cultivation and purification strategy together with small scale validation of both the Sortase Coupling and Fiber Formation indicate promising use of the method for partial protein labeling.</p><p>In addition to ssNMR, the CT-domains ability to form fibers and its dependency on pH was investigated using Circular Dichrosim (CD). The results indicate that low pH (~5) facilitates structural transition from a primarily α-helical structure to β-sheet within 2 hours incubation at room temperature, and that pH plays and important role in the fiber formation mechanism.</p><p>The increased structural knowledge obtained of the CT-domain can facilitate future engineering efforts for the production of biomaterials that better mimic the native spider silk and its desirable properties. A successful partial protein labeling could also broaden the applications of NMR and provide novel possibilities for the determination of protein structure and function.</p>

w='Dichrosim' val={'c': 'Dichroism', 's': 'diva2:1774447', 'n': 'no full text'}

corrected abstract:
<p>Dragline silk is a fiber naturally produced by spiders that demonstrate characteristics desirable in a biomaterial. Limitations of extraction from spiders has led to the development of recombinantly produced versions of dragline silk proteins for various biomedical applications. A smaller version of the native spider silk protein, 4RepCT, has been engineered and expressed in Escherichia coli to produce fibers similar to that of native spider silk. Structural characterization of the 4RepCT structure is crucial to assess functional properties of the silk and to further develop materials that better mimics the native fibers. As of now, there is a lack of information regarding the structure of the C-terminal (CT) domain and its role in fiber formation. </p><p>The CT-domain structure is herein investigated using Circular Dichroism (CD) and solid state Nuclear Magnetic Resonance (ssNMR). The latter is limited in its ability to distinguish the CT-domain from the 4Rep part of the protein, resulting in a spectrum lacking in structural information for the CT-domain. This report presents a partial labelling strategy using Sortase A coupling to overcome this problem. The results show that production of 4Rep coupled with a Sortase signal (4Rep-SRT) and uniformly 13C and 15N labled CT-domain with an N-terminal glycine (G-CT) covalently coupled using Sortase A resulted in a fiber, although too weak to be removed and analyzed by ssNMR. To successfully produce fibers to send for ssNMR, higher protein concentrations are required and the purification protocol for 4Rep-SRT needs to be optimized to make the process more efficient. Despite the lack of final product sufficient for ssNMR analysis, the cultivation and purification strategy together with small scale validation of both the Sortase Coupling and Fiber Formation indicate promising use of the method for partial protein labeling.</p><p>In addition to ssNMR, the CT-domains ability to form fibers and its dependency on pH was investigated using Circular Dichroism (CD). The results indicate that low pH (~5) facilitates structural transition from a primarily α-helical structure to β-sheet within 2 hours incubation at room temperature, and that pH plays and important role in the fiber formation mechanism.</p><p>The increased structural knowledge obtained of the CT-domain can facilitate future engineering efforts for the production of biomaterials that better mimic the native spider silk and its desirable properties. A successful partial protein labeling could also broaden the applications of NMR and provide novel possibilities for the determination of protein structure and function.</p>
----------------------------------------------------------------------
In diva2:1044142 abstract is: <p>Malignant melanoma is an aggressive type of skin cancer. High resistance to common therapies results in poor prognosis in later stages of the disease, which leaves early detection as crucial for patients’ survival. During the past decades, an increase in incidence of malignant melanoma has been observed worldwide. Majority of tumors shows heterogeneity and new analysis methods are needed for better understanding of cancer evolution. One such method is Spatial Trancriptomics.</p><p>Most melanoma biopsies contain dark, melanin-rich regions, which involves several challenges for the Spatial Transcriptomics technology. The aim of this project was to develop a protocol that allows for analysis of dark malignant melanoma tissue samples.</p>

w='Trancriptomics' val={'c': 'Transcriptomics', 's': 'diva2:1044142', 'n': 'no full text'}

corrected abstract:
<p>Malignant melanoma is an aggressive type of skin cancer. High resistance to common therapies results in poor prognosis in later stages of the disease, which leaves early detection as crucial for patients’ survival. During the past decades, an increase in incidence of malignant melanoma has been observed worldwide. Majority of tumors shows heterogeneity and new analysis methods are needed for better understanding of cancer evolution. One such method is Spatial Transcriptomics.</p><p>Most melanoma biopsies contain dark, melanin-rich regions, which involves several challenges for the Spatial Transcriptomics technology. The aim of this project was to develop a protocol that allows for analysis of dark malignant melanoma tissue samples.</p>
----------------------------------------------------------------------
In diva2:1617578 abstract is: <p>Since the first human spaceflight in 1961, hundreds of humans have been in space. Microgravity and high radiation are the main spaceflight hazards. The space environment is known to impact several aspects of human health, such as bone density and cognitive performance. However, the effects of long­duration spaceflights on a cellular and molecular level, utilizing biosamples and multiomic approaches, is poorly studied.</p><p>In this project, the method Spatial Transcriptomics has been utilized to compare brain tissue from the hippocampus region of mice that have been in space with a control group of mice that have stayed on Earth. Spatial Transcriptomics allow for the quantification of gene expression, while maintaining the spatial information of the transcriptome. The results of this study suggest that spaceflights cause mitochondrial stress.   This thesis work is part of a more extensive study in collaboration with NASA, and more studies will be conducted to investigate the effects of spaceflights further. If these findings are confirmed, medicines used on Earth to treat patients with mitochondrial dysfunction could increase the well­being of astronauts in space.</p>

w='long\xadduration' val={'c': 'long-duration', 's': 'diva2:1617578'}

corrected abstract:
<p>Since the first human spaceflight in 1961, hundreds of humans have been in space. Microgravity and high radiation are the main spaceflight hazards. The space environment is known to impact several aspects of human health, such as bone density and cognitive performance. However, the effects of long-duration spaceflights on a cellular and molecular level, utilizing biosamples and multiomic approaches, is poorly studied.</p><p>In this project, the method Spatial Transcriptomics has been utilized to compare brain tissue from the hippocampus region of mice that have been in space with a control group of mice that have stayed on Earth. Spatial Transcriptomics allow for the quantification of gene expression, while maintaining the spatial information of the transcriptome. The results of this study suggest that spaceflights cause mitochondrial stress. This thesis work is part of a more extensive study in collaboration with NASA, and more studies will be conducted to investigate the effects of spaceflights further. If these findings are confirmed, medicines used on Earth to treat patients with mitochondrial dysfunction could increase the well­being of astronauts in space.</p>
----------------------------------------------------------------------
In diva2:743504 abstract is: <p>The technical development of computed tomography (CT) with faster image collection frequencies and lower radiation dose while maintaining or increasing spatial and tissue-specific resolution, has enabled timeresoluted dynamic CT for the study of body parts in motion.</p><p> </p><p>The development of this technology has come to the attention of the supervisors at the Karolinska Institute and the Royal Institute of Technology. They were interested to investigate the possibility of using today´s technology to calculate the kinematics of skeletal foot segments, based on a dynamic CT collection of a human foot. Thus a method to detect, extract and track landmarks in sequences of 3-dimensional (3D) volumes, and the description of foot kinematics during plantarflexion and dorsiflexion was developed in this thesis.</p><p> </p><p>The developed method intended to automatically track skeletal foot segments and calculate their relative movement and rotation in the frontal, sagittal and transverse plane. A program was developed using commercially available software that isolated a selected skeletal segment and detected its landmarks. The automatic landmark detection useful for tracking needs further development and therefore complementary manual detection was performed. These landmarks were used to calculate the kinematics by tracking individually created local coordinate system for each structure. As an example the maximum rotation stated between calcaneus and talus was 51 degrees within the sagittal plane. The thesis illustrates methods used and presents the developed program and all the results of the individual and relative motion of the calcaneus and talus. The method used in this thesis has a good future potential with possibilities to visualize and calculate normal and pathological musculoskeletal motion.</p>

w='timeresoluted' val={'c': 'time resolved', 's': 'diva2:743504', 'n': 'no full text'}

corrected abstract:
<p>The technical development of computed tomography (CT) with faster image collection frequencies and lower radiation dose while maintaining or increasing spatial and tissue-specific resolution, has enabled time resolved dynamic CT for the study of body parts in motion.</p><p> </p><p>The development of this technology has come to the attention of the supervisors at the Karolinska Institute and the Royal Institute of Technology. They were interested to investigate the possibility of using today´s technology to calculate the kinematics of skeletal foot segments, based on a dynamic CT collection of a human foot. Thus a method to detect, extract and track landmarks in sequences of 3-dimensional (3D) volumes, and the description of foot kinematics during plantarflexion and dorsiflexion was developed in this thesis.</p><p> </p><p>The developed method intended to automatically track skeletal foot segments and calculate their relative movement and rotation in the frontal, sagittal and transverse plane. A program was developed using commercially available software that isolated a selected skeletal segment and detected its landmarks. The automatic landmark detection useful for tracking needs further development and therefore complementary manual detection was performed. These landmarks were used to calculate the kinematics by tracking individually created local coordinate system for each structure. As an example the maximum rotation stated between calcaneus and talus was 51 degrees within the sagittal plane. The thesis illustrates methods used and presents the developed program and all the results of the individual and relative motion of the calcaneus and talus. The method used in this thesis has a good future potential with possibilities to visualize and calculate normal and pathological musculoskeletal motion.</p>
----------------------------------------------------------------------
In diva2:572719 abstract is: <p>Abstract</p><p>Structure of membrane protiens can be determined by diffirent techniques.Electron Crystallography is one of the commonly used techniques to determine their structure in atomic or near atomic resolution.Due to the crystal disorder and poor CTF correction techniques the resolution obtained from this technique is not ideal.To push the resolution to the ideal, single particle refinement with local averaging for crystal disorder and improved CTF correction methodology for tilted data sets has been applied.With this approach applied to microsomal glutathione S-transferase 1(MGST1) membrane Protien data set comparable resolution with fewer data sets to the previous reconstruction has been achieved.</p>

w='diffirent' val={'c': 'different', 's': 'diva2:572719', 'n': 'correct in original'}
w='protiens' val={'c': 'proteins', 's': 'diva2:572719', 'n': 'correct in original'}
w='Protien' val={'c': 'protein', 's': 'diva2:572719', 'n': 'correct in original'}

Note there were also some capitalization differences, the below is as set in the thesis.
corrected abstract:
<p>Structure of membrane proteins can be determined by different techniques. Electron crystallography is one of the commonly used techniques to determine their structure in atomic or near atomic resolution. Due to the crystal disorder and poor CTF correction techniques the resolution obtained from this technique is not the ideal. To push the resolution to the ideal, single particle refinement with local averaging for crystal disorder and Improved CTF correction methodology for tilted data sets has been applied. With this approach applied to microsomal glutathione S-transferase 1(MGST1) membrane protein data set comparable resolution with fewer data sets to the previous reconstructions has been achieved.</p>
----------------------------------------------------------------------
In diva2:730241 abstract is: <p>A fourth generation multiple site Ziegler-Natta catalyst was used to synthesize ethylene and propylene homo-and copolymers in the presence of hydrogen. This type of catalysts produce polymers with broader molecular weight distribution (MWD), chemical composition distribution (CCD) and stereoregularity than other coordination polymerization catalysts since it has more than one active site.</p><p>The ratio of propylene/ethylene was varied to study its effect on polymer microstructure. In addition, by having two different electron donors, namely diisopropyldimethoxysilane (P) and dicyclopentyldimethoxysilane (D), the molecular weight distribution (MWD) and stereospecificity of the synthesized polymers were examined.</p><p>The polymer samples were characterized using <sup>13</sup>C-NMR and high-temperature gel permeation chromatography (GPC). Using the <sup>13</sup>C-NMR data, the triad distribution for the copolymers and also the isotactic triad distribution for homo-polymers were calculated.</p><p>The effects of electron donors on different feed ratios of ethylene and propylene in the synthesis were investigated. Co-polymer produced with D-donors showed higher isospecificity and also higher content of ethylene in the final polymer. In contrast, polymers produced using with P-donor showed lower polydispersity indices (PDI), and had higher contents of propylene in final polymer. In addition, the “Deconvolution method” was applied to GPC data in order to determine the number of sites on the Ziegler-Natta catalyst; which showed that 4 active site types were adequate to explain the molecular weight distributions.</p>

w='C-NMR' val={'c': '<sup>13</sup>C-NMR', 's': 'diva2:730241'}
Some words were missing from the DIVA abstraact.

corrected abstract:
<p>A fourth generation multiple site Ziegler-Natta catalyst was used to synthesize ethylene and propylene homo-and copolymers in the presence of hydrogen. This type of catalysts produce polymers with broader molecular weight distribution (MWD), chemical composition distribution (CCD) and stereoregularity than other coordination polymerization catalysts since it has more than one active site.</p><p>The ratio of propylene/ethylene was varied to study its effect on polymer microstructure. In addition, by having two different electron donors, namely diisopropyldimethoxysilane (P) and dicyclopentyldimethoxysilane (D), the molecular weight distribution (MWD) and stereospecificity of the synthesized polymers were examined.</p><p>The polymer samples were characterized using carbon-13 nuclear magnetic resonance (<sup>13</sup>C-NMR) and high-temperature gel permeation chromatography (GPC). Using the <sup>13</sup>C-NMR data, the triad distribution for the copolymers and also the isotactic triad distribution for homo-polymers were calculated.</p><p>The effects of electron donors on different feed ratios of ethylene and propylene in the synthesis were investigated. Co-polymer produced with D-donors showed higher isospecificity and also higher content of ethylene in the final polymer. In contrast, polymers produced using with P-donor showed lower polydispersity indices (PDI), and had higher contents of propylene in final polymer. In addition, the “Deconvolution method” was applied to GPC data in order to determine the number of sites on the Ziegler-Natta catalyst; which showed that 4 active site types were adequate to explain the molecular weight distributions.</p>
----------------------------------------------------------------------
In diva2:1117705 abstract is: <p>The lithium-ion battery technology is commercialized since more than 20 years and is approaching its the­ oretical limits. Hence, new technologies are needed to prevent  a stagnation of the maximal energy density of batteries [l]. To answer this need, the use of lithium metal has been investigated since the 1970s [2]. In addition of having one of the lowest negative electrochemical potential (-3.040Vs1m [3]), it also has a very high theoretical specific capacity (3860 mAh.g·1 )  due to its low density (0.534 g.cm·3 )  [l]. Thus, extensive research are conducted on lithium-air as well as lithium-sulfur battery to achieve longer autonomy for electrical vehicles that are in high demand in the current market [4] [5] [6] [7]. All these factors make the research on metallic lithium anode a demanding topic nowadays [8].</p><p> </p><p>The current knowledge on lithium metal electrochemical behavior creates several issues concerning its utiliza­tion in batteries.  It is known to quickly loose coulombic efficiency while cycling due to side reactions, creating a poorly-conductive passivation layer, as well as preferably forming dendrites during the plating process, leading to short circuits in a small number of cycles [l] (5] [6]. These issues prevent the use of lithium metal anodes in classical batteries without firstly gathering information on the lithium plating process.</p><p> </p><p>Numerous parameters are known to influence the lithium deposition, the most important ones being the elec­trolyte composition, the substrate and the physical parameters. The latter are the least studied parameters and research need to be undertaken to understand fully their influences on the deposition.</p><p> </p><p>In this report the current lithium-ions batteri es technology will be briefly int roduced. It is followed by a pre­sentation of the different parameters influencing the deposition of lithium. The current density influence on the deposit morphology will then be investigated. Finally, the result will be analyzed to determine how to achieve a uniform lithium electrodeposition.</p>

w='utiliza\xadtion' val={'c': 'utilization', 's': 'diva2:1117705'}
w='elec\xadtrolyte' val={'c': 'electrolyte', 's': 'diva2:1117705'}
w='oretical' val={'c': 'theoretical', 's': 'diva2:1117705', 'n': 'no full text'}
w='pre\xadsentation' val={'c': 'presentation', 's': 'diva2:1117705'}
w='roduced' val={'c': 'introduced', 's': 'diva2:1117705', 'n': 'no full text'}

corrected abstract:
<p>The lithium-ion battery technology is commercialized since more than 20 years and is approaching its theoretical limits. Hence, new technologies are needed to prevent  a stagnation of the maximal energy density of batteries [l]. To answer this need, the use of lithium metal has been investigated since the 1970s [2]. In addition of having one of the lowest negative electrochemical potential (-3.040Vs1m [3]), it also has a very high theoretical specific capacity (3860 mAh.g·1 )  due to its low density (0.534 g.cm·3 )  [l]. Thus, extensive research are conducted on lithium-air as well as lithium-sulfur battery to achieve longer autonomy for electrical vehicles that are in high demand in the current market [4] [5] [6] [7]. All these factors make the research on metallic lithium anode a demanding topic nowadays [8].</p><p> </p><p>The current knowledge on lithium metal electrochemical behavior creates several issues concerning its utilization in batteries.  It is known to quickly loose coulombic efficiency while cycling due to side reactions, creating a poorly-conductive passivation layer, as well as preferably forming dendrites during the plating process, leading to short circuits in a small number of cycles [l] (5] [6]. These issues prevent the use of lithium metal anodes in classical batteries without firstly gathering information on the lithium plating process.</p><p> </p><p>Numerous parameters are known to influence the lithium deposition, the most important ones being the electrolyte composition, the substrate and the physical parameters. The latter are the least studied parameters and research need to be undertaken to understand fully their influences on the deposition.</p><p> </p><p>In this report the current lithium-ions batteri es technology will be briefly introduced. It is followed by a presentation of the different parameters influencing the deposition of lithium. The current density influence on the deposit morphology will then be investigated. Finally, the result will be analyzed to determine how to achieve a uniform lithium electrodeposition.</p>
----------------------------------------------------------------------
In diva2:848291 abstract is: <p>The endeavor to have more efficient solar cells and as environmentally beneficial as possible are the driving forces for this work. The way to reach this is by research to better the understanding of the mechanisms and parameters that govern the performance of solar cells. New materials are essential to develop because the current ones lack stability and are water, temperature and UV-radiation sensitive. In this work the lead (Pb2+), which is poisonous and hazardous is intended to be replaced in the organic metal halide (OMH) perovskite structure. This is tested with gold or silver combined with bismuth and silver by itself. Also trimethylsulfonium gold or silver iodides are investigated. The methylammonium cation is also substituted to cesium. The perovskite material both absorbs light and transports charges in the solar cells. Materials based on AuI/AgI, BiI3 and CH3NH3I and AuI/AgI and [Me3S]I and AgI, BiI3 and CsI were synthesized and analyzed by XRD on thin film and mesoporous substrate and Raman spectroscopy to determine material structure and bonding. J-V measurements were performed to see the function in solar cells. After this conductivity and absorption parameters were determined by an electrical conductivity test and UV-vis absorption spectroscopy.</p><p>XRD measurements indicate that the perovskite structure could have been obtained because the materials match with the XRD spectra of [20] foremost T3, T5 and T6, Cs1 and Cs2. In T7 some new structure is formed. The bismuth could be partially substituted by silver as the metal cation. The samples are quite amorphous, but still containing crystalline peaks, the product material could be a mixture of a crystalline and an amorphous phase. The crystalline phase could have the desired perovskite structure. To have mesoporous TiO2 as substrate seem to enhance a more crystalline structured material. All the materials seem to have formed some new structures because the pure reactants does not seem to be present, exceptions could be P1 and T1 that contained AuI. The change of cation from methylamine to cesium though results in a shift of the peak positions because of the change of cation size as in [20], but the structure is most likely the same.</p><p>Raman spectroscopy indicate that there is a change in structure, some new bond being present, when increasing the methylamine ratio for the presumed methylammonium silver bismuth iodide perovskites. This concerns materials T5, T6, T7 with increasing ratio of methylamine. This new bond is most pronounced in T7 where the methylamine content is the highest. Both Silver and bismuth iodide bonds seem to be present and cannot be coupled to be the pure reactants recrystallizing and some new bonds of these are present in all materials to some extent. The organic bond vibration has low intensity and might indicate that there is not so much organic cation present in the product and thus the probability of having the desired product anion decreases.</p><p>The solar cells made with Spiro-OMeTAD were 700-4000 times more efficient than those made with Sulphur polymer HTM.</p><p>Solar cells made with Spiro-OMeTAD as HTM gives slightly higher efficiency when increasing the methylammonium cation ratio. For cesium as cation the combined metal cation constellation with bismuth and silver gives a little higher efficiency than bismuth alone. Methylammonium as cation gives a higher efficiency than cesium.</p><p>Solar cells made with Sulphur polymer HTM show approximately 3-30 times higher efficiency with methylammonium as cation compared to cesium as cation. HTM material seem to affect the perovskite material making some of the cells completely transparent and some of them paler, water in the solvent chlorobenzene can be a possible explanation. The transparency can be the reason for the low efficiency obtained for the solar cells. Also the measurement methodology of these solar cells can also have been false, measuring the contacts, and the etching procedure could be another source of this.</p><p>The solar cells had quite low efficiencies compared to [20], although same presumed material and procedure has been used and thus there might be something wrong in the accuracy of the manufacturing. The cells should probably been made several times and possible sources of error should be analyzed and corrected for.</p><p>The materials were all relatively conductive. P1 gave the highest conductivity, almost three times higher than for methylammonium lead iodide that has a conductivity of 1,1x10-4 s/cm [3]. Increasing the methylammonium ratio gave an increase of the conductivity both with bismuth and silver as metal cations and silver alone. The increase of the methylammonium ratio might result in a new structure formed which has lattice planes that are more conductive. A change of gold to silver for the trimethylsulfonium iodide materials gave a large decrease in conductivity.</p><p>The materials have different absorption curves meaning that they have different bandgaps and this indicates differences in structure. The bandgaps of all materials are indirect contrary to what is proven to be the case for perovskites that are believed to have direct bandgaps in general. To have indirect bandgaps requires a shift in momentum in the electronic transitions and is not as beneficial as having direct bandgaps. Compared to methylammonium lead iodide that has a direct bandgap of 1,6 eV, the bandgaps are at least 0,5 eV higher and range between 2,2-2,36 eV. P1 had a low bandgap of 1,6 eV meaning it absorbs a wide range of wavelengths.</p><p>The conductivity does not seem to be the obstacle and the cells that are not transparent absorb light. It is highly possible that the low solar cell performance, at least to a certain extent, has to do with the production process. The low scan rate could also affect the low efficiencies and HTM Spiro-OMeTAD should be used.</p><p>Currently the efficiency of the perovskite materials with silver/bismuth, gold/bismuth and silver are too low, and not able to substitute lead in the perovskite structure solar cells. Neither trimethylsulfonium gold or silver iodide cells nor cesium perovskites have enough efficiency at present. The conductivities for the materials are promising and the materials that are not completely transparent absorb light.</p>

w='[Me3S]I' val={'c': '[Me<sub>3</sub>S]I', 's': 'diva2:848291', 'n': 'correct in original'}
There were some additional missing subscripts and a superscript.

corrected abstract:
<p>The endeavor to have more efficient solar cells and as environmentally beneficial as possible are the driving forces for this work. The way to reach this is by research to better the understanding of the mechanisms and parameters that govern the performance of solar cells. New materials are essential to develop because the current ones lack stability and are water, temperature and UV-radiation sensitive. In this work the lead (Pb<sup>2+</sup>), which is poisonous and hazardous is intended to be replaced in the organic metal halide (OMH) perovskite structure. This is tested with gold or silver combined with bismuth and silver by itself. Also trimethylsulfonium gold or silver iodides are investigated. The methylammonium cation is also substituted to cesium. The perovskite material both absorbs light and transports charges in the solar cells. Materials based on AuI/AgI, BiI3 and CH<sub>3</sub>NH<sub>3</sub>I and AuI/AgI and [Me<sub>3</sub>S]I and AgI, BiI3 and CsI were synthesized and analyzed by XRD on thin film and mesoporous substrate and Raman spectroscopy to determine material structure and bonding. J-V measurements were performed to see the function in solar cells. After this conductivity and absorption parameters were determined by an electrical conductivity test and UV-vis absorption spectroscopy.</p><p>XRD measurements indicate that the perovskite structure could have been obtained because the materials match with the XRD spectra of [20] foremost T3, T5 and T6, Cs1 and Cs2. In T7 some new structure is formed. The bismuth could be partially substituted by silver as the metal cation. The samples are quite amorphous, but still containing crystalline peaks, the product material could be a mixture of a crystalline and an amorphous phase. The crystalline phase could have the desired perovskite structure. To have mesoporous TiO2 as substrate seem to enhance a more crystalline structured material. All the materials seem to have formed some new structures because the pure reactants does not seem to be present, exceptions could be P1 and T1 that contained AuI. The change of cation from methylamine to cesium though results in a shift of the peak positions because of the change of cation size as in [20], but the structure is most likely the same.</p><p>Raman spectroscopy indicate that there is a change in structure, some new bond being present, when increasing the methylamine ratio for the presumed methylammonium silver bismuth iodide perovskites. This concerns materials T5, T6, T7 with increasing ratio of methylamine. This new bond is most pronounced in T7 where the methylamine content is the highest. Both Silver and bismuth iodide bonds seem to be present and cannot be coupled to be the pure reactants recrystallizing and some new bonds of these are present in all materials to some extent. The organic bond vibration has low intensity and might indicate that there is not so much organic cation present in the product and thus the probability of having the desired product anion decreases.</p><p>The solar cells made with Spiro-OMeTAD were 700-4000 times more efficient than those made with Sulphur polymer HTM.</p><p>Solar cells made with Spiro-OMeTAD as HTM gives slightly higher efficiency when increasing the methylammonium cation ratio. For cesium as cation the combined metal cation constellation with bismuth and silver gives a little higher efficiency than bismuth alone. Methylammonium as cation gives a higher efficiency than cesium.</p><p>Solar cells made with Sulphur polymer HTM show approximately 3-30 times higher efficiency with methylammonium as cation compared to cesium as cation. HTM material seem to affect the perovskite material making some of the cells completely transparent and some of them paler, water in the solvent chlorobenzene can be a possible explanation. The transparency can be the reason for the low efficiency obtained for the solar cells. Also the measurement methodology of these solar cells can also have been false, measuring the contacts, and the etching procedure could be another source of this.</p><p>The solar cells had quite low efficiencies compared to [20], although same presumed material and procedure has been used and thus there might be something wrong in the accuracy of the manufacturing. The cells should probably been made several times and possible sources of error should be analyzed and corrected for.</p><p>The materials were all relatively conductive. P1 gave the highest conductivity, almost three times higher than for methylammonium lead iodide that has a conductivity of 1,1x10<sup>-4</sup> s/cm [3]. Increasing the methylammonium ratio gave an increase of the conductivity both with bismuth and silver as metal cations and silver alone. The increase of the methylammonium ratio might result in a new structure formed which has lattice planes that are more conductive. A change of gold to silver for the trimethylsulfonium iodide materials gave a large decrease in conductivity.</p><p>The materials have different absorption curves meaning that they have different bandgaps and this indicates differences in structure. The bandgaps of all materials are indirect contrary to what is proven to be the case for perovskites that are believed to have direct bandgaps in general. To have indirect bandgaps requires a shift in momentum in the electronic transitions and is not as beneficial as having direct bandgaps. Compared to methylammonium lead iodide that has a direct bandgap of 1,6 eV, the bandgaps are at least 0,5 eV higher and range between 2,2-2,36 eV. P1 had a low bandgap of 1,6 eV meaning it absorbs a wide range of wavelengths.</p><p>The conductivity does not seem to be the obstacle and the cells that are not transparent absorb light. It is highly possible that the low solar cell performance, at least to a certain extent, has to do with the production process. The low scan rate could also affect the low efficiencies and HTM Spiro-OMeTAD should be used.</p><p>Currently the efficiency of the perovskite materials with silver/bismuth, gold/bismuth and silver are too low, and not able to substitute lead in the perovskite structure solar cells. Neither trimethylsulfonium gold or silver iodide cells nor cesium perovskites have enough efficiency at present. The conductivities for the materials are promising and the materials that are not completely transparent absorb light.</p>
----------------------------------------------------------------------
In diva2:853815 abstract is: <p>Labelling of antibodies is traditionally done by conjugation to free amine or carboxyl groups, whish may result in unspecific labelling and the risk of influecning the antigen binding site of the antibody. In this project, the use of an immunoglobulin binding protein domain, C2, modified with a photoactivable amino acid analogue, benzoylphenylalanine (BPA), enables covalent coupling to the antibody Fab fragment. This can be used as a new strategy for site-specific labelling. Previous studies include an introduction of a double mutation to the C2 domain that removes the Fc-binding properties of the rotein, making sure that the binding occurs only at the antibody Fab fragment. Incorporation of a BPA gives the domain crosslinking abilities to human antibodies. In this project, an affinity analysis shows that this previous BPA mutation removes the affinity of C2 for the murine Fab fragment, which could explain the absence of crosslinking abilities towards mouse antibodies. This thesis includes solid phase peptide synthesis of novel C2.BPA variants where the most promising candidate, with an additional benzoylphenylalanine substitution, shows crosslinking towards both human and murine antibodies of various subclasses.</p>

w='influecning' val={'c': 'influencing', 's': 'diva2:853815', 'n': 'no full text'}
w='rotein' val={'c': 'protein', 's': 'diva2:853815', 'n': 'no full text'}

corrected abstract:
<p>Labelling of antibodies is traditionally done by conjugation to free amine or carboxyl groups, whish may result in unspecific labelling and the risk of influencing the antigen binding site of the antibody. In this project, the use of an immunoglobulin binding protein domain, C2, modified with a photoactivable amino acid analogue, benzoylphenylalanine (BPA), enables covalent coupling to the antibody Fab fragment. This can be used as a new strategy for site-specific labelling. Previous studies include an introduction of a double mutation to the C2 domain that removes the Fc-binding properties of the protein, making sure that the binding occurs only at the antibody Fab fragment. Incorporation of a BPA gives the domain crosslinking abilities to human antibodies. In this project, an affinity analysis shows that this previous BPA mutation removes the affinity of C2 for the murine Fab fragment, which could explain the absence of crosslinking abilities towards mouse antibodies. This thesis includes solid phase peptide synthesis of novel C2.BPA variants where the most promising candidate, with an additional benzoylphenylalanine substitution, shows crosslinking towards both human and murine antibodies of various subclasses.</p>
----------------------------------------------------------------------
In diva2:1758817 abstract is: <p>Supercapacitors are energy storage devices that have drawn attention for the past decade. Some of the advantages of these devices are higher power density storage, extended life cycles, and fast charge and discharge times. However, supercapacitors are still limited in energy density compared to batteries. To obtain higher power and energy densities, a hybrid asymmetric supercapacitor is a good alternative. This device consists of one carbon-based electrode for non-faradaic reactions, and one carbon electrode combined with metal oxides for redox reactions. The material choice is important for the capability of a hybrid asymmetric supercapacitor. In this study, four different commercial carbons are investigated. The specific surface area, pore sizes, and morphology are compared. In addition, metal oxide nanoparticles MnO<sub>2</sub> are synthesised, and crystal structure is investigated. Furthermore, the MnO<sub>2</sub> particles are deposited on the four carbons and the growth of those is studied. Finally, the interaction between ionic liquid 1-butyl-3-methylimidazolium tetrafluoroborate (BMIM[BF<sub>4</sub>]) as an electrolyte and the different carbons is studied.</p>

w='BMIM[BF' val={'c': '1-butyl-3-methylimidazolium tetrafluoroborate (BMIM[BF<sub>4</sub>]', 's': 'diva2:1758817'}

corrected abstract:
<p>Supercapacitors are energy storage devices that have drawn attention for the past decade. Some of the advantages of these devices are higher power density storage, extended life cycles, and fast charge and discharge times. However, supercapacitors are still limited in energy density compared to batteries. To obtain higher power and energy densities, a hybrid asymmetric supercapacitor is a good alternative. This device consists of one carbon-based electrode for non-faradaic reactions, and one carbon electrode combined with metal oxides for redox reactions. The material choice is important for the capability of a hybrid asymmetric supercapacitor. In this study, four different commercial carbons are investigated. The specific surface area, pore sizes, and morphology are compared. In addition, metal oxide nanoparticles MnO<sub>2</sub> are synthesised, and crystal structure is investigated. Furthermore, the MnO<sub>2</sub> particles are deposited on the four carbons and the growth of those is studied. Finally, the interaction between ionic liquid 1-butyl-3-methylimidazolium tetrafluoroborate (BMIM[BF<sub>4</sub>]]) as an electrolyte and the different carbons is studied.</p>
----------------------------------------------------------------------
In diva2:1595752 abstract is: <p>The demand for better and more sustainable material is increasing. More efficient materials will be needed to meet the growing global need. Hybrid organic-inorganic materials are one type of materials that have been of great interest recently, which can be described as a class of materials that mix organic and inorganic components. This thesis focused on hybrid organic-inorganic materials inspired by the classical perovskite crystal structure ABX<sub>3</sub>, where component A is an organic cation, component B is a divalent metal cation and component X is an anion. Hybrid organic-inorganic materials based on the classical perovskite structure may have various functional properties and may have a broad range of potential applications. Some examples of those properties as well as some and possible applications include good photoconductivity and power conversion efficiency for photovoltaic devices, excellent emission properties for light emitting diodes and tunable dielectric properties for electronic switches and sensors. </p><p>The physical properties of the hybrid organic-inorganic material are determined by the crystal structure of the material, which in turn will be decided by the choice of components. With the many possible choices for organic and inorganic components, there is an opportunity to synthesize completely new hybrid organic-inorganic compounds that may display new or superior physical properties.</p><p>Current hybrid organic-inorganic materials based on the perovskite crystal structure mainly use lead as the divalent metal, since it currently gives the best performance. The toxicity of lead is a major drawback for current lead-based hybrid organic-inorganic materials. The possibility to replace lead with another divalent metal has been explored during this project. For this thesis, the organic cation cyclohexylammonium (CHA) has been of focus as the organic component.</p><p>The aim of this thesis was to design, synthesize and characterize novel hybrid organic-inorganic compounds. The hybrid organic-inorganic compounds CHAZnBr<sub>3</sub> and (CHA)<sub>2</sub>ZnBr<sub>4</sub> were synthesized for the first time, to the best of our knowledge, and will be the focus of this thesis. The two new hybrid organic-inorganic compounds were structurally characterized by X-ray Diffraction (XRD) and thermally characterized by Thermal Gravimetric Analysis (TGA) and Differential Scanning Calorimetry (DSC). </p><p>The first compound, CHAZnBr<sub>3</sub>, could be determined to be orthorhombic at 298 K. The compound was found to be thermally stable up 490 K, and to undergo a phase transition at 445 K.  The second compound, (CHA)<sub>2</sub>ZnBr<sub>4</sub>, could not be fully structurally solved at either 100 K or 298 K. The compound was found to be thermally stable up to 490 K, and to undergo a phase transition at 230 K.  Further characterization will be needed to better understand the properties of these two compounds and their possible applications.</p>

w='CHAZnBr' val={'c': 'CHAZnBr<sub>3</sub>', 's': 'diva2:1595752'}
Note that the original does not indicate degree Kelvin (ᵒK) but simply says "K".

corrected abstract:
<p>The demand for better and more sustainable material is increasing. More efficient materials will be needed to meet the growing global need. Hybrid organic-inorganic materials are one type of materials that have been of great interest recently, which can be described as a class of materials that mix organic and inorganic components. This thesis focused on hybrid organic-inorganic materials inspired by the classical perovskite crystal structure ABX<sub>3</sub>, where component A is an organic cation, component B is a divalent metal cation and component X is an anion. Hybrid organic-inorganic materials based on the classical perovskite structure may have various functional properties and may have a broad range of potential applications. Some examples of those properties as well as some and possible applications include good photoconductivity and power conversion efficiency for photovoltaic devices, excellent emission properties for light emitting diodes and tunable dielectric properties for electronic switches and sensors.</p><p>The physical properties of the hybrid organic-inorganic material are determined by the crystal structure of the material, which in turn will be decided by the choice of components. With the many possible choices for organic and inorganic components, there is an opportunity to synthesize completely new hybrid organic-inorganic compounds that may display new or superior physical properties.</p><p>Current hybrid organic-inorganic materials based on the perovskite crystal structure mainly use lead as the divalent metal, since it currently gives the best performance. The toxicity of lead is a major drawback for current lead-based hybrid organic-inorganic materials. The possibility to replace lead with another divalent metal has been explored during this project. For this thesis, the organic cation cyclohexylammonium (CHA) has been of focus as the organic component.</p><p>The aim of this thesis was to design, synthesize and characterize novel hybrid organic-inorganic compounds. The hybrid organic-inorganic compounds CHAZnBr<sub>3</sub> and (CHA)<sub>2</sub>ZnBr<sub>4</sub> were synthesized for the first time, to the best of our knowledge, and will be the focus of this thesis. The two new hybrid organic-inorganic compounds were structurally characterized by X-ray Diffraction (XRD) and thermally characterized by Thermal Gravimetric Analysis (TGA) and Differential Scanning Calorimetry (DSC).</p><p>The first compound, CHAZnBr<sub>3</sub>, could be determined to be orthorhombic at 298 K. The compound was found to be thermally stable up 490 K, and to undergo a phase transition at 445 K.  The second compound, (CHA)<sub>2</sub>ZnBr<sub>4</sub>, could not be fully structurally solved at either 100 K or 298 K. The compound was found to be thermally stable up to 490 K, and to undergo a phase transition at 230 K.  Further characterization will be needed to better understand the properties of these two compounds and their possible applications.</p>
----------------------------------------------------------------------
diva2:1185488 no changes
----------------------------------------------------------------------
In diva2:1587325 abstract is: <p>Peptidoglycan (PGN) is a bacterial cell wall component and known to be recognized by various receptors or enzymes to lead the activation immune system. The general structure of PGN consists of sugar chains including N-acetylglutamine (GlcNAc), N-acetylmuramic acid (MurNAc) and cross-linked peptide chains. PGN fragments having D-Lac terminus peptides have been found from vancomycin-resistant enterococcus, but a chemically synthesized PGN fragment having a D-Lac terminus peptide has not been examined in detail. Thus, we focused on the synthesis of PGN fragment structures that include a D-Ala-D-Lac residue at the terminal part of the peptide chain. In order to synthesize these fragment structures, we planned to combine solid-phase synthesis (for the peptide- Lac part) and solution-phase synthesis (for glycan preparation and the condensation). This approach is advantageous for the preparation of peptidoglycan fragments having complex branched peptide moiety. First, we prepared the sugar moiety MurNAc derivative in solution-phase synthesis from a glucose derivative. While, the Lac-containing peptide was prepared with solid-phase peptide synthesis using 2-chlorotrityl chloride resin. Having this compound, the condensation of these two compounds gave the desired D-Lac-terminated peptidoglycan fragment.</p>

w='peptideLac' val={'c': 'peptide-Lac', 's': 'diva2:1587325', 'n': 'appears as "peptide- Lac" in original'}

corrected abstract:
<p>Peptidoglycan (PGN) is a bacterial cell wall component and known to be recognized by various receptors or enzymes to lead the activation immune system. The general structure of PGN consists of sugar chains including N-acetylglutamine (GlcNAc), N-acetylmuramic acid (MurNAc) and cross-linked peptide chains. PGN fragments having D-Lac terminus peptides have been found from vancomycin-resistant enterococcus, but a chemically synthesized PGN fragment having a D-Lac terminus peptide has not been examined in detail. Thus, we focused on the synthesis of PGN fragment structures that include a D-Ala-D-Lac residue at the terminal part of the peptide chain. In order to synthesize these fragment structures, we planned to combine solid-phase synthesis (for the peptide- Lac part) and solution-phase synthesis (for glycan preparation and the condensation). This approach is advantageous for the preparation of peptidoglycan fragments having complex branched peptide moiety. First, we prepared the sugar moiety MurNAc derivative in solution-phase synthesis from a glucose derivative. While, the Lac-containing peptide was prepared with solid-phase peptide synthesis using 2-chlorotrityl chloride resin. Having this compound, the condensation of these two compounds gave the desired D-Lac-terminated peptidoglycan fragment.</p>
----------------------------------------------------------------------
In diva2:855425 abstract is: <p>Two series of new mononuclear ruthenium complexes with hydrophobic or hydrophilic ligands [Ru(bda)L2] and [Ru(pdc)L3] (H2bda = 2,2'-bipyridine-6,6'-dicarboxylic acid; H2pdc = 2,6-pyridinedicarboxylic acid; L = pyridyl ligands) were synthesized and their electrochemical properties and catalytic activity toward water oxidation were examined. It was revealed that the hydrophobic ligands introduced to [Ru(bda)L2 ] improved the catalytic performance, ahnost twofold TON and TOF values were achieved compared to the [Ru(bda)] catalyst with hydrophilic ligands. The cyclic voltammogram of [Ru(bda)L2] exhibited marginal difference between the catalysts with hydrophobic ligands and hydrophilic ones, implying that the hydrophobic ligands promoted the catalytic activity by :lacilitating formation of a reaction intermediate dimer.</p>

w='pdc' val={'c': 'H<sub>2</sub>pdc = 2,6-pyridinedicarboxylic acid', 's': 'diva2:855425'}
w='ahnost' val={'c': 'almost', 's': 'diva2:855425', 'n': 'correct in original'}


corrected abstract:
<p>Two series of new mononuclear ruthenium complexes with hydrophobic or hydrophilic ligands [Ru(bda)L2] and [Ru(pdc)L3] (H2bda = 2,2'-bipyridine-6,6'-dicarboxylic acid; H2pdc = 2,6-pyridinedicarboxylic acid; L = pyridyl ligands) were synthesized and their electrochemical properties and catalytic activity toward water oxidation were examined. It was revealed that the hydrophobic ligands introduced to [Ru(bda)L2] improved the catalytic performance, almost twofold TON and TOF values were achieved compared to the [Ru(bda)] catalyst with hydrophilic ligands. The cyclic voltammogram of [Ru(bda)L2] exhibited marginal difference between the catalysts with hydrophobic ligands and hydrophilic ones, implying that the hydrophobic ligands promoted the catalytic activity by :lacilitating formation of a reaction intermediate dimer.</p>
----------------------------------------------------------------------
In diva2:1097854 abstract is: <p>The need of efficient and sustainable chemical processes in the field of organic chemistry is essential in order to meet the environmental demands that societies and industries require from us. One key chemical reaction that is widely used in industrial plants is olefin metathesis, a reaction that involves exchange and formation of C-C double bonds. It is a crucial reaction in polymerization reactions and as a means of connecting building blocks together. Consequently it is necessary to enable these reactions to work in aqueous media for increased sustainability and for applications in biochemistry. Carbenes are a class of compounds that constitute a crucial role as ligands in olefin metathesis catalysts. This project will aim to explore the water applicability of a certain category of carbenes, cyclic (alkyl)-(amino)carbenes (CAAC), which recently have caught attention due to their remarkable catalytic activity in organometallic catalysis. The CAAC based catalysis display high stability towards air and moisture, and thus making them attractive alternatives for industrial applications. Throughout the project, synthetic routes towards water soluble CAAC:s have been designed and investigated. Moreover, an approach of evaluating a CAAC-ruthenium catalyst in micellar solutions have been investigated and illustrated promising results regarding activity and stability.</p>


w='CAAC:s' val={'c': 'CAACs', 's': 'diva2:1097854', 'n': 'no full text'}

corrected abstract:
<p>The need of efficient and sustainable chemical processes in the field of organic chemistry is essential in order to meet the environmental demands that societies and industries require from us. One key chemical reaction that is widely used in industrial plants is olefin metathesis, a reaction that involves exchange and formation of C-C double bonds. It is a crucial reaction in polymerization reactions and as a means of connecting building blocks together. Consequently it is necessary to enable these reactions to work in aqueous media for increased sustainability and for applications in biochemistry. Carbenes are a class of compounds that constitute a crucial role as ligands in olefin metathesis catalysts. This project will aim to explore the water applicability of a certain category of carbenes, cyclic (alkyl)-(amino)carbenes (CAAC), which recently have caught attention due to their remarkable catalytic activity in organometallic catalysis. The CAAC based catalysis display high stability towards air and moisture, and thus making them attractive alternatives for industrial applications. Throughout the project, synthetic routes towards water soluble CAACs have been designed and investigated. Moreover, an approach of evaluating a CAAC-ruthenium catalyst in micellar solutions have been investigated and illustrated promising results regarding activity and stability.</p>
----------------------------------------------------------------------
In diva2:820958 abstract is: <p>There's a high risk that an unauthorized party can gain access to and use a consumer's iden-tity. This while the ability to control how and when a personal identity is used is small. The question to be answered were regarding how identity theft and online fraud could be mi-nimized and give the consumers a greater influence and more control over the management of their identity online.</p><p>The goal was to centralize and establish a common approach for identity management on-line, with greater benefits for consumers. Through central service individual consumers would be able to set conditions for which online shops and services would be able to access their identities and grant access in each specific transaction. This would remove the need for non-central control of identities and as a result remove the need for independent storage of identity information.</p><p>The solution would result in a system model with the potential to authenticate the consu-mer, managing conditions for how individual identity documents may be used online and to provide the consumer with a online history by sending notifications of events that has occurred with regard to a specific identity.</p><p>A prototype was developed to demonstrate the basic functionality in practice. This included the functionality to authenticate the consumer with Mobile BankID, send notifications about events and check existing conditions regarding a specific identity. This prototype came to consist of a simplified system according to the model developed, an associated API, and two models representing an online store and a payment provider that would utilize the functionality of the system by calling the API.</p><p>The proposed solution was evaluated through two interviews with experts in the fields of IT Security and e-commerce. The conclusion was that identity fraud would probably drop drastically and the individual consumer influence and awareness would be fortified. The main reason for this was considered to be primarily through the consistent and standardized way for authentication of and communication with the consumer. This would remove the individual risk for online services.</p><p>The challenge with this proposed solution is believed to be getting consumers, online re-tailers and payment providers to accept a central solution instead of relying on internally developed and disconnected solution.</p>

w='iden-tity' val={'c': 'identity', 's': 'diva2:820958', 'n': 'hyphenation at the end of a line'}

corrected abstract:
<p>There's a high risk that an unauthorized party can gain access to and use a consumer's identity. This while the ability to control how and when a personal identity is used is small. The question to be answered were regarding how identity theft and online fraud could be minimized and give the consumers a greater influence and more control over the management of their identity online.</p><p>The goal was to centralize and establish a common approach for identity management online, with greater benefits for consumers. Through central service individual consumers would be able to set conditions for which online shops and services would be able to access their identities and grant access in each specific transaction. This would remove the need for non-central control of identities and as a result remove the need for independent storage of identity information.</p><p>The solution would result in a system model with the potential to authenticate the consumer, managing conditions for how individual identity documents may be used online and to provide the consumer with a online history by sending notifications of events that has occurred with regard to a specific identity.</p><p>A prototype was developed to demonstrate the basic functionality in practice. This included the functionality to authenticate the consumer with Mobile BankID, send notifications about events and check existing conditions regarding a specific identity. This prototype came to consist of a simplified system according to the model developed, an associated API, and two models representing an online store and a payment provider that would utilize the functionality of the system by calling the API.</p><p>The proposed solution was evaluated through two interviews with experts in the fields of IT Security and e-commerce. The conclusion was that identity fraud would probably drop drastically and the individual consumer influence and awareness would be fortified. The main reason for this was considered to be primarily through the consistent and standardized way for authentication of and communication with the consumer. This would remove the individual risk for online services.</p><p>The challenge with this proposed solution is believed to be getting consumers, online retailers and payment providers to accept a central solution instead of relying on internally developed and disconnected solution.</p>
----------------------------------------------------------------------
In diva2:1519525 abstract is: <p>Climate change is the single largest challenge facing humanity in the 21st century. To tackle this challenge, renewable energies are seeing a large increase in primary energy share globally. The natural variableness of solar and wind requires energy storage to be used in conjuction with them for an energy system transition. Power-to-Gas (PtG) technologies offer an attractive solution by allowing conversion of electrical energy to hydrogen or methane, enabling cross-energy-network and cross-sectoral integration. This thesis investigates profitability of a PtG plant with a primary application of producing synthetic methane (SNG) for natural gas (NG) grid injection. A techno-economical model was created to simulate plant operation over one year and extrapolate the results for the project lifespan. The model was designed based off of a pilot project being developed in France named HYCAUNAIS and used partner as well as literature data for processing. Due to limitations inlocal NG grid capacity, several scenarios were investigated that included adding additional investments that allow increased operational time and revenue streams, including: fixed electrical price or day-ahead (DA) market participation; mesh upgrade for increased NG grid capacity; and CH4 and H2 mobility. Electrolyser participation in the frequency containment reserve (FCR) was also considered for increased profitability. The results determined the standard case scenario (no additional investments) with participation in the DA electricity market was the most attractive in terms of three objectives investigated: net present value (NPV), payback period (PBP) and levelized cost of methane (LCOM). The operational hours of the standard case was found to be approximately 90% of the year; production was not hindered by limited grid capacity sufficiently to deem additional investments necessary. Further, participation in the DA market should be determined by a cut-off willingness to pay (WTP) for electricity as opposed to marginal profit (MP). Using WTP as the determining factor allowed increased operational hours and lower LCOM. However, in all of the scenarios investigated, none were profitable; meaning that market conditions still need to greatly improve before PtG can gain momentum. A sensitivity analysis was done on the standard case scenario to see which parameters influence profitability the most and should be the focus of further research and development. The SNG tariff was found to be the most influential on NPV, requiring a tariff of at least 188 e=MWh (120 e=MWh was used for modeling) to be profitable. Electricity price was the second most inuential and required an average market price of 25 e=MWh to be profitable. As PtG technologies can provide several external benefits that are not economically realized by investors, monetization of them could provide a means of improving profitability. This includes, grid balancing and exibility, decarbonization, lower grid costs and improved energy security. Inconclusion, capital costs of equipment, electricity prices and fees associated to them, and tariffs for green gases all need to improve dramatically for SNG production tobe an attractive solution for electricity curtailment and decarbonization.</p>

w='exibility' val={'c': 'flexibility', 's': 'diva2:1519525', 'n': 'missing ligature'}
w='conjuction' val={'c': 'conjunction', 's': 'diva2:1519525', 'n': 'error in original'}
w='inuential' val={'c': 'influential', 's': 'diva2:1519525', 'n': 'missing ligature'}

corrected abstract:
<p>Climate change is the single largest challenge facing humanity in the 21st century. To tackle this challenge, renewable energies are seeing a large increase in primary energy share globally. The natural variableness of solar and wind requires energy storage to be used in conjuction with them for an energy system transition. Power-to-Gas (PtG) technologies offer an attractive solution by allowing conversion of electrical energy to hydrogen or methane, enabling cross-energy-network and cross-sectoral integration. This thesis investigates profitability of a PtG plant with a primary application of producing synthetic methane (SNG) for natural gas (NG) grid injection. A techno-economical model was created to simulate plant operation over one year and extrapolate the results for the project lifespan. The model was designed based off of a pilot project being developed in France named HYCAUNAIS and used partner as well as literature data for processing. Due to limitations in local NG grid capacity, several scenarios were investigated that included adding additional investments that allow increased operational time and revenue streams, including: fixed electrical price or day-ahead (DA) market participation; mesh upgrade for increased NG grid capacity; and CH<sub>4</sub> and H<sub>2</sub> mobility. Electrolyser participation in the frequency containment reserve (FCR) was also considered for increased profitability. The results determined the standard case scenario (no additional investments) with participation in the DA electricity market was the most attractive in terms of three objectives investigated: net present value (NPV), payback period (PBP) and levelized cost of methane (LCOM). The operational hours of the standard case was found to be approximately 90% of the year; production was not hindered by limited grid capacity sufficiently to deem additional investments necessary. Further, participation in the DA market should be determined by a cut-off willingness to pay (WTP) for electricity as opposed to marginal profit (MP). Using WTP as the determining factor allowed increased operational hours and lower LCOM. However, in all of the scenarios investigated, none were profitable; meaning that market conditions still need to greatly improve before PtG can gain momentum. A sensitivity analysis was done on the standard case scenario to see which parameters influence profitability the most and should be the focus of further research and development. The SNG tariff was found to be the most influential on NPV, requiring a tariff of at least 188 €MWh (120 €MWh was used for modeling) to be profitable. Electricity price was the second most influential and required an average market price of 25 €MWh to be profitable. As PtG technologies can provide several external benefits that are not economically realized by investors, monetization of them could provide a means of improving profitability. This includes, grid balancing and flexibility, decarbonization, lower grid costs and improved energy security. In conclusion, capital costs of equipment, electricity prices and fees associated to them, and tariffs for green gases all need to improve dramatically for SNG production to be an attractive solution for electricity curtailment and decarbonization.</p>
----------------------------------------------------------------------
In diva2:1876253 abstract is: <p>Hemophilia A is a X-linked hereditary disease that effects the coagulation of the blood and is due to the absence or deficiency of the protein factor VIII (FVIII). Nuwiq® is a recombinant FVIII B-domain deleted product from Octapharma AB that treats hemophilia A and are produced with human embryonic kidney cell lines. The fVIII protein is 170 kDa full length but are cleaved into a 90 kDa chain and an 80 kDa chain before release into circulation in the body. Due to regulations the 170 kDa chain needs to be below a certain limit during the cultivation process of the pharmaceutical product and the relative distribution between the three isoforms needs to be measured. Today the sample is prepared and analysed through a drop column and reverse-phase high performance liquid chromatography (HPLC). The drop column is a time-consuming process and the aim for this project was to find a method that can reduce the time for this analysis step. This study investigated the possibilities to use Western Blot as a method and to use an ÄKTA pure system for the preparation of the samples and then analyse with HPLC. Results from Western Blot showed similar results as the HPLC analysis and a linearity, accuracy and precision study were performed. The linearity study showed promising results for a sample concentration of 2 IU/mL to be used. Accuracy testing showed similar but slightly different results between the two methods and some differences are expected since the HPLC analyses on hydrophobicity and Western Blot uses antibodies and chemiluminescence. There would need to be a more extensive precision study before any decision could be made about changing the analysis method. Western Blot showed promising results and would shorten the method time with around 1-2 days. The first ÄKTA purification runs with cultivation samples did not show any protein levels in the eluate, but with a higher protein amount put on the column from a sample with less cell debris the eluate had good amount of protein in it. The analysis of the eluate with Western Blot showed good results. Further testing of the ÄKTA purific tion is needed to consider it an option to use at the laboratory for analysis of factor VIII protein. </p>

w='purific' val={'c': 'purification', 's': 'diva2:1876253', 'n': 'correct in original'}

corrected abstract:
<p>Hemophilia A is a X-linked hereditary disease that effects the coagulation of the blood and is due to the absence or deficiency of the protein factor VIII (FVIII). Nuwiq® is a recombinant FVIII B-domain deleted product from Octapharma AB that treats hemophilia A and are produced with human embryonic kidney cell lines. The fVIII protein is 170 kDa full length but are cleaved into a 90 kDa chain and an 80 kDa chain before release into circulation in the body. Due to regulations the 170 kDa chain needs to be below a certain limit during the cultivation process of the pharmaceutical product and the relative distribution between the three isoforms needs to be measured. Today the sample is prepared and analysed through a drop column and reverse-phase high performance liquid chromatography (HPLC). The drop column is a time-consuming process and the aim for this project was to find a method that can reduce the time for this analysis step. This study investigated the possibilities to use Western Blot as a method and to use an ÄKTA pure system for the preparation of the samples and then analyse with HPLC. Results from Western Blot showed similar results as the HPLC analysis and a linearity, accuracy and precision study were performed. The linearity study showed promising results for a sample concentration of 2 IU/mL to be used. Accuracy testing showed similar but slightly different results between the two methods and some differences are expected since the HPLC analyses on hydrophobicity and Western Blot uses antibodies and chemiluminescence. There would need to be a more extensive precision study before any decision could be made about changing the analysis method. Western Blot showed promising results and would shorten the method time with around 1-2 days. The first ÄKTA purification runs with cultivation samples did not show any protein levels in the eluate, but with a higher protein amount put on the column from a sample with less cell debris the eluate had good amount of protein in it. The analysis of the eluate with Western Blot showed good results. Further testing of the ÄKTA purification is needed to consider it an option to use at the laboratory for analysis of factor VIII protein.</p>
----------------------------------------------------------------------
In diva2:945315 abstract is: <p>In older landfills several substances exist that slowly leaches from the landfill and has a negative impact on the environment. The project has tested a new method, TreeWell, for leachate treatment by bioremediation installed at Ragn-Sells landfill in Brista. The project has documented the installation, initiated the sampling and analysis of incoming and outgoing leachate and focus in this study was the reduction of BOD<sub>7</sub>, nitrogen, ammonium nitrogen, phosphorus and pH levels in the water. The result will be the basis for further studies of the technology.</p><p>A landfill goes through several phases during its lifetime, which affects the leachate character. The leachate from an old landfill has a large amount of organic compounds and it is common to use a leachate treatment plant with biological treatment. The leachate treatment on Brista today consists of two leachate ponds and irrigation of a land and plant systems.</p><p>TreeWell is a biological treatment method that previously has been tested, and is used today, for the purification of water from individual sewers. Both leachate and wastewater is rich in nutrients and organic compounds and therefore TreeWell should be suitable for the purification of leachate. The technology is based on two tanks in series, a biofilm reactor and an airliftbioreactor. In the system lives bacteria, protozoa and plants that together purify the water. The system in Brista is installed near the landfill where the leachate exits the landfill. The tanks are partly buried to facilitate the sampling.</p><p>For the pilot plant of TreeWell, the investment cost amounted to 400 000 SEK and the installations cost to 125 000 SEK. This plant is expected to handle up to 12 m<sup>3</sup> per day. Annuity for the system is estimated to 70 000 SEK which equals a cost per cubic meter of water at 16 SEK/m<sup>3 </sup>over a period of 20 years.</p><p>Today’s requirements for better water treatment for landfills and lower levels of compounds in the water demands more effective and new treatment approaches.</p><p>Analyzes on the water in- and outflow from the system is a base package for landfills with the addition of BOD<sub>7</sub>, metals, easily soluble and less soluble organic compounds, phenols and organic analyzes according to Environmental Protection Agency guidelines. Ragn-Sells cooperate with Alcontrol and it is from their catalog that the package has been selected. Samples for organic compounds have been analyzed once and the other parameters have been analyzed twice during the project. Samples for ammonia nitrogen were taken weekly and analyzed at Ragn-Sells laboratory at Högbytorp.</p><p>TreeWell has been successfully installed in Brista and started to test run the process in November 2015. TreeWell has a good reduction of BOD<sub>7</sub>, phosphorus and TOC compared to the common techniques. Although the reduction of nitrogen and ammonium nitrogen is lower for TreeWell than other studied leache treatment plants in this study.</p><p>During the initial period of testing the system after installation, which is included in this report, have not enough information been collected to determine if TreeWell pass the emissions requirement at Brista today. Derpite this TreeWell show good potential and during the first sampling the level of BOD<sub>7</sub>, phosphorusand pH level have been reduced and is below the level allowed for emissions. Also the content of several metals and organic compounds have been reduced. The level varies with several factors such as temperature, flow and retention time.</p>

w='leache' val={'c': 'leaches', 's': 'diva2:945315', 'n': 'correct in original'} # first instance, the last instance should stay as "leche"
I would actually think that the word to use is "leachate". See for example: https://www.lunduniversity.lu.se/lup/publication/4675844

w='Derpite' val={'c': 'Despite', 's': 'diva2:945315', 'n': 'error in original'}

corrected abstract:
<p>In older landfills several substances exist that slowly leaches from the landfill and has a negative impact on the environment. The project has tested a new method, TreeWell, for leachate treatment by bioremediation installed at Ragn-Sells landfill in Brista. The project has documented the installation, initiated the sampling and analysis of incoming and outgoing leachate and focus in this study was the reduction of BOD<sub>7</sub>, nitrogen, ammonium nitrogen, phosphorus and pH levels in the water. The result will be the basis for further studies of the technology.</p><p>A landfill goes through several phases during its lifetime, which affects the leachate character. The leachate from an old landfill has a large amount of organic compounds and it is common to use a leachate treatment plant with biological treatment. The leachate treatment on Brista today consists of two leachate ponds and irrigation of a land and plant systems.</p><p>TreeWell is a biological treatment method that previously has been tested, and is used today, for the purification of water from individual sewers. Both leachate and wastewater is rich in nutrients and organic compounds and therefore TreeWell should be suitable for the purification of leachate. The technology is based on two tanks in series, a biofilm reactor and an airlift bioreactor. In the system lives bacteria, protozoa and plants that together purify the water. The system in Brista is installed near the landfill where the leachate exits the landfill. The tanks are partly buried to facilitate the sampling.</p><p>For the pilot plant of TreeWell, the investment cost amounted to 400 000 SEK and the installations cost to 125 000 SEK. This plant is expected to handle up to 12 m<sup>3</sup> per day. Annuity for the system is estimated to 70 000 SEK which equals a cost per cubic meter of water at 16 SEK/m<sup>3</sup> over a period of 20 years.</p><p>Today’s requirements for better water treatment for landfills and lower levels of compounds in the water demands more effective and new treatment approaches.</p><p>Analyzes on the water in- and outflow from the system is a base package for landfills with the addition of BOD<sub>7</sub>, metals, easily soluble and less soluble organic compounds, phenols and organic analyzes according to Environmental Protection Agency guidelines. Ragn-Sells cooperate with Alcontrol and it is from their catalog that the package has been selected. Samples for organic compounds have been analyzed once and the other parameters have been analyzed twice during the project. Samples for ammonia nitrogen were taken weekly and analyzed at Ragn-Sells laboratory at Högbytorp.</p><p>TreeWell has been successfully installed in Brista and started to test run the process in November 2015. TreeWell has a good reduction of BOD<sub>7</sub>, phosphorus and TOC compared to the common techniques. Although the reduction of nitrogen and ammonium nitrogen is lower for TreeWell than other studied leache treatment plants in this study.</p><p>During the initial period of testing the system after installation, which is included in this report, have not enough information been collected to determine if TreeWell pass the emissions requirement at Brista today. Derpite this TreeWell show good potential and during the first sampling the level of BOD<sub>7</sub>, phosphorus and pH level have been reduced and is below the level allowed for emissions. Also the content of several metals and organic compounds have been reduced. The level varies with several factors such as temperature, flow and retention time.</p>
----------------------------------------------------------------------
In diva2:778984 abstract is: <p>An advanced aftertreatment system is applied in heavy duty trucks in order to reach the strict requirements of the Euro VI emission standard. The NH3 emission limit is managed by installation of Ammonia Slip Catalysts (ASC) downstream of the NOX reduction catalyst (SCR). ASC’s could be exposed to high temperatures and sulfur (S) during operation. Therefore, ASC’s must be resistant to thermal deactivation and chemical poisoning that otherwise could lead to changes in activity and selectivity.</p><p>The different effects of accelerated hydrothermal aging and SO2 poisoning on two ASCs were investigated. The ASCs are made up by dual-layers with Pt/Al2O3 on the bottom for NH3 oxidation and a metal-based zeolite for NOX reduction on top. Cu-ASC has a Cu-based zeolite and Fe-ASC a Fe-based zeolite as NOX reduction layer. Catalyst performance tests and several characterization methods were used, i.e. surface area analysis, scanning electron microscopy with energy dispersive x-ray (SEM-EDX), X-ray florescence (XRF), X-ray diffraction (XRD), temperature programmed desorption of NH3 (NH3-TPD) and time-of-flight secondary ion mass spectrometry (TOF-SIMS).</p><p>Both ASCs reached the 10 ppm requirement of NH3 outlet concentration throughout all accelerated aging treatments. Cu-ASC had significantly lower NOX and N2O selectivity than Fe-ASC, i.e. better catalyst performance. However, Cu-ASC exhibited less thermal resistance than Fe-ASC. The catalyst performances approached each other as the hydrothermal treatment became tougher. The proposed thermal deactivation mechanisms of Cu-ASC is Cu-sintering, while carrier sintering was moderate. The NH3-TPD curve showed loss of weak NH3 adsorption sites (Cu-sites). Furthermore, XRD, SEM and NH3-TPD indicated that the zeolite structure remains unaffected. The major deactivation mechanisms for Fe-ASC are dealumination and sintering revealed by SEM, XRD, NH3-TPD and surface area analysis. Pt sintering is believed to be absent because of unaffected NH3 activity and N2O selectivity for both Cu-ASC and Fe-ASC.</p><p>Cu-ASC were severely deactivated by SO2 poisoning resulting in higher NOX and N2O selectivity. Fe-ASC on the other hand was promoted by SO2 poisoning with decreased NOX- and N2O selectivity. The results could be attributed to formation of ammonium sulfates. TOF-SIMS and EDX could verify sulfates in both washcoat layers but with accumulation in the Pt/Al2O3 washcoat for Fe-ASC while sulfates were present in the zeolite only for Cu-ASC.</p>

w='Pt/Al2O3' val={'c': 'Pt/Al<sub>2</sub>O<sub>3</sub>', 's': ['diva2:895301', 'diva2:778984'], 'n': 'correct in original'}

corrected abstract:
<p>An advanced aftertreatment system is applied in heavy duty trucks in order to reach the strict requirements of the Euro VI emission standard. The NH<sub>3</sub> emission limit is managed by installation of Ammonia Slip Catalysts (ASC) downstream of the NO<sub>X</sub> reduction catalyst (SCR). ASC’s could be exposed to high temperatures and sulfur(S) during operation. Therefore, ASC’s must be resistant to thermal deactivation and chemical poisoning that otherwise could lead to changes in activity and selectivity.</p><p>The different effects of accelerated hydrothermal aging and SO<sub>2</sub> poisoning on two ASCs were investigated. The ASCs are made up by dual-layers with Pt/Al<sub>2</sub>O<sub>3</sub> on the bottom for NH<sub>3</sub> oxidation and a metal-based zeolite for NO<sub>X</sub> reduction on top. Cu-ASC has a Cu-based zeolite and Fe-ASC a Fe-based zeolite as NO<sub>X</sub> reduction layer. Catalyst performance tests and several characterization methods were used, i.e. surface area analysis, scanning electron microscopy with energy dispersive x-ray (SEM-EDX), X-ray florescence (XRF), X-ray diffraction (XRD), temperature programmed desorption of NH<sub>3</sub> (NH<sub>3</sub>-TPD) and time-of-flight secondary ion mass spectrometry (TOF-SIMS).</p><p>Both ASCs reached the 10 ppm requirement of NH<sub>3</sub> outlet concentration throughout all accelerated aging treatments. Cu-ASC had significantly lower NO<sub>X</sub> and N<sub>2</sub>O selectivity than Fe-ASC, i.e. better catalyst performance. However, Cu-ASC exhibited less thermal resistance than Fe-ASC. The catalyst performances approached each other as the hydrothermal treatment became tougher. The proposed thermal deactivation mechanisms of Cu-ASC is Cu-sintering, while carrier sintering was moderate. The NH<sub>3</sub>-TPD curve showed loss of weak NH<sub>3</sub> adsorption sites (Cu-sites). Furthermore, XRD, SEM and NH<sub>3</sub>-TPD indicated that the zeolite structure remains unaffected. The major deactivation mechanisms for Fe-ASC are dealumination and sintering revealed by SEM, XRD, NH<sub>3</sub>-TPD and surface area analysis. Pt sintering is believed to be absent because of unaffected NH<sub>3</sub> activity and N<sub>2</sub>O selectivity for both Cu-ASC and Fe-ASC.</p><p>Cu-ASC were severely deactivated by SO<sub>2</sub> poisoning resulting in higher NO<sub>X</sub> and N<sub>2</sub>O selectivity. Fe-ASC on the other hand was promoted by SO<sub>2</sub> poisoning with decreased NO<sub>X</sub>- and N<sub>2</sub>O selectivity. The results could be attributed to formation of ammonium sulfates. TOF-SIMS and EDX could verify sulfates in both washcoat layers but with accumulation in the Pt/Al<sub>2</sub>O<sub>3</sub> washcoat for Fe-ASC while sulfates were present in the zeolite only for Cu-ASC.</p>
----------------------------------------------------------------------
In diva2:1592494 abstract is: <p>Flow reactors with immobilized enzymes were prepared through non-covalent adsorption of a polymer-enzyme conjugate on silicate surfaces. Horseradish peroxidase (HRP) and poly-D-lysine (PDL) were first modified separately and then conjugated together via bis-aryl hydrazone (BAH) bond formation, followed by the immobilization of the conjugate (PDL-HRP) on the inner surface of glass micropipettes and mesoporous silica monoliths. The incubation protocol was optimized, and the enzymatic stability of the reactors was systematically investigated by exposing the reactors to various stresses. The immobilization method showed good reproducibility, and the reactors prepared showed good robustness against various types of stress.</p><p>Then, the reactors were used to quantify hydrogen peroxide present in diluted honey. The diammonium salt of 2, 2‐azino‐bis(3‐ethylbenzothiazoline‐6‐sulfonic acid) (ABTS<sup>2-</sup>) and 2’,7’-dichlorodihydrofluorescein (DCFH<sub>2</sub>) were used as chromogenic substrates, which reacted with H<sub>2</sub>O<sub>2</sub> in the presence of HRP and their products could be easily quantified by UV/Vis spectrophotometry. The hydrogen peroxide concentration was determined using a calibration curve and the standard addition method. While the radical-scavenging ability of honey made the use of ABTS<sup>2-</sup> as a chromogenic substrate challenging during the quantification of hydrogen peroxide, DCFH<sub>2</sub> proved to be suitable thanks to the stability of its product (2’,7’-Dichlorofluorescein (DCF)) in diluted honey and was therefore used for the quantification with immobilized enzymes. Compared to the quantification using native HRP in bulk solution, the flow analysis using reactors with immobilized enzymes showed similar results but with higher reproducibility.</p>

w='ABTS' val={'c': 'ABTS<sup>2-</sup>', 's': 'diva2:1592494'}

corrected abstract:
<p>Flow reactors with immobilized enzymes were prepared through non-covalent adsorption of a polymer-enzyme conjugate on silicate surfaces. Horseradish peroxidase (HRP) and poly-D-lysine (PDL) were first modified separately and then conjugated together via bis-aryl hydrazone (BAH) bond formation, followed by the immobilization of the conjugate (PDL-HRP) on the inner surface of glass micropipettes and mesoporous silica monoliths. The incubation protocol was optimized, and the enzymatic stability of the reactors was systematically investigated by exposing the reactors to various stresses. The immobilization method showed good reproducibility, and the reactors prepared showed good robustness against various types of stress.</p><p>Then, the reactors were used to quantify hydrogen peroxide present in diluted honey. The diammonium salt of 2, 2‐azino‐bis(3‐ethylbenzothiazoline‐6‐sulfonic acid) (ABTS<sup>2-</sup>) and 2’,7’-dichlorodihydrofluorescein (DCFH<sub>2</sub>) were used as chromogenic substrates, which reacted with H<sub>2</sub>O<sub>2</sub> in the presence of HRP and their products could be easily quantified by UV/Vis spectrophotometry. The hydrogen peroxide concentration was determined using a calibration curve and the standard addition method. While the radical-scavenging ability of honey made the use of ABTS<sup>2-</sup> as a chromogenic substrate challenging during the quantification of hydrogen peroxide, DCFH<sub>2</sub> proved to be suitable thanks to the stability of its product (2’,7’-Dichlorofluorescein (DCF)) in diluted honey and was therefore used for the quantification with immobilized enzymes. Compared to the quantification using native HRP in bulk solution, the flow analysis using reactors with immobilized enzymes showed similar results but with higher reproducibility.</p>
----------------------------------------------------------------------
In diva2:849535 abstract is: <p>In the tumor microenvironment there are many different cell types present and among these, immune cells display a large proportion. Central players in the tumor immunity are macrophages that come in two different phenotypes, the M1 and M2 macrophages. M1 polarized macrophages are tumor suppressive while M2 polarized macrophages support tumor growth. The factors that contribute to the skewing of macrophages from one phenotype to another are under investigation. Interestingly, our lab has identified Immune Semaphorin 3A (Sema3A) as a participating plaer in regulating the accumulation of anti-tumoral M1 macrophages leading to a suppression of tumor growth.</p><p> </p><p>In light of these data this thesis has focused on the role of endogenous Sema3A in the tumor microenvironment. A tumor cell line expressing shRNA against Sema3A mRNA was generated using lentiviral mediated gene therapy. This knockdoen cell line showed 72% lower mRNA expression compared to control and was evaluated in vivo by monitoring tumor progression in female BALB/c mice. The immune cell composition of the tumors was analysed using flow cytometry. The results from the in vivo experiment show that endogenous Sema3A has a limited effect on tumor progression. A slight shift to a more tumor supportive immune profile was observed in the knockdown tumors. Moreover, a virus for transducing cells to overecpress Sema3A under asuitable promoter for systemic delivery was generated.</p>

w='knockdoen' val={'c': 'knockdown', 's': 'diva2:849535', 'n': 'correct in original'}
w='overecpress' val={'c': 'overexpress', 's': 'diva2:849535', 'n': 'correct in original'}
w='plaer' val={'c': 'player', 's': 'diva2:849535', 'n': 'correct in original'}

corrected abstract:
<p>In the tumor microenvironment there are many different cell types present and among these, immune cells display a large proportion. Central players in the tumor immunity are macrophages that come in two different phenotypes, the M1 and M2 macrophages. M1 polarized macrophages are tumor suppressive while M2 polarized macrophages support tumor growth. The factors that contribute to the skewing of macrophages from one phenotype to another are under investigation. Interestingly, our lab has identified Immune Semaphorin 3A (Sema3A) as a participating player in regulating the accumulation of anti-tumoral M1 macrophages leading to a suppression of tumor growth.</p><p>In light of these data this thesis has focused on the role of endogenous Sema3A in the tumor microenvironment. A tumor cell line expressing shRNA against Sema3A mRNA was generated using lentiviral mediated gene therapy. This knockdown cell line showed 72 % lower mRNA expression compared to control and was evaluated <em>in vivo</em> by monitoring tumor progression in female BALB/c mice. The immune cell composition of the tumors was analyzed using flow cytometry. The results from the <em>in vivo</em> experiment show that endogenous Sema3A has a limited effect on tumor progression. A slight shift to a more tumor supportive immune profile was observed in the knockdown tumors. Moreover, a virus for transducing cells to overexpress Sema3A under a suitable promoter for systemic delivery was generated.</p>
----------------------------------------------------------------------
In diva2:1770889 abstract is: <p>One ofthe ways many companies in have tried to mitigate environmental impacts, contribution to climate change and carbon footprint is to phase out products that are derived from fossil fuel. Procudan is a danish packaging company that have decided to phase out one of their petroleum products, namely mineral oil waxes used for their food packaging. As an substitute for their current product Procudan has instead made a biobased wax with beeswax as its main component. Before utilising this wax however their thermal properties need to be analysed. Mettler-Toledo was tasked to perform the analysis of these new biobased wax formulations, their raw material components, additives as well as the currently used waxes and raw materials. The properties that were determined was when the waxes melted and crystallised, the enthalpies of each reactions, and the degradation effects of oxygen and nitrogen respectively. These analysis were done using DSC and OJT, TGA and GC-MS. After these results were evaluated they were compared with that of the mineral oil waxes. What resulted from the DSC analysis was to show that the biobased waxes contained a lot of thermal history and required somewhat more energy to melt and crystallise, which potentially will increase the energy costs for cus(omers. At the same time the TGA and OIT showed that there was little concern for early degradation oj the biobased waxes by Oxygen and Nitrogen gases. Complete degradation of any of the waxes did not occur until the waxes reached temperatures far higher than any production parameters would have. As the GC-MS showed, when this eventual degradation does occur in oxygen environments Acetic acid is the most common product; followed by a variety of alkenes, aldehydes and alkanes and other organic compounds. At the end of the study the biobased waxes does seem rather viable with the production parameters, with the biobased wax formulations 2 and 5 being more recommended for future applications as they had similar properties to mineral oil waxes.</p>

w='omers' val={'c': 'customers', 's': 'diva2:1770889', 'n': 'no full text'}
w='OJT' val={'c': 'OIT', 's': 'diva2:1770889', 'n': 'no full text'} # Oxygen Induction Time.

corrected abstract:
<p>One of the ways many companies in have tried to mitigate environmental impacts, contribution to climate change and carbon footprint is to phase out products that are derived from fossil fuel. Procudan is a danish packaging company that have decided to phase out one of their petroleum products, namely mineral oil waxes used for their food packaging. As an substitute for their current product Procudan has instead made a biobased wax with beeswax as its main component. Before utilising this wax however their thermal properties need to be analysed. Mettler-Toledo was tasked to perform the analysis of these new biobased wax formulations, their raw material components, additives as well as the currently used waxes and raw materials. The properties that were determined was when the waxes melted and crystallised, the enthalpies of each reactions, and the degradation effects of oxygen and nitrogen respectively. These analysis were done using DSC and OIT, TGA and GC-MS. After these results were evaluated they were compared with that of the mineral oil waxes. What resulted from the DSC analysis was to show that the biobased waxes contained a lot of thermal history and required somewhat more energy to melt and crystallise, which potentially will increase the energy costs for customers. At the same time the TGA and OIT showed that there was little concern for early degradation oj the biobased waxes by Oxygen and Nitrogen gases. Complete degradation of any of the waxes did not occur until the waxes reached temperatures far higher than any production parameters would have. As the GC-MS showed, when this eventual degradation does occur in oxygen environments Acetic acid is the most common product; followed by a variety of alkenes, aldehydes and alkanes and other organic compounds. At the end of the study the biobased waxes does seem rather viable with the production parameters, with the biobased wax formulations 2 and 5 being more recommended for future applications as they had similar properties to mineral oil waxes.</p>
----------------------------------------------------------------------
In diva2:444133 abstract is: <p>The purpose of this diploma work was to compound different field grading material by adding fillers (ZnO residue and carbon black) into two different polymers polyamide 6,6 and poly (butylenes terepthalate) (PBTP) and investigate the effect of filler concentration on mechanical and electrical properties. Three different concentration (15vol.%, 25vol.% and 30vol.%) of ZnO residue in combination with two different concentration (5vol.%, 10vol.%) of carbon black were mixed in into the polymer. The composites were compounded using a twin screw extruder and pellitizer.  Sheet samples for testing were prepared by compression moulding. The dielectric response, electrical breakdown strength and tensile properties were evaluated.</p>
<p>The dielectric response properties were tested in the frequency range of 0.01 Hz- 1 kHz and at 30, 60 and 90 °C. Filler concentration into the matrix increased the permittivity and loss factor. The highest permittivity and loss was obtained at low frequencies (f&lt; 1 Hz), due to electrical conduction. At 30 °C, PBTP with 15vol.% ZnO and 5vol.% CB showed the minimum permittivity (12.9) and loss (0.19) at 46Hz. whereas corresponding PA composite showed lower permittivity 7.6 and loss 0.03.</p>
<p>The breakdown strength decreased with the addition of filler materials. PBTP composite having 15vol.% ZnO and 5vol.% CB showed the highest strength (16 kV/mm) among the PBTP composites, but it had lower strength than the corresponding PA 6,6 composite (21 kV/mm). Addition of black reduced the breakdown strength more than ZnO.</p>
<p>Addition of ZnO and CB increased the tensile strength of the material up to a certain concentration; 25% ZnO and 10% CB concentration showed the maximum strength (43 MPa) in PBTP composites. But with same composition the PA 6,6 composite showed higher tensile strength (67 MPa) than the PBTP composite (30 MPa). In elongation PBTP composite showed value (0.3%) which is 12.5% of elongation in PA 6,6 composite.</p>
<p>In conclusion the most promising formulation for field grading application is PA 6,6 filled with 15% ZnO and 5% CB, exhibiting a real permittivity of 7.6 and a dielectric loss of 0.03 at 46 Hz at 50C; electrical breakdown strength of 21 kV/mm, and tensile strength and elongation at break of 67 MPa and 0.4% respectively.</p>

w='terepthalate' val={'c': 'terephthalate', 's': 'diva2:444133', 'n': 'no full text'}
w='Hz- 1' val={'c': 'Hz 1', 's': 'diva2:444133', 'n': 'no full text'}
w='Hz1' val={'c': 'Hz 1', 's': 'diva2:444133', 'n': 'no full text'}

corrected abstract:
<p>The purpose of this diploma work was to compound different field grading material by adding fillers (ZnO residue and carbon black) into two different polymers polyamide 6,6 and poly (butylenes terephthalate) (PBTP) and investigate the effect of filler concentration on mechanical and electrical properties. Three different concentration (15vol.%, 25vol.% and 30vol.%) of ZnO residue in combination with two different concentration (5vol.%, 10vol.%) of carbon black were mixed in into the polymer. The composites were compounded using a twin screw extruder and pellitizer.  Sheet samples for testing were prepared by compression moulding. The dielectric response, electrical breakdown strength and tensile properties were evaluated.</p>
<p>The dielectric response properties were tested in the frequency range of 0.01 Hz - 1 kHz and at 30, 60 and 90 °C. Filler concentration into the matrix increased the permittivity and loss factor. The highest permittivity and loss was obtained at low frequencies (f &lt; 1 Hz), due to electrical conduction. At 30 °C, PBTP with 15 vol.% ZnO and 5 vol.% CB showed the minimum permittivity (12.9) and loss (0.19) at 46Hz. whereas corresponding PA composite showed lower permittivity 7.6 and loss 0.03.</p>
<p>The breakdown strength decreased with the addition of filler materials. PBTP composite having 15vol.% ZnO and 5vol.% CB showed the highest strength (16 kV/mm) among the PBTP composites, but it had lower strength than the corresponding PA 6,6 composite (21 kV/mm). Addition of black reduced the breakdown strength more than ZnO.</p>
<p>Addition of ZnO and CB increased the tensile strength of the material up to a certain concentration; 25% ZnO and 10% CB concentration showed the maximum strength (43 MPa) in PBTP composites. But with same composition the PA 6,6 composite showed higher tensile strength (67 MPa) than the PBTP composite (30 MPa). In elongation PBTP composite showed value (0.3%) which is 12.5% of elongation in PA 6,6 composite.</p>
----------------------------------------------------------------------
In diva2:1455922 abstract is: <p>In commercial lithium ion batteries graphite is often used in the anode material, mainly because it can reversibly contain lithium ions between the layers. However, graphite has a limited energy density and research is therefore being performed to find methods that will increase the energy density of the anode material. Silicon, which has a significantly higher energy density, can be intercalated in the graphite to increase the energy density. However, in order to avoid a destructive volume expansion that occurs during the lithiation, the intercalation must be performed with silicon nanoparticles. A scalable method for producing silicon nanoparticles on nanographite flakes has been developed and presented in an article by Phadatare et al.</p><p>The purpose behind this work has been to lay the foundation for large-scale production of silicon- based anodes intended to be used in lithium-ion batteries to provide them with increased capacity. The aim of the work was to present whether the method is repeatable and how different parameters affect the results for an upscaling, which was done through laboratory investigations. The results confirmed that the method presented in the article is repeatable and that the process should be carried out using a tube furnace. The results also showed that the temperature of the oven process should not be lowered to 600 °C, but that there is potential to lower it from the current 800 °C. At the current temperature, the percentage of silicon should not be increased to ≥ 47 %, but is considered to have the potential to be increased from the current 33 %. Only a small proportion of silicon nanoparticles was formed when a silicon powder of a different quality than that described in the article was used and showed that the choice of the silicon starting material is of great importance. The results further revealed that the dispersant polyvinyl alcohol, PVA, cannot be excluded. The results showed that there is potential to reduce the proportion of PVA, but that it should not be reduced to as much as half the concentration that is presented in the article. The results showed that the process must be carried out in an oxygen-poor environment in order not to cause oxidation of the nanographite. However, the results also indicated that the mechanism for the formation of silicon nanoparticles is not oxygen sensitive and that the mechanism, if the existing theory is correct, does not appear to be dependent on PVA beeing the source of hydrogen.</p>

w='beeing' val={'c': 'being', 's': ['diva2:862341', 'diva2:739841', 'diva2:1455922']}
The actual thesis uses the word "beeing" in the last sentence.


corrected abstract:
<p>In commercial lithium ion batteries graphite is often used in the anode material, mainly because it can reversibly contain lithium ions between the layers. However, graphite has a limited energy density and research is therefore being performed to find methods that will increase the energy density of the anode material. Silicon, which has a significantly higher energy density, can be intercalated in the graphite to increase the energy density. However, in order to avoid a destructive volume expansion that occurs during the lithiation, the intercalation must be performed with silicon nanoparticles. A scalable method for producing silicon nanoparticles on nanographite flakes has been developed and presented in an article by Phadatare et al.</p><p>The purpose behind this work has been to lay the foundation for large-scale production of silicon-based anodes intended to be used in lithium-ion batteries to provide them with increased capacity. The aim of the work was to present whether the method is repeatable and how different parameters affect the results for an upscaling, which was done through laboratory investigations. The results confirmed that the method presented in the article is repeatable and that the process should be carried out using a tube furnace. The results also showed that the temperature of the oven process should not be lowered to 600 °C, but that there is potential to lower it from the current 800 °C. At the current temperature, the percentage of silicon should not be increased to ≥ 47 %, but is considered to have the potential to be increased from the current 33 %. Only a small proportion of silicon nanoparticles was formed when a silicon powder of a different quality than that described in the article was used and showed that the choice of the silicon starting material is of great importance. The results further revealed that the dispersant polyvinyl alcohol, PVA, cannot be excluded. The results showed that there is potential to reduce the proportion of PVA, but that it should not be reduced to as much as half the concentration that is presented in the article. The results showed that the process must be carried out in an oxygen-poor environment in order not to cause oxidation of the nanographite. However, the results also indicated that the mechanism for the formation of silicon nanoparticles is not oxygen sensitive and that the mechanism, if the existing theory is correct, does not appear to be dependent on PVA beeing the source of hydrogen.</p>
----------------------------------------------------------------------
In diva2:801733 abstract is: <p>This work is focused in the study of three different ways of perfusion. The spin tube system was used in order to see metabolic differences between commercial cultivation media for CHO culture. A preliminary selection was carried out by vector analysis. This method was proposed in order to select the media with most singular features. 11 media were selected for a 12-day SpinTube experiment. Some correlations between matabolic rates were established. The media presenting the most unique characteristics were tested in STR experiment: acoustic settler and ATF. The first one required parameter adjustments to obtain high cell densities. Dut to time constrains, the filtration system was switxhed to ATF. Both systems were connected to the bioreactor with dip tube ended by a U-shaped profile, which jeopardized the operation due to mis-dimensioning. Two media could be analyzed and a correlation between STR and SpinTube sysstems was established. Finally, a small bioreactor prototype for stem cell culture was used. A useful system was designed and mounted for this prototype. Afterwards, a functional testing was carried out and some issues regarding temperature, pH, evaporation, and leakage were detected. Some solutions were proposed and put into practice, which contributed to the improvement and development of this promising prototype.</p>

w='matabolic' val={'c': 'metabolic', 's': 'diva2:801733', 'n': 'no full text'}
w='switxhed' val={'c': 'switched', 's': 'diva2:801733', 'n': 'no full text'}
w='sysstems' val={'c': 'systems', 's': 'diva2:801733', 'n': 'no full text'}
w='sstems' val={'c': 'systems', 's': 'diva2:801733', 'n': 'no full text'}
w='Dut' val={'c': 'Due', 's': 'diva2:801733', 'n': 'no full text'}

corrected abstract:
<p>This work is focused in the study of three different ways of perfusion. The spin tube system was used in order to see metabolic differences between commercial cultivation media for CHO culture. A preliminary selection was carried out by vector analysis. This method was proposed in order to select the media with most singular features. 11 media were selected for a 12-day SpinTube experiment. Some correlations between metabolic rates were established. The media presenting the most unique characteristics were tested in STR experiment: acoustic settler and ATF. The first one required parameter adjustments to obtain high cell densities. Due to time constrains, the filtration system was switched to ATF. Both systems were connected to the bioreactor with dip tube ended by a U-shaped profile, which jeopardized the operation due to mis-dimensioning. Two media could be analyzed and a correlation between STR and SpinTube systems was established. Finally, a small bioreactor prototype for stem cell culture was used. A useful system was designed and mounted for this prototype. Afterwards, a functional testing was carried out and some issues regarding temperature, pH, evaporation, and leakage were detected. Some solutions were proposed and put into practice, which contributed to the improvement and development of this promising prototype.</p>
----------------------------------------------------------------------
In diva2:854226 abstract is: <p>Cyanobateria are promising cell factories due to their minimal nutrient requirements and utilization of asmospheric carbon di-oxide as its sole carbon source. In particular, polyhydroxybutyrate (PHB) is an industrially useful bio plastic that is produced naturally by some cyanobacteria. Furthermore, PHB biosynthetic pathway is a starting point for production of the biofuel, 1-butanol. There has been much genetic engineering effort toward increasing the production of PHB from cyanobacteria. These have been focused on increasing the pool of acetyl-CoA precursor, or increasing the amount of the reductant NADPH. The upstream process for increasing these reactants is complex and involves many genes. In this contect, cyanobacteria libraries will contribute to reveal genes or gene fragments that are responsible for production of PHB, alkanes and other high value compounds. In pursuit of finding these novel genes or genefragments, a transcription factor library is created in this study with 50 transcription factors. Furthermore, the process is optimized towards the creation of genomic fragment library and metagenomic fragment library with 26 diverse strains. Membersof the transcription factor library are over-expressed by a PHB - producing host Synechocystis PCC 6803 and the process towards creation of genomic and metagenomic libraries is optimized. The members of the metagenomic library can be screened for increased PHB, alkanes, lactate and other high value products and the potential members can be isolated and characterized.  </p>

w='asmospheric' val={'c': 'atmospheric', 's': 'diva2:854226', 'n': 'correct in original'}
w='contect' val={'c': 'context', 's': 'diva2:854226', 'n': 'error in original'}

corrected abstract:
<p>Cyanobacteria are promising cell factories due to their minimal nutrient requirements and utilization of atmospheric carbon di-oxide as its sole carbon source. In particular, polyhydroxybutyrate (PHB) is an industrially useful bio plastic that is produced naturally by some cyanobacteria. Furthermore, PHB biosynthetic pathway is a starting point for production of the biofuel, 1-butanol. There has been much genetic engineering effort toward increasing the production of PHB from cyanobacteria. These have been focused on increasing the pool of acetyl-CoA precursor, or increasing the amount of the reductant NADPH. The upstream process for increasing these reactants is complex and involves many genes. In this context, cyanobacteria libraries will contribute to reveal genes or gene fragments that are responsible for production of PHB, alkanes and other high value compounds. In pursuit of finding these novel genes or gene fragments, a transcription factor library is created in this study with 50 transcription factors. Furthermore, the process is optimized towards the creation of genomic fragment library and metagenomic fragment library with 26 diverse strains. Members of the transcription factor library are over-expressed by a PHB - producing host Synechocystis PCC 6803 and the process towards creation of genomic and metagenomic libraries is optimized. The members of the metagenomic library can be screened for increased PHB, alkanes, lactate and other high value products and the potential members can be isolated and characterized.</p>
----------------------------------------------------------------------
In diva2:860677 abstract is: <p>Over the past several years it has been revealed that microRNAs (miRNAs) play a role in various diseases from cancer to cardiovascular disease. These short, noncoding RNAs act by regulating gene expression at transcriptional or post-transcriptional level and show potential as biomarkers and as therapeutic targets.</p><p>Traditional miRNA expression analysis relies on measuring the mean miRNA content of a whole tissue sample, resulting in that altered miRNA expression of a rare cell type is masked because of the large number of normal cells.</p><p>Therefore, a method for spatially resolved single-cell miRNA sequencing is being developed. By placing a one-cell-layer tissue section on a microarray covered with pre-adenylated surface probes, miRNAs can be specifically immobilized to the surface by ligation. Surface probes are arranged into singl-cell-sized clusters with unique positional tags, which enables the miRNA expression data to be traced back to the cell from which it originated. In this master thesis project, a protocol for spatially resolved miRNA expression analysis has been evaluated, using in-house arrays without positional tags. The study was performed on the mouse olfactory bulb (MOB) model system, and includes new strategies for miRNA surface capture and library preparation.</p>


w='singl-cell-sized' val={'c': 'single-cell-sized', 's': 'diva2:860677', 'n': 'no full text'}

corrected abstract:
<p>Over the past several years it has been revealed that microRNAs (miRNAs) play a role in various diseases from cancer to cardiovascular disease. These short, noncoding RNAs act by regulating gene expression at transcriptional or post-transcriptional level and show potential as biomarkers and as therapeutic targets.</p><p>Traditional miRNA expression analysis relies on measuring the mean miRNA content of a whole tissue sample, resulting in that altered miRNA expression of a rare cell type is masked because of the large number of normal cells.</p><p>Therefore, a method for spatially resolved single-cell miRNA sequencing is being developed. By placing a one-cell-layer tissue section on a microarray covered with pre-adenylated surface probes, miRNAs can be specifically immobilized to the surface by ligation. Surface probes are arranged into single-cell-sized clusters with unique positional tags, which enables the miRNA expression data to be traced back to the cell from which it originated. In this master thesis project, a protocol for spatially resolved miRNA expression analysis has been evaluated, using in-house arrays without positional tags. The study was performed on the mouse olfactory bulb (MOB) model system, and includes new strategies for miRNA surface capture and library preparation.</p>
----------------------------------------------------------------------
In diva2:855806 abstract is: <p>Industrial production of biofuels using microorganisms is facing many challenges. Among these low stress toerlance, low growth rate and low productivity are found. To address these problems transcriptome engineering is a method of increasing interest. In this study transcriptome engineering was employed to find ways to create industrial strains of the cyanobacteria Synechocystis sp. PCC6803 and Synechococcus elongatus PCC7942. Transcription factors know to be involved in stress response or in carbon fixating regulatory mechanisms in Synechocystis sp. PCC6803 were overexpressed, deleted or mutated to evaluate their effect of improving solvent tolerance in particular, but also tolerance to high temperature and oxidative stress, or of improving the growth rate and partitioning carbon fluxes towards biofuels. A 2-3 fold increase in heat tolerance of strains overexpressing Hik34 or SigB and a Hik34 knock out strain was found, a library of Ycf30 of 10 000 members with 4-7 mutations per gene was created, cultivation conditions facilitating growth of large number of cultures were established and methods for assessing butanol tolerance was determined. </p>

w='toerlance' val={'c': 'tolerance', 's': 'diva2:855806', 'n': 'no full text'}

corrected abstract:
<p>Industrial production of biofuels using microorganisms is facing many challenges. Among these low stress tolerance, low growth rate and low productivity are found. To address these problems transcriptome engineering is a method of increasing interest. In this study transcriptome engineering was employed to find ways to create industrial strains of the cyanobacteria Synechocystis sp. PCC6803 and Synechococcus elongatus PCC7942. Transcription factors know to be involved in stress response or in carbon fixating regulatory mechanisms in Synechocystis sp. PCC6803 were overexpressed, deleted or mutated to evaluate their effect of improving solvent tolerance in particular, but also tolerance to high temperature and oxidative stress, or of improving the growth rate and partitioning carbon fluxes towards biofuels. A 2-3 fold increase in heat tolerance of strains overexpressing Hik34 or SigB and a Hik34 knock out strain was found, a library of Ycf30 of 10 000 members with 4-7 mutations per gene was created, cultivation conditions facilitating growth of large number of cultures were established and methods for assessing butanol tolerance was determined. </p>
----------------------------------------------------------------------
In diva2:1468587 abstract is: <p>Transparent papers made from cellulose nanofibrils (CNF), derived from e.g. wood, show great potential to replace petroleum-based plastics in many application areas, such as packaging for foods and goods. CNF, also known as nanocellulose, combine important cellulose properties with the unique features of nanoscale materials, gaining paper-like materials with outstanding mechanical properties and high transparency. However, nanocellulose faces various challenges in order to make the products commercially competitive. One of the main challenges is accompanied with nanocelluloses’ high affinity for water, which makes processing difficult. Dewatering of a nanocellulose dispersion in order to produce transparent paper may take up to several hours. To overcome this obstacle, the Fibre technology division at KTH Royal Institute of technology and BillerudKorsnäs AB have recently developed a new concept of self-fibrillating fibres (SFFs). This material enables fast-dewatering papermaking using fibres of native dimensions and conversion into nanocellulose after the paper has been prepared. In order to obtain SFFs, proper amounts of charged groups and aldehyde groups need to be introduced into the cellulose backbone. When SFFs are exposed to high alkali concentration, i.e. &gt; pH=10, the fibres self-fibrillates into CNFs.</p><p>In the original study, the functional groups were introduced through sequential TEMPO oxidation and periodate oxidation. In this work, alternative chemical routes have been examined to prepare SFFs with the same functional groups as introduced with the TEMPO-periodate system. The aim of the thesis has been to answer: how does different chemical routes to prepare transparent nanopaper made from SFFs affect the chemical and physical properties of the modified fibres, as well as the final physical properties of the transparent papers? To answer the question, fibres with similar carboxyl and aldehyde contents were prepared using three chemical routes: 1) TEMPO oxidation followed by periodate oxidation (which was used as reference system); 2) periodate oxidation followed by chlorite oxidation; 3) carboxymethylation followed by periodate oxidation. The properties of the fibres were examined regarding aldehyde and carboxyl content, dewatering potential and self-fibrillating ability. Papers were produced using a vacuum filtration set-up and the properties investigated were the mechanical; tensile strength, strain at failure and Young’s modulus, the optical properties; transparency and haze, as well as the oxygen permeability. In order to investigate the impact of the fibrillation of the papers, the properties were measured for both unfibrillated and fibrillated samples. Furthermore, the gravimetric yield after each chemical modification procedure was examined, as well as the dewatering time during sheet making.</p><p>Fibres obtained from all three chemistries demonstrated self-fibrillating properties in alkaline solutions. This strengthens the hypothesis that the strategical introduction of aldehydes and carboxyl groups is the main feature responsible for the self-fibrillating ability of the fibres. Transparent papers made from fibres treated through TEMPO-periodate oxidation and periodate-chlorite oxidation showed excellent mechanical, optical and barrier properties, comparable to those seen in nanocellulose papers. The properties were further increased after fibrillation. The carboxymethylated-periodate oxidized fibres, on the other hand, behaved differently from the others. While the TEMPO-periodate and periodate-chlorite pulp was semi-translucent and gel-like, the carboxymethylated-periodate oxidized fibres resembled more the unmodified material. Likewise, the properties of those papers resembled conventional paper and no fibrillationwas experienced after immersing the papers in alkaline solution, according to the same protocol developed for the other two chemistries.</p><p>The dewatering time during sheet making ranged from 4–60 seconds (carboxymethylation-periodate oxidation showing the fastest dewatering rates). The increased dewatering time compared to earlier studies is believed to mainly be due to the use of a filtration membrane on the vacuum filtration set-up, instead of a metallic wire with larger pores.</p><p>Overall, SFFs was successfully produced using three different chemical routes. SFFs enables production of fast-dewatering transparent nanocellulose papers that shows the potential to replace oil-based plastics in many packaging applications.</p>

corrected abstract:
<p>Transparent papers made from cellulose nanofibrils (CNF), derived from e.g. wood, show great potential to replace petroleum-based plastics in many application areas, such as packaging for foods and goods. CNF, also known as nanocellulose, combine important cellulose properties with the unique features of nanoscale materials, gaining paper-like materials with outstanding mechanical properties and high transparency. However, nanocellulose faces various challenges in order to make the products commercially competitive. One of the main challenges is accompanied with nanocelluloses’ high affinity for water, which makes processing difficult. Dewatering of a nanocellulose dispersion in order to produce transparent paper may take up to several hours. To overcome this obstacle, the Fibre technology division at KTH Royal Institute of technology and BillerudKorsnäs AB have recently developed a new concept of self-fibrillating fibres (SFFs). This material enables fast-dewatering papermaking using fibres of native dimensions and conversion into nanocellulose after the paper has been prepared. In order to obtain SFFs, proper amounts of charged groups and aldehyde groups need to be introduced into the cellulose backbone. When SFFs are exposed to high alkali concentration, i.e. &gt; pH=10, the fibres self-fibrillates into CNFs.</p><p>In the original study, the functional groups were introduced through sequential TEMPO oxidation and periodate oxidation. In this work, alternative chemical routes have been examined to prepare SFFs with the same functional groups as introduced with the TEMPO-periodate system. The aim of the thesis has been to answer: how does different chemical routes to prepare transparent nanopaper made from SFFs affect the chemical and physical properties of the modified fibres, as well as the final physical properties of the transparent papers? To answer the question, fibres with similar carboxyl and aldehyde contents were prepared using three chemical routes: 1) TEMPO oxidation followed by periodate oxidation (which was used as reference system); 2) periodate oxidation followed by chlorite oxidation; 3) carboxymethylation followed by periodate oxidation. The properties of the fibres were examined regarding aldehyde and carboxyl content, dewatering potential and self-fibrillating ability. Papers were produced using a vacuum filtration set-up and the properties investigated were the mechanical; tensile strength, strain at failure and Young’s modulus, the optical properties; transparency and haze, as well as the oxygen permeability. In order to investigate the impact of the fibrillation of the papers, the properties were measured for both unfibrillated and fibrillated samples. Furthermore, the gravimetric yield after each chemical modification procedure was examined, as well as the dewatering time during sheet making.</p><p>Fibres obtained from all three chemistries demonstrated self-fibrillating properties in alkaline solutions. This strengthens the hypothesis that the strategical introduction of aldehydes and carboxyl groups is the main feature responsible for the self-fibrillating ability of the fibres. Transparent papers made from fibres treated through TEMPO-periodate oxidation and periodate-chlorite oxidation showed excellent mechanical, optical and barrier properties, comparable to those seen in nanocellulose papers. The properties were further increased after fibrillation. The carboxymethylated-periodate oxidized fibres, on the other hand, behaved differently from the others. While the TEMPO-periodate and periodate-chlorite pulp was semitranslucent and gel-like, the carboxymethylated-periodate oxidized fibres resembled more the unmodified material. Likewise, the properties of those papers resembled conventional paper and no fibrillation was experienced after immersing the papers in alkaline solution, according to the same protocol developed for the other two chemistries.</p><p>The dewatering time during sheet making ranged from 4–60 seconds (carboxymethylation-periodate oxidation showing the fastest dewatering rates). The increased dewatering time compared to earlier studies is believed to mainly be due to the use of a filtration membrane on the vacuum filtration set-up, instead of a metallic wire with larger pores.</p><p>Overall, SFFs was successfully produced using three different chemical routes. SFFs enables production of fast-dewatering transparent nanocellulose papers that shows the potential to replace oil-based plastics in many packaging applications.</p>
----------------------------------------------------------------------
In diva2:1685176 abstract is: <p>This project presents greywater (GW) treatment techniques and reuse applications for different purposes. The aim is to suggest a suitable process design for treating GW in the urban city environment, Loudden, Norra Djurgårdsstaden (NDS).</p><p>The research studied relevant literature and four demo sites where GW is treated and reused in various applications. GW data were collected, and an average was used to emulate a GW inflow characteristic. The GW flow was constructed based on estimated daily water consumption in Sweden of approximately 140 liter/person/day.</p><p>Four treatment process units were selected. The treatment processes units were combined with pre- treatment, a standard treatment to protect the system. Two of these treatment processes were modified because, individually, they would not be sufficient to reduce contaminants and secure the end-user application. The performance of the four process units was evaluated based on two criteria: EU quality criteria for reclaimed water under class C and operating parameters. A list ranking the most critical variables was created to sort out the best process unit.</p><p>An evaluation process showed the suitability of the designs for the required treatment. "Process Combination 1" was chosen because of its robust performance and did not present any significant drawbacks. It consists of a fine screen, a submerged membrane bioreactor, and a chlorination step. A process design including heat recovery and reuse applications for irrigation of the plantation fiels in the area and process industry was established. The characterized inflow and constructed GW flow were then used for technical and economic calculations.</p><p>The estimated investment cost for the proposed process design was approximately SEK 31 million. It generates an annual income of SEK 5 million and will, according to the Net Present Value method, with an interest rate of 5%, be fully paid after 9 years.</p>

w='fiels' val={'c': 'fields', 's': 'diva2:1685176', 'n': 'correct in the original'}

corrected abstract:
<p>This project presents greywater (GW) treatment techniques and reuse applications for different purposes. The aim is to suggest a suitable process design for treating GW in the urban city environment, Loudden, Norra Djurgårdsstaden (NDS).</p><p>The research studied relevant literature and four demo sites where GW is treated and reused in various applications. GW data were collected, and an average was used to emulate a GW inflow characteristic. The GW flow was constructed based on estimated daily water consumption in Sweden of approximately 140 liter/person/day.</p><p>Four treatment process units were selected. The treatment processes units were combined with pretreatment, a standard treatment to protect the system. Two of these treatment processes were modified because, individually, they would not be sufficient to reduce contaminants and secure the end-user application. The performance of the four process units was evaluated based on two criteria: EU quality criteria for reclaimed water under class C and operating parameters. A list ranking the most critical variables was created to sort out the best process unit.</p><p>An evaluation process showed the suitability of the designs for the required treatment. "Process Combination 1" was chosen because of its robust performance and did not present any significant drawbacks. It consists of a fine screen, a submerged membrane bioreactor, and a chlorination step. A process design including heat recovery and reuse applications for irrigation of plantation fields in the area and process industry was established. The characterized inflow and constructed GW flow were then used for technical and economic calculations.</p><p>The estimated investment cost for the proposed process design was approximately SEK 31 million. It generates an annual income of SEK 5 million and will, according to the Net Present Value method, with an interest rate of 5%, be fully paid after 9 years.</p>
----------------------------------------------------------------------
In diva2:505431 abstract is: <p>Odorous volatile organic compounds emissions from fried-food industries posed severed pollution problems both to the workers and the surrounding inhabitants. These industries need to look for cost effective and efficient methods to reduce these emitted gases.  Several solutions such as the use of centrifugation, scrubbers, ion exchangers, biofiltration, condensation, adsorption, absorption, and incineration have been exploited to reduce these smelling gases. Centriair in collaboration with KTH aim at using UV light in combination with ozone and hydrogen peroxide to degrade these odorous VOCs emitted from the frying of meat balls (SCAN) and chips.</p>
<p>Several volatile organic compounds which are odorants with low threshold values were identified in the emitted gases from meat frying which includes: aldehydes, sulphur containing compounds, ketones, pyrazines, and alcohols. The type and concentration of these odorants emitted depends among other things primarily on the type of oil used during the frying process. </p>
<p>This work focuses on the use of advanced oxidation processes to abate theses odorous gases. The effect of UV dosage and the use of hydrogen peroxide were tested in a flow reactor. Ozone producing UV lamps were used for the treatment of 2,4-decadienal, Hexanal, furfural, and 2,5-dimethylpyrazine. A simultaneous chemical and odour analysis was done using a GC/MS Olfactometry system.</p>
<p>UV/Ozone/H<sub>2</sub>O<sub>2</sub> was effective in reducing the volatile organic compounds tested thus reducing the odor concentration. The percent removal was proportional to the energy dosage.</p>


w='UV/Ozone/H' val={'c': 'UV/Ozone/H<sub>2</sub>O<sub>2</sub>', 's': 'diva2:505431', 'n': 'correct in original'}

corrected abstract:
<p>Odorous volatile organic compounds emissions from fried-food industries posed severed pollution problems both to the workers and the surrounding inhabitants. These industries need to look for cost effective and efficient methods to reduce these emitted gases.  Several solutions such as the use of centrifugation, scrubbers, ion exchangers, biofiltration, condensation, adsorption, absorption, and incineration have been exploited to reduce these smelling gases. Centriair in collaboration with KTH aim at using UV light in combination with ozone and hydrogen peroxide to degrade these odorous VOCs emitted from the frying of meat balls (SCAN) and chips.</p><p>Several volatile organic compounds which are odorants with low threshold values were identified in the emitted gases from meat frying which includes: aldehydes, sulphur containing compounds, ketones, pyrazines, and alcohols. The type and concentration of these odorants emitted depends among other things primarily on the type of oil used during the frying process.</p><p>This work focuses on the use of advanced oxidation processes to abate theses odorous gases. The effect of UV dosage and the use of hydrogen peroxide were tested in a flow reactor. Ozone producing UV lamps were used for the treatment of 2,4-decadienal, Hexanal, furfural, and 2,5-dimethylpyrazine. A simultaneous chemical and odour analysis was done using a GC/MS Olfactometry system.</p><p>UV/Ozone/H<sub>2</sub>O<sub>2</sub> was effective in reducing the volatile organic compounds tested thus reducing the odor concentration. The percent removal was proportional to the energy dosage.</p>
----------------------------------------------------------------------
In diva2:1571882 abstract is: <p>This research work studied the use of CAZymes for the sustainable valorization of wheat bran as a potential source for bioactive carbohydrates, in the form feruloylated-arabinoxylans (F-AXs) present in this cereal by-product. In order to extract the recalcitrant fraction of F-AXs, specific CAZymes have been tested. Feruloyl esterases (FAEs) have been characterized in model and commercial substrates, showing that the FAE with highest activity and substrate specificity is the enzyme 29492, the one that, together with the FAE U2BHX, also has an unexpected activity rapidly creating other compounds, like a dimer 5,5-diFA’ from free FA. Also, Xylanases and FAEs have been analyzed in insoluble real substrate (wheat bran), demonstrating that the best enzymes to extract this antioxidant biopolymers are FAE 22198 and xylanase AcXyn10A CBM1C, due to the presence of CBM in both structures. Then, the use of CAZymes, combined with destaching pre-treatments and SWE extractions would allow a better use of residual matter, thus promoting a more eco-friendly circular economy.</p>

w='destaching' val={'c': 'destarching', 's': 'diva2:1571882'} # seems the "r" is missing

corrected abstract:
<p>This research work studied the use of CAZymes for the sustainable valorization of wheat bran as a potential source for bioactive carbohydrates, in the form feruloylated-arabinoxylans (F-AXs) present in this cereal by-product. In order to extract the recalcitrant fraction of F-AXs, specific CAZymes have been tested. Feruloyl esterases (FAEs) have been characterized in model and commercial substrates, showing that the FAE with highest activity and substrate specificity is the enzyme 29492, the one that, together with the FAE U2BHX, also has an unexpected activity rapidly creating other compounds, like a dimer 5,5-diFA’ from free FA. Also, Xylanases and FAEs have been analyzed in insoluble real substrate (wheat bran), demonstrating that the best enzymes to extract this antioxidant biopolymers are FAE 22198 and xylanase AcXyn10A CBM1C, due to the presence of CBM in both structures. Then, the use of CAZymes, combined with destarching pre-treatments and SWE extractions would allow a better use of residual matter, thus promoting a more eco-friendly circular economy.</p>
----------------------------------------------------------------------
In diva2:442696 abstract is: <p>Topical treatment containing undeclared corticosteroids and illegal topical treatment with corticosteroid content have been seen on the Swedish market. In creams and ointments corticosteroids in the category of glucocorticosteroids are used to reduce inflammatory reactions and itchiness in the skin. If the inflammation is due to bacterial infection or fungus, complementary treatment is necessary. Side effects of corticosteroids are skin reactions and if used in excess suppression of the adrenal gland function. Therefore the Swedish Medical Products Agency has published related warnings to make the public aware. There are many similar structures of corticosteroids where the anti-inflammatory effect is depending on substitutions on the corticosteroid molecular skeleton. In legal creams and ointments they can be found at concentrations of 0.025 ‑ 1.0 %, where corticosteroids with fluorine substitutions usually are found at concentrations up to 0.1 % due to increased potency.</p>
<p>At the Medical Products Agency <sup>19</sup>F-NMR and <sup>1</sup>H-NMR have been used to detect and quantify corticosteroid content in creams and ointments. Nuclear Magnetic Resonance, NMR, is an analytical technique which is quite sensitive and can have a relative short experimental time. The low concentration of corticosteroids makes the signals detected in NMR small and in <sup>1</sup>H‑NMR the signals are often overlapped by signals from the matrix. With <sup>1</sup>H‑NMR characteristic signals could be detected in a less crowded spectral window between 5.96 ‑ 6.40 ppm where overlapping signals from the matrix often are absent. Since fluorine is less common in molecules, the option of using <sup>19</sup>F‑NMR increases the possibility of finding fluorine-containing corticosteroids in creams and ointments. The corticosteroid signals in <sup>19</sup>F‑NMR are detected at -165 ppm and -187 ppm, depending on where fluorine is located on the structure.</p>
<p>Quantifying with <sup>1</sup>H-NMR and <sup>19</sup>F-NMR gave similar result with an accuracy of 98‑116 % and 89-106 % respectively, and RSD values between 2‑35 %, depending on the kind and amount of corticosteroid. Relations between the structure and some signals in <sup>1</sup>H‑NMR were found, making it easier to determine the basic structure of unknown corticosteroids in creams and ointments. Screening experiments were performed on creams and ointments with known concentration corticosteroid in order to find minimum NS for analyzing products which might contain corticosteroids. In order to detect a corticosteroid concentration of 0.05 % <sup>19</sup>F‑NMR needed 64 NS with an experimental time of 2 min and <sup>1</sup>H-NMR needed 160 NS with an experimental time of 17 min. Concentrations of 0.025 % could for some corticosteroids be detected with these parameters. The possibility of spiking samples in order to discriminate between corticosteroids was also investigated. The corticosteroids available at the MPA could be discriminated from each other with at least one of the methods <sup>1</sup>H‑NMR or <sup>19</sup>F-NMR, and in most cases with both. A market research was done in order to search for counterfeits and salespersons in different health food stores were asked to recommend the best product to treat eczema or psoriasis. Nine recommended products were bought where one was found illegally containing a corticosteroid. In previous experiments at the MPA there had been occurrences of a split signal in <sup>19</sup>F-NMR when analyzing creams. The split <sup>19</sup>F‑NMR<strong> </strong>signal was shown to be related both to the presence of water and structural effects of the corticosteroid</p>

w='F‑NMR' val={'c': 'F-NMR', 's': 'diva2:442696'}

corrected abstract:
<p>Topical treatment containing undeclared corticosteroids and illegal topical treatment with corticosteroid content have been seen on the Swedish market. In creams and ointments corticosteroids in the category of glucocorticosteroids are used to reduce inflammatory reactions and itchiness in the skin. If the inflammation is due to bacterial infection or fungus, complementary treatment is necessary. Side effects of corticosteroids are skin reactions and if used in excess suppression of the adrenal gland function. Therefore the Swedish Medical Products Agency has published related warnings to make the public aware. There are many similar structures of corticosteroids where the anti-inflammatory effect is depending on substitutions on the corticosteroid molecular skeleton. In legal creams and ointments they can be found at concentrations of 0.025 ‑ 1.0 %, where corticosteroids with fluorine substitutions usually are found at concentrations up to 0.1 % due to increased potency.</p><p>At the Medical Products Agency <sup>19</sup>F-NMR and <sup>1</sup>H-NMR have been used to detect and quantify corticosteroid content in creams and ointments. Nuclear Magnetic Resonance, NMR, is an analytical technique which is quite sensitive and can have a relative short experimental time. The low concentration of corticosteroids makes the signals detected in NMR small and in <sup>1</sup>H‑NMR the signals are often overlapped by signals from the matrix. With <sup>1</sup>H‑NMR characteristic signals could be detected in a less crowded spectral window between 5.96 ‑ 6.40 ppm where overlapping signals from the matrix often are absent. Since fluorine is less common in molecules, the option of using <sup>19</sup>F-NMR increases the possibility of finding fluorine-containing corticosteroids in creams and ointments. The corticosteroid signals in <sup>19</sup>F-NMR are detected at -165 ppm and -187 ppm, depending on where fluorine is located on the structure.</p><p>Quantifying with <sup>1</sup>H-NMR and <sup>19</sup>F-NMR gave similar result with an accuracy of 98‑116 % and 89-106 % respectively, and RSD values between 2‑35 %, depending on the kind and amount of corticosteroid. Relations between the structure and some signals in <sup>1</sup>H‑NMR were found, making it easier to determine the basic structure of unknown corticosteroids in creams and ointments. Screening experiments were performed on creams and ointments with known concentration corticosteroid in order to find minimum NS for analyzing products which might contain corticosteroids. In order to detect a corticosteroid concentration of 0.05 % <sup>19</sup>F-NMR needed 64 NS with an experimental time of 2 min and <sup>1</sup>H-NMR needed 160 NS with an experimental time of 17 min. Concentrations of 0.025 % could for some corticosteroids be detected with these parameters. The possibility of spiking samples in order to discriminate between corticosteroids was also investigated. The corticosteroids available at the MPA could be discriminated from each other with at least one of the methods <sup>1</sup>H‑NMR or <sup>19</sup>F-NMR, and in most cases with both. A market research was done in order to search for counterfeits and salespersons in different health food stores were asked to recommend the best product to treat eczema or psoriasis. Nine recommended products were bought where one was found illegally containing a corticosteroid. In previous experiments at the MPA there had been occurrences of a split signal in <sup>19</sup>F-NMR when analyzing creams. The split <sup>19</sup>F-NMR signal was shown to be related both to the presence of water and structural effects of the corticosteroid</p>
----------------------------------------------------------------------
In diva2:1584719 abstract is: <p>Sydvästra stockholmsregionens va-verksaktiebolag – Syvab owns and operates the Himmerfjärdsverket plant, which is currently undergoing a large-scale refurbishment and expansion to cope with both increased loadings and the stricter treatment requirements expected in the future. The plant will be converted to an MBR process, i.e. Membrane BioReactor. In conjunction with this, an MBR pilot plant has been commissioned. In the pilot plant, the wastewater is distributed in three successive cascades, each equipped for complete nitrogen removal. </p><p>The aim of this project was to investigate why lower denitrification rates were observed in the pilot plant compared to the design data when an external carbon source (methanol) was added. To investigate this, a literature study was first carried out to gain an understanding of the processes and the parameters affecting denitrification. </p><p>Thereafter three different types of experiments were performed. One was a post- denitrification experiment to monitor the maximum denitrification rate of the sludge. The second test examined the linear relationship for the temperature dependence of the denitrification rate. Finally, nitrogen profiles were made to calculate denitrification rates in the process by setting up mass balances over the analysed parameters. The data from the results were then evaluated together with previously collected data from the pilot plant. </p><p>The results from the post-denitrification experiments and the nitrogen profiles showed that the denitrification rate was faster than in the previously performed experiment (maximum of 2,0 mg NO3/gVSS, h was achieved) but still lower than the designed rate for the process of 3,5 mg NO3/gVSS, h. According to the temperature experiments performed, the denitrification rate increases by 10.3% per °C. </p><p>The faster denitrification rates can be explained partly due to higher temperatures in the process but mainly due to lower oxygen levels in the process which may have altered the bacterial composition of the sludge in favor of denitrification. Based on the experiments carried out, the designed denitrification rate appears to be higher than what can be achieved in the pilot plant. </p>

w='NO3/gVSS' val={'c': 'NO<sub>3</sub>/gVSS', 's': 'diva2:1584719', 'n': 'correct in original'}

corrected abstract:
<p>Sydvästra stockholmsregionens va-verksaktiebolag – Syvab owns and operates the Himmerfjärdsverket plant, which is currently undergoing a large-scale refurbishment and expansion to cope with both increased loadings and the stricter treatment requirements expected in the future. The plant will be converted to an MBR process, i.e. Membrane BioReactor. In conjunction with this, an MBR pilot plant has been commissioned. In the pilot plant, the wastewater is distributed in three successive cascades, each equipped for complete nitrogen removal.</p><p>The aim of this project was to investigate why lower denitrification rates were observed in the pilot plant compared to the design data when an external carbon source (methanol) was added. To investigate this, a literature study was first carried out to gain an understanding of the processes and the parameters affecting denitrification.</p><p>Thereafter three different types of experiments were performed. One was a post-denitrification experiment to monitor the maximum denitrification rate of the sludge. The second test examined the linear relationship for the temperature dependence of the denitrification rate. Finally, nitrogen profiles were made to calculate denitrification rates in the process by setting up mass balances over the analysed parameters. The data from the results were then evaluated together with previously collected data from the pilot plant.</p><p>The results from the post-denitrification experiments and the nitrogen profiles showed that the denitrification rate was faster than in the previously performed experiment (maximum of 2,0 mg NO<sub>3</sub>/gVSS,h was achieved) but still lower than the designed rate for the process of 3,5 mg NO<sub>3</sub>/gVSS,h. According to the temperature experiments performed, the denitrification rate increases by 10.3% per °C.</p><p>The faster denitrification rates can be explained partly due to higher temperatures in the process but mainly due to lower oxygen levels in the process which may have altered the bacterial composition of the sludge in favor of denitrification. Based on the experiments carried out, the designed denitrification rate appears to be higher than what can be achieved in the pilot plant.</p>
----------------------------------------------------------------------
In diva2:849460 abstract is: <p>Proteases are important enzymes in the biotechnology due to their specific cleavage of substrates. HRV 3C, sortase A and TEV are some examples of cysteine proteases which become more of use lately in applications as removal of affinity tags (3C/TEV) and labelling of proteins (sortase). Here an investigation was made on the proteases by displaying them on two different prokaryotic hosts; E. coli and S. carnosus and to use these to cleave away affinity proteins (Affibody molecule) from other cells with an incorporated cleavage site. Constructs were cloned and incorporated into expressing strains which were then cultivated and induced. Analysis of surface expression was done by flow cytometer. Cleavage was made by cultivating combinations with cleavable bacteria and bacteria displaying proteases. A functional protease would lead to the presence of Affibody molecules in the supernatant. Flow cytomtery analysis was first made to inevstigate signal difference in Affibody binding by the addition of flurophores. Secondly SDS-PAGE was made on the centrifuged supernatant to investigate the presence of a product. Finally analysis of the bacteria was made by examining the reaction with soluble substrate and comparing activity with soluble enzyme. All of the enzymes were able to be displayed on the surface of bacteria with a clear separation from control. The cleavage analysis showed however varying results yet no clear evidence of product. Best flow cytometer results were seen for 3C but SDS-PAGE/MS did not show any cleaved product. For Sortase SDS-PAGE showed positive result but analysis with MS showed no product. TEV was concluded not to be funcional at all hence the failing to cleave soluble substrate  when condition seemed near optimal and faulty flow cytometer data. Even though the lack of success there is still many further studies that can be done on the proteases in order to prove its absence/presence  of activity.</p>

w='cytomtery' val={'c': 'cytometry', 's': 'diva2:849460', 'n': 'error in original'}
w='flurophores' val={'c': 'fluorophores', 's': 'diva2:849460', 'n': 'error in original'}
w='funcional' val={'c': 'functional', 's': 'diva2:849460', 'n': 'correct in original'}
w='inevstigate' val={'c': 'investigate', 's': 'diva2:849460', 'n': 'correct in original'}

corrected abstract:
<p>Proteases are important enzymes in the biotechnology due to their specific cleavage of substrates. HRV 3C, sortase A and TEV are some examples of cysteine proteases which become more of use lately in applications as removal of affinity tags (3C/TEV) and labelling of proteins (sortase). Here an investigation was made on the proteases by displaying them on two different prokaryotic hosts; <em>E. coli</em> and <emS. carnosus</em> and to use these to cleave away affinity proteins (Affibody molecule) from other cells with an incorporated cleavage site. Constructs were cloned and incorporated into expressing strains which were then cultivated and induced. Analysis of surface expression was done by flow cytometer. Cleavage was made by cultivating combinations with cleavable bacteria and bacteria displaying proteases. A functional protease would lead to the presence of Affibody molecules in the supernatant. Flow cytomtery analysis was first made to investigate signal difference in Affibody binding by the addition of flurophores. Secondly SDS-PAGE was made on the centrifuged supernatant to investigate the presence of a product. Finally analysis of the bacteria was made by examining the reaction with soluble substrate and comparing activity with soluble enzyme. All of the enzymes were able to be displayed on the surface of bacteria with a clear separation from control. The cleavage analysis showed however varying results yet no clear evidence of product. Best flow cytometer results were seen for 3C but SDS-PAGE/MS did not show any cleaved product. For Sortase SDS-PAGE showed positive result but analysis with MS showed no product. TEV was concluded not to be functional at all hence the failing to cleave soluble substrate when condition seemed near optimal and faulty flow cytometer data. Even though the lack of success there is still many further studies that can be done on the proteases in order to prove its absence/presence of activity.</p>
----------------------------------------------------------------------
In diva2:882778 abstract is: <p>Duchenne muscular dystrophy is caused primarily by mutations in the dystrophin gene with possible involvement of the autoantibodies in the aggravating or initiating the disease and there is a great need for sensitive biomarkers for the disease. Suspension bed arrays enables high throughput of autoantibody reactivities toward multiple targets proteins simultaneously and the aim of this project is to identify, autoantibodies associated with Duchenne Muscular Dystrophy. 54 target proteins were selected through a hypothesis-driven selection for which 91 protein fragments fused to a His<sub>6</sub>ABP-tag were received. These were transformed then expressed in Escherichia Coli Rosetta (DE3) with ZYP-5052 auto-induction medium, purified with immobilised metal affinity chromatography and then immobilised on carboxylated magnetic beads. Serological assays were done with labelled plasma samples for binding assessment and non-labelled for accurate of autoantibodies detection. Out of 91 protein fragments, 85% (77) were successfully transformed, expressed, purified and coupled to magnetic beads with the developed pipeline. In the final assay, autoantibody reactivities toward 67 analytes were analysed in 83 plasma samples. Data from the serological assay were mostly inconclusive with mild indications of reactivities associated with DMD. In brief, we have developed an efficient pipeline from antigen production to immobilisation of magnetic beads and used a bead-based serological assay for the purpose of analysing autoantibody reactivities in plasma samples. A total of 83 samples were analysed and although primary results are inconclusive there are some indications suggesting that conslusive results could be achieved in a further improved assay. </p>

w='conslusive' val={'c': 'conclusive', 's': 'diva2:882778', 'n': 'no full text'}
I assumed the usual italics for Latin words.

corrected abstract:
<p>Duchenne muscular dystrophy is caused primarily by mutations in the dystrophin gene with possible involvement of the autoantibodies in the aggravating or initiating the disease and there is a great need for sensitive biomarkers for the disease. Suspension bed arrays enables high throughput of autoantibody reactivities toward multiple targets proteins simultaneously and the aim of this project is to identify, autoantibodies associated with Duchenne Muscular Dystrophy. 54 target proteins were selected through a hypothesis-driven selection for which 91 protein fragments fused to a His<sub>6</sub>ABP-tag were received. These were transformed then expressed in <em>Escherichia Coli Rosetta</em> (DE3) with ZYP-5052 auto-induction medium, purified with immobilised metal affinity chromatography and then immobilised on carboxylated magnetic beads. Serological assays were done with labelled plasma samples for binding assessment and non-labelled for accurate of autoantibodies detection. Out of 91 protein fragments, 85% (77) were successfully transformed, expressed, purified and coupled to magnetic beads with the developed pipeline. In the final assay, autoantibody reactivities toward 67 analytes were analysed in 83 plasma samples. Data from the serological assay were mostly inconclusive with mild indications of reactivities associated with DMD. In brief, we have developed an efficient pipeline from antigen production to immobilisation of magnetic beads and used a bead-based serological assay for the purpose of analysing autoantibody reactivities in plasma samples. A total of 83 samples were analysed and although primary results are inconclusive there are some indications suggesting that conclusive results could be achieved in a further improved assay.</p>
----------------------------------------------------------------------
In diva2:1350183 abstract is: <p>This report represents a bachelor thesis at the Royal Institute of Technology, where the goal was to create a handheld prototype. The functions of the prototype were measuring pulse in a finger using the optical sensor TCRT1010 and electrocardiogram(ECG) with bipolar limb-lead. In this thesis, the focus was put on trying to develop a user-friendly and cost-effective prototype for use in developing countries, that can be found in Africa and Asia. The reason is because there's a lack of cheap and easy to use medical equipment in those areas.</p><p>The work was performed by developing two circuits, given by our employer, that was designed on a circuitboard and constructed by a Swedish circuitboard constructor. All components were soldered on the circuitboard and was mounted in a box with socket and batteries. The finished prototype consists of a plastic box containing one circuitboard with the function to measure either the puls of a finger or ECG, which the user can choose between using a switch. The prototype is also equipped with a removable lid, two BNC plugs for connecting an oscilloskope, one for each ciruit, three sockets for the limb lead for the ECG and finally a 4-pole socket for the optical sensor. The signals acquired are not free of noise but contains the most important parameters of the two signals for guidance if further medical diagnostic is needed.</p><p>The prototypes final cost was 1815,5 SEK, which was not considered achieving the goal for cost-efficiency, was still believed to be useful in developing countries because of its simpel design with removable lid and easily to change batteries. It's equipped with only one switch and the compact size makes it easy to handle. The signal processing is tested to give a clear signal which enables the health professionals to identify if further diagnostics are needed. Several improvements was identified for future work, such as to reduce the cost by buying cheaper components or buying the circuitboard in greater numbers, making a wireless connection to the measuring equipment or further reducing the noise.</p>

w='ciruit' val={'c': 'circuit', 's': 'diva2:1350183', 'n': 'error in original'}
w='oscilloskope' val={'c': 'oscilloscope', 's': 'diva2:1350183', 'n': 'error in original'}

no changes
----------------------------------------------------------------------
In diva2:851710 abstract is: <p>Due to existing limitations of diagnosis and clinical management of muscular dystrophies there is an urgent need a clinical assay which is capable of diagnosis, monitoring disease progression and severity and individual patients' responses to treatment outcome. An immunoassay requires minute amount of sample and is simpler and less expensive than today's clinical tests, which is a tremendous advantage when handling rare diseases since both the sample cohorts and the volumes are limited. This is addressed by developing a microspere-based sandwich immunoassay targeting biomarkers for Duchenne muscular dystrophy, which has been assessed in previous reports. four protein biomarkers have been target in this study; Carbonic anhydrase 3 (CA3), Malate dehydrogenase 2 (MDH2), Electron transfer flavoprotein a (ETFA) and Myosin light chain 3 (MYL3). Antibodies against these proteins have been evaluated, their target binding capacity first in a single-binder assay and eventual in a dual-binder assay have been tested, using target spike-in samples and patient samples. One prominent antibody pair targeting CA3 was developed. Antibody pairs targetin MDH2 and ETFA generates signal in patients sample although optimization is needed. These sandwich immunoassay hold great promise in the managmenet of the challenging DMD disease., by facilitating diagnosis and the monitoring of the disease progressiona dn is capable of patient stratification. The immunoassays can progress to a clinical test that offers a robust and easily assessed severity biomarker detection system.</p>

w='managmenet' val={'c': 'management', 's': 'diva2:851710', 'n': 'no full text'}
w='microspere-based' val={'c': 'microsphere-based', 's': 'diva2:851710', 'n': 'no full text'}
w='targetin' val={'c': 'targeting', 's': 'diva2:851710', 'n': 'no full text'}

corrected abstract:
<p>Due to existing limitations of diagnosis and clinical management of muscular dystrophies there is an urgent need a clinical assay which is capable of diagnosis, monitoring disease progression and severity and individual patients' responses to treatment outcome. An immunoassay requires minute amount of sample and is simpler and less expensive than today's clinical tests, which is a tremendous advantage when handling rare diseases since both the sample cohorts and the volumes are limited. This is addressed by developing a microsphere-based sandwich immunoassay targeting biomarkers for Duchenne muscular dystrophy, which has been assessed in previous reports. four protein biomarkers have been target in this study; Carbonic anhydrase 3 (CA3), Malate dehydrogenase 2 (MDH2), Electron transfer flavoprotein a (ETFA) and Myosin light chain 3 (MYL3). Antibodies against these proteins have been evaluated, their target binding capacity first in a single-binder assay and eventual in a dual-binder assay have been tested, using target spike-in samples and patient samples. One prominent antibody pair targeting CA3 was developed. Antibody pairs targeting MDH2 and ETFA generates signal in patients sample although optimization is needed. These sandwich immunoassay hold great promise in the management of the challenging DMD disease, by facilitating diagnosis and the monitoring of the disease progression and is capable of patient stratification. The immunoassays can progress to a clinical test that offers a robust and easily assessed severity biomarker detection system.</p>
----------------------------------------------------------------------
In diva2:1216721 abstract is: <p>Small scale hydropower is a big user of reactive power, mainly because of their use of asynchrounous generators. Power companies are charging their customers a fee for an extensive use of reactive power. Which gives rise to a need of correction of the power factor as well as reduction of distorsion in the network. It is also in the interest of the general public to acquire a good electric quality in terms of keeping distorsion and interruptions in the network to a minimum. This can be achieved by using different methods for power factor correction and filtration of harmonics.</p><p>These methods have their own inherent advantages and disadvantages described after the facility’s specific needs and requirements. These methods are a capacitor bank, passive and active regulation, overexcited synchronous machine, static var compensator (SVC) and static synchronous compensator (STATCOM). Simulations and economical calculations have been used to determine these properties. A capacitor bank has been proven to be a good economical investment, but it has high transients during switching conditions. Active regulation also shows a good profitability and provides a fast, continuous regulation of the reactive power, though it has higher operating costs and low life expectancy. The overexcited synchronous generator has a positive outlook in economic terms, with the drawback of a slower response time. SVC and STATCOM are more applicable to larger facilities or weak networks.</p>

w='asynchrounous' val={'c': 'asynchronous', 's': 'diva2:1216721', 'n': 'error in original'}

 no changes
----------------------------------------------------------------------
In diva2:628440 abstract is: <p>The aim of this project is to evaluate the network security at J Bil AB. The focus will be on both social and technical issues. For the employees to be able to con-nect to remote servers and external services and perform their daily work tasks, secure connections is needed. J Bil Ab has no IT manager who actively maintains and monitors the network; rather they consult a computer company when changes and implementations are required. The projects’ goal is to identify gaps, come up with suggestions for improvement and to some extent implement so-lutions. To do this, an observation of the employees hav been made, an inter-view have been held, and several attacks on the network have been performed. Based on the data collected, it was concluded that the company has shortcom-ings in IT security. Above all, the social security appeared to have major gaps in it and that is mainly because the lack of knowledge among the employees and they have never been informed of how to manage their passwords, computers and IT issues in general. Suggestions for improvement have been given and some implementations have been performed to eliminate the deficiencies.</p>

w='con-nect' val={'c': 'connect', 's': 'diva2:628440'}
w='inter-view' val={'c': 'interview', 's': 'diva2:628440', 'n': 'hyphen at end of line'}

corrected abstract:
<p>The aim of this project is to evaluate the network security at J Bil AB. The focus will be on both social and technical issues. For the employees to be able to connect to remote servers and external services and perform their daily work tasks, secure connections is needed. J Bil Ab has no IT manager who actively maintains and monitors the network; rather they consult a computer company when changes and implementations are required. The projects’ goal is to identify gaps, come up with suggestions for improvement and to some extent implement solutions. To do this, an observation of the employees hav been made, an interview have been held, and several attacks on the network have been performed. Based on the data collected, it was concluded that the company has shortcomings in IT security. Above all, the social security appeared to have major gaps in it and that is mainly because the lack of knowledge among the employees and they have never been informed of how to manage their passwords, computers and IT issues in general. Suggestions for improvement have been given and some implementations have been performed to eliminate the deficiencies.</p>
----------------------------------------------------------------------
In diva2:1352383 abstract is: <p>The subway is a very important means of transport in Stockholm and is in operation</p><p>most of the day. The Red and Blue lines of the metro use an older signal system from</p><p>the 60s. The system used for the signaling of the two lines is of the type AC power</p><p>line with coding and uses relay switches.</p><p>The track circuit that detects the position of different trains on the track use the frequency</p><p>75 Hz. When converting the frequency 50 Hz to 75 Hz, carbon dust and heat</p><p>losses is released to the space from the frequency converter. The carbon dust emitted</p><p>into the air risks affecting relay functionality as it enters the relay housing and settles</p><p>on the surface of the relay contacts in the space next to the frequency converter.</p><p>The Traffic Administration have plans to upgrade the Red Line's signaling system in</p><p>order to use the existing signaling system for another 25 years. The track circuit system</p><p>positions and gives speed messages to the trains. A literature study has been</p><p>conducted on the construction of the track circuit system and previous research has</p><p>been conducted on frequency inverters to safely find a new frequency inverter that</p><p>does not risk relay functionality. Through fact-finding from the Traffic Administration,</p><p>new suitable frequency converters that could be replaced with the existing ones</p><p>were investigated in this research.</p><p>Static frequency converters were chosen to be investigated. According to the requirements</p><p>of the Traffic Administration, four products were identified from different</p><p>suppliers. None of the static frequency converters was directly compatible with existing</p><p>systems because the system was built for a rotary frequency converter. "HZ-</p><p>50-1105" from GoHz this model is available in 1-phase input and 1-phase output respectively.</p><p>"FR-D 700" from Mitsubishi and "Micromaster 440" from Siemens which</p><p>were equal in and out connection. "ACS-150" from ABB this model had a maximum</p><p>rated power up to 4 kVA, which is only suitable for frequency inverters with a rated</p><p>power of 2.5 kVA. The various types of static frequency inverters loaded the network</p><p>in an asymmetrical way. Therefore, it was important to be able to distribute the load</p><p>as evenly as possible on each phase. It was difficult to be able to rank the selected</p><p>products without just showing the choices that existed when replacing rotary frequency</p><p>converters. However, the use of static frequency inverters eliminates the</p><p>risks of a rotating frequency converter. Static frequency converters can more accurately</p><p>generate different frequencies and they have a shorter commissioning time;</p><p>however, they load the network asymmetrically compared to rotating frequency converters.</p>

w='HZ-</p><p>50-1105' val={'c': 'HZ-50-1105', 's': 'diva2:1352383', 'n': 'correct in original'}

corrected abstract:
<p>The subway is a very important means of transport in Stockholm and is in operation most of the day. The Red and Blue lines of the metro use an older signal system from the 60s. The system used for the signaling of the two lines is of the type AC power line with coding and uses relay switches.</p><p>The track circuit that detects the position of different trains on the track use the frequency 75 Hz. When converting the frequency 50 Hz to 75 Hz, carbon dust and heat losses is released to the space from the frequency converter. The carbon dust emitted into the air risks affecting relay functionality as it enters the relay housing and settles on the surface of the relay contacts in the space next to the frequency converter.</p><p>The Traffic Administration have plans to upgrade the Red Line's signaling system in order to use the existing signaling system for another 25 years. The track circuit system positions and gives speed messages to the trains. A literature study has been conducted on the construction of the track circuit system and previous research has been conducted on frequency inverters to safely find a new frequency inverter that does not risk relay functionality. Through fact-finding from the Traffic Administration, new suitable frequency converters that could be replaced with the existing ones were investigated in this research.</p><p>Static frequency converters were chosen to be investigated. According to the requirements of the Traffic Administration, four products were identified from different suppliers. None of the static frequency converters was directly compatible with existing systems because the system was built for a rotary frequency converter. "HZ-50-1105" from GoHz this model is available in 1-phase input and 1-phase output respectively. "FR-D 700" from Mitsubishi and "Micromaster 440" from Siemens which were equal in and out connection. "ACS-150" from ABB this model had a maximum rated power up to 4 kVA, which is only suitable for frequency inverters with a rated power of 2.5 kVA. The various types of static frequency inverters loaded the network in an asymmetrical way. Therefore, it was important to be able to distribute the load as evenly as possible on each phase. It was difficult to be able to rank the selected products without just showing the choices that existed when replacing rotary frequency converters. However, the use of static frequency inverters eliminates the risks of a rotating frequency converter. Static frequency converters can more accurately generate different frequencies and they have a shorter commissioning time; however, they load the network asymmetrically compared to rotating frequency converters.</p>
----------------------------------------------------------------------
In diva2:851774 abstract is: <p>The most common cancer form among men i prostate cancer and measurement of circulating tumor cells (CTCs) in the blood is a useful tool to evaluating the responset o treatment and progress of disease. CTC’s are cells that have detached from the original tumor and have spread in the blood sstem. In this master thesis a panel of 16 genes at single cell levels are analyzed using Reverse Transciptase Multiplexed Ligation-dependent Probe Amplification (RT-MLPA). The method uses gene specific reverse transcription primers to generate and amplify cDNA which MLPA probes are hybridized to. Correctly hybridized probes are ligated and amplified so that relative expression profiles can be calculated. One additional MLPA probe was designed to add to the existing panel. The results show that the MLPA reaction generates products from all genes in the panel when performed on synthetic MLPA prbe targets with equal concentrations. Results from totatl RNA on cell lines show that the reverse transcription and amplification of cDNA need further optimizations. When the whole assay is working it will be possible to evaluate gene expression from CTC’s that can help us understand the  progression and spread of prostate cancer in the body.</p>

w='prbe' val={'c': 'probe', 's': 'diva2:851774', 'n': 'correct in original'}
w='responset' val={'c': 'response to', 's': 'diva2:851774'}
w='sstem' val={'c': 'system', 's': 'diva2:851774', 'n': 'correct in original'}
w='totatl' val={'c': 'total', 's': 'diva2:851774', 'n': 'error in original'}
w='Transciptase' val={'c': 'Transcriptase', 's': 'diva2:851774', 'n': 'correct in original'}

corrected abstract:
<p>The most common cancer form among men is prostate cancer and measurement of circulating tumor cells (CTCs) in the blood is a useful tool to evaluating the response to treatment and progress of disease. CTCs are cells that have detached from the original tumor and have spread in the blood system. In this master thesis a panel of 16 genes at single cell levels are analyzed using Reverse Transcriptase Multiplexed Ligation-dependent Probe Amplification (RT-MLPA). The method uses gene specific reverse transcription primers to generate and amplify cDNA which MLPA probes are hybridized to. Correctly hybridized probes are ligated and amplified so that relative expression profiles can be calculated. One additional MLPA probe was designed to add to the existing panel. The results show that the MLPA reaction generates products from all genes in the panel when performed on synthetic MLPA probe targets with equal concentrations. Results from total RNA on cell lines show that the reverse transcription and amplification of cDNA need further optimizations. When the whole assay is working it will be possible to evaluate gene expression from CTCs that can help us understand the progression and spread of prostate cancer in the body.</p>
----------------------------------------------------------------------
In diva2:1859331 abstract is: <p>Surface water preparation in Vombs water begins with pumping water from lake Vomb through micro-strainers that separate reeds, silt, and larger particles into several infiltration beds. The raw water is then allowed to slowly permeate the infiltration bed, which consists of coarser layers of gravel and finer layers of sand and alluvium that naturally filter the water from minor physical impurities. The filtered water is collected in wells which can then be pumped up for further treatment. The reason why this can be done on water from lake Vomb is because of the low NOM (Natural Organic Material) content in the Vomb lake water. NOM is the most important building block for microbiological growth and originates from decomposed material such as leaves, branches, and dead animals. A high content of it in the water will lead to clogging of the infiltration bed and thus must be separated before it is pumped to the beds. Vombs waterworks want to start mixing water from Lake Bolmen into the water to be purified to increase capacity and redundancy, but this is a problem as raw water from Bolmen has a high NOM content. The most effective and used method to separate NOM from the water is to use a precipitation chemical such as ALG (Aluminum sulfate tetra decahydrate) or PAX-XL100 (Poly aluminum chloride). A preparatory process must therefore be introduced to Vombs waterworks where the water is first mixed, treated with coagulants and undergoes rapid sedimentation before it is pumped out to the infiltration bed due to the newly increased NOM content from lake Bolmen. In this work, the efficiency of PAX-XL100 and ALG to separate NOM from different mixtures of water from Bolmen and Vomb will be measured, compared, and discussed.</p><p>The choice of coagulant plays a major role in water purification as they are often different in their effectiveness to separate different kinds of impurities together with several other advantages and disadvantages. Testing of these coagulants is carried out in several samples with water that is aimed to be purified, on a smaller scale that is easy to control, these tests are called jar tests. At Sydvatten AB, which is a municipally owned water treatment company, it was of interest to evaluate if a mixture of two coagulants, PAX-XL100 and ALG, could be used to purify different water mixtures of lake Bolmen and lake Vomb.</p><p>This purification was evaluated with a spectrophotometric analysis of the water at two wavelengths. The first wavelength measured was 254nm and is in the ultraviolet spectrum. Conjugated and aromatic carbon compounds absorb light at this wavelength and are associated with the presence of dissolved organic matter in the water. The second wavelength is 436nm and belongs to the visual spectrum. This wavelength is measured because substances that are observed to be yellow in ordinary sunlight absorb at this wavelength and the presence of organic material often colors water yellow. Thus, a higher value of 436nm will show an increased presence of organic material. The limits set for the water to be considered sufficiently purified at these wavelengths were A<sub>254nm</sub>=10m<sup>−1</sup> and A<sub>436nm</sub>=0.700m<sup>−1</sup>.</p><p>After tests were done, the results showed that a mixture of 50:50 (PAX-XL100:ALG) purified the water below these limits with 𝐴<sub>254𝑛𝑚</sub>=8.12, 𝐴<sub>436𝑛𝑚</sub>=0.363 on a water mixture of 20:80(Vomb:Bolmen) and 𝐴<sub>254𝑛𝑚</sub>=7.56, 𝐴4<sub>36𝑛𝑚</sub>=0.324 at 50:50(Vomb:Bolmen). The difference in NOM separation between pure PAX-XL100 and ALG is 1.00±0.25 m<sup>-1</sup>. No NaOH needed to be added when a treatment with pure PAX-XL100 was used in a 20:80 water mixture (Vomb:Bolmen) along with a smaller carbon footprint. This means that the question for future use is whether it is worth separating 1.00±0.25 m<sup>-1</sup> extra NOM from the water with ALG and spend 6.6 million kroner in NaOH costs per year together with an increased carbon footprint or reducing the NOM separation with PAX -XL100 and reduce costs and carbon footprint.</p>

w='PAX- XL100X' val={'c': 'PAX-XL100', 's': 'diva2:1859331'}
w='XL100' val={'c': 'PAX-XL100', 's': 'diva2:1859331'}

corrected abstract:
<p>Surface water preparation in Vombs water begins with pumping water from lake Vomb through micro-strainers that separate reeds, silt, and larger particles into several infiltration beds. The raw water is then allowed to slowly permeate the infiltration bed, which consists of coarser layers of gravel and finer layers of sand and alluvium that naturally filter the water from minor physical impurities. The filtered water is collected in wells which can then be pumped up for further treatment. The reason why this can be done on water from lake Vomb is because of the low NOM (Natural Organic Material) content in the Vomb lake water. NOM is the most important building block for microbiological growth and originates from decomposed material such as leaves, branches, and dead animals. A high content of it in the water will lead to clogging of the infiltration bed and thus must be separated before it is pumped to the beds. Vombs waterworks want to start mixing water from Lake Bolmen into the water to be purified to increase capacity and redundancy, but this is a problem as raw water from Bolmen has a high NOM content. The most effective and used method to separate NOM from the water is to use a precipitation chemical such as ALG (Aluminum sulfate tetra decahydrate) or PAX-XL100 (Poly aluminum chloride). A preparatory process must therefore be introduced to Vombs waterworks where the water is first mixed, treated with coagulants and undergoes rapid sedimentation before it is pumped out to the infiltration bed due to the newly increased NOM content from lake Bolmen. In this work, the efficiency of PAX-XL100 and ALG to separate NOM from different mixtures of water from Bolmen and Vomb will be measured, compared, and discussed.</p><p>The choice of coagulant plays a major role in water purification as they are often different in their effectiveness to separate different kinds of impurities together with several other advantages and disadvantages. Testing of these coagulants is carried out in several samples with water that is aimed to be purified, on a smaller scale that is easy to control, these tests are called jar tests. At Sydvatten AB, which is a municipally owned water treatment company, it was of interest to evaluate if a mixture of two coagulants, PAX-XL100 and ALG, could be used to purify different water mixtures of lake Bolmen and lake Vomb.</p><p>This purification was evaluated with a spectrophotometric analysis of the water at two wavelengths. The first wavelength measured was 254nm and is in the ultraviolet spectrum. Conjugated and aromatic carbon compounds absorb light at this wavelength and are associated with the presence of dissolved organic matter in the water. The second wavelength is 436nm and belongs to the visual spectrum. This wavelength is measured because substances that are observed to be yellow in ordinary sunlight absorb at this wavelength and the presence of organic material often colors water yellow. Thus, a higher value of 436nm will show an increased presence of organic material. The limits set for the water to be considered sufficiently purified at these wavelengths were A<sub>254nm</sub>=10m<sup>−1</sup> and A<sub>436nm</sub>=0.700m<sup>−1</sup>.</p><p>After tests were done, the results showed that a mixture of 50:50 (PAX-XL100:ALG) purified the water below these limits with 𝐴<sub>254𝑛𝑚</sub>=8.12, 𝐴<sub>436𝑛𝑚</sub>=0.363 on a water mixture of 20:80(Vomb:Bolmen) and 𝐴<sub>254𝑛𝑚</sub>=7.56, 𝐴4<sub>36𝑛𝑚</sub>=0.324 at 50:50(Vomb:Bolmen). The difference in NOM separation between pure PAX-XL100 and ALG is 1.00±0.25 m<sup>-1</sup>. No NaOH needed to be added when a treatment with pure PAX-XL100 was used in a 20:80 water mixture (Vomb:Bolmen) along with a smaller carbon footprint. This means that the question for future use is whether it is worth separating 1.00±0.25 m<sup>-1</sup> extra NOM from the water with ALG and spend 6.6 million kroner in NaOH costs per year together with an increased carbon footprint or reducing the NOM separation with PAX-XL100 and reduce costs and carbon footprint.</p>
----------------------------------------------------------------------
In diva2:744703 abstract is: <p>Gliomas are brain tumors of glial origin and the most aggressive form is called glioblastoma multiforme (GBM). In Sweden, 300 new GBM cases are reported each year. Although advance treatments including radiotherapy, chemotherapy and surgery, are available, the survival of GBM is less than two years from diagnosis. Cell lines derived from GBM and cultured as neurospheres are known to be very heterogenic, that is they comprise many different cells, therefore it is difficult to study what factors may influence their proliferation, differentiation, apoptosis etc.</p><p>Several recent studies have suggested Notch signalling as a potential therapeutic target [1]. Notch signalling occurs via direct cell-cell interactions and influence many different cell fate decisions including, proliferation, differentiation, apoptosis and migration. Abnormal function in this pathway is related to many different diseases including cancer. Furthermore, Notch play a crucial role in normal development of central nervous system and is involved in regulation response to hypoxia and angiogenesis which are associated with the development of GBM [2, 3]. Notch signalling is far from fully understood. Moreover antibodies that specifically block signalling through individual Botch receptors can provide tools to investigate this in detail, potentially leading to new cancer treatments.</p><p>The aim of this study was to utilize antibodies to specifically target Notch1, Notch2 and Notch3 in glioma cell lines, derived from glioma neural stem (GNS) cells, grown in both proliferating and differentiating conditions. The blocking antibodies were previously generated from a scFv phage display library and shown to interfere with Notch in a receptor-specific way [4]. After adherent culturing of glioma cell lines, the anti-Notch antibodies were employed to inhibit Notch signalling. Moreover, the gene expression of different markers related to Notch signalling pathway were analysed using RT-PCR and a gel-based semi-quantitative method.</p><p>We produced antibodies and verified their full activity after being incubated at 37°C during four weeks and also after long term storage at 4°C. Durign the project, a protocol for adherent culturing of three different GNS cell lines, in both proliferating and differentiating conditions, were established. Further, gene expression of several Notch related proteins, including Notch1-3 and the ligands Jag1, Jag2, D111, D113 were verified using RT-PCR suggesting Notch to be highly active in GNS cell lines. On the contrary, Notch4 receptor and D114 ligand were not expressed in these cell linces. Hence these systems provide a good platform addressing the role of Notch signalling in initiation and progression of GBM. The involvement of Notch in these cells i further indicated by the fact that eve after several days in differentiating conditions, Notch target genes such as Hes5 and Hey1 were not fully down regulated. In addition, inhibition of single receptors with blocking antibodies does not seem to be enough and therefore additional approaches are required for a significant down regulation of Notch signalling GNS cells.</p>

w='linces' val={'c': 'lines', 's': 'diva2:744703', 'n': 'no full text'}
w='Durign' val={'c': 'During', 's': 'diva2:744703', 'n': 'no full text'}

corrected abstract:
<p>Gliomas are brain tumors of glial origin and the most aggressive form is called glioblastoma multiforme (GBM). In Sweden, 300 new GBM cases are reported each year. Although advance treatments including radiotherapy, chemotherapy and surgery, are available, the survival of GBM is less than two years from diagnosis. Cell lines derived from GBM and cultured as neurospheres are known to be very heterogenic, that is they comprise many different cells, therefore it is difficult to study what factors may influence their proliferation, differentiation, apoptosis etc.</p><p>Several recent studies have suggested Notch signalling as a potential therapeutic target [1]. Notch signalling occurs via direct cell-cell interactions and influence many different cell fate decisions including, proliferation, differentiation, apoptosis and migration. Abnormal function in this pathway is related to many different diseases including cancer. Furthermore, Notch play a crucial role in normal development of central nervous system and is involved in regulation response to hypoxia and angiogenesis which are associated with the development of GBM [2, 3]. Notch signalling is far from fully understood. Moreover antibodies that specifically block signalling through individual Botch receptors can provide tools to investigate this in detail, potentially leading to new cancer treatments.</p><p>The aim of this study was to utilize antibodies to specifically target Notch1, Notch2 and Notch3 in glioma cell lines, derived from glioma neural stem (GNS) cells, grown in both proliferating and differentiating conditions. The blocking antibodies were previously generated from a scFv phage display library and shown to interfere with Notch in a receptor-specific way [4]. After adherent culturing of glioma cell lines, the anti-Notch antibodies were employed to inhibit Notch signalling. Moreover, the gene expression of different markers related to Notch signalling pathway were analysed using RT-PCR and a gel-based semi-quantitative method.</p><p>We produced antibodies and verified their full activity after being incubated at 37°C during four weeks and also after long term storage at 4°C. During the project, a protocol for adherent culturing of three different GNS cell lines, in both proliferating and differentiating conditions, were established. Further, gene expression of several Notch related proteins, including Notch1-3 and the ligands Jag1, Jag2, D111, D113 were verified using RT-PCR suggesting Notch to be highly active in GNS cell lines. On the contrary, Notch4 receptor and D114 ligand were not expressed in these cell lines. Hence these systems provide a good platform addressing the role of Notch signalling in initiation and progression of GBM. The involvement of Notch in these cells i further indicated by the fact that eve after several days in differentiating conditions, Notch target genes such as Hes5 and Hey1 were not fully down regulated. In addition, inhibition of single receptors with blocking antibodies does not seem to be enough and therefore additional approaches are required for a significant down regulation of Notch signalling GNS cells.</p>
----------------------------------------------------------------------
In diva2:936265 abstract is: <p>Scania AB owns the complex test systems that generate complex and large amounts of data. To take advantage of this data, it must be stored in a suitable database, de-pending on the data type. One of these test systems is Hardware-In-the-Loop which tests electrical system and produces large amounts of electrical signals. The data of these electrical signals will be saved in the form of JSON files.In the current system, these data are stored and visualized in an inefficient manner. The purpose of this study was to investigate 4 relevant databases that can handle JSON files. The second part of this work was to visualize signals in the web brow-ser.Results of investigations show that two databases were more qualified to replace the current database. CouchDB and PostgreSQL was a test-item to measure their performance in terms of response time in relation to file size and the number of signals.Results of the tests show that CouchDB has higher performance for retrieving data than PostgreSQL. Then, a prototype was developed in order to visualize the signals in the web browser.</p>

w='brow-ser' val={'c': 'brow-ser', 's': 'diva2:936265'}

corrected abstract:
<p>Scania AB owns the complex test systems that generate complex and large amounts of data. To take advantage of this data, it must be stored in a suitable database, depending on the data type. One of these test systems is Hardware-In-the-Loop which tests electrical system and produces large amounts of electrical signals. The data of these electrical signals will be saved in the form of JSON files.</p><p>In the current system, these data are stored and visualized in an inefficient manner. The purpose of this study was to investigate 4 relevant databases that can handle JSON files. The second part of this work was to visualize signals in the web browser.</p><p>Results of investigations show that two databases were more qualified to replace the current database. CouchDB and PostgreSQL was a test-item to measure their performance in terms of response time in relation to file size and the number of signals.</p><p>Results of the tests show that CouchDB has higher performance for retrieving data than PostgreSQL. Then, a prototype was developed in order to visualize the signals in the web browser.</p>
----------------------------------------------------------------------
