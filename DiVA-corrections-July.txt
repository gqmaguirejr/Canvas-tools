In diva2:873936 there is an error in the title  'arambient' should be 'Ar ambient' - as can be see on the cover of the thesis.

Additionally, the abstract should be:

"We grow epitaxial graphene on 6H-SiC (0001) substrates in Ar background and intercalate hydrogen to reform the carbon buffer layer into single and multilayered graphene. We will analyze the graphene using a combination of techniques including optical microscopy, micro-Raman spectroscopy, Atomic Force Microscopy (AFM), and reflectance mapping and contactless measurements of sheet carrier density and charge carrier mobility. We have studied in detail, the influence of growth parameters and in-situ surface preparation of substrate on the thickness uniformity and surface morphology of graphene. Additionally, as-grown graphene layers were intercalated with H to obtain quasi-free standing layers of graphene with enhanced charge carrier mobility."
----------------------------------------------------------------------
In diva2:1216876 a number of words are merged, the abstract should be:

The aim of this research project was to create a conversational interface for retrieving information from rulebooks. This conversational interface takes the shape of an assistant named OLGA (short for Open Legend Game Assistant) to whom you can give enquiries about the rules of any game loaded into the program. We tuned and designed the assistant around a specific type of board games called TRPGs (tabletop role playing games), hence the conversational interface is focused around game rulebooks. By giving the assistant the rules for a game in the form of a raw text document the program can extract key concepts and words from the rules which we call entities. The process of extracting entities and all other functions of the assistant were calibrated on the TRPG called Open Legend, hence the name Open Legend Game Assistant. When the user sends a query to the assistant it is first sent to the web service Dialogflow for interpretation. In Dialogflow we enter our extracted entities to assist the service in recognizing key words and concepts in the queries. Dialogflow then returns an object with information telling the assistant what the intent of the user’s query was and any additional information provided. The assistant then responds to the query. The standard response for a request for information about an entity is what we call a streak search. The assistant locates parts of the rules that contain the entity and sorts them by a relevance score, then the results are presented in order of relevance. When testing on people with no prior knowledge of the game it was concluded that the assistant indeed could be helpful in finding answers to rule questions in the limited amount of time provided. Generalization being one of our goals the program was also applied on another rule system in the TRPG genre, Pathfinder, applied on this system the assistant worked as intended without altering any algorithm.
----------------------------------------------------------------------
In  diva2:1818546 \emph{outliers} indicated the word 'outliers' should be emphasize in italics or bold.

In diva2:1871154 'MITRAC/TC1500TM' should be 'MITRAC/TC1500™', i.e, the trademark character rather than "TM"

In diva2:742615,  in the abstract there are multiple spaces between words, some words are merged such as "andtoday"while others have been split apart, such as "per    cent"- the "full text" is not the PDF of the thesis, but rather a PDF containing images of files on someone's desktop! Additionally, there is no examiner listed in the DiVA entry.

In diva2:847246 in the abstract a number of words are merged together.
----------------------------------------------------------------------
In diva2:1295396 many words have been run together and the paragraphs are not kept. The abstract should be:

 <p>Uplink power control is a resource management function that controls the signal’s transmit power from a user device, i.e. mobile phone, to a base-station tower. It is used to maximize the data-rates while reducing the generated interference.</p><p>Reinforcement learning is a powerful learning technique that has the capability not only to teach an artificial agent how to act, but also to create the possibility for the agent to learn through its own experiences by interacting with an environment.</p><p>In this thesis we have applied reinforcement learning on uplink power control, enabling an intelligent software agent to dynamically adjust the user devices’ transmit powers. The agent learns to find suitable transmit power levels for the user devices by choosing a value for the closed-loop correction signal in uplink. The purpose is to investigate whether or not reinforcement learning can improve the uplink power control in the new 5G communication system.</p><p>The problem was formulated as a multi-armed bandit at first, and then extended to a contextual bandit. We implemented three different reinforcement learning algorithms for the agent to solve the problem. The performance of the agent using each of the three algorithms was evaluated by comparing the performance of the uplink power control with and without the agent. With this approach we could discover whether the agent is improving the performance or not. From simulations, it was found out that the agent is in fact able to find a value for the correction signal that improves the data-rate, or throughput measured in Mbps, of the user devices average connections. However, it was also found that the agent does not have a significant contribution regarding the interference.</p>

Similarly for diva2:1188687, the abstract should be:

<p>This degree project, conducted at Volvo Cars, investigates whether closed-loop re-simulation (CLR) methods can provide a safety proof for the autonomous driving (AD) functions based on previously collected driving data. The elements under study for this closed loop approach are model-in-loop based Simulation Platform Active Safety (SPAS) environment and Active Safety (AS) software.</p><p>The prerequisites for securing the closed loop re-simulation environment are performing open-loop simulations with AS software under test and preparing a validated vehicle model constituting the sensors and actuators. The validated vehicle model against a set of physical data ensures high confidence in the CAE environment. This results in high correlation between physical and simulated data for the closed loop tests performed for testing the Active Safety algorithms.</p><p>This thesis work focuses on preparing the vehicle model in SPAS with the emphasison performance of auto-brake functionality in CLR. The vehicle model in SPAS was prepared by tuning the brake model focusing on the EuNCAP cases in which CLR environment was subsequently tested with respect to Eu-NCAP scenarios.</p<<p>In the procedure of securing CLR methods, it was crucial to design the scenarios in virtual test environment as close as possible to field test conditions to make reliable comparison with the reality. Therefore, the verification of CLR environment was carried out by subjecting the CAE Environment to EuNCAP braking scenarios with dry surfaces, host vehicle velocities up to 80 km/h and target vehicle deceleration levels being 2m/s<sup>2</sup> and 6m/s<sup>2</sup>.</p><p>As a result of all these virtual tests, it was empirically verified that CLR environment can be used to predict braking behaviour of the vehicle in certain traffic scenarios for the verification of autonomous driving functions.</p>


----------------------------------------------------------------------
 in diva2:1271491, merged words and a Roman numeral page number The  abstract should be:

 <p>There is currently a high demand for electric power from renewable sources. One source that remains relatively untapped is the motion of ocean waves. Anders Hagnestål has been developing a uniquely efficient and simplified design for a point-absorb buoy generator by converting its linear motion directly into alternating electric power using a linear PM engine. To test this method, a smaller prototype is built. Its characteristics present some unusual challenges in the design and construction of its winding.</p><p>Devices of this type typically use relatively low voltage (690V typically for a wind turbine, compared to the 10kV range of traditional power plants). To achieve high power, they need high current, which in turn requires splitting the conductors in the winding into isolated parallel strands to avoid losses due to eddy currents and current crowding. However, new losses from circulating currents can then arise. In order to reduce said losses, the parallel conductors should be transposed in such a way that the aggregate electromotive force the circuits that each pair of them forms is minimized.</p><p>This research and prototyping was performed in absence of advanced industrial means of construction, with limited space, budget, materials, manpower, know-how, and technology. Manual ingenuity and empirical experimentation were required to find a practical implementation for: laying the cables, fixing them in place, transferring them to the machine, stripping their coating at the ends and establishing a reliable connection to the current source.</p><p>Using theoretical derivations and FEM simulation, a sufficiently good transposition scheme is proposed for the specific machine that the winding is built for. A bobbin replicating the shape of the engine core is built to lay down the strands.</p><p>The  parallel strands are then organized each into their respective bobbin, with a bobbin rack and conductor funneling device being designed and constructed to gather them together into a strictly-organized bundle. An adhesive is found to set the cables in place.</p><p>Problems with maintaining the orientation and configuration of the cables in the face of repeated torsion are met and solved. A chemical solution is used to strip the ends of the conductors, and a reliable connection is established by crimping the conductors into a bi-metal Cu-Al lug.</p><p>In conclusion, the ideal transposition schemes required to cancel out circulating currents due to magnetic flux leakage are impossible to put in practice without appropriate technological means. The feasible transposition scheme turns out to be a simple mirroring of conductors’ positions, implemented by building each half of the winding separately around replicas of the core and then connecting them using crimping lugs.</p>
Note that diva2:654273 is one of the many theses that does not have full text in DiVA, but rather therre is a link to http://www.csc.kth.se/utbildning/kandidatexjobb/datateknik/2011/rapport/hassel_olle_OCH_janse_petter_K11049.pdf
 - it is likely that the www.csc.kth.se server will go away and this this and other full text will be lots (much the same as happened when the nada server went away).
----------------------------------------------------------------------

In diva2:860672, there were merged words and missing ligatures, the abstract should be:

<p>The efficiency of today's after treatment system relies on the exhaust gas temperature.The Selective Catalytic Reduction system, which is a part of the after treatment system has an operating temperature window which starts at approximately 200C in order to work as efficiently as possible. Rolling downhill without burning fuel could cause the exhaust after treatment system to cool down below this window, which gives higher emissions when burning fuel again.This thesis focuses on investigating the potential of using the exhaust brake, a throttle between the turbine and the exhaust after treatment system, in slopes and investigates and evaluates possible strategies. The exhaust brake is controlled using Model Predictive Control with models of acceleration and temperature with respect to exhaust brake pressure, mass and road incline. Different strategies are investigated and it is concluded that controlling the exhaust brake to keep constant velocity down the slope gives the highest temperature at the exhaust after treatment system. It is also concluded that there is no strategy which could guarantee the exhaust after treatment temperature to stay within the temperature window just using the exhaust brake.</p>

In diva2:1485487, merged words and missing subscript, the abstract should be: 

<p>This thesis considers stabilization of constant power loads (CPLs) fed by a dc power source through an input filter, using model predictive control (MPC). Train propulsion systems generally utilize electrical motors whose output torque is tightly regulated by power converters. Often, these systems behave as CPLs. When a CPL is coupled with an input filter it can lead to a stability problem known as the negative impedance instability problem. Current state of the art regulators deal with this problem using classical frequency domain optimization-based controllers, such as H<sub>∞</sub>. This thesis instead proposes a linear parameter-varying model predictive controller (LPV-MPC). This advanced control method solves the negative impedance instability problem while also being capable of explicitly addressing signal constraints, which often exist in power converter applications. The regulator is evaluated in MATLAB/Simulink as well as in a software-in-the-loop (SIL) simulator. It has furthermore been realized in a real-time hardware-in-the-loop (HIL) simulator and tested in a power laboratory. Theoretical results show improved performance over conventional H<sub>∞</sub> controllers, in terms of damping and control input use, under certain operating conditions where the control input is limited. The results can be used as a benchmark of theoretical performance limits for design of other regulators.</p>

In diva2:1414451, 'coreferencere': should be 'coreference', so the abstract would be:

: <p>This degree project examines and evaluates the performance of various ways of improving contextualization of text span representations within a general multi-task learning framework for named entity recognition, coreference solution and relation extraction. A span-based approach is used in which all possible text spans are enumerated, iteratively refined and finally scored. This work examines which ways of contextualizing the span representations are beneficial when using the text embedder BERT. Furthermore, I evaluate to what degree graph propagations can be used together with BERT to enhance performance further, and observe F1-score improvements over previous work. The architecture sets new state-of-the-art results on four datasets from different domains - SciERC, ACE2005, GENIA and WLPC. Qualitative examples are provided to highlight model behaviour and reasons for the improvements are discussed.</p>
----------------------------------------------------------------------
----------------------------------------------------------------------
In diva2:818797 each line of the abstract had been treated as a paragraph, but there are only three paragraphs. There are also some words merged. The abstract should be::

<p>The objective of this thesis is to derive an analytical model representing a reduced form of a mine hoist hydraulic braking system. Based primarily on fluid mechanical and mechanical physical modeling, along with a number of simplifying assumptions, the analytical model will be derived and expressed in the form of a system of differential equations including a set of static functions. The obtained model will be suitable for basic simulation and analysis of system dynamics, with the aim to capture the fundamentals regarding feedback control of the brake system pressure.</p><p>The thesis will mainly cover hydraulic servo valve and brake caliper modeling including static modeling of brake lining stress-strain and disc spring deflection force characteristics. Nonlinearities such as servo valve hysteresis, saturation, effects of under- or overlapping spool geometry, flow forces, velocity limitations and brake caliper frictional forces have intentionally been excluded in order not to make the model overly complex. The hydraulic braking system will be described in detail and basic theory that is needed regarding fluid properties and fluid mechanics will also be covered so as to facilitate the reader in his understanding of the material presented in this work.</p><p>Overall, the scope of this thesis is broad and more work remains in order to complement the model of the system both qualitatively and quantitatively. Although not complete in its simplified form and with known nonlinearities aside, the validity of the model in the lower frequency domain is confirmed by results given in form of measurements and dynamic simulation. Static analysis of the brake caliper model is also verified to be essentially correct when comparing calculated characteristics against actual measurements, as is also the case for the static models of the brake lining and disc-spring characteristics.</p>

In diva2:1298737 there were merged words and missing ligatures, the abstract should be:

 <p>The recent increase of electric cars adoption will influence the electricity demand in the distribution networks which risks to be higher than the maximum power available in the grid, if not well planned. For this reason, it is on the DSOs and TSOs's interest to plan carefully coordinated charging of a bulk of EVs as well as assess the possibility of EVs acting as energy storages with the Vehicle-to-Grid (V2G) or Vehicle-to-Building (V2B) capability. When parked and plugged into the electric grid, EVs will absorb energy and store it, being also able to deliver electricity back to the grid/building (V2G/B system). This can be an optimized process, performed by an aggregator, gathering multiple EVs that discharge the battery into the grid at peak time and charge when there is low demand i.e. overnight and off-peak hours.</p><p>Numerous studies have investigated the possibility of aggregating multiple EVs and optimizing their charging and discharging schedules for peak load reduction or energy arbitrage with participation in the electricity market. However, no study was found for optimizing a shared fleet of EVs with daily reservations for different users trying to perform V2B. In this study an optimization modelling algorithm (mixed integer linear problem - MILP) that manages the possible reservations of the shared fleet of EVs, coordinates the charging and discharging schedules, and provides V2B (Vehicle-to-Building), with the objective of minimizing energy costs and accounting with battery ageing has been developed. A case study with real data for a building is carried out modelling different number of EVs for two different days in year 2017, one in March and other in June.</p><p>Results show that the profits are higher for all cases when introducing V2B as compared to a no optimization scenario: V2B with battery degradation (50 ore/kWh) has decreased daily variable electricity costs between 54 and 59% in March and 60 and 63% for June when compared without smart charging. Integration of battery degradation cost in V2B applications is necessary and influences significantly the charging and discharging strategies adopted by EV and finally the total daily costs: The total daily cost increase by maximal 10% for the day in March and 13% for the day in June when comparing the scenario that has stationary battery and uses only-charging model for EVs with the scenario applying V2B mode considering a degradation cost of 80 ore/kWh.</p>

In diva2:489848, there were merged words and the "ff" ligature. The abstract should be: 

<p>The sliding contact between catenary and pantograph has to transfer a large amount of current and power to the locomotive reliably. Sometimes detachment and attachment occur between the contact wire and the pantograph.This is related to the speed of the train as well as the lifting force of the pantograph. It leads to the creation of arcs whose visibility is also increased by the current level and their probability of appearance by the presence of a layer of ice (under harsh winter conditions) that could be formed on the bottom of the contact wire, which creates a separation between pantograph and catenary. The ﬁrst goal of the work is to create a model of the arc impedance, based on previously done experimental investigations. The model aims to simulate arc voltages (output) from experimental arc currents (input) which differentiate from each other by the RMS value, the voltage level, the powerfactor or the speed of the train. A strong link between the arc current waveform characteristics and the corresponding arc voltage has been brought to light, leading to very good simulations: for each arc period, the voltage is strongly related to the extreme value of the current over the same period. Once the model has been built and validated, it has been found that the simulated arc voltages were quite similar to the experimental voltages, and thus that the main part of the information about the arc phenomenon is contained in the arc current itself. The transient overvoltages as well as the average values are well modeled and approximated. Then, the modeled arc voltages have been studied in order to see how the impedance model behaves when the arc currents have different characteristics. Both thermal and dielectric reignitions (breakdowns) are correctly recreated, and the presence of a net DC voltage component (originating from transients and asymmetrically distorted waveforms) was also conﬁrmed for all the test runs. Finally, an electric circuit model is elaborated. Starting from a simple frequency analysis, an RLC circuit has been simulated and the encouraging results lead the way to a further study.</p>

In diva2:930984 there was a "ff" and "fi" ligature, the abstract should be:

 <p>Handsets have experienced a tremendous growth over the past few years, with users have more and more options in applications. The push notification has replaced the email grown in importance as a new bridge for application service provider to engage with users. The essential purpose of push notifications is to enable an app to inform its users that it has something for them when the app is not running in the foreground. For example, a message or an upcoming appointment [1]. However, in practice, the notification channel would get ﬂooded by commercials regardless of which platform that was used. Aimless push notifications would only result in pushing the users away from the applications. This leads to the rise of the concept of contextual notifications, sending notifications at the right place, right time, regarding the right piece of information to the targeted audience. The concept of contextual notifications is not entirely new, yet the power of contextual notifications has not been fully developed, especially for video on demand applications. By sending more personal, contextual notifications to the user, it could prevent users from not opening the application or even deleting the application. Moreover, since users nowadays are used to having a large range of options in browsing videos over different platforms as well, contextual notifications for cross platforms are also of interest. Users can use their companion mobile devices to interact with the video that is being played on the ﬁrst screen, even the two devices are different platforms, the companion device still can receive the relative contextual notification about the video on the ﬁrst screen.</p><p>The thesis work presents the result of the exploratory study and design development, then a discussion and conclusion will is presented based on the result. The development consists of three main parts. First of all, a high-level design and a framework are proposed based on pre-study of Accedo Appgrid product and the other existing system. Secondly, an implementation on proof of concept of contextual notifications server is performed. Thirdly, a demonstration application is implemented in order to test the proof of concept contextual notification server.</p><p>In the end, the functional evaluation is presented and a conclusion can be made that the contextual notification solution is achievable.</p>

In diva2:1263117 there was an "ff" ligature, a missing italics,  and the paragrpahs were all merged, the abstract should be:

 <p>The recent emergence of a distributed technology named blockchain, clearly created a new point of view in the data storing and data distribution fields. If on one hand blockchain is mainly known for Bitcoin (an auto-regulated decentralized digital currency), on the other hand it has the potential to set up an auto regulated economy.</p><p>In this thesis, the blockchain technology will be analyzed and described starting from P2P architecture and its origin in 2009 Satoshi Nakamoto’s whitepaper, and leading to the most up to date blockchains. The advantages and disadvantages of such architecture will be pointed out keeping in mind the security, speed and cost of such infrastructure.</p><p>While Real Estate companies have often anticipated the technological innovations, land registries, instead, derive and keep a working manner which is extremely old and out of date: made of unclear procedures and wet signatures. The market needs and legislation will be researched mainly referring to other works and integrated with a technical point of view with particular focus on the decentralization of such systems.</p><p>After analyzing the flow, problems and flaws of the current system, a new proposal will be researched, in particular trying to minimize the dead time in between the different steps of the mortgage, increase transparency, as well as reducing dependence on the central authorities, leading to more convenient interactions among the properties’ stakeholders. An attractive low capitalization decentralized financial product will also be proposed and implemented able to lower the interest rate and create a profitable investment with low risk, low interest and durable in time.</p><p>Secure and <em>ad-hoc</em> algorithms will be presented and, in a later section, analyzed in combination with different blockchain technologies. Scalability and performance will also be evaluated, taking into account all the current technology limitations and the near future opportunities.</p>

In diva2:1368334 there was an "ff" ligature, a missing italics. The abstract should be:

 <p>In the last years, the demands on different models using deep learning to generate textual data conditionally have increased, where one would like to control what textual data to generate from a deep learning model. For this purpose, a couple of models have been developed and achieved state-of-art performance in the ﬁeld of generating textual data conditionally. Therefore, the purpose of this study was to develop a new model that could outperform the relevant baseline models with respect to the BLEU metric. The alternative model combined some of the properties from the state-of-art models and was given the name the <em>Variational Attribute-to-Sequence decoder model</em> (shortened to the V-Att2Seq model) that paraphrases the name of one of the state-of-art models and "variational" refers to its application of variational recurrent autoencoders (VRAE). The data set used in this study contained drug reviews that were written by patients to express their opinion about the drug that they have used to treat a certain condition. The drug review texts were accompanied by the following attributes: the (name of the) drug, the condition, and the rating that the patient has given to the drug. The results in this study show that the V-Att2Seq model did not outperform all the baseline models, which concluded that the V-Att2Seq model did not satisfy the requirements imposed on the model itself. However, there are some future work that is suggested by this study to hopefully improve the performance of the V-Att2Seq model in the future such as including other mechanisms that are present in the state-of-art models, testing with e.g. other sizes and settings of the V-Att2Seq model, and testing different strategies forgenerating sequences since there is still potential that has been observed in the model that should be further investigated to improve its performance.</p>

In diva2:1586228 there was an "ff" and "fi" ligatures the paragrpahs were all merged, the abstract should be:

 <p>Databases of user generated data can quickly become unmanageable. Klarna faced this issue, with a database of around 700,000 customer reviews. Ideally, the database would be cleaned of uninteresting reviews and the remaining reviews categorized. Without knowing what categories might emerge, the idea was to use an unsupervised clustering algorithm to find categories.</p><p>This thesis describes the work carried out to solve this problem, and proposes a solution for Klarna that involves artificial neural networks rather than unsupervised clustering. The implementation done by us is able to categorize reviews as either interesting or uninteresting. We propose a workﬂow that would create means to categorize reviews not only in these two categories, but in multiple.</p><p>The method revolved around experimentation with clustering algorithms and neural networks. Previous research shows that texts can be clustered, however, the datasets used seem to be vastly different from the Klarna dataset. The Klarna dataset consists of short reviews and contain a large amount of uninteresting reviews.</p><p>Using unsupervised clustering yielded unsatisfactory results, as no discernible categories could be found. In some cases, the technique created clusters of uninteresting reviews. These clusters were used as training data for an artificial neural network, together with manually labeled interesting reviews. The results from this artificial neural network was satisfactory; it can with an accuracy of around 86% say whether a review is interesting or not. This was achieved using the aforementioned clusters and five feedback loops, where the model’s wrongfully predicted reviews from an evaluation dataset was fed back to it as training data.</p><p>We argue that the main reason behind why unsupervised clustering failed is that the length of the reviews are too short. In comparison, other researchers have successfully clustered text data with an average length in the hundreds. These items pack much more features than the short reviews in the Klarna dataset. We show that an artificial neural network is able to detect these features despite the short length, through its intrinsic design.</p><p>Further research in feature extraction of short text strings could provide means to cluster this kind of data. If features can be extracted, the clustering can thus be done on the features rather than the actual words. Our artificial neural network shows that the arbitrary features interesting and uninteresting can be extracted, so we are hopeful that future researchers will find ways of extracting more features from short text strings. In theory, this should mean that text of all lengths can be clustered unsupervised. </p>

In diva2:1205396 there was a "ff" ligrautre, the abstract should be:

 <p>Nowadays it is more clear that the Internet of things (IoT) is not a transient trend but a completely new industry. The internet of things has the capability to enhance current industries (Industry 4.0), as well as to help protecting the environment and people. The latter is the case with the system developed and described in this thesis.</p><p>The possibilities that IoT brings are due to the interconnection of heterogeneous embedded devices to the internet. This thesis focus on LPWANs (Low Power Wide Area Networks), which is a new set of technologies specifically design for the needs of IoT devices.Due to the recent deploy of NB-IoT (Narrow Band IoT) networks it has become more diﬃcult to know what LPWAN is best for a certain application. Thus, the first half of this thesis involves the comparative study of NB-IoT and LoRaWAN LPWANs. This comparison required an in depth study of each technology, specially on the physical and datalink layers. The comparison briefly displays the main characteristics of each technology and explain the main conclusions in a concise manner.</p><p>The second part of the thesis describes the development of a GNSS tracker. This tracker will be used on train wagons carrying goods that are dangerous for people and the environment. This thesis report describes the different steps taken, from the requirement specification to the partial development of the software.</p>

In diva2:931627  there was a "ff" ligrautre and merged paragraphs, the abstract should be

<p>Autonomous driving vehicles introduce challenging research areas combining different disciplines. One challenge is the detection of obstacles with different sensors and the combination of information to generate a comprehensive representation of the environment, which can be used for path planning and decision making.</p><p>The sensor fusion is demonstrated using two Velodyne multi beam laser scanners, but it is possible to extend the proposed sensor fusion framework for different sensor types. Sensor fusion methods are highly dependent on an accurate pose estimate, which can not be guaranteed in any case. A fault tolerant sensor fusion based on the Dempster Shafer theory to take the uncertainty of the pose estimate into account is discussed and compared using an example, although not implemented on the test vehicle.</p><p>Based on the fused occupancy grid map, dynamic obstacles are tracked to give a velocity estimate without the need of any object or track association methods. Experiments are carried out on real world data as well as on simulated measurements, for which a ground truth reference is provided.</p><p>The occupancy grid mapping algorithm runs on central- and graphical-processing units, which allows to give a comparison between the two approaches and to stress out which approach is preferably used, depending on the application.</p>
----------------------------------------------------------------------
In diva2:1590753 there was a "ff" ligature and the paragraphs were merged, the abstract should be:

 <p>In general, electric propulsion offers very high eﬃciency but relatively low thrust. To remedy this, several ion engines can be assembled in a clustered configuration and operated in parallel. This requires the careful design of a frame to accommodate the individual propulsion systems. This frame must be modular to be used in different cluster sizes, and verify thermal and mechanical requirements to ensure the nominal operation of the thrusters.</p<<p>The present report aims to show the design process of such a frame, from preliminary modelling to the experimental study of a prototype. This document features an overview of the iterative design process driven by thermal simulations rendered on COMSOL Multiphysics. This process led to the conception of a 2-thruster and 4-thruster cluster frame. A lumped-parameter model of the electric propulsion system was also created to model its complex thermal behaviour. In addition, the 2-thruster frame was studied mechanically with analytical calculations and simulations of simple load cases on SolidWorks. Lastly, a prototype based on the 2-thruster frame model was assembled. The prototype was used to conduct temperature measurements while hosting two operating thrusters inside a vacuum chamber. The temperature distribution in the cluster was measured, and compared to simulation results.</p<<p>Thermal simulations of the 2-thruster and 4-thruster frame showed promising results, while mechanical simulations of the 2-thruster version met all requirements. Moreover, experimental results largely agreed with thermal simulations of the prototype. Finally, the lumped-element model proved instrumental in calibrating the models, with its high flexibility and quick computation time. </p>

In diva2:1046844:

 <p>Diabetes mellitus is a disease that leads to an unstable blood glucose concentration, with oscillations and peaks higher than the normal range. It is a worldwide problem that is currently affecting 387 million people. To avoid further complications associated to the disease, patients have to monitor their blood glucose level multiple times per day, making this compound the most commonly tested analyte. Self-monitoring blood glucose strips (SMBG strips) and continuous glucose monitoring systems (CGMS) are the most widespread glucose monitoring devices nowadays. CGMS, detecting glucose in interstitial fluid (ISF), even if more advantageous than traditional devices, are still relatively invasive and painful, due to their size and needle-based insertion mechanism.</p><p>For the correct functioning of amperometric glucose sensors, having a reference electrode (RE) which is stable in the physiological environment is a crucial requirement. In this work the development of stable and selective miniaturized electrodes to be integrated in biosensors for ISF glucose monitoring is presented. In particular, a method to produce stable miniaturized iridium/iridium oxide (Ir/IrOx) quasi-reference electrode (quasi-RE) was firstly developed. Cyclic voltammetry was used for this purpose, and the process allowed the realization of miniaturized electrodes, stable for several days under physiological conditions. By using this technology it is possible to solve some of the common problems arising from the miniaturization of traditional RE.</p><p>Secondly, miniaturized working electrodes (WE) for selective glucose detection in presence of interfering species have been developed. In fact, to detect the analyte, amperometric glucose sensors need the application of a potential, which also oxidizes interfering species and results in an overestimation of glucose reading. To solve the problem, anti-interference membranes based on different materials and deposition techniques have been developed. In particular, in this study three different membranes have been studied. The performed techniques allowed to realize reproducible films on top of miniaturized WE. As a future step, the realized electrodes should be integrated and tested as complete miniaturized glucose sensors, to prove the feasibility of further miniaturized CGMS.</p>


In diva2:931267 "ff" , "fl", and "ffi" ligature and missing italics and the note was missing (the URL is currently invalid), the abstract should be:

 <p>Digital technologies have increased the influence of technology in business, even changing business models and strategies of organisations. This influence, called <em>Digital Transformation of Business (DT)</em>, happens when there is an increase of the number of digital connections, information and interactions. This phenomena has been defined as <em>Digital Density (DD)</em> and aims to provide an assessment of the digitalization status of an organization. With the concept of <em>DD</em> we pro-pose the <em>DD Framework</em>, that has two parts: qualitative and quantitative. In order to support both parts we simulate the effect that a platform for smart-phones called Waze has on the traffic flow of a city. For the qualitative side, we redefine the concept of <em>DD</em> and with Business Model CANVAS we compare the changes introduced by Waze in the business model. The quantitative part of the simulation is performed with MATLAB and its goal is to understand how an agent chooses the path to drive from point <em>A</em> to point <em>B</em>. As expected, we prove that with more information agents can estimate better the costs of the different paths. Despite, even if our simulation seems to prove that the more information the agents have the better the system works, we introduce an analogy with the <em>Braess Paradox</em> to find potential problems and provide solutions for them. We conclude with a proposal of future work.</p><p><em>Note: This report, the presentation of this master thesis and the Matlab files used for the simulation can be found in next hyperlink:<br>
<a href="www.kth.se/profile/188107/page/master-thesis-digital-density-as-the-d/">www.kth.se/profile/188107/page/master-thesis-digital-density-as-the-d/</a></em></p>


In diva2:1040704 there were ligatures, the abstract should be

: <p>The objective of this report is to investigate the fringing flux around the air gap of a high frequency reactor and what correlation it has with losses, air gap length and frequency. A computer model is made using a finite element analysis software and a prototype reactor is built and tested on to verify the model. Variables such as air gap length, ripple frequency and current are changed in order to investigate different relationships. Results show that the computer model is sufficiently valid, and that trends regarding core losses versus frequency and air gap length correlates with theory. From test and simulation results conclusions are made for making designers aware of different measures for mitigating unwanted fringing flux effects.</p>


In diva2:1247549 there were ligatures and the delta character was missing, the abstract should be:

 <p>In order to accurate calculate currents during unsymmetrical faults in the electric power grid information about all components positive, negative and zero sequence impedances are important. Transformers are one key component in the power grid and accurate information about sequence impedances for power transformers are therefore essential. However, to analytically derive the zero sequence impedance is extremely hard. Therefore are the transformers tested before they are connected.‌</p><p>The Swedish TSO (Transmission System Operator), Svenska kraftnät, Svk, are responsible for operating and balancing the Swedish transmission grid. For Svk it is crucial that the grid is operated in a reliable and safe way so that the society always have available electricity at the same time as no persons working on the grid are exposed to unnecessary risks. Having accurate current calculations during unsymmetrical faults is one crucial part. Unfortunate, for some transformers in Svk area of responsibility the information about zero sequence impedance is missing and must therefore be estimated.</p><p>This work propose, by applying a case study on a existing group of transformers (three limb design with no &Delta; windings), a method for estimating the zero sequence impedance. The aim of the the analysis is to investigate if there are any relationship between the zero sequence impedance and some more basic operating data for the transformers. From an initial literature study it was concluded that the following parameters would affect the zero sequence impedance: rated power and voltage, no-load power and currents and information about winding configuration, manufacture and production year.</p><p>In the case study linear relationships was found based on a least square method and was tuned using a robust linear square method called Bisquare. The results showed that several of the tested quantities showed a growing trend when they were correlated with the zero sequence impedance. The no-load impedance was identified as one interesting quantity that was investigated further.</p><p>The tested transformers were divided into different groups based on number of windings and winding configuration. In the next step the transformers were ordered based on rated power. New linear relationships were derived for every group and the final results were compared between the different transformer groups.</p><p>The reliability of the proposed algorithm is dependent on the size of the tested transformer group and also on the data quality. The data quality was investigated by trying to identify transformer with abnormal values. The results showed that abnormal behaviour was very rare in the tested group of transformers.</p><p>The results from the case study indicates that a relationship between the no-load impedance and the zero sequence impedance exists for the common types of transformers, the YNyn0, YNauto0 and YNyn0yn0. The results shows that these relationships differ depending on the rated power of the high voltage winding. The case study also concludes that it is possible to estimate a winding’s zero sequence impedance if the zero sequence impedance for a second winding is known as well as the rated voltage of the two windings. The presented relationships have all some uncertainty related to them, for some relationships the uncertainty is relatively high whereas for others it is very low. The relationship with the lowest uncertainty found was the function describing a winding’s zero sequence impedance as a function of another windings zero sequence impedance and the rated voltages</p>

In diva2:1095893:

<p>Today the Internet of Things (IoT) lacks universal standards for communication and interaction between devices. There is a wide collection of diverse software architectures for IoT applications on the market, where smart devices from different manufacturers are often unable to interact with each other.</p><p>A standards organization within IoT gaining recognition is the Open Connectivity Foundation (OCF), an industry group delivering an IoT software framework specification and a product certification program. Open Connectivity Foundation (OCF) is funding an open source reference implementation of the specification called <em>IoTivity</em>, which runs as middleware intended to be portable to all operating systems and connectivity platforms. The goal of the OCF is to enable interoperability between IoT devices regardless of manufacturer, operating system, chipset or physical transport.</p><p>Through a literature review, the key functional and non-functional requirements for IoT middleware architectures were found. Functionality requirements identified were data management, resource management, resource discovery, and context-awareness. The quality attributes were found to be interoperability, adaptability, scalability, security, and real-time behavior.In this thesis project, IoTivity was evaluated with respect to these requirements with the scenario-based Method for Evaluating Middleware Architectures (MEMS). As a part of MEMS, a case study of implementing a building management system (BMS) with IoTivity was conducted.</p><p>Results showed that, within the framework of the case study, IoTivity complied with three out of four functional requirements, and three out of five quality requirements identified for IoT middleware architectures. One of the quality requirements, security, was not evaluated in this study.</p>


In diva2:855986 ligrature(s), should be

: <p>This thesis deals with the task of identifying implicit citations between scientific publications. Apart from being useful knowledge on their own, the citations may be used as input to other problems such as determining an author’s sentiment towards a reference, or summarizing a paper based on what others have written about it. We extend two recently proposed methods, a Machine Learning classifier and an iterative Belief Propagation algorithm. Both are implemented and evaluated on a common pre-annotated dataset. Several changes to the algorithms are then presented, incorporating new sentence features, different semantic text similarity measures as well as combining the methods into a single classifier. Our main finding is that the introduction of new sentence features yield significantly improved F-scores for both approaches.</p>

In diva2:576409 ligature(s) and some werhed words, should be:

 <p>Similar to many technological developments, wireless sensor networks have emerged from military needs and found its way into civil applications. Today, wireless sensor networks has become a key technology for different types of ”smart environments”, and an intense research effort is currently underway to enable the application of wireless sensor networks for a wide range of industrial problems. Wireless networks are of particular importance when a large number of sensor nodes have to be deployed, and/or in hazardous situations.</p><p>Localization is important when there is an uncertainty of the exact location of some fixed or mobile devices. One example has been in the supervision of humidity and temperature in forests and/or fields, where thousands of sensors are deployed by a plane, giving the operator little or no possibility to influence the precise location of each node. An effective localization algorithm can then use all the available information from the wireless sensor nodes to infer the position of the individual devices. Another application is the positioning of a mobile robot based on received signal strength from a set of radio beacons placed at known locations on the factory floor.</p><p>This thesis work is carried out on the wireless automation testbed at the S3. Focusing on localization processes, we will first give an overview of the state of the art in this area. From the various techniques, one idea was found to have significant bearing for the development of a new algorithm. We present analysis and simulations of the algorithms, demonstrating improved accuracy compared to other schemes although the accuracy is probably not good enough for some high-end applications. A third aspect of the work concerns the feasibility of approaches based on received signal strength indication (RSSI). Multiple measurement series have been collected in the lab with the MoteIV wireless sensor node platform. The measurement campaign indicates significant fluctuations in the RSSI values due to interference and limited repeatability of experiments, which may limit the reliability of many localization schemes, especially in an indoor environment.</p>


In diva2:576407 ligatures and merged words, should be:

 <p>Look ahead cruise control is a relatively new concept. It deals with the possibility of making use of recorded road slope data in combination with GPS, in order to improve vehicle cruise control. This thesis explores the possibility of estimating road slope as well as investigating the sensitivity of two look ahead controllers, with respect to errors in estimation of mass, wheel radius and road slope.</p><p>A filter using GPS and standard vehicle sensors is used for estimation of road slope. The filter is robust to losses in data since redundant information is available. Possible errors in estimation caused by the filter are identified.</p><p>Two previously published look ahead controllers using different strategies to control a heavy vehicle are investigated. A description of controller behaviour in perfect conditions is presented. Sensitivity analysis is performed identifying erroneous control behaviour inflicted by errors in the used vehicle model and road slope. Further, effects on controllers caused by errors in road slope estimation estimation are detected. Conclusions about the two look ahead strategies are drawn.</p>

In diva2:812042 ff ligrature , should be:

 <p>In this paper we compare various Sudoku solving algorithms in order to determine what kind of run-time improvements different optimizations can give. We will also examine what kind of effect the existence of multiple solutions in the puzzles has on our result.</p>

In diva2:1400696, ff ligature, should be:

 <p>Neurala nätverk är kända för att memorera delar av sina träningsset. När känslig information är inblandad kan därför offentliggörandet av ett tränat nätverk innebära att sekretessen bryts. I detta examensarbete använder vi differential privacy för att träna neurala nätverk som bevisligen skyddar deltagarnas identitet. I synnerhet tar vi upp de problem som uppstår inom området bildsegmentering. Här har tidigare metoder behövt lägga till orimligt höga brusnivåer för att skydda sekretessen på grund av den höga dimensionaliteten. Vi använder dimensionalitetsreduktion för att sänka den nödvändiga brusnivån, vilket resulterar i en bättre avvägning mellan sekretessbevarande och användbarhet. Vi bevisar integritetsgarantin formellt och utvärderar den prediktiva prestandan empiriskt på ett syntetiskt dataset.</p>


In diva2:1049369, ff ligrature, should be:

 <p>Wave power is currently a hot topic of research, and has shown great potential as a renewable energy source. There have been lot of progress made in developing cost effective Wave Energy Converters (WECs) that can compete with other sources of energy in regard to price and electrical power. Theoretical studies has shown that optimal control can increase the generated power for idealized WECs. This thesis is done in collaboration with CorPower Ocean, and investigates the use of economic Model Predictive Control (MPC) to control the generator torque in a light, point-absorbing, heaving WEC that is currently under development. The objective is to optimize the generator torque, such that the average generated power is maximized while maintaining a small ratio between maximum and average generated power. This results in a nonconvex cost function. Due to the highly nonlinear and nonsmooth dynamics of the WEC, two controllers are proposed. The first controller consists of a system of linear MPCs, and the second controller is a nonlinear MPC. Relevant forces acting on the WEC are identified and the system dynamics are modelled from a force perspective. The models are discretized and the controllers are implemented in Simulink. The WEC, together with the controllers, is simulated in an extensive Simulink model developed by CorPower Ocean. Several different types of ocean waves are considered, such as its energy content and its regularity. In the majority of cases, the controllers do not increase the performance of the WEC compared to a simple, well tuned controller previously developed by CorPower Ocean. Finally, possible improvements of how to reduce existing model errors are proposed.</p>

In diva2:576430, ff  and flligature, should be:

 <p>Chemical reactors are a part of modern industry and the catalytic tubular fixed bed reactor examined in this work is an important reactor for chemicals production.</p><p>In this work two different types of models for the reactor are studied; a pseudohomogeneous model and a heterogeneous model. The goal is to find differences in behaviour between these two types of reactor models and explain these.</p><p>In a real reactor there exists two phases, a solid catalyst and a fluid reactant. Both these phases are in the pseudohomogeneous model treated as a single phase, a pseudofluid. In the heterogeneous model the two phases are treated separately.</p><p>When comparing these types of models a few structural differences exist, void fraction, heat exchange between two phases, and heat dispersion in the phases, and all of these will affect the behaviour of the models differently.</p><p>The models are studied using bifurcation analysis and linear analysis. Bifurcation theory is used to find and track different solutions depending on a certain parameter and to get a good overall picture of a system’s solutions and their type, steady state or sustained oscillation.</p><p>Linear analysis is used to study linearization around a specific solution and to determine stability and frequency dependency.</p><p>It is found that the concept of void fraction in the reactor model affects the behaviour only as a time scaling, while the concept of interfacial heat exchange affects the stability. The distribution of heat dispersion between phases has a significant impact on the reaction behaviour. Feedback is determined as the main cause for instabilities and oscillative solutions.</p>

In diva2:65431 ff ligature1

: <p>This report evaluates possible performance differences between Java and native C on the operating system Android, by developing tests and analyzing the execution. The ambition is that each test should evaluate the performance of a certain task, such as memory access or arithmetic operations of different data types. The results were in some cases unexpected and show that the executed implementations were faster on C compared to Java on one of the test devices, but not the other. The conclusion partly opposes earlier research and this is probably partly due to the fact that the Java Virtual Machine has been improved vastly in the latest versions of Android.</p>

The full text is at:

https://www.csc.kth.se/utbildning/kandidatexjobb/datateknik/2011/rapport/ulvesand_andreas_OCH_eriksson_daniel_K11009.pdf

In diva2:918152 ff and ffi ligature, should be:

 <p>This Thesis details the research on Machine Learning techniques that are central in performing Anomaly and Masquerade attack detection. The main focus is put on Web Applications because of their immense popularity and ubiquity. This popularity has led to an increase in attacks, making them the most targeted entry point to violate a system. Specifically, a group of attacks that range from identity theft using social engineering to cross site scripting attacks, aim at exploiting and masquerading users. Masquerading attacks are even harder to detect due to their resemblance with normal sessions, thus posing an additional burden.</p><p>Concerning prevention, the diversity and complexity of those systems makes it harder to define reliable protection mechanisms. Additionally, new and emerging attack patterns make manually configured and Signature based systems less effective with the need to continuously update them with new rules and signatures. This leads to a situation where they eventually become obsolete if left unmanaged. Finally the huge amount of traffic makes manual inspection of attacks and False alarms an impossible task. To tackle those issues, Anomaly Detection systems are proposed using powerful and proven Machine Learning algorithms.</p><p>Gravitating around the context of Anomaly Detection and Machine Learning, this Thesis initially defines several basic definitions such as user behavior, normality and normal and anomalous behavior. Those definitions aim at setting the context in which the proposed method is targeted and at defining the theoretical premises. To ease the transition into the implementation phase, the underlying methodology is also explained in detail.</p><p>Naturally, the implementation is also presented, where, starting from server logs, a method is described on how to pre-process the data into a form suitable for classification. This preprocessing phase was constructed from several statistical analyses and normalization methods (Univariate Selection, ANOVA) to clear and transform the given logs and perform feature selection. Furthermore, given that the proposed detection method is based on the source and1request URLs, a method of aggregation is proposed to limit the user privacy and classifier over-fitting issues. Subsequently, two popular classification algorithms (Multinomial Naive Bayes and Support Vector Machines) have been tested and compared to define which one performs better in our given situations.</p><p>Each of the implementation steps (pre-processing and classification) requires a number of different parameters to be set and thus a method called Hyper-parameter optimization is defined. This method searches for the parameters that improve the classification results. Moreover, the training and testing methodology is also outlined alongside the experimental setup. The Hyper-parameter optimization and the training phases are the most computationally intensive steps, especially given a large number of samples/users. To overcome this obstacle, a scaling methodology is also defined and evaluated to demonstrate its ability to handle larger data sets.</p><p>To complete this framework, several other options have been also evaluated and compared to each other to challenge the method and implementation decisions. An example of this, is the "Transitions-vs-Pages" dilemma, the block restriction effect, the DR usefulness and the classification parameters optimization. Moreover, a Survivability Analysis is performed to demonstrate how the produced alarms could be correlated affecting the resulting detection rates and interval times.</p><p>The implementation of the proposed detection method and outlined experimental setup lead to interesting results. Even so, the data-set that has been used to produce this evaluation is also provided online to promote further investigation and research on this field.</p>

In diva2:576413 ff ligature, should be:

 <p>The fully automated milking system VMS has different functions which complements the actual milking of cows. This master thesis presents a method to improve the calculation of milk yield in dairy cows for the VMS. This report also investigates if it is possible to improve the algorithm for finding cows with mastitis (udder inflammation). The correctness of the prediction of milk yield is important for a couple of actions in the VMS. For example, valuable time can be saved if teatcups are attached first to high yielding teats. Only cows with an attained minimum level of predicted yield should be allowed to enter the VMS and get milked. Milking has traditionally been an event to monitor the condition of the cows. Therefore methods that determine the condition are demanded for any automatic milking systems. Mastitis is a costly illness and a working test for ill cows should be implemented in the VMS in order to know which cows that are ill.</p><p>The goal of this thesis work is to develop two new algorithms for the VMS. First, an improved algorithm for the prediction of secretion rate is presented. The improved algorithm uses a Kalman-filter to update the secretion-rate. The improved method has a lower total prediction in most cases. The Kalman-filter was tested and developed for five farms and was verified on one farm.</p><p>Second, this report investigates if a cusum test can be used to detect ill cows. The method turns out to be slightly better than the current algorithm. A test for cows which are milked on three or two teats is evaluated. In this test the number of milkings with high conductivity and low secretion rate are weighted together. This algorithm is slightly better than the current algorithm used for detection of ill cows.</p>


In diva2:576428, ff ligrature and merged words, should be:

 <p>As part of design of a personal robot for operation in an everyday environment there is need to endow the system with facilities to track a person as it is taken on a tour of the environment.</p><p>In the literature a number of different methods based on laser tracking and computer vision have been presented. However, most of these methods are not particularly robust and in many cases the methods do not operate in realtime.</p><p>In this master thesis a system for people detecting and tracking has been implemented using laser and vision. The information given by both scanners are used for two different purposes, laser range data are used to detect persons and the images grabbed by the camera are used to confirm the hypotheses made by the laser.</p><p>Methods for people tracking based on laser and vision have two main problems. The ones based on laser are not very robust and the ones based on vision hardly ever operate in real time.</p><p>This project is aimed at taking the main advantages of both methods:</p><p><strong>Laser</strong></p><p><em>Advantage: </em>High Speed =&gt; Works in real time</p><p><em>Disadvantage:</em> Not robust =&gt; Not reliable 100%</p><p></p><p><strong>Vision</strong><strong>:</strong></p><p><em>Advantage: </em>Robust =&gt; More reliable</p><p><em>Disadvantage: </em>Low speed =&gt; Does not work in real time</p>

In diva2:654196, ff ligature and hyphenation being kept, should be:

 <p>In this master thesis report a new method for simulating waters surface waves is presented. The method is well adapted for real-time applications and has been developed with computer games in mind. By simulating the water surface at several different resolutions simultaneously using a construction similar to Laplacian Pyramids dispersion is handled approximately resulting in a complex behaviour.</p><p>The simulation is also extended with a dynamic level of detail method and phenomenological models for boundaries and high frequency waves. This method is pro-totyped inside the Frostbite™ engine developed at EA™ DICE™ and runs at 3 ms per time step on a single core of a Intel™ Xeon™ processor with high quality results.</p>

The NADA link is no longer valid: https://www.nada.kth.se/utbildning/grukth/exjobb/rapportlistor/2011/rapporter11/ottosson_bjorn_11105.pdf

In diva2:1164152 ff ligature,should be:

 <p>The Miniature Student Satellite (MIST) is a project at the Royal Institute of Technology in Stockholm, Sweden. The goal of the project is to launch a satellite designed and constructed by different student teams. The satellite carries seven scientific experiments that continuously collects experiment data as the satellite orbits the Earth. When the satellite is launched, the communication link between the satellite and the ground station is an essential part. This communication consists of a radio link between the on-board computer through a radio module and the ground station on Earth. Satellite communication is not a new field and there exist predefined communication standards and protocols. These standards and protocols are quite extensive and need to be tailored for the specific mission.</p><p>Once a satellite is launched, it is out of reach for further development. This makes it crucial that the software running on the on-board computer are well tested and correctly integrated with the mission control system (MCS) that are used the send commands to control the satellite from Earth.</p><p>Since satellite radio equipment is expensive, this bachelor thesis describes how to set up, implement and test an end-to-end communication chain between the satellite on-board computer and the MCS using a hardware simulator. The simulator both mimics the functionality of an on-board radio and replaces the ground station and radio link. Communication standards and protocols are studied and investigated, alongside with on-board pre-implemented subsystem libraries and an MCS named Elveit from Solenix. As the simulator also replaces the radio link, data transfer errors such as data loss, data corruption and, connection time windows can be simulated and tested.</p><p>The simulator development results in a feasible end-to-end communication chain between the on-board computer and the MCS. This includes mimicking and acting as a radio module against the on-board computer, simulation of the radio link with the possibility to add transmission errors and, acting as a ground station against the MCS. To ensure that the simulator performs as the on-board radio module, the simulator performance is tested against the on-board computer. These results can be compared with on-board radio module performance to make sure that the behavior is similar.</p>


In diva2:1046447 ff ligature, hyphen kept,  and merged paragraphs, should be:

 <p>Anomaly detectors in control systems are used to detect system faults and they are typically based on an analytical system model, which generates residual signals to find a fault. The detectors are designed to detect randomly occurring faults but not coordinated malicious attacks on the system.</p><p>Therefore three different anomaly detectors, namely a detector solely based on the last residual, a multivariate exponentially weighted moving average filter and a cumulative sum, are investigated to determine which detector yields the smallest worst-case impact of a time-limited data injection attack.</p><p>For this reason optimal control problems are formulated to characterize the worst-case attack under different anomaly detectors, which lead to non-convex optimization problems. Relaxations to convex problems are proposed and solved numerically and in special cases also analytically.</p><p>The detectors are compared by solving the optimal control problems for a simple simulation example as well as a quadruple-tank process. Simulations and experiments show that the cumulative sum seems to be the detector to choose, if one wants to limit the worst-case attack impact.</p>

In diva2:1421282, ff ligature, merged words,  and unnecessary spaces, should be:

 <p>Attitude Determination and Control System (ADCS) is often a complex system on-board any satellite which needs validation and testing to prove its operability and verify its software compatibility with hardware and other subsystems. One failure in orbit is extremely expensive in terms of cost and time due to payload preparation and launch. The ideal test bench would be the one that perfectly simulates the space environment and all its main factors such as weightlessness, Earth’s Magnetic Field (EMF), vacuum, neutral particles, plasma and radiation, among others. The target in this case was the Earth’s Magnetic Field (EMF), solved with a Helmholtz Cage in a Merritt Configuration, and weightlessness, not implemented but analysed in detail where different alternatives are proposed, similar to market solutions.As derived from literature and simulations executed along this M. Sc. Thesis, the Merritt Cage seems beneficial against any other configuration in terms of magnetic field uniformity and effective volume. After the design and assembly of the test bench, both properties were verified and successfully achieved, despite the lack of calibration, not executed because of time limitation, and tiny issues encountered along the full evolution of the project.</p>

In diva2:721641, ff ligature and missing superscripts:

 <p>Two implementations of the backtracking algorithm for solving Sudoku puzzles as well as their dependence on the representations of the problem have been studied in order to ascertain pros and cons of different approaches. For each backtracking step, empty cells could be assigned numbers sequentially or, by using a greedy heuristic, by the probability that guessed numbers were more likely to be correct. Representations of the Sudoku puzzles varied from a n<sup>2</sup> matrix to a n<sup>3</sup> matrix, as well as a combination of both. This study shows that (1) a sequential approach has better best case times but poor worst case behaviour, and a n<sup>3</sup> representation does not benefit over a n<sup>2</sup> representation; (2) a greedy heuristic approach has superior worst case times but worse best case, and n<sup>3</sup> representations sees great benefits over n<sup>2</sup> representations. A combination of n<sup>2</sup> and n<sup>3</sup> representations grants the best overall performance with both approaches.</p>

In diva2:1038731 ff ligature and hyphens left, should be:

 <p>With the coming growth in Internet of Things (IoT) applications, we can expect environments with many independent networks operating in nearby locations. Wireless Sensor Networks (WSN), which have become popular during the last few years, are the main type of networks used in IoT. The IEEE 802.15.4 protocol designed for low-rate wireless personal area networks has been widely adopted for this kind of network. Together with ZigBee, this protocol is gaining increasing interest from the industry, as they are considered a universal solution for low-cost, low-power, wireless connected monitoring and control devices. Internetwork interference issues in IEEE 802.15.4 networks can be a major problem because of the extensive use of wireless channels. In this thesis, an in-depth simulation study of the internetwork interferences is performed using Castalia, a widely used network simulator. We focus on the beacon collision problem, as it has been proved to be the main cause of performance degradation for coexisting networks. We carry out a prestudy of the main node simulation parameters to setup the different scenarios. Then, we evaluate how the overlap of the active periods and the location of the nodes affect the network performance. We continue with a network coexistence analysis to study the inter-action of two networks of two nodes and their performance regarding the beacon reception rate. We show that there are significantly different operation regions, depending on the network location. Following this, a probabilistic analysis is carried out in order to obtain an average beacon reception rate depending on the size of the area considered. Finally, we discuss available beacon collisions avoidance methods, taking into account the detailed simulation results. Our conclusions have theoretical and practical implications for the design of wireless sensor networks, and for the evaluation of beacon collisions avoidance schemes.</p>
----------------------------------------------------------------------
In diva2:1421238, unnecessary dashes from hyphenation and a missing space, should be:

 <p>In this thesis, we study the active estimation of the attention level of an approaching human driver from the perspective of an autonomous vehicle.   The  focus  is  on  how to  plan  information  gathering  actions for probing the human driver. The responses to these actions are then used for the actual model-based estimation of the attention level.In the first part of this thesis, a non-cooperative game framework is proposed for modelling the interaction between a human driver and an autonomous vehicle.  The best response algorithm is employed to find the equilibrium strategy of the game.  The equilibrium strategy is then used by the autonomous vehicle to actively learn the attention level of the human driver.In the second part of this thesis, the active learning of driver’s attention level is studied in a cooperative adaptive cruise control (CACC) scenario.  A novel CACC protocol is proposed which allows the platoon of vehicles to simultaneously achieve the platooning and learning objectives.In both cases for validating the proposed approach some simulations are performed using a simulated human driver which behaves as an attentive or distracted driver.  Then some real-time case studies are  performed  with  a  real  human  driving  the  car  in  the simulation. Both simulations and real-time case studies endorse the effectiveness of the proposed framework for the active learning of human driver’s attention level.</p>


In diva2:1821985 there were merged words, the abstract should be:

<p>The goal of this project is to examine how a higher penetration of power electronics-based forms of power production affects system stability and damping. The project covers how power system stability can be improved by using a Thyristor Controlled Series Capacitor (TCSC) with Power Oscillation Damping (POD). In order to reach these aims, a small Kundur's two-area power system is analysed. A fault is implemented in a system with four synchronous generators, this is done using the simulation software SIMPOW. By replacing portions of the synchronously generated power with power electronics based generation, the impacts of renewable energy sources on power system stability can be evaluated. The results of the project show that introducing power electronics based generation to a system generally worsens the stability of said system. Furthermore, it is discovered that the oscillations in the system response can be efficiently damped by using a TCSC. On the other hand, when a certain amount of renewable energy is integrated into the system the method used in this project is no longer sufficient.</p>

In diva2:1499079 there are merged words, merged paragraphs, and missing italics/emphesis, the abstract should be:

 <p>The dielectric antenna is an interesting alternative to a metallic antenna. This is mainly due to its low manufacturing cost and the possibility to fabricate complex antenna geometry with the aid of additive manufacturing (AM). Sophisticated AM technology provides new degrees of freedom in shaping the outer and inner geometry of antennas. This feature can be utilized to optimize various properties of antenna, such as its bandwidth, radiation pattern etc[.], while maintaining a compact geometry.</p><p>This master thesis investigates the possibility of improving the bandwidth of a compact dielectric antenna by modifying its geometry. Specifically, dielectric resonator antennas (DRAs) have been considered here. In this connection, two embedded cylindrical DRAs operating within 8 GHz-17 GHz frequency band have been designed and simulated using <em>Ansys HFSS</em>. For the first design (<em>Design-1</em>), a bandwidth (corresponding to reflection coefficient ≤ -10dB) of approximately 63% has been obtained and the second design (<em>Design-2</em>) has a bandwidth (corresponding to reflection coefficient ≤ -10dB) of about 57%. However, in terms of radiation characteristics, the performance of <em>Design-2</em> has been found to be superior compared to <em>Design-1</em>, mainly due to its symmetrical geometry. Furthermore, the two designs have been compared to an existing compact rectangular embedded DRA. It has been found that both <em>Design-1</em> and <em>Design-2</em> have comparatively wider bandwidth. With respect to the radiation characteristics, the performance of the reference antenna and <em>Design-2</em> are similar. While, the radiation performance of the reference antenna is found to be better than<em> Design-1</em>.</p>

In diva2:1635738 merged words and merged paragraphs, abstract should be ('enterpriseLang' is a special purpose language):

 <p>As the number of digital systems grows yearly, there is a need for good cyber security. Lack of such security can be attributed to the demand on resources or even knowledge. To fill this gap, tools such as enterpriseLang can be used by the enduser to find flaws within his system, which he can revise. This allows a user with inadequate knowledge of cyber security to create safer IT architecture. The authors of this paper took part in the development of enterpriseLang and its improvement. This was done by suggesting improvements based on certain design guidelines, as well as attempting to achieve 100% attack coverage and improving the defense coverage.</p><p>The results show a coverage increase of 0.6% for a specific model’s attack steps. Further more, we find that nearly 84.6%of the compiled guidelines are met, followed by 7.7% that were not fully met and a similar amount that were non-applicable to enterpriseLang. As the language is still in development, there remains much work that can improve it. A few suggestions would be to increase the attack coverage by 100%, increasing the defense coverage and improving enterpriseLang to fulfill thed esign guidelines, which would ultimately ease future projects within this domain.</p>

In diva2:497435 there was some incorrect formatting and a right quote mark (rather than a double quote mark), the abstract should be:

 <p>Today the extent and value of electronic data is constantly growing. Dealing across the internet depends on how secure consumers believe their personal data are. And therefore, information security becomes essential to any business with any form of web strategy, from simple business-to-consumer, or business-to-business to the use of extranets, e-mail and instants messaging. It matters too any organization that depends on computers for its daily existence.</p>
<p>This master thesis has its focus on Information Security Governance. The goal of this thesis was to study different Information Security processes within the five objectives for Information Security Governance in order to identify which processes that organizations should prioritize in order to reduce negative consequences on the data, information and software of a business from security incidents. By surveying IT experts, it was possible to gather their relative opinion regarding the relationship between Information Security Governance processes and security incidents.</p>
<p>By studying the five desired objectives for Information Security Governance,
<em>Strategic Alignment</em>, <em>Risk Management</em>, <em>Resource Management</em>, <em>Performance Measurement</em>
and
<em>Value Delivery</em>
the result indicated that some processes within <em>Performance Measurement<em>s have a difference in relation to other processes. For those processes a conclusion can be made that they are not as important as the processes which they were compared to. A reason for this can be that the processes within performance measurement are different in such a way that they measure an incident after it has actually happened.
While other processes within the objectives for ISG are processes which needs to be fulfilled in order to prevent that an incident happens. This could obviously explain why the expert&#8217;s choose to value the processes within performance measurement less important compared to other processes.</p>
<p>However, this conclusion cannot be generalized, since the total amount of completed responses where less than expected. More respondents would have made the result more reliable. The majority of the respondents were academicals and their opinion and experience may be different from the IT experts within the industry, which have a better understanding of how it actually works in reality within an organization.</p> 
----------------------------------------------------------------------
In diva2:1635763, there were merged words, the abstract should be:

 <p>For the future implementation of high speed communication, safety remains one of the main concerns. To ensure the safety of new applications, specifically the new 5G antennas, it is crucial to know that they will not cause any harm to the human body. There are a few ways to test how safe a system using high frequency radiation is but the industry standard is by using the Specific Absorption Rate (SAR). The SAR is directly correlated to the initial temperature rise in the volume exposed to radiation which is what the method used in this report is based on. The temperature rise in a skin-like phantom due to 5 GHz exposure was recorded using an IR-camera, which in turn was used to calculate the SAR. The purpose of this report was to test if this method is a valid way of obtaining the peak surface SAR. It was concluded that the method is valid but there are some uncertainties in regards to abstracting the method to far-field exposure for our considered frequency. The SAR value that is achieved in this report is 333.4 W/Kg which is high in relation to the SAR-limits in IEEE guidelines, although the set up is not supposed to reflect a realistic use of the antenna. This is due to the fact that the waveguide in the setup is close to measurement sample, and has a higher intensity than is to be expected from real world applications. The method may be applicable for far-field exposure with a higher frequency as that would concentrate the measurable heat to the surface of the measurement sample and would also carry more energy by default.</p>


In diva2:1822765 there were merged word, the abstract should be:

<p>Experimentally, it has been difficult to identify the exact physical processes at work in the magnetosphere that drive the strong currents that later give rise to auroras. The aim of this report has been to address this question by analyzing data gathered from Cluster-, MMS-, and DMSP satellite missions. The focus has specifically been on finding time intervals between the years 2018 and 2023 where Cluster and MMS had magnetic field-line conjunctions that occurred within the plasma sheet. Multiple conjunctions were found, and the most promising event, occurring on 2018-06-24, was further investigated. The field-aligned current was calculated using the curlometer method. The current parallel to the magnetic field was related to the shear velocity of the magnetic field and the ion bulk velocity. The experimental findings of this report confirmed the theoretical framework regarding the generation of field-aligned currents. In addition, during the investigated time interval, auroral kilometric radiation was observed while DMSP registered auroral acitivity. In conclusion, it has been shown that there is an experimental connection between field-aligned currents and auroral formation.</p>

In diva2:675977 there were unnecessary paragraph divisions and a missing "-" in 'field-of-view', the abstract should be:

 <p>This essay details a 3D simulation of a number of control methods used for maneuvering of teleoperated USAR robots. The implementation was produced in the Unity3D engine. The simulation implemented different variations on field-of-view angle, turning algorithms, and camera view perspectives. An evaluation using volunteer test operators was conducted and discussed. The sample size was too small to draw any definitive conclusions. Further testing is advised.</p>

Note also that there is a space missing in the title, it should be: "Efficiency Evaluation of Simulated USAR Control Methods"

Also note that the Swedish abstract also has unnecessary paragraph divisions and should be:

<p>Denna uppsats behandlar en 3D-simulering samt användartester av flera olika kontrollmetoder som används vid fjärrstyrning av obemannade räddningsrobotar. Implementationen skapades med Unity3D-plattformen. De styrmetoder som testades var olika stora synfältsvinklar på kameran, olika algoritmer för att styra robotens svängning, samt olika kameraperspektiv. Användartester med frivilliga testförare genomfördes och diskuteras. Provstorleken var för liten för att kunna dra några definitiva slutsatser. Ytterligare tester rekommenderas.</p>

In diva2:547622 unnecessary paragraph divisions and missing subscripts, abstract should be:

 <p>Heating, Ventilation and Air Conditioning (HVAC) systems consist of all the equipment that control the conditions and distribution of indoor air. Indoor air must be confortable and healthy for the occupants to maximize their productivity. Moreover, HVAC energy consumption is between 20% and 40% of the total energy consumption in developed countries and accounts around 33% of the global CO<sub>2</sub> emissions. So the study of HVAC systems plays an important role in building science.</p><p>The aim of this project is to identify mathematical models that will be employed by intelligent control algorithms which guarantee human comfort indoors, energy saving and less CO<sub>2</sub> emissions at the same time. Three models, based on first-principle physical knowledge, are proposed for CO<sub>2</sub> concentration, temperature and humidity, respectively, for a room in the Q-building at KTH. Thermodynamic equations and an original estimation of the number of the occupiers of the room are employed.</p><p>Validation shows that models have really good performances, even with a short training dataset. Discussions on the obtained results are given and some ideas for future work are proposed.</p>

Note the word 'confortable' is misspell in the original abstract, so I have kept it in the above.

In diva2:616632, there are spaces missing in the title, it should be: "Computations on the French transmission grid in order to improve the voltage security assessment"'

The abstract had merged words and should be:

diva2:616632: <p>As the electric consumption increases and the investments are hard to make, electricity networks are operated closer to their limits. In such conditions, a generator or a transmission line outage can have tremendous impact, leaving a great number of people without electricity. It is therefore a matter of prime importance to ensure power system security and in particular voltage stability. Static criteria used in on-line simulations as well as protection and defense devices such as load-shedding devices play a critical role for voltage stability and are thus crucial for the network security. The core of this project is to determine efficient tools to detect undesirable conditions in operational context and to determine a pertinent activation level for an automatic load-shedding device used for the system protection against voltage instability.</p><p>In the first part of this report, theoretical background regarding voltage stability is presented, followed by the software and methodologies used during the Master’s thesis work.</p><p>The second part of this report focuses on case studies conducted for the French power system. From an initial objective of updating static criteria, the results have actually led to the withdrawal of these criteria and a switch to dynamic simulations for the North-East and East areas, as well as to the improvement of Astre software database. Simulations on the most stressed conditions from last winter allowed the updating of the activation level for the automatic load-shedding device. These changes have been validated and will be applied for voltage security assessment of the French network in the future.</p>

In diva2:1168320: there were merged words and paragraphs, the abstract should be 

<p>With the continuous progress on autonomous vehicle and remote driving techniques, connection quality demands are changing compared with conventional quality of service. Vehicle to everything communication, as the connectivity basis for these applications, has been built up on Long Term Evolution basis, but due to various ethical and environmental issues, few implementations have been made in reality. Therefore simulation approaches are believed to provide valuable insights.</p><p>To fully model an LTE vehicular network, in this work we first provide a comparison study to select the preferable LTE simulator. Aiming to integrate communication nodes with mobility, a solution for simulation framework is developed based on a state-of-art comparison study on the existing simulator frameworks. We then further develop the network simulator, and complement it with hybrid wireless channel modeling, channel and quality of service aware scheduler, and admission control strategies. In terms of instant optimization of the network, real-time access is emulated for external devices to communicate with the simulator. In this thesis, the evaluation of the framework performance considers two aspects: the performance of the simulator in LTE V2X use case and the feasibility of the service, specifically, remote driving, under realistic network capacity. For our framework, the results indicate that it is feasible to realize remote driving in an LTE urban scenario, but, as an example, we show that for an area of Kista, five vehicles could be hold by a base-station with guaranteed service at most.</p>


In diva2:1479353 there were merged words and paragraphs, the abstract should be:

 <p>As the size and complexity of the internet increased dramatically in recent years,the burden of network service management also became heavier. The need for an intelligent way for data analysis and forecasting becomes urgent. The wide implementation of machine learning and data analysis methods provides a new way to analyze large amounts of data.</p><p>In this project, I study and evaluate data forecasting methods using machine learning techniques and time series analysis methods on data collected from the KTH testbed. Comparing different methods with respect to accuracy and computing overhead I propose the best method for data forecasting for different scenarios.</p><p>The results show that machine learning techniques using regression can achieve better performance with higher accuracy and smaller computing overhead. Time series data analysis methods have relatively lower accuracy, and the computing overhead is much higher than machine learning techniques on the datasets evaluated in this project.</p>


In diva2:1331911 there were merged words and paragraphs, the abstract should be:

 <p>Anticipating the future positions of the surrounding vehicles is a crucial task for an autonomous vehicle in order to drive safely. To foresee complex manoeuvres for longer time horizons, a framework that relies on high-level properties of motion and is able to incorporate, e.g. contextual features, is needed. In this thesis, the problem of predicting the trajectories of the surrounding vehicles on a highway is tackled by using machine learning. The objective is to evaluate the performance of recurrent neural networks for trajectory prediction, specifically long-short term memory neural networks. Moreover, the goal is to investigate if contextual features can improve the predictions.</p><p>The problem of predicting future trajectories is solved by using two different approaches, which are compared by using the same framework. The first approach is based on the vehicle states of the surrounding vehicles relative to the ego-vehicle, where the reference system is in the ego-vehicle. The second approach is based on the velocities of the vehicles relative to the ground, where the reference system is in the ground. The results show that, with the proposed architecture, the latter approach results in a lower RMSE in the longitudinal direction compared with the former approach. The results also show that the proposed models, overall, outperform a simple model, which is based on polynomial fitting, particularly in the lateral direction where the proposed models are significantly better than the polynomial models. Furthermore, contextual features do not improve the predictions significantly. However, the results indicate that contextual information has a positive impact on the predictions in specific scenarios.</p>

In diva2:1548712 there were merged words, the abstract should be:

 <p>One of the main focus of robotics is to integrate robotic tasks and motion planning, which has an increased significance due to their growing number of application fields in transportation, navigation, warehouse management and much more. A crucial step towards this direction is to have robots automatically plan its trajectory to accomplish the given task. In this project a multi-layered approach was implemented to accomplish it. Our framework consists of a discrete high-level planning layer that is designed for planning, and a continuous low-level search layer that uses a sampling-based method for the trajectory searching. The layers will interact with each other during the search for a solution. In order to coordinate for multi-agent system, velocity tuning is used to avoid collisions, and different priority are assigned to each robot to avoid deadlocks. As a result, the framework trades off completeness for efficiency. The main aim of this project is to study and learn about high-level motion planning and multi-agent system, as an introduction to robotics and computer science.</p>

In diva2:1634504 there were merged words, the abstract should be:

 <p>With the rise of global sustainability energy initiatives,the implementation of renewable energy sources in future electrical grids is increasing. Many of the renewable energy sources are however intermittent, meaning they provide varying levels of power. As grids meet the demand of larger loads of intermittent renewable energy sources, small signal instability arises as result of the power oscillations. Small signal instability occurs when a system cannot return to steady state after being exposed to small disturbances. One method to damp power oscillations in an unstable system is by using a Power System Stabilizer (PSS). The goal of this project is to tune a PSS or PSSs required to successfully damp out the power oscillations in a system which is small signal unstable without any PSSs connected. The PSSs are tuned through a trial and error approach, and the system is a Kundur two-area four-machine MATLAB Simulink model. Overall, the trial and error method is successful in tuning PSSs, which damp out the system’s power oscillations. Other methods of tuning are discussed and compared in terms of efficiency to damp out power oscillations.</p>
----------------------------------------------------------------------
In diva2:1723189 words were merged, the abstract should be:

<p>In this report, the effect that a higher penetration of renewable energy sources has on electric power grid stability is evaluated. The report also compares different methods of stabilizing an unstable grid. The model used is a two-area four-machine system and the main objective is to stabilize the synchronous generators such that they revert back to synchronism after being subjugated to a small signal disturbance. The stabilization methods consists of supplementary Power System Stabilizers (PSSs) complementing the exciter systems of the synchronous machines, as well as two types of converter-based controllers in the renewable energy source: Grid-Following (GFL) converters and Grid-Forming (GFM) converters. The results show that a system with renewable energy sources is more sensitive to disturbances and has a larger rotor angle deviation from a steady state when using only GFLs compared to the conventional grid without PSSs. It is also found that a conventional grid requires supplementary PSSs to be stable. This is also the case for a system with renewable energy controlled by GFL. The system with GFM controllers does however not need supplementary PSS to be stable. This leads to the conclusion that GFM is more preferable than GFL to control a grid with a higher penetration of renewable energy.</p>
----------------------------------------------------------------------
In diva2:1619556 words and paragraphs were merged, the abstract should be:

<p>The automotive industry is evolving with the advent and development of autonomous cars. For the operation of these vehicles, sensors are needed, and utilising radar technology is a good solution. Slotted waveguide arrays in gap waveguide technology provide a good solution for the radar antennas; their low losses, high performance while keeping a compact design and its ease and cost-efficient manufacturing are the main features that make this antenna powerful.</p><p>This work aims to facilitate the design process of the antenna through a synthesis method that manages to find an antenna with desired characteristics more efficiently. These characteristics include central frequency, impedance bandwidth, sidelobe level and beamwidth. The synthesis method is implemented and verified throughout the work in three waveguide structures, rectangular, ridge, and ridged gap waveguide. The implementation is done in <em>Matlab</em> and the simulations are carried on <em>CST Microwave Studio</em>.</p><p>Two four-slots waveguide arrays in gap waveguide technology with their central frequencies at 28 GHz are designed and manufactured. The difference is in the SLL, which is -15 dB for one and -25 dB for the other. The objective is to verify that the synthesis method matches the simulations and measurements. </p>
----------------------------------------------------------------------
In diva2:1391129, 'framework1' should be 'framework<sup>1</sup>'
where 1 is for a footnote saying "<sup>1</sup>https://yukaichou.com/gamification-examples/octalysis-complete-gamificationframework/"

Note: The cover is in Swedish, but the thesis in in English.
----------------------------------------------------------------------

In diva2:1380818 words were merged, the abstract should be:

<p>This thesis examines the problem of downlink power allocation in dense 5G networks, and attempts to develop a data-driven solution by employing deep reinforcement learning. We train and test multiple reinforcement learningagents using the deep Q-networks (DQN) algorithm, and the so-called Rainbow extensions of DQN. The performance of each agent is tested on 5G Urban Macro simulation scenarios, and is benchmarked against a fixed power allocation approach. Our test results show that the DQN models are successful at improving data rates at cell-edge, while generalizing well to previously unseen simulation scenarios. In addition, the agents induce throughput balancing effects, i.e., achieve fairness among users, in networks with full-downlink-buffer traffic by properly designing the reward signal.</p>
----------------------------------------------------------------------

In diva2:680026 words and paragraphs were merged, the abstract should be:

<p>The giant ever increasing demand for higher data rates and better Quality of Service (QoS) is rapidly growing and operators’ main concern is to support the growth of mobile data traffic and address the users’ expectations while at the same time keeping the costs of services reasonable [1], [2], [3]. This is more vital in residential and dense urban areas where the reception of themacro signal level becomes weak [4]. Therefore, the implementation of ultra dense networks becomes a promising approach which is expected to provide good indoor coverage and higher capacity in residential areas. Nevertheless, the potential degradation of network performance due to severe interference originated from nearby networks should be deeply studied prior to full-scale implementation of ultra dense networks. The main concern of this thesis work is to investigate the coexistence between two operators in Time Division Duplex (TDD) system which are using adjacent frequency channels and implemented in the same geographical area. For this purpose, the system level simulation based on Monte Carlo method is performed to reveal the impact of critical parameters including <em>Adjacent Channel Interference power Ratio (ACIR)</em>, <em>Uplink-Downlink synchronization between operators<em>, <em>Base-Stations positioning</em>, and <em>Internal walls existence</em> on the system performance. Afterwards, the effect of densification on the previous findings is studied.</p><p>Results show that in downlink and uplink, approximately 30 dB and 55 dB of ACIR is required, respectively, in order to eliminate the impact of adjacent channel interference. Furthermore, in uplink, synchronization is necessary when base stations of operators are collocated. In downlink, however, synchronization and collocation is beneficial when signal quality is poor. On the other hand, it is shown that densification is feasible provided base stations employ adjustable transmission power model. Moreover, internal walls can improve system performance due to attenuation of interferences originated from surrounding cells.</p>
----------------------------------------------------------------------
In diva2:1748023 the paragraphs were merged and there were missing ff ligatures, the abstract should be:
<p>Block partitioning is a computationally heavy step in the video coding process. Previously, this stage has been done using a full-search-esque algorithm. Recently, Artificial Neural Networks (ANN) approaches to speed-up block partitioning in encoders compliant to the Versatile Video Coding (VVC) standard have shown to significantly decrease the time needed for block partitioning.</p><p>In this degree project, a state of the art Convolutional Neural Network (CNN) was ported to VTM16. It was ablated into 7 new models which were trained and tested. The eects of the ablations were compared and discussed with respect to the number of Multiply-Accumulate operations (MAC) a model required, the speed-up in the encoding stage as well as the quality of the encoding.</p><p>The results show that the number of MACs can be substantially decreased from that of the state of the art model while having low negative eects on the quality of the encoding. Furthermore, the results show that the two tested approaches of reducing the computational complexity of the model were effective. Those were: 1) reducing the image’s resolution earlier in the model. 2) reducing the number of features in the beginning layers. The results point towards the first approach being more effective.</p>
----------------------------------------------------------------------

In diva2:860991 missing italics and bold face, the abstract should be:
<p>Nashorn is a JavaScript engine that compiles JavaScript source code to Java bytecode and executes it on a Java Virtual Machine. The new bytecode instruction <strong>invokedynamic</strong> that was introduced in Java 7 to make it easier for dynamic languages to handle linking at runtime is used frequently by Nashorn. Nashorn also has a type system that optimizes the code by using primitive bytecode instructions where possible. They are known to be the fastest implementations for particular operations.</p><p>Either types are proved statically or a method called <em>optimistic type guessing</em> is used. That means that expressions are assumed to have an <strong>int</strong> value, the narrowest and fastest possible type, until that assumption proves to be wrong. When that happens, the code is deoptimized to use types that can hold the current value.</p><p>In this thesis a new architecture for Nashorn is presented that makes Nashorn’s type system reusable to other dynamic language implementations. The solution is an intermediate representation very similar to bytecode but with untyped instructions. It is referred to as Nashorn bytecode in this thesis.</p><p>A TypeScript front-end has been implemented on top of Nashorn’s cur-rent architecture. TypeScript is a language that is very similar to JavaScript with the main difference being that it has type annotations. Performance measurements which show that the type annotations can be used to improve the performance of the type system are also presented in this thesis. The results show that it indeed has an impact but that it is not as big as anticipated.</p>
----------------------------------------------------------------------
In diva2:1660097 there were instances of "grid- -", the abstract should be:
<p>The modern power system is aiming to progress away from conventional synchronous machine  based power generation towards converter dominated system that leads to extensively high penetration of renewable energy such as wind and PV. This transition of modern power system toward converter based renewable energy comes with new challenges as the conventional synchronous generation is being replaced by converter based power system (CBPS). The converter is commonly interfaced to the power system with Phase Locked Loop (PLL) technique to synchronize the converter with the grid voltage angle and inject the current at the right angle. Therefore, this approach is called grid-following converter; this type of configuration of converters may lead to some power system instabilities (e.g., voltage instability, frequency instability, synchronous and sub­synchronous instabilities). In order to overcome the limitation of the grid-following converters, another converter control concept become present in the literature as a grid-forming converter where the synchronizing method to the grid eliminates the need for PLL .In this thesis, a grid-forming controlled power converter is implemented with an energy storage system to emulate the inertia of the synchronous generator through the VSM control concept. An electromagnetic transient (EMT) simulation has been modeled in the PSCAD simulation environment. The model is the well­ known four-­machine two-­area power system. The model has been tested by incrementally replacing the synchronous machines with wind farms connected through power converters; this weakens the grid and may lead to frequency instability during a disturbing event. An Energy Storage System (ESS) has been implemented and added to the system to mitigate the loss of the kinetic energy of the rotating masses of the synchronous generators. The ESS is integrated with a grid-forming converter that is controlled to mimic the dynamic behavior of a synchronous generator. Thus, the ESS is synchronized to the system based on the swing equation of the synchronous generator. The results show significant improvements in the frequency stability of the system under study.</p>
----------------------------------------------------------------------
In diva2:1600236 the paragraphs were merged, the abstract should be:
<p>Mobile manipulators are changing the way companies and industries complete their work. Untrained end users risk facing unfunctional and nonuser- friendly Graphical User Interfaces. Recently, there has been shortages of people and talent in the heathcare industry where these applications would benefit in being used to accomplish easy and low level tasks. All these reasons contribute to the need of finding functional robot-user ways of communicating that allow the expansion of mobile manipulation applications. This thesis addresses the problem of finding an intuitive way to deploy a mobile manipulator in a laboratory environment.</p><p>This thesis has analyzed whether it is possible to permit the user to work with a manipulator efficiently and without too much effort via a functional graphical user interface. Creating a modular interface based on user needs is the innovation value of this work. It allows the expansion of mobile manipulator applications that increases the number of possible users. To accomplish this purpose a Graphical User Interface application is proposed using an explanatory research strategy. First, user data was acquired using an ad hoc research survey and mixed with literature implementations to create the right application design. Then, an iterative implementation based on code-creation and tests was used to design a valuable solution. Finally, the results from an observational user study with non-roboticist programmers are presented.</p><p>The results were validated with the help of 10 potential end users and a validation matrix. This demonstrated how the system is both functional and user-friendly for novices, but also expressive for experts. </p>

Note that in the 2rd sentence 'healthcare' is misspelled, so it perhaps could be 'hea[l]thcare' - to reflect that the error is in the original abstract in the thesis.
----------------------------------------------------------------------
In diva2:1443829 there were merged words and paragraphs, the abstract should be:

<p>With increasing demand for autonomous systems and self-driving heavy-duty vehicles there is an even more increasing demand for safety. In order to achieve desired safety level on the public roads, engineers have to tackle many technical issues, like decision making, object detection and perception. In order to detect an object or to have an understanding of its surroundings, autonomous heavy-duty vehicles are equipped with different types of sensors. These sensors are placed on different parts of the autonomous truck. The fact that some parts of the truck are highly dynamical introduces additional disturbances to the signals coming from onboard sensors. One of the most dynamic parts of every truck is its cabin. Moving cabin may induce additional disturbances into data coming from sensors attached to it. This corrupted data may lead the autonomous trucks to make wrong decisions. In the worst case, such decisions may be fatal.</p><p>This thesis uses a data driven modeling approach for creating a mathematical description of cabin movements based on data from onboard sensors. For that purpose, tools from system identification field are used. The resulting models are aimed to be used for implementation of real-time estimation algorithm for the cabin dynamics, which in turn can be used for real-time compensation of the disturbances.</p>
----------------------------------------------------------------------
In diva2:1076208 there were merged words, the abstract should be:
<p>Comets are a key to understanding the early stages of the solar system. They were here at its formation and have not evolved ever since, which means they are our best shot at learning the processes that led to the formation of the solar system as we know it today. Yet, our knowledge about these bodies is very limited. They are far from the Earth and small, which makes it complicated and expensive to reach them. But the study of the chemistry and geology of comets is not the only goal of the scientific community. The plasma environment of these astronomical bodies could also give answers to many questions regarding the science of plasma physics, such as the interaction of the solar wind with plasmas. Answering some of these questions was an objective of the Rosetta Mission. Before its launch, only three space missions out of eight targetting comets had plasma instruments onboard. Rosetta carried several instruments designed to analyse the plasma environment of comet 67P/Churyumov-Gerasimenko. We were able to perform a statistical analysis of the electric field spectrum in the vicinity of comet 67P/Churyumov-Gerasimenko. This allowed us to determine two regions of high spectral activity using the two probes of the LAP instrument and to propose several theories about the physical processes that were active.</p>
----------------------------------------------------------------------
In diva2:789016 the title and abstract are missing spaces, the title should be:
"Evaluation of IEC 61175 for semantic interpretation in OPAL-RT reference distribution network and Jess-OPC"

the abstract should be:
diva2:789016: <p>In the past few decades the electric power system has seen tremendous growth in terms technology such as integration of DG and advanced power electronics devices. Such developments mainly include renewable energy, increased use of HVDC transmission, state of the art communication systems and adaptation of information modeling standards applied to power systems. In this thesis work a RBTS distribution grid network model has been implemented in OPAL-RT to integrate automation and ICT with more devices. These motivate use of standardization like IEC 61850 and IEC 61175. This network model is used for verification of earlier developed monitoring and control methods at the ICS department. The verification setup involves interfacing of the network models in OPAL-RT with Jess rule engine via OPC. The focus of this master project work is evaluation of the use of IEC 61175 for consistent signal naming in the network model. It is done through detailed signal modeling in the Simulink model for use in the real time simulation and implementation of a graphical user interface to show all the signals along with their measured value in a tree view. Second part of the work focuses on utilization of the signal modeling in high-level applications. The applications can utilize the consistent signal naming done by using IEC 61175 to implement a semantic reasoning model for interpretation of the signals and thereby improve the monitoring and control functions. An interface has been implemented to generate an XML file of the mapped signals along with the last measured value for the parser implemented in Java. After parsing the XML file all the data is used for the Jess rule engine to fire the particular rule in a specific situation. Some rules are implemented for monitoring the system voltage and for finding out the signals associated with distributed generation unit to calculate their actual generation in the network. In this project an ontology based approach is also performed in JADE to get a clear idea of integration of information sources. Results of the work show that proper signal designation in a network according the standard IEC 61175 can potentially help reduce the complexity and improve reliability of operations.</p>
----------------------------------------------------------------------
In diva2:1248837 there were merged words and a ligature, the abstract should be:

<p>The worldwide automotive market is constantly evolving due to digitalization. Vehicles manufacturers introduce more and more digitalized human-machines interfaces by adding screens in cars. Nowadays a majority of cars have two screens: one used for navigation and another one to display technical parameters such as speed or gas consumption. When the same image has to be displayed on both screens it is not possible to generate two images adapted for both screens due to hardware limitations. This implies that only one image adapted for a specific screen is generated; to be displayed on the other screen this image has to be rescaled. This resizing is often source of image distortion and reduced user perceived image quality. The aim of this Master Thesis is to use sub-pixel rendering in resizing algorithms to improve perceived quality. Sub-pixel rendering takes the human visual system and the screen properties into account to improve image perceived quality after resizing. This thesis adapts the sub-pixel rendering for resizing and compares different existing upscaling algorithms with the sub-pixel rendering upscaling algorithms in terms of perceived quality. The perceived image quality after resizing is assessed both by a reference less image quality algorithm and a sample-group subjective evaluation composed of automotive engineers and designers. The evaluation highlights an improvement of perceived quality when the sub-pixel rendering algorithms is used for resizing.</p>
----------------------------------------------------------------------
In diva2:1142915 merged words, the abstract should be:

<p>In this bachelor thesis project, the problem of image classification with convolutional neural networks is considered. In several fields of biology, automatized cell detection is a helpful tool for facilitating the process of cellular analysis. This report answers the question whether a computer program can tell if an image contains muscle stem cells or not. Analogously to the neurons of the human brain, the creation of such a program involves training thousands of mathematically modeled artificial neurons to maximize the likelihood of producing correct classifications.This report covers how such a network is implemented and shows how its performance dependens on the network’s dimensions. It is revealed that a neural network indeed can replace and speed up the manual process of classifying images. With an image dataset of cells, the best performing networks manage to classify images with an accuracy of up to 90%.</p>
----------------------------------------------------------------------
In diva2:1135693 merged words, the abstract should be:

<p>In this paper, we propose a novel active learning algorithm for short-text (Chinese) classification applied to a deep learning architecture. This topic thus belongs to a cross research area between active learning and deep learning. One of the bottlenecks of deep learning for classification is that it relies on large number of labeled samples, which is expensive and time consuming to obtain. Active learning aims to overcome this disadvantage through asking the most useful queries in the form of unlabeled samples to be labeled. In other words, active learning intends to achieve precise classification accuracy using as few labeled samples as possible. Such ideas have been investigated in conventional machine learning algorithms, such as support vector machine (SVM) for image classification, and in deep neural networks, including convolutional neural networks (CNN) and deep belief networks (DBN) for image classification. Yet the research on combining active learning with recurrent neural networks (RNNs) for short-text classification is rare. We demonstrate results for short-text classification on datasets from Zhuiyi Inc. Importantly, to achieve better classification accuracy with less computational overhead,the proposed algorithm shows large reductions in the number of labeled training samples compared to random sampling. Moreover, the proposed algorithm is a little bit better than the conventional sampling method, uncertainty sampling. The proposed active learning algorithm dramatically decreases the amount of labeled samples without significantly influencing the test classification accuracy of the original RNNs classifier, trained on the whole data set. In some cases, the proposed algorithm even achieves better classification accuracy than the original RNNs classifier.</p>
----------------------------------------------------------------------
In diva2:853249: "importanceof" should be "importance of"
----------------------------------------------------------------------
In diva2:1638177 merged paragraphs and one case of merged words, the abstract should be:

<p>Along with the growth and popularity of mobile networks, users enjoy more convenient connection and communication. However, exposure of user presence in mobile networks is becoming a major concern and motivated a plethora of LPPM Location Privacy Protection Mechanisms (LPPMs) have been proposed and analysed, notably considering powerful adversaries with rich data at their disposal, e.g., mobile network service providers or Location Based Services (LBS). In this thesis, we consider a complementary challenge: exposure of users to their peers or other nearby devices. In other words, we are concerned with devices in the vicinity that happen to eavesdrop (or learn in the context of a peer-to-peer protocol execution) MAC/IP addresses or Bluetooth device names, to link user activities over a large area (e.g., a city), and especially when a small subset of the mobile network devices parasitically logged such encounters, even scattered in space and time, and collaboratively breach user privacy. The eavesdroppers can be honest-but-curious network infrastructures such as wireless routers, base stations, or adversaries equipped with Bluetooth or WiFi sniffers. The goal of this thesis is to simulate location privacy attacks for mobile network and measure the location privacy exposure under these attacks. We consider adversaries with varying capabilities, e.g., number of deployable eavesdroppers in the network and coverage of eavesdropper, and evaluate the effect of such adversarial capabilities on privacy exposure of mobile users.</p><p>We evaluate privacy exposure with two different metrics, i.e., Exposure Degree and Average Displacement Error (ADE).We use Exposure Degree as a preliminary metric to measure the general coverage of deployed eavesdroppers in the considered area. ADE is used to measure the average distance between user’s actual trace points and user’s trajectory predictions. We simulate three attack cases in our scheme. In the first case, we assume the attacker only acquires the collected data from users. We vary the number of receivers to test attack capacity. Exposure Degree is used to evaluate location privacy in this case. For the second and third cases, we assume the attacker also has some knowledge about users’ history traces. Thus, the attacker can utilize machine learning models to make prediction about user’s trace. We leverage Long Short-Term Memory (LSTM) neural network and Hidden Markov Model (HMM) to conduct real-time prediction and Heuristic LSTM to reconstruct more precise user trajectories. ADE is used to evaluate the degree of location privacy exposure in this cases. The experiment results show that LSTM performs better than HMM on trace prediction in our scheme. Higher number of eavesdroppers would decrease the ADE of LSTM model (increase user location privacy exposure). The increase of communication range of receiver can decrease ADE but will incur ADE increase if communication range successively increases. The Heuristic LSTM model performs better than LSTM to abuse user location privacy under the situation that the attacker reconstructs more precise users trajectories based on the incomplete observed trace sequence. </p>
----------------------------------------------------------------------
In diva2:1582903 merged paragraphs and "in-sight" should be "insight", the abstract should be:

<p>This study examines the correlation between self-regulated learning (SRL) and psychological well-being (PWB) in students in the online learning environment. Previous research suggests that these concepts are positively correlated, i.e. that application of SRL contributes to better PWB or vice versa. However, most studies on this relation have been performed with the traditional/in-person learning environment as context. Therefore, there is a lack of insight into how this relation might behave in the online learning environment, which is currently employed by many universities due to the ongoing Covid-19 pandemic. Thus, in this study, SRL and PWB were measured across 6 subscales each in students at KTH in order to perform a correlation analysis between the subscales of the two concepts. The aim is to gain insight into the relationship between SRL and PWB, which could be useful for teachers in assisting their students’ utilization of SRL and in turn improve their PWB.</p><p>Data was gathered through an online survey which was administered to students at KTH through a course and was published on an online forum for KTH students on Facebook in the year 2021. The survey received 103 responses. The data was analyzed using Spearman rank correlation analysis, which revealed mostly statistically insignificant correlations, or statistically significant but weak correlations between the subscales within SRL and PWB. The results show an overall weaker correlation between SRL and PWB compared to the results of previous studies. The large number of insignificant correlations might indicate that the sample size was insufficient for the method and tools used.</p><p>In conclusion, the results of this study did not reveal much meaningful information on the relation between SRL and PWB in students in the online learning environment. </p>
----------------------------------------------------------------------
In diva2:618087 merged words, the abstract should be:

<p>Mobile communication network is consuming 0.5 percent of the global energy supply alone these days. While the unrelenting increase in user capacity requirement, which is expected to grow 1000 times in 10 years, will lead to even more energy consumption in this filed. On the one hand, the energy cost which has covered half of the operators’ operation fee will continually increase; On the other hand, global warming problem may be still more severe due to the rising amount of carbon emissions caused by the increasing energy consumption. Although the system spectrum efficiency is improved significantly by the introduction of latest cellular network standards 3G and 4G, and the incremental enhancements in electronics and signal processing are bringing the energy consumption down in base stations, these improvements are still not enough to match the huge increase in energy consumption which is realted to the surging increase demands for more capacity. It is clear that solutions have to be found at the architectural layer. Besides, more and more capacity requirement is generating from indoor environment, it has been a vital concern for operators to consider how to provide enough service coverage with the existing macro-only network due to the complex indoor environment and high wall penetration loss. All these cause urgent demand for more energy efficient cellular networks to deal with the capacity challenges. One of the promising technologies to this situation is macro-femto heterogeneous networks by offloading indoor traffic to femtocells. Considering the shorten distance between indoor users and base stations and spacial frequency reuse advantages, as well as the drawbacks on introduced interference and extra power consumption by femto base stations, this report proposes a simulation framework to study whether this network topology could enhance the system energy efficiency or not. The results suggest that the introduction of femtocells may be a promising method to save energy consumption in the future and meet the increasing user data rate access requirements, especially in high user capacity demand networks, macro-femto deployment could save more energy consumption.</p>
----------------------------------------------------------------------
In diva2:1635560 merged words, the abstract should be:

p>Real-time monitoring of vehicle data during testing can drastically cut down on test times as well as improve the quality of testing by facilitating the implementation of run-time compliance verification with expected model behavior, along with anomaly detection in both hardware and software. By providing a wireless communication link between vehicles and a monitoring base station, this project aims to build the groundwork for more sophisticated testing proceedings in the future. The wireless communication system implemented in this project mirrors data from the two CAN data busses on the vehicle and transmits them via a licence-free 868 MHz ISM band. The receiver is connected to a computer where the data can be visualized and analyzed in real-time. The project goals were exceeded in both throughput and range. Early testing has shown that data rates of 150 kbit/s and ranges 1.2 km and beyond are achievable. The project has set a solid foundation upon which wireless testing routines can now be developed. Hardware and software developed in this project can be built upon and optimized further in future revisions to achieve even higher data rates and longer ranges.</p>
----------------------------------------------------------------------
In diva2:1836839, in the English keywords, one should remove "keywords here: "
----------------------------------------------------------------------
In diva2:1634343 merged words, the abstract should be:

<p>Systems consisting of multiple robots are traditionally difficult to optimize. This project considers such a system in a simulated warehouse setting, where the robots are to deliver boxes while avoiding collisions. Adding such collision constraints complicates the problem. For dynamical multi-agent systems as these, reinforcement learning algorithms are often appropriate. We explore and implement a reinforcement learning algorithm, called multi-agent rollout, that allows for re-planning during operation. The algorithm is paired with a base policy of following the shortest path. Simulation results with up to 10 robots indicates that the algorithm is promising for large-scale multi-robot systems. We have also discussed the possibility of using neural networks and partitioning to further increase performance.</p>
----------------------------------------------------------------------
In diva2:1453604 merged words and paragraphs as well as missing ligatures, the abstract should be:

<p>For a ship sailing between destinations on a schedule, having a speed controller that can sail the ship according to a speed reference offers extended capabilities for the crew. It is also a necessary tool for sailing the ship, according to an optimally planned speed reference. A method used for speed control of large sea-going vessels today is PID-control. This solution, however, tends to offer poor performance: the speed of the vessel tends to overshoot the reference during reference changes and the controller has a hard time tracking the reference under disturbances from waves and wind.</p><p>This thesis investigates the possibility of using Model Predictive Control to achieve better control performance while satisfying the constraints of the ship's propulsion system. The solution presented in this thesis is to use a Model Predictive Controller that uses a linear model obtained at each sampling instance from a ship's nonlinear surge dynamics. The required thrust, calculated by the controller, is then mapped to a fuel consumption setpoint for the ship's main engines. The results show that the MPC can in simulations achieve offset free tracking without overshoots or undershoots. Full scale test on a ship with a length of 171 [<em>m</em>] shows that the controller achieves good performance at speeds over 13 [<em>kn</em>] and in steady-state operation under modest disturbances, but is unable to achieve satisfactory tracking during accelerations or decelerations at speeds less than 13 [<em>kn</em>].</p>
----------------------------------------------------------------------
In diva2:705687 unnecessary hyphen spaces, the abstract should be:

<p>The unmanned helicopter Skeldar relies on model based control to per form its tasks.  System identification is an integral component  in the process  of  deriving models of helicopter dynamics.  This thesis aims to investigate how nonparametric methods of system identification can support the current  modelling  and system identification  practices at Saab.</p><p>Nonparametric system identification does not require a pre-defined model structure. Models estimated with this methodology may be used to validate parametric models, which are necessary for the implementation of the model based control system. This thesis examines several nonparametric methods of system identification in both the frequency and time domains. The theory of these methods is presented and their performance is analyzed on data from flight tests as well as from simulated systems.</p><p>Analysis of the results shows that models are highly dependent on the choice of input signal spectrum.  To best take advantage of nonparametric system identification in this application, experiments should be designed with special regard to the system properties sought to be modelled. Nonparametric system identification can then be used to provide a good understanding of the system properties in the excited frequency region.</p><p>In the specific  case of helicopter dynamics, of which the principles are very well understood at Saab, it can be concluded  that the existing system identification process is sufficient to provide well performing models. However, a nonparametric model could be estimated and used as a tool for comparison and validation in the process of identifying a paramteric model.</p>
----------------------------------------------------------------------
In diva2:690824 merged words, the abstract should be:

<p>With the dynamic changes in technological advancements, wireless communication technologies has made a tremendous progress from simple to complex systems that are able to communicate across multiple networks platform. As these systems continue to prove their proficiency and benefits, it is strongly asserted that wireless technologies will continue to play an even more critical and vital role compared to wired connections in the future.</p><p>The most relevant question now regarding the future of wireless technologies is whether it going to dominate the wireline transmission or be a complementary to wireline where it’s difficult for any reason to have wireline like wireless backhaul. With today’s wireless data rate speeds it would be difficult to imagine it replacing wireline in the near future, but technically speaking it is feasible to achieve those data rates with use of wider spectrum.</p><p>Moreover, wireless communication technologies particularly that of wireless mobile phone technology, is continuously more preferred in communication today, making it the first priority of modern day lifestyle. Modern communication system standards have therefore been subjected to evaluation and analysis to establish a more profound understanding of these various technologies.The proposed study presents an overview of various wireless communication systems such as: Global system for mobile communications (GSM), high-speed packet access (HSPA), long-term evolution (LTE), mobile WiMAX, ultra wideband (UWB) technology, ultra mobile broadband (UMB), wireless local area network (WLAN), Bluetooth wireless technology, and, Wi-Fi.</p>
----------------------------------------------------------------------
In diva2:1453632 merged words and bulleted paragraphs, the abstract should be:

<p>This degree project work has been carried out in the context of in the Hybrid Innovative Powertrain (HIP) project presently executed by the company Altran Technologies S.A.. Its purpose is to provide clients with a modular platform to model and simulate their vehicle as accurately and as fast as possible. The idea is to build a custom Simulink library with a block for each major component found in any road vehicle. The platform must enable clients to model battery electric vehicle as well as plug-in hybrid or fuel cell vehicles with different powertrain topologies. It must provides component blocks for each layer of control, from high-level energy efficiency algorithms embedded in high authority hardware to low-level component control solutions. The work presented here contributes to the development of the modular powertrain electric part with the electric motor, the inverter and the chopper. It is composed of three main steps:
<ul>
<li>Building an electric powertrain model with its proper control schemes in order to enable fast simulations. The model must be split into blocks that become part of the project library.</li>
<li>Building a more realistic model that takes into account the discontinuous switching dynamics and the discrete nature of the actual controllers, in order to verify and validate the control effectiveness.</li>
<li>Designing a diagnostic method for inverter faults.</li>
</ul>
The first part of this work is dedicated to a literature review of automotive electric drives solutions as well as the research in progress in the domain, notably in fault detection schemes. The mathematical models and controller designs used for the simulations are developed in a second part. The last part presents the software implementation aspects and the analysed simulation results.</p>

I suspect that there are also some merged paragraphs, but since there is no full text in DiVA, I cannot be sure.
----------------------------------------------------------------------
In diva2:869163 merged words and paragraphs, the abstract should be:

<p>Testing of information systems is an essential part of the system development process to minimize errors and improve the reliability of systems. Trafikverket IT unit had a structured testing in the test phase high-level, however, they had not a structured testing in the development phase, low-level tests. We were assigned to examine methods and working methods in low-level test. We also would compare systems that had undergone a structured testing in low- and high-level test against systems that had undergone an unstructured low-level test and structured high-level test.</p><zp>The goal of the thesis was to propose appropriate method/methods in low-level test for Trafikverket IT unit. The goal was also to make a recommendation if a structured testing in low- and high-level were to be recommended in comparison with systems that had undergone unstructured low-level test and structured high-level test. Through literary studies and interviews with Trafikverket employees we reached our result.</p><zp>Our recommendation for Trafikverket IT is that they should use test-driven development because developers were unsure of what should be tested and the method would make this clear. The developers also wanted to have options and guidelines that would give them a definite work structure. We also recommend an adaptation of the Self-Governance framework from where activities can be selected from each project manager (Scrum Master) that determines which activities will be performed in individual- and group level for each project.</p>
----------------------------------------------------------------------
In diva2:1324244 merged words and paragraphs, the abstract should be:

<p>The interlock Beam Position Monitor (BPM) system in the Large Hadron Collider(LHC) is responsible for monitoring the particle beam position at the point of the beam dump kicker magnets and is part of the machine protection system. The current interlock BPM system has some limitations and because of this, an upgrade project has been initiated. This master thesis describes the development of the analog front end electronics of this system, consisting mainly of two parts: A delay line based microwave filter and a high isolation and highly balanced power combiner circuit.</p<<p>The filter has been validated with real LHC beam measurements and is found to work as expected. More work however needs to be done to ensure the effect that the filter itself has on the beam measurements as the filter could introduce some ringing effects on the signal. The highly balanced high isolation power combiner has been tested through lab measurements and also shows promising results but long-term tests need to be conducted to ensure the reliability of the component as it will need to endure very high signal levels over long periods of time.</p>
----------------------------------------------------------------------
In diva2:771132 the title is missing a space, it should be:
"Challenges With Session to Session Management in Brain Computer Interfaces"


In diva2:771132 there were merged words, the abstract should be:

<p>Brain computer interfaces (BCIs) enable communication between a brain and a computer, without the need for any motor actions. Electroencephalography (EEG) signals can be used as input for a BCI, but they need to go through a number of steps in the BCI to create useable output. One of the most critical steps is the classification algorithm, which is the step that is investigated in this report. A linear and a nonlinear Support Vector Machine (SVM), together with a Linear Discriminant Analysis (LDA), are investigated in how well they can handle session to session performance when classifying EEG data from three different recording sessions of three different test subjects. The results show that the average performance of the classifiers are in most cases similar, slightly above 60%. The performance of the investigated algorithms differed depending on subject and session. The sometime slow performance of the classification algorithms may be due to the low signal-to-noise ratio in the EEG signals, or possibly even due to bad performance in producing recognizable EEG patterns by the test subjects. The conclusion drawn from the project is that data from different sessions can vary quite extensively, and in this project it was handled best by the nonlinear SVM with RBF kernel, with the highest average classification accuracy.</p>
----------------------------------------------------------------------
In diva2:1273039 unnecessary hyphens and missing hyphens, the abstract should be:

<p>Equation-based object-oriented modeling languages represent a highly composable class of modeling languages. In these languages models are expressed as differential-algebraic equations with no explicit causal relation between variables. Modeling of structurally varying systems in such languages is typically done by defining modes that describe the continuous evolution of the system, coupled with mode-switches describing structural changes. Specifically, structural changes can give rise to discontinuities and impulses, which can result in additional changes to the system. This thesis formalizes semantics for the treatment of structurally varying systems in such languages, including automatic handling of discontinuities and impulses from the theory of non-linear circuits. The semantics are implemented as part of an equation-based modeling language, where the treatment of impulses is based on backwards-Euler. The expressiveness of the implementation is evaluated on a number of structurally varying systems, both in the electrical and mechanical domains. We conclude that the semantics are expressive enough to describe some structurally varying systems, but are sensitive to numerical errors. Furthermore, more work is needed to allow the semantics to express inelastic collision in a satisfactory manner.</p>
----------------------------------------------------------------------
In diva2:809641 merged words and unnecessary paragraphs, the abstract should be:

<p>Given the demand for mobility in our society, the cost of building additional infrastructures and the increasing concerns about the sustainability of the traffic system, traffic managers have to come up with new tools to optimize the traffic conditions within the existing infrastructure. This study considered to optimize the durations of the green light phases in order to improve several criteria such as the ability of the network to deal with important demands or the total pollutant emissions.</p><p>Because the modeling of the problem is difficult and computationally demanding, a stochastic micro-simulator called ’Simulation of Urban MObility’ (SUMO) has been used with a stochastic optimization process, namely a Genetic Algorithm (GA).</p><p>The research objective of the study was to create a computational framework based on the integration of SUMO and a Multi-Objective Genetic-Algorithm (MOGA). The proposed framework was demonstrated on a medium-size network corresponding to a part of the town of Rouen, France. This network is composed of 11 intersections, 168 traffic lights and 40 possible turning movements. The network is monitored with 20 sensors, spread over the network. The MOGA considered in this study is based on NSGA-II. Several aspects have been investigated during the course of this thesis.</p><p>An initial study shows that the proposed MOGA is successful in optimizing the signal control strategies for a medium-sized network within a reasonable amount of time.</p><p>A second study has been conducted to optimize the demand-related model of SUMO in order to ensure that the behavior in the simulated environment is close to the real one. The study shows that a hybrid algorithm composed of a gradient searchalgorithm combined with a GA achieved a satisfactory behavior for a medium-size network within a reasonable time. The demand is defined as the number of cars</p>
----------------------------------------------------------------------
diva2:809641 and diva2:816306 seem to be duplicates with the latter having more downloads
----------------------------------------------------------------------
In diva2:1272223 merged words, the abstract should be:

<p>Rising penetration of renewable energy sources in electric power grids is both a challenge and an opportunity to optimally utilize the potential of either wind or PV energy sources, to stabilize operation of future power systems. Bi-directional flows between distribution and transmission system operators cause signicant problems with keeping the voltages in the grid within admissible limits. This paper contains description of Oland's island medium- and low-voltage electric power grid, ranging from 0.4 kV to 130 kV in the purpose of quasi-static analysis of active and reactive power flows in the system. Goal of the analysis is to optimize reactive power exchange at the point of connection with the mainland grid. In the analyzed grid system, there is an enormous, 190% penetration of wind sources. Capacity of the wind parks connected to dedicated buses totals to 136.1 MW, that supply up to 90.5 MW of load. With industry-wise reactive power capability limits, total contribution of wind parks reaches almost 66 MVAr, enabling to compensate deficits and extra surpluses of the reactive power in the grid. Presented system is connected to the mainland's grid through one point of connection, which is simulated as Thevenin equivalent circuit. Main objective of the thesis is to test and analyze viable solutions to minimize reactive power exchange at the point of connection at Stavlo substation connecting Oland's and Sweden's electric grid keeping valid all necessary contingencies enforced by current grid codes applied in Sweden as well as thermal limits of the lines and voltage limits of the system. Furthermore, state of the art of current reactive power compensation methodologies and most promising techniques to eciently and effectively control reactive power flow are outlined. Droop control methodologies, with focus on global and local objectives, and smartgrid solutions opportunities are being tested and modeled by the authors and are comprehensively presented in this paper. Moreover, economic costs of control methods are compared. Analysis of active power losses in the system as well as cost of implementation of alternative solutions is presented, where most financially viable solutions are outlined, giving brief outlook into future perspectives and challenges of electric power systems. It is shown that controllability of reactive power support by wind turbine generators can enhance operation of electric power grids, by keeping the reactive power ow minimized at the boundary between grids of distribution and transmission system operators. Furthermore, results indicate that extra reactive power support by wind turbine generators can lead to diminishment of active power losses in the system. Presented system is being modeled in the PSS/E software dedicated for power system engineers with use of Python programming languages. Analysis of data was done either in Python or R related environments. Thesis was written with cooperation between KTH and E.On Energidistribution AB.</p>
----------------------------------------------------------------------

The above were all sent on or before 2024-08-01
======================================================================
In diva2:868398 merged words and paragraphs, as well as a missing ligature, along with the page number "i", the abstract should be:

<p>Manufactures of power system products face an increased pressure to reduce the time to market of their development process without compromising quality. Moreover the operation of power systems needs to be performed in a secure and reliable manner. One of the key systems to guarantee those stringent requirements is the protection system. The objective of this Master’s thesis is the development of a protection system, which solidly relies on Commercial-off-the-Shelf (COTS) components as well as on the developed protection functions. Thereby it is shown that the tight cost requirements can be fulfilled without jeopardising the reliability and security performance.</p><p>This project comprises the development of a definite-time directional overcurrent and earth fault protection. The applied development process is based on a model-based-design approach, which comprises the definition of the requirements, the design phase, the implementation on the target system and the test phase. As part of this thesis each stage is described and executed. Moreover MATLAB/Simulink was used as development environment, since it perfectly supports the model-based-design approach. The considered functional requirements are mostly based on the standard IEC 60255-151. The developed protection algorithm runs on a realtime linux system and the interface to the process is based on the EtherCAT protocol and their corresponding I/O modules. Lastly, the test phase is based on a functional performance test, a type test according to IEC 60255-151, a long term test and an evaluation of the EMC performance of the used I/O modules.</p><p>The results of the type tests showed that a IEC 60255-151 compliant solution is yield. Moreover the functional performance test proofed that the developed protection function operate as intended for various fault scenarios. Lastly, the realtime performance of protection system has to be further analysed and adapted in order to achieve satisfactory behaviour.</p>
----------------------------------------------------------------------
In diva2:1110818 the title should be: "Electromagnetic modelling and testing of a Thomson coil based actuator"
----------------------------------------------------------------------
In diva2:1034057 merged words, the abstract should be: <p>Software Defined Networks (SDN) constitute the new communication paradigm of programmable computer networks. By decoupling the control and date plane the network management is easier and more flexible. However, the new architecture is vulnerable to a number of security threats, which are able to harm the network. Network monitoring systems are pivotal in order to protect the network. To this end, the evaluation of a network monitoring system is crucial before the deployment of it in the real environment. Network simulators are the complementary part of the process as they are necessary during the evaluation of the new system’s performance at the design time.</p><p>This work focuses on providing a complete simulation framework which is able to (i) support SDN architectures and the OpenFlow protocol, (ii) reproduce the impact of cyber and physical attacks against the network and (iii) provide detection and mitigation techniques to address Denial-of-Service (DoS) attacks. The performance of the designed monitoring system will be evaluated in terms of accuracy, reactiveness and effectiveness.The work is an extension of INET framework of OMNeT++ network simulator.</p>----------------------------------------------------------------------
In diva2:874540, missing spaces, the abstract should be

<p>Recently, unique combination of electro- and magneto-optic, piezoelectric, and coupled acousto-optic properties makes complex metal oxide films to be important players in emerging heterogeneous integration technologies. Nowadays, bulk-single crystal- quality thin film heterostructures can be fabricated directly on a semiconductor platform. Integration of dissimilar classes of high performance functional materials into a single system enables optical and electrical signal processing and storage: e.g., waferlevel integration of tunable narrowband lasing sources, electro-optic modulators, magneto-optic isolators, logic and memory devices. As a result, well-known photonic materials own new functionalities. In addition, interesting effect of so called resistive switching is observed in several transition metal oxide films might be also considered for the application in novel optical devices. Functional properties of transition metal oxide films can be tailored in a wide range. Standard plasma dry etching technique can be employed to pattern these films to fabricate optical waveguides and optical integrated circuits. Therefore, a complex research program that includes films processing, characterization, theoretical modeling and device simulation is required for material validation and cost efficiency and should precede further practical exploration.</p><p>Three kinds of dielectric oxide films, Er<sub>2</sub>O<sub>3</sub>, WO<sub>3</sub>, and Ta<sub>2</sub>O<sub>5</sub> were deposited on a transparent borosilicate glass substrate via radio-frequency magnetron sputtering. Optical properties of grown films were characterized by ellipsometer technique within the range 1.5-6 eV (850 -200 nm) and using optical transmissionspectrometer 1.1 -6 eV (200-1100nm). Inherent transition peaks at 379 and 524nm caused by a single trivalent Er<sup>3+</sup> ion is observed in transmission spectrum of Er<sub>2</sub>O<sub>3</sub> film. Two different approaches, computation using the ellipsometer DeltaPsi 2 software and Matlab simulation based on the microscopic electric dipole theory were employed to obtain dispersion relations of refractive index and extinction coefficient. We found and explained several mismatches of refractive indexes and extinction coefficients obtained by these two methods.</p><p>Optical characteristics were obtained with a reasonable precision and were set into optical waveguide simulations as input parameters. Modeling of light propagation in 2D planar waveguide with different widths yields requirements for an optical insertion loss. We compared the performance of the waveguides made from the fabricated functional oxides with currently achieved optical properties. Variation of processing parameters leads to the variation of film stoichiometry, critically influences film refractive index and strongly effects a light confinement. With Comsol multiphysics we modeled C-bandlight amplification in 2D waveguides made from Er-doped Ta<sub>2</sub>O<sub>5</sub> and WO<sub>3</sub>.The threshold values of the fractional population of 4I<sup>13/2</sup> level of Er<sup>3+</sup>-ions were found for currently achieved Ta<sub>2</sub>O<sub>5</sub> and WO<sub>3</sub>film properties.</p><p>Advanced device modeling and design should include a tailoring of optical films properties through the optimization of processing conditions furnished with a detailed feedback of comprehensive films characterization.</p>
----------------------------------------------------------------------
In diva2:567676
"model-parameter-invariant" should be
"model-parameter-invariant"
----------------------------------------------------------------------
In diva2:1570274 merged words and merged paragraphs, the abstract should be:

<p>It is a notoriously difficult task to find winning strategies for multi-agent games. Especially if one or multiple agents lack the information required to determine which state the game is in. When this type of uncertainty arises in a game it is referred to as a multi-agent game of imperfect information.</p<<p>In this project we designed and built a tool for strategy synthesis of multi-agent games against nature. The strategy synthesis was knowledge-based and therefore a multi-agent extension of the Knowledge Based Subset Construction, built by a previous project group, was applied to the input games. This construction creates a new knowledge-based game, with reduced uncertainty compared to the initial multi-agent game of imperfect information. We constructed the tool using a forward search heuristic which meant that it would locate all existing winning strategies.</p<<p>We study the performance of the tool by comparing it to a baseline approach relying solely on randomisation. This comparison was performed on five different games. Our tool found every relevant strategy for each game at least 35% faster than the baseline found the same amount of unique winning strategies. If a strategy can win without transitioning through a state, then that state is not relevant and is not part of the strategy. The comparison test for this game shows that the tool is working very well.</p>
----------------------------------------------------------------------
In diva2:615538 merged words and paragraphs, the abstract should be:

<p>The concept of interference alignment has recently become one of the important tools to analyze the capacity of many multiuser communication networks, e.g. K-user interference channel, wireless X networks, multi hop interference networks, etc. The idea is to consolidate the interference into smaller dimensions of signal space at each receiver and use the remaining dimensions to transmit the desired signals. Furthermore, most progress in understanding of the wireless networks capacity has been made on the single hop schemes and multi-hop multi-cast networks. However, there has not been as much progress in multi-hop multi-flow networks where all messages are not required by all destination nodes. One of the basic problems in this area, is the capacity of 2 × 2 × 2 interference channel. It is proved that the upper bound value of 2 degrees of freedom (DoF) for this channel can be achieved using the so called “aligned interference neutralization” method.</p<<p>In the proposed interference alignment schemes for network problems which we mentioned in the above, including 2 × 2 × 2 interference channel, there are some theoretical assumptions which seem to be difficult to apply in practice, e.g. high transmit power, asymptotic symbol extension of the channel, global and perfect channel state information (CSI), etc. Among these assumptions the availability of CSI specially at transmitter, is crucial for performing the interference alignment technique. The CSI at transmitter (CSIT) is usually available through feedback from receiver and it is used to estimate the current channel state, given that the channel coherence-time is long enough. However,it has been shown recently that the delayed CSIT, which is assumed to be independent of current channel state, still can be used to increase DoF of some specific network settings.</p<<p>In this work, we consider the 2 × 2 × 2 interference channel where two source nodes communicate with corresponding destination nodes via two relay nodes. We investigated the degrees of freedom of 2×2×2 interference channel with delayed CSIT and we derived the upper bound on the degrees of freedom of the channel under this condition. Furthermore, we showed that this upper bound can be achieved using interference alignment technique. We also showed that this completely out-of-date information of the channel can still be useful to achieve higher rate compared to the situation where no CSIT is available at the source nodes. Moreover, we observed that using relay nodes in interference channel can improve DoF compared to one hop interference channel where transmitters and receivers directly communicate with each other.</p>
----------------------------------------------------------------------
In diva2:1141375 merged words and paragraphs, the abstract should be:

<p>The ITEA3 OpenCPS (Open Cyber-Physical System Model-Driven Certified Development) project focuses on interoperability between the Modelica/Unified Modeling Language (UML)/Functional Mock-up Interface (FMI) standards, improved (co-)simulation execution speed, and verified code generation. The project aims to develop a modeling and simulation framework for cyber-physical and multi-domain systems. One of the main use cases for the framework, is the multi-domain equation-based modeling and simulation of detailed gas turbine power plants (including the explicit equation-based modeling of turbomachinery dynamics) and the electrical power grid.</p><p>In this work, UML class diagrams based on the Common Information Model (CIM) standard are used to describe the semantics of the electrical power grid. An extension based on the standard ISO 15926 has been proposed to derive the multi-domain semantics required by the models that integrate the electrical power grid with the detailed gas turbine dynamics.</p><p>Furthermore, the multi-domain physical modeling and simulation Modelica language has been employed to create the equation-based models of the use case of this project. A comparative analysis between the Single-Domain and Multi-Domain model responses has been performed both in time and frequency. The results show some interesting differences between the turbine dynamics representation of the commonly used GGOV1 standard model and the less simplified model of a gas turbine.</p><p>Finally, the models from each domain can be exchanged between two different stakeholders by means of Functional Mock-Up Units (FMUs), defined by the FMI standard. Promising test results were obtained with different simulation tools that support the standard, which demonstrates the feasibility of exchanging unambiguous multi-domain models with a detailed gas turbine representation. This shows the potential of the FMI standard for manufacturers to exchange equation-based multi-domain models, while at the same time protecting their intellectual property.</p>
----------------------------------------------------------------------
In diva2:1141736 merged words, the abstract should be:

<p>Over the course of the last few years, the fashion industry has begun to focus more resources on their digital transition. For a fashion e-commerce business, it is essential to know whether or not to invest money and time in building a modern web application. This master's thesis aims at finding practical results on how transitioning from a multi-page website towards a Single Page Application can have an impact on the business performances of the company, as measured by Conversion Rate, Page Views and Gross Sales. In collaboration with the development and product team of a fashion company, this master's thesis is based on the six-months development of a new Single Page Application using the Javascript framework React.js, building on known User-Experience Design principles and Human-Computer Interface heuristics. The live data collection from the website's audience allowed a quantitative analysis of the transition's effect on business performance, which showed positive impact on business performance. A qualitative user survey was then conducted in order to further elaborate on the causes of the aforementioned impact: all respondents praised the Single Page Application as compared to the multi-page website, and noted lower Response Time, efficient filter-and search system and high user interaction as advantages that played in favour of their browsing experience and their will-to buy a product. The impact of lowering Response Time even more was discussed, as well as the different limitations due to the scope of this thesis. A list of user suggestions for further improvements was also compiled.</p>
----------------------------------------------------------------------
In diva2:1412847, 
   multi-tenancy‟s
should be
   multi-tenancy's
----------------------------------------------------------------------
In diva2:618110 merged words, the abstract should be:

<p>Traditional design methods could not cope with the recent development of multiprocessor systems-on-chip (MPSoC). Especially, hard real-time systems that require time-predictability are cumbersome to develop. What is needed, is an efficient, automatic process that abstracts away all the implementation details. ForSyDe, a design methodology developed at KTH, allows this on the system modelling side. The NoC System Generator, another project at KTH, has the ability to create automatically complex systems-on-chip based on a network-on-chip on an FPGA. Both of them support the synchronous model of computation to ensure time-predictability. In this thesis, these two projects are analysed and modelled. Considering the characteristics of the projects and exploiting the properties of the synchronous model of computation, a mapping process to map processes to the processors at the different network nodes of the generated system-on-chip was developed. The mapping process is split into three steps: (1) Binding processes to processors, (2) Placement of the processors on net network nodes, and (3) scheduling of the processes on the nodes. An implementation of the mapping process is described and some synthetic examples were mapped to show the feasibility of algorithms.</p>
----------------------------------------------------------------------
In diva2:703998 merged words and pagragraphs, as well as missing emphasis (italics) and numbered list items, the abstract should be:

<p>IP traffic increase has resulted in a demand for greater capacity of the underlying Ethernet network. As a consequence, not only Internet Service Providers (ISPs) but also telecom operators have migrated their mobile back-haul networks from legacy SONET/SDH circuit-switched equipment to packet-based networks.</p><p>This inevitable shift brings higher throughput efficiency and lower costs; however, the guaranteed QoS and minimal delay and packet delay variation (PDV) that can only be offered by circuit-switched technologies such as SONET/SDH are still essential and are becoming more vital for transport and metro networks, as well as for mobile back-haul networks, as the range and demands of applications increase.</p><p>Fusion network offers "both an Ethernet wavelength transport and the ability to exploit vacant wavelength capacity using statistical multiplexing <em>without interfering</em> with the performance of the wavelength transport" [RVH] by dividing the traffic into two service classes while still using the capacity of the same wavelength in a wavelength routed optical network (WRON) [SBS06]:<ol><li>A <em>Guaranteed Service Transport</em> (GST) service class supporting QoS demands such as no packet loss and fixed low delay for the circuit-switched traffic.</li><li>A <em>statistical multiplexing</em> (SM) service class offering high bandwidth efficiency for the best-effort packet-switched traffic.</li></ol></p><p>Experimentation was carried out using two TransPacket's H1 nodes and the Spirent Test-Center as a packet generator/analyzer with the objective of demonstrating that the fusion technology, using TransPacket's H1 muxponders allow transporting GST traffic with circuit QoS; that is with no packet loss, no PDV and minimum delay independent of the insertion of statistically multiplexed traffic.</p><p>Results indicated that the GST traffic performance is completely independent of the added SM traffic and its load. GST was always given absolute priority and remained with a constant average end-to-end delay of 21.47 μs, no packet loss and a minimum PDV of 50 ns while SM traffic load increased, increasing the overall 10GE lightpath utilization up to 99.5%.</p>
----------------------------------------------------------------------
In diva2:1556726
   naphtha-lene
should be
   naphthalene
----------------------------------------------------------------------
In diva2:1039090 paragraphs were merged, the abstract should be:

<p>The Internet of the 21th century is a different version from the original Internet. The Internet is becoming more and more a huge distribution network for large quantities of data (Photos, Music, and Video) with different types of connections and needs. TCP/IP the work horse for the Internet was intended as a vehicle to transport best effort Connection oriented data where the main focus is about transporting data from point A to point B regardless of the type of data or the nature of path.<7p><p>Information Centric Networking (ICN) is a new paradigm shift in a networking where the focus in networking is shifted from the host address to the content name. The current TCP/IP model for transporting data depends on establishing an end to end connection between client and server. However, in ICN, the client requests the data by name and the request is handled by the network without the need to go each time to a fixed server address as each node in the network can serve data. ICN works on a hop by hop basis where each node have visibility over the content requested enabling it to take more sophisticated decisions in comparison to TCP/IP where the forwarding node take decisions based on the source and destination IP addresses.<7p><p>ICN have different implementations projects with different visions and one of those projects is Named Data Networking (NDN) and that’s what we use for our work. NDN/ICN architecture consists of different layers and one of those layers is the Forwarding Strategy (FS) layer which is responsible for deciding how to forward the coming request/response. In this thesis we implement and simulate three Forwarding Strategies (Best Face Selection, Round Robin, and Weighted Round Robin) and investigate how they can adapt to changes in link bandwidth with variable traffic rate. We performed a number of simulations using the ndnSIMv2.1 simulator. We concluded that Weighted Round Robin offers high throughput and reliability in comparison to the other two strategies. Also, the three strategies offer better reliability than using a single static face and offer lower cost than using the broadcast strategy. We also concluded that there is a need for a dynamic congestion control algorithm that takes into consideration the dynamic nature of ICN. </p>

----------------------------------------------------------------------
In diva2:726393 merged words and missing ligrature, the abstract should be:

<p>A near-field generalization of Friis transmission equation has previously been proposed in the literature. Using this generalization, it is possible to calculate the mutual coupling between two antennas as a weighted integral over the antenna far-fields. In this thesis, a change of variables is used to remove the singularity in the integrand and a normalization of the antenna far-field is suggested to take mismatch and thermal losses into account. The resulting non-singular integral has been implemented in a computer program that can be used to calculate the mutual coupling between two arbitrarily polarized antennas given the antenna far-fields and the geometrical separation between the antennas. The program has several advantages compared to previous programs based on the near-field generalization of Friis transmission equation. Firstly, this program can calculate the mutual coupling between two arbitrarily polarized and oriented antennas whereas previous programs could only be used for linearly polarized and polarization-matched antennas. Secondly, the advantage of the non-singular form is the improved numerical stability. The mutual coupling calculated using this program is demonstrated to agree well with results from full three-dimensional simulations of antennas located in each others near-fields using commercial software. Finally, we investigate for the first time if this integral relation can be used to calculate approximate values of the mutual coupling between antennas on an electrically large vehicle.</p>
----------------------------------------------------------------------
In diva2:1354488 merged words, merged paragraphs, missing emphasis, and missing ligratures, the abstract should be:

<p>To investigate near-field contributions for installed antennas, an in-house code is written to incorporate near-field terms in <em>Shooting and Bouncing Rays</em> (SBR). SBR is a method where rays are launched toward an object and scatter using <em>Geometrical Optics</em> (GO). These rays induce currents on the object, from which the total scattered field can be found.</p><p>To gauge the effect of near-field terms, the in-house code can be set to exclude near-field terms. Due to this characteristic, the method is named <em>SBR Including or Excluding near-field Terms</em> (SIENT). The SIENT implementation is thoroughly described. To make SIENT more flexible, the code works with triangulated meshes of objects. Antennas are represented as near-field sources, allowing complex antennas to be represented by simple surface currents. Further, some implemented optimizations of SIENT are shown.</p><p>To test the implemented method, SIENT is compared to a reference solution and comparable commercial SBR solvers. It is shown that SIENT compares well to the commercial options. Further, it is shown that the inclusion of near-field terms acts as a small correction to the far-field of the installed antenna.</p>

----------------------------------------------------------------------
In diva2:1635607 merged words and paragraphs, the abstract should be:

<p>A modular Recurrent Bayesian Confidence Propagating Neural Networks (BCPNN) with two synaptic time traces is a computational neural network that can serve as a model of biological short term memory. The units in the network are grouped into modules called hypercolumns within which there is a competitive winner-takes-all mechanism.</p><p>In this work, the network’s capacity to store sequential memories is investigated while varying the size of and number of hyperocolumns in the network. The network is trained on sets of temporal sequences where each sequence consist of a set of symbols represented as semi-stable attractor state patterns in the network and evaluated by its ability to later recall the sequences.</p><p>For a given distribution of training sequence the networks’ ability to store and recall sequences was seen to significantly increase with the size of the hypercolumns. As the number of hypercolumns was increased, the storage capacity increased upto a clear level in most cases. After this point it was observed to remain constant and did not improve by adding any more hypercolumns (for a given sequence distribution). The storage capacity was also seen to depend a lot on the distribution of the sequences.</p>
----------------------------------------------------------------------
In diva2:865901 merged paragraphs, the abstract should be:

<p>Openratio offers a mobile platform as a service (mPaaS) to build secure enterprise apps that use legacy systems from vendors such as Microsoft, IBM and SAP. The platform creates native mobile applications and mobile websites using a simple drag and drop interface without writing code.</p><p>Openratio continuously develops their platform in short design cycles, and started a major system upgrade in 2014. The old mobile website was incompatible with the new specifications and upgrading it was time intensive, leading to a 6 months delay behind other system components’ progress.</p><p>This thesis covers the process of analyzing the previous mobile website server and implementing a new mobile website rendering server. The server is developed from scratch using node.js, a javascript platform for developing web applications, and jQuery mobile, an HTML 5 based interface for responsive mobile websites.</p><p>The designed server uses real-time configuration settings to dynamically render each page’s layout, navigation and content with enterprise-grade security. In addition, the server handles user authentication and session management, content fetching from external data sources and uses styling and website templating engines.</p><p>The new implemented server shortened the updating time for each development cycle and decoupling the mobile website rendering server from the main server allowed team members to work on several platform components in parallel.</p>
----------------------------------------------------------------------
In diva2:1375911 missing emphasis, the abstract should be:

<p>Wave surfing is a popular sport that requires minimal financial investment, while it can still be enjoyable from the very first attempt. At the same time, the demand for smart devices that enhance the experience of doing sports by analyzing and evaluating the activities is growing. For surf sport, there are some solutions that are able to collect statistics about activities being done during a surf session, but none of them is able to recognize specific maneuvers that are performed during wave riding.</p><p>The goal of this Master Thesis is to improve a currently existing surf activity monitoring solution by extending it with the ability to identify the two most common surf maneuvers during a wave riding session, namely cutback and snap. The solution is using the user’s smartphone to collect IMU sensor data and feed it to a classification pipeline.</p><p>The implemented algorithm takes raw sensor data as an input, performs various preprocessing steps, splits the input stream into segments, extracts features from these segments and feed them into a hierarchical classification tree. The implemented pipeline is able to classify <em>non-maneuver</em>, <em>cutback</em> and <em>snap segments</em> with 78% accuracy on a self-collected dataset.</p>
----------------------------------------------------------------------
In diva2:1295364 merged words, the abstract should be:

<p>Customized truck is a relevant and high-profit part of Scania’s market. Nowadays designers do not have a self-developed electric control unit for introducing non-standard functionality. This thesis is intended to investigate the specification of a new system and an approach to implement it in current truck’s electrical system. The adopted methods in this research are systems analysis, functional specification, HW &amp; SW research and comparison and validation test of prototype system. The results obtained in this research include the potential functions and corresponding requirement specification, 5 types of hardware alternatives and 2 recommended software platform. Their feasibility is verified by a prototype system with 2 typical functions, control the motor of inward sliding door and combine the communication of different system. 3 recommended schemes and 2 directions for future research are given in the end. Based on investigation results, developers are able to know new system’s specification preliminarily, understand the architectural requirements and suitable tools and materials for implementation. This research will help to improve Scania’s truck’s electrical system and product manufacturing in the future.</p>
----------------------------------------------------------------------
In diva2:606245 merged words, the abstract should be:

<p>The study of dust dynamics in tokamaks has been carried out by means of the DDFTU numerical code solving the coupled equations of motion, charging and heat balance for a dust grain immersed in plasmas with given profiles. The code has been updated to include (i) a non-steady state heat balance model and phase transitions, (ii) geometrical properties of the vessel such as gaps, (iii) realistic boundary conditions for dust-wall collisions. The models for secondary electron emission (SEE), thermionic emission and black body radiation have also been refined, and sensitivity of the results to the SEE strength is demonstrated.</p><p>The DDFTU code has been used for the first time to explore a large range of initial conditions (position, velocity and radius) for dust grains of various tokamak-relevant materials. This study confirmed the impact of the drag force as one of the main factors in dust dynamics and allowed to estimate average lifetimes, to locate preferred sites for dust deposition and to judge the sensitivity to initial conditions. This is a first step towards the use of the code as a predictive tool for devices of importance, such as JET and ITER.</p><p>Preliminary simulations of scenarios relevant for dust injection experiments in TEXTOR have yielded results in remarkable agreement with experimental data.</p><p>These preliminary studies allowed to identify the most crucial issues affecting dust dynamics, lifetime, deposition rate and contribution to impurities, which are to be pursued in future studies.</p>
----------------------------------------------------------------------
In diva2:1332086 merged words, the abstract should be:

<p>The European Space Agency’s Rosetta spacecraft followed the comet 67P/Churyumov-Gerasimenko from August 2014 to September 2016, providing observations of the comet ionosphere at varying heliocentric distances. Measurements from the Rosetta mission have shown a multitude of non-thermal electron distributions in the cometary environment, challenging the previously assumed origin and plasma interaction mechanisms near a cometary nucleus. In this thesis, we discuss electron trapping near a weakly outgassing comet from a fully kinetic (particle-in-cell) perspective which self consistently describe the ambipolar field. Using electromagnetic fields derived from the simulation, we characterize the trajectories of trapped electrons in the potential well surrounding the cometary nucleus and identify the distinguishing features in their respective velocity and pitch angle distributions. In accordance with theoretical findings in space plasma, our analysis allows us to define a clear boundary in velocity phase space between the distributions of trapped and passing electrons.</p>

----------------------------------------------------------------------
In diva2:1453586 merged words and ":" instead of ".", the abstract should be:

<p>Recent advances in 3D printing, with new techniques and materials, are an opportunity for the design of innovative microwave devices, as requirements in electromagnetic discretion and bandwidth are becoming increasingly strict. To this end, an electromagnetic absorber that is omnidirectional, wideband in the super high frequency domain and compact has to be devised. Additionnally, as part of a material by design approach, the ideal properties of future printable materials, dedicated to the design of absorbers, have to be investigated. We design and simulate a hyperbolic metamaterial microwave absorber made of metal-dielectric cavities stacked in a pyramidal fashion, to be fabricated with fused deposition modeling (FDM) materials. It achieves over 90% power absorption in 8.9 – 18.6 GHz for a thickness of 6.3mm. Taking advantage of the flexibility provided by 3D printing, we show that a geometry using stacked parallepipedic cavities with such dimensions that the corresponding absorption peaks are evenly spread in the absorption band yields better results than the existing tapered metamaterial absorbers. Finally, we evidence the need fora high dielectric permittivity to improve performances in oblique incidence. This work opens new perspectives for compact and efficient radiofrequency devices and should drive the development of future materials.</p>
----------------------------------------------------------------------
In diva2:1499100 merged words, the abstract should be:

<p>As the era of Industry 4.0 arises, the number of devices that are connected to a network has increased. The devices continuously generate data that has various information from power consumption to the configuration of the devices. Since the data have the raw information about each local node in the network, the manipulation of the information brings a potential to benefit the network with different methods. However, due to the large amount of non-IID data generated in each node, manual operations to process the data and tune the methods became challenging. To overcome the challenge, there have been attempts to apply automated methods to build accurate machine learning models by a subset of collected data or cluster network nodes by leveraging clustering algorithms and using machine learning models within each cluster. However, the conventional clustering algorithms are imperfect in a distributed and dynamic network due to risk of data privacy, the nondynamic clusters, and the fixed number of clusters. These limitations of the clustering algorithms degrade the performance of the machine learning models because the clusters may become obsolete over time. Therefore, this thesis proposes a three-phase clustering algorithm in dynamic environments by leveraging 1) GAN-based clustering, 2) cluster calibration, and 3) divisive clustering in federated learning. GAN-based clustering preserves data because it eliminates the necessity of sharing raw data in a network to create clusters. Cluster calibration adds dynamics to fixed clusters by continuously updating clusters and benefits methods that manage the network. Moreover, the divisive clustering explores the different number of clusters by iteratively selecting and dividing a cluster into multiple clusters. As a result, we create clusters for dynamic environments and improve the performance of machine learning models within each cluster.</p>
----------------------------------------------------------------------
In diva2:1471509 merged paragraphs, the abstract should be:

<p>An automated AI solution for out-door Telecom equipment segmentation is beneficial to most of the workflow for site survey and engineering performed by human. AI solutions that perform segmentation tasks are today trained with supervised learning which requires manually labeled images. However, labeling images is both time consuming and expensive, which makes semisupervised learning attractive where unlabeled data is used to further improve the performance of models.</p><p>To determine if semi-supervised learning can be used to improve the performance of instance segmentation, the effectiveness of a semi-supervised learning approach called FixMatch was tested for instance segmentation using a custom dataset. The dataset contains 590 labeled and 1000 unlabeled drone-captured images of Telecom equipment. An extension was made to FixMatch where the predicted bounding boxes and masks are augmented like the images, which makes it possible to use FixMatch for instance segmentation. The extension was evaluated with mean Average Precision (mAP) but only achieved 1 point higher mAP than without using the extension. The small improvement in performance shows that this semi-supervised approach is not suitable for instance segmentation of Telecom equipment where the amount of unlabeled data is twice the labeled.</p>


Note "out-door" is in the original, rather than "outdoor".
----------------------------------------------------------------------
In diva2:1230122
   'over-all'
should be
   'overall'
----------------------------------------------------------------------
In diva2:850230 unnecessary hyphens and unnecessary paragraphs, the abstract should be:

<p>Responsive web design is a popular approach to support devices with varying characteristics (viewport size, input mechanisms, media type, etc.) by conditionally style the content of a document by such criteria using CSS media queries. To reduce complexity it is also popular to develop web applications by creating reusable modules. Unfortunately, responsive modules require the user of a module to define the conditional styles since only the user knows the layout of the module. This implies that responsive modules cannot be encapsulated (i.e., that modules cannot perform their task by themselves), which is important for reusability and reduced complexity. This is due to the limitation of CSS media queries that elements can only be conditionally styled by the document root and device properties. In order to create encapsulated responsive modules, elements must be able to be conditionally styled by element property criteria, which is known as element queries.</p><p>Participants of the main international standards organization for the web, the W3C, are interested in solving the problem and possible solutions are being discussed. However, they are still at the initial planning stage so a solution will not be implemented natively in the near future. Additionally, implementing element queries imposes circularity and performance problems, which need to be resolved before writing a standard.</p><p>This thesis presents the issues that element queries impose to layout engines and shows some approaches to overcome the problems. In order to enable developers to create encapsulated responsive modules, while waiting for native support, a third-party element queries JavaScript library named ELQ has been developed. As presented in this thesis, the library provides both performance and usage advantages to other related libraries. An optimized subsystem for detecting resize events of elements has been developed using a leveled batch processor, which is significantly faster than similar systems. As part of the empirical evaluation of the developed library the Bootstrap framework has been altered to use element queries instead of media queries by altering ~50 out of ~8500 lines of style code, which displays one of the advantages of the library.</p>

Note that the Swedish abstract also needs to be corrected:

diva2:850230: <p>Responsiv webbutveckling är ett populärt sätt att stödja enheter med varierande egenskaper (storlek av visninsområdet, inmatningsmekanismer, mediumtyper, etc.) genom att ange olika stilar för ett dokument beroende på enhetens egenskaper med hjälp av CSS media queries. Det är också populärt att utveckla webbapplikationer genom att skapa återanvändbara moduler för minskad komplexitet. Tyvärr kräver responsiva moduler att användaren av en modul definierar de olika responsiva stilarna eftersom endast användaren vet i vilket kontext modulen används. Detta implicerar att responsiva moduler inte är enkapsulerade (alltså att de inte fungerar av sig själva), vilket är viktigt för återanvändning och reduktion av komplexitet. Det beror på CSS media queries begränsningar att det endast går att definiera olika stilar för element beroende på dokumentets rot och enhetens egenskaper. För att kunna skapa enkapsulerade responsiva moduler måste olika stilar kunna definieras för ett element beroende på ett elements egenskaper, vilket är känt som element queries.</p><p>Deltagare av det internationella industrikonsortiet för webbstandardisering, W3C, är intresserade av att lösa problemet och möjliga lösningar diskuteras. De är dock endast i det initiala planeringsstadiet, så det kommer dröja innan en lösning blir implementerad. Dessutom är det problematiskt att implementera element queries eftersom de medför problem gällande cirkularitet samt prestanda, vilket måste lösas innan en standard skapas.</p><p>Denna rapport presenterar de problem för webbläsares renderingsmoterer som element queries medför och visar sätt att övervinna vissa av problemen. För att möjliggöra skapandet av enkapsulerade responsiva moduler, i väntan på webbläsarstöd, har ett tredjepartsbibliotek för element queries namngett ELQ skapats i JavaScript. Biblioteket erbjuder både prestanda- och användningsfördelar jämfört med andra relaterade bibliotek. Ett optimerat delsystem för att detektera förändringar av elements storlekar har utvecklats som använder en nivåuppdelad <em>batch</em>-processerare vilket medför att delsystemet erbjuder signifikant bättre prestanda än relaterade system. Som del av den empiriska utvärderingen har det populära ramverket Bootstrap modifierats att använda element queries istället för media queries genom att ändra ~50 utav ~8500 rader stilkod, vilket visar en av fördelarna med det utvecklade biblioteket.</p>


The title is missing two spaces:
    'ELQ: Extensible Element Queries forModular Responsive WebComponents'
should be:
    'ELQ: Extensible Element Queries for Modular Responsive Web Components'
----------------------------------------------------------------------
In diva2:1095622: 

   peerto-peer
should be:
   peer-to-peer
----------------------------------------------------------------------
In diva2:1620371 merged words and multiple hyphens were missing, the abstract should be:

<p>Building occupancy estimation has become an important topic for sustainable buildings that has attracted more attention during the pandemics. Estimating building occupancy is a considerable problem in computer vision, while computer vision has achieved breakthroughs in recent years. But, machine learning algorithms for computer vision demand large datasets that may contain users’ private information to train reliable models. As privacy issues pose a severe challenge in the field of machine learning, this work aims to develop a privacy-preserved machine learning-based method for people counting using a low-resolution thermal camera with 32 × 24 pixels. The method is applicable for counting people in different scenarios, concretely, counting people in spaces smaller than the field of view (FoV) of the camera, as well as large spaces over the FoV of the camera. In the first scenario, counting people in small spaces, we directly count people within the FoV of the camera by Multiple Object Detection (MOD) techniques. Our MOD method achieves up to 56.8% mean average precision (mAP). In the second scenario, we use Multiple Object Tracking (MOT) techniques to track people entering and exiting the space. We record the number of people who entered and exited, and then calculate the number of people based on the tracking results. The MOT method reaches 47.4% multiple object tracking accuracy (MOTA), 78.2% multiple object tracking precision (MOTP), and 59.6% identification F-Score (IDF1). Apart from the method, we create a novel thermal images dataset containing 1770 thermal images with proper annotation.</p>
----------------------------------------------------------------------
In diva2:1249540 merged words and paragraphs, the abstract should be:

<p>This master thesis aimed to design a permanent magnet assisted synchronous reluctance machine (PMaSynRM) rotor for pump applications which were to be implemented in an existing Induction Machine stator. The machine were to be compared with a similar permanent magnet synchronous machine (PMSM) with similar torque production in terms of cost and performance.</p><p>This thesis goes through the theory of the Synchronous Reluctance Machine and the Permanent Magnet assistance. A rotor was designed by utilizing existing design approaches and simulation of performance by use of finite element analysis. A demagnetization study was conducted on the added permanent magnets in order to investigate the feasiblity of the design.</p><p>The final design of the PMaSynRM was thereafter compared to the equivalent surface-mounted PMSM in terms of performance and cost. The performance parameters was torque production, torque ripple, efficiency and power factor. Due to the lower torque density of the PMaSynRM, for equal torque production the PMSM had a 40% shorter lamination stack than the PMaSynRM.</p><p>The economic evaluation resulted in that when utilizing ferrite magnets in the PMa-SynRM it became slightly cheaper than the PMSM, up to 20%. However, due to the fluctuating prices of NdFeB magnets, there exist breakpoints below which the PMaSynRM is in fact more expensive than the PMSM or where the price reduction of the PMaSynRM is not worth the extra length of the motor. However, it was shown that the PMaSynRM is very insensitive to magnet price fluctuations and thereby proved to be a more secure choice than the PMSM</p>
----------------------------------------------------------------------
In diva2:766162, title is missing a space
   Classify Swedish bank transactions withearly and late fusion techniques
should be:
   Classify Swedish bank transactions with early and late fusion techniques

Missing paragraphs, spaces, and ligratures, the abstract should be:

diva2:766162: <p>Categorising bank transactions to predefined categories are essential for getting a good overview of ones personal finance. Tink provides a mobile app for automatic categorisation of bank transactions. Tink's categorisation approach is a clustering technique with longest prefix match based on merchant.</p><p>This thesis will examine if a machine learning model can learn to classify transactions based on its purchase, what was bought, instead of merchant.</p><p>This thesis classifies bank transactions in a supervised learning setting by exploring early and late fusion schemes on three types of modalities (text, amount, date) found in Swedish bank transactions. Experiments are carried out with Naive Bayes, Support Vector Machines and Decision Trees. The different fusion schemes are compared with no fusion, learned on only one modality, and stacked classification, learning models in a pipe-lined fashion.</p><p>The early fusion concatenation schemes shows all worse performance than no fusion on the text modality. The late fusion experiments on the other hand shows no impact of modality fusion.</p><p>Suggestions are made to change the feedback loop from user, to get more data labeled by users, which would potentially boost the other modalities importance</p>

Note "pipe-lined" is used in the original, although it should be "pipelined".
----------------------------------------------------------------------
In diva2:1713353, missing italics, missing first hyphen in pixel-by-pixel, the degree symbols were centered vertically (rather than the correct position), merged paragraphs, the abstract should be:

<p>Ray tracing comes together with a tremendous computational cost [1]. Therefore, Keller <em>et al.</em>, expressed that possible cost reduction appears when a hybrid rendering pipeline is implemented by combining rasterization and ray tracing, which have already been introduced to the film and game industries. Such a rendering method within Grand Strategy Games (GSG) has been an unexplored task.</p><p>The standard rendering method of GSG has been rasterization. Implementing hybrid rendering for GSG would allow this niche to follow the continuously developing rendering techniques. Therefore, this thesis examined the advantages and disadvantages of hybrid rendering compared to a path traced pipeline. The study measured different camera angles applied to three GSG-inspired scenes by rendering time and quality according to pixel-by-pixel comparison focusing on effects like shadows and reflections. Closeup images have been taken on the rendered scenes to evaluate interesting pieces in the scenes.</p><p>Steady time performance for all angles was the significant advantage of the hybrid pipeline. The angles at lower grades resulted in an increased difference in shadows and reflections for two out of three scenes. Additionally, the entire pixel-by-pixel comparison did not generate more than ten percent difference for any scene and not more than twelve percent difference on closeup images. Still, differences were noticeable to the eye since the path tracer was superior for developing sharp shadows. The hybrid pipeline generated a massive reflection compared to the path tracer. Since the path tracer was defined as the ground truth, this quantity of reflections was not considered positive.</p><p>The thesis concludes that a simple hybrid rendering pipeline could be an exciting future for GSG, especially for angles above 67.25°. Additionally, improving the sharpness of the shadows for the hybrid rendering pipeline could increase the interest in hybrid rendering for GSG even at angles below 67.25°. Some interesting future work is rendering advanced 3D map-based GSG scenes, including more shadows and reflections. Another suggestion is a qualitative analysis of users playing a game with the two rendering pipelines before attending a user study about their possible improved graphical experience and how the game experience has been affected.</p>

Some of the same problems in Swedish abstract, it should be:

<p>Strålspårning kommer tillsammans med en stor beräkningskostnad [1]. Därför har Keller m.fl. uttryckt att kostnaderna kan reduceras genom att implementera en hybridrenderingsmetod baserad på en kombination av rastrering och strålspårning, vilket redan har introducerats till film- och spelindustrin. En sådan renderingsmetod inom Grand Strategy Games (GSG) har dock varit ett outforskat område.</p><p>Standard renderingsmetoden för GSG har varit rastrering. Implementering av hybridrendering för GSG skulle tillåta denna nisch att följa de ständigt utvecklande renderingsteknikerna. Därför undersöker denna avhandling fördelarna och nackdelarna med hybridrendering jämfört med en renderingspipeline baserad på strålspårning. Studien har mätt olika kameravinklar applicerade på GSG-inspirerade scener mätt med renderingstid och kvalitet enligt pixel-för-pixel-jämförelse och med fokus på effekter som skuggor och reflektioner. Närbilder har tagits på de renderade scenerna för att utvärdera intressanta delar i scenerna.</p><p>Stabil tidprestanda av samtliga vinklar var den betydande fördelen med hybridpipelinen. Vinklarna vid lägre grader resulterade i ökad differens av skuggor och reflektioner för två av tre scener. Dessutom resulterade hela pixeljämförelsen inte mer än tio procents skillnad för någon av scenerna och inte mer än tolv procents skillnad på närbilderna. Ändå var skillnaderna märkbara för ögat eftersom strålspårningen var överlägsen för att generera skarpa skuggor. Hybridlösningen genererade en stor andel reflektion jämfört med strålspårningen. Eftersom strålspårningen definierades som målbilden var denna mängd reflektioner inte positiva.</p><p>Avhandlingen drar slutsatsen att en enkel hybridmetod kan vara en spännande framtid för GSG, speciellt för vinklar över 67,25° . Dessutom kan en förbättring av skärpan på skuggorna för hybridrenderingen öka intresset för hybridrendering för GSG även vid vinklar under 67,25° . Intressanta framtida arbeten är rendering av avancerade GSG scener, som inkluderar fler skuggor och reflektioner. Ett till förslag är kvalitativ analys av användare som spelar ett spel med de två renderingsmetoder följt av användarstudie om deras möjliga förbättrade grafiska upplevelse och om spelupplevelsen har drabbats.</p>
----------------------------------------------------------------------
In diva2:1548999 merged words, the abstract should be:

<p>In order to meet the increasing demand of energy in today’s society while at the same time minimizing the environmental impact, renewable energy sources will be required to be integrated into the existing energy mix. Technological advances in high voltage direct current (HVDC) grids play a crucial role in making this possible. Therefore the purpose of this project has been to validate the properties of basic control strategies in terms of how they respond to four different simulation cases. All simulations have been conducted on a simplified version of the CIGR ́E B4 test grid, consisting of four monopolar HVDC converters. After analyzing the results obtained from each control strategy it became evident that provided if the benefits of the redundancy introduced by a multi-terminal grid are to be fully utilized, a distributed voltage control should be used. Moreover, after substituting one of the four internal controllers with an external one, it became clear that simply deciding the droop constants based on results from the simulation model wouldn’t be sufficient for real worl dapplications.</p>
----------------------------------------------------------------------
In diva2:1381398 merged words and merged paragraphs, the abstract should be:

<p>Pitch-shifting lowers or increases the pitch of an audio recording. This technique has been used in recording studios since the 1960s, many Beatles tracks being produced using analog pitch-shifting effects. With the advent of the first digital pitch-shifting hardware in the 1970s, this technique became essential in music production. Nowadays,it is massively used in popular music for pitch correction or other creative purposes. With the improvement of mixing and mastering processes, the recent focus in the audio industry has been placed on the high quality of pitch-shifting tools. As a consequence, current state-of-the-art literature algorithms are often outperformed by the best commercial algorithms. Unfortunately, these commercial algorithms are ”black boxes” which are very complicated to reverse engineer.</p><p>In this master thesis, state-of-the-art pitch-shifting techniques found in the literature are evaluated, attaching great importance to audio quality on musical signals. Time domain and frequency domain methods are studied and tested on a wide range of audio signals. Two offline implementations of the most promising algorithms are proposed with novel features. Pitch Synchronous Overlap and Add (PSOLA), a simple time domain algorithm, is used to create pitch-shifting, formant-shifting, pitch correction and chorus effects on voice and monophonic signals. Phase vocoder, a more complex frequency domain algorithm, is combined with high quality spectral envelope estimation and harmonic-percussive separation to design a polyvalent pitch-shifting and formant-shifting algorithm. Subjective evaluations indicate that the resulting quality is comparable to that of the commercial algorithms.</p>
----------------------------------------------------------------------
In diva2:1130024 merged words, the abstract should be:

<p>A spacecraft needs to simultaneously provide orbital and attitude control but these are in general treated as separate systems. Normally the attitude control is conducted via reaction wheels but can in scenarios with high manoeuvrability demands be handed over to pure thruster control. In specific cases the reaction wheels are removed from the spacecraft to save mass. If both the orbital and attitude control is regulated with thrusters, there is a potential to save fuel in a combined control strategy. Model predictive control has been shown to be a viable method for orbital control with a fuel minimising objective. This thesis investigates a combined orbital and attitude model predictive control strategy. Three test cases are simulated with a specific thruster configuration; maintaining a passive orbit relative to a target, large-angle reorientation and repositioning, and rendezvous. Preliminary results show that including the coupled dynamics lowers the overall fuel consumption while satisfying requirements on position and attitude in scenarios where the timescale of the orbital and attitude control is similar.</p>
----------------------------------------------------------------------
In diva2:1272290 merged words and paragraphs, the abstract should be:

<p>Model predictive control (MPC) is an advanced control technique that requires solving an optimization problem at each sampling instant. Several emerging applications require the use of short sampling times to cope with the fast dynamics of the underlying process. In many cases, these applications also need to be implemented on embedded hardware with limited resources. As a result, the use of model predictive controllers in these application domains remains challenging.</p><p>This work deals with the implementation of an interior point algorithm for use in embedded MPC applications. We propose a modular software design that allows for high solver customization, while still producing compact and fast code. Our interior point method includes an efficient implementation of a novel approach to constraint softening, which has only been tested in high-level languages before. We show that a well conceived low-level implementation of integrated constraint softening adds no significant overhead to the solution time, and hence, constitutes an attractive alternative in embedded MPC solvers.</p>
----------------------------------------------------------------------
In diva2:1354472 merged words, the abstract should be:

<p>Industrial robots are becoming an integral part of the production industry. Efficient operation with respect to fast movements is critical to increase the economic benefits of automating the production line. Facilitating near optimality with regards to time has high computational demands however and multiple frameworks have been suggested to remedy this. In this thesis we consider one of these frameworks, namely the elastic band framework. We investigate how the elastic band time optimal control framework performs regarding computational time for point-to-point movements on a SCARA type robot with three revolute and one prismatic joint. We compare an unconstrained elastic band formulation with a constrained formulation in the open loop, along with simulating performance in the closed-loop. We show that a constrained formulation which considers the sparseness of underlying matrices in the optimization problem has the lowest computational time. Additionally, we show that the unconstrained formulation benefits from early stopping. Finally, we show that a controller implementing this formulation can be used in a model predictive controller, although the computational time is still too high for commercial use on the hardware used in testing.</p>
----------------------------------------------------------------------
In diva2:1140559 merged words and missing ligrature, the abstract should be:

<p>The aim of the project is to design an active safety system for passenger vehicles for mitigating secondary collisions after an initial impact. The control objective is to minimize the lateral deviation from the known original path while achieving a safe heading angle after the initial collision. A hierarchical controller structure is proposed: the higher layer is formulated as a linear time varying model predictive controller that defines the virtual control moment input; the lower layer deploys a rule-based controller that realizes the requested moment. The designed control system is then tested and validated in Simulink as well as in IPG CarMaker, a high delity vehicle dynamics simulator.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-08
======================================================================
In diva2:1472471 merged paragraphs, the abstract should be:

<p>Elevated heart rate is considered to be an indicator of stress. Thus, noticing one’s own heartbeat can have a negative connotation. Yet, the heartbeat is simply a physiological function, neither positive nor negative in itself, that is experienced in diverse contexts, such as medical, athletic, or intimate. This study uses first-person  research through design and soma design to increase awareness of the heartbeat from both an individual and social angle and examines the potential benefits of using external sensory stimuli to convey biofeedback information. It also opens up the design space around the heartbeat and sensory stimuli and reflects upon comfort and relaxation, biofeedback and digital mindfulness, the Sensiks sensory reality pod as a tool and space, and the heartbeat as a spectrum and a way of getting to know people. The study results in four deliverables: a design critique of the Sensiks sensory reality pod, a design fiction publication, a design proposal, and an experience prototype.</p><p>The study proposes the design for the Gallery of Heartbeats – a sensory experience aimed at externalising and sharing the heartbeat of self and others. The Gallery of Heartbeats supports individual reflections, providing the user with real-time numerical, graphical, and auditory biofeedback on their heart rate. It also encourages social communication of this commonly unnoticed physiological feature, allowing users to record and store their heartbeat to an archive and experience the pre-recorded heartbeats of others in a multisensory way.</p><p>The evaluation of the Gallery of Heartbeats prototype shows that the design succeeds in making people more aware of their cardiovascular activity, triggers their curiosity, and increases empathy. However, the Gallery of Heartbeats also makes the users want to control or change their heart rate which goes against the mindfulness principles of presence-in and presence-with the design was inspired by. Sensory stimuli, especially sound and visuals, are assessed as beneficial for creating feelings of immersion, whereas different representations of the biofeedback information have different effects and use cases.</p>

----------------------------------------------------------------------
In diva2:654254 the paragraphs were merged and the text does not match that in tbe thesis, the abstract should be:

<p>When it comes to authenticating a user on the Internet today, the de facto standard is to do so with an alphanumeric password. This method is simple to use both from a user- and from a developers prespective. However there are growing concerns about the security of this scheme.</p><p>There exists researches about alternatives to the alphanumeric scheme, some of these focus on the user's ability to remember and identifying pictures. Such a scheme is reserched and implemented in this thesis and a small study about the useability of a such scheme is presented. The results, though intresting, were to slim to do any statistically secure conclusion.</p>

"to slim" should be "too slim" but "to" is used in the abstract in the thesis.
'prespective' should be 'perspective', but the former is used in the abstract in the thesis.
----------------------------------------------------------------------
In diva2:1823470 merged words, the abstract should be:

<p>As machine learning models affect our lives more strongly every day, developing methods to train these models becomes paramount. In our paper, we focus on the problem of minimizing a sum of functions, which lies at the heart of most - if not all - of these training methods. This problem was formulated in terms of a decentralized consensus optimization, with the terms of the sum belonging to different agents. We examined the efficency and privacy-preserving properties of methods to solve this problem, as well as conducted numerical experiments on several variations of the I-ADMM algorithm. Our results show that utilizing encryption is inefficient compared to PI-ADMM1, while PI-ADMM1 converges at the same speed as I-ADMM.</p>
----------------------------------------------------------------------
In diva2:1835948, the title is missing spaces:
   Membership Inference Attacksagainst GenerativeAdversarial Networks
should be:
   Membership Inference Attacks against Generative Adversarial Networks

In the abstract:
   privacypreserving
should be:
   privacy preserving

----------------------------------------------------------------------
In diva2:654167
   'projet'
should be:
   'project'
This is an error in the original.
----------------------------------------------------------------------
In diva2:1247558
   'projetct'
should be:
   'project'
This is an error in the original.

There should not be a "</p><p>" before MATLAB.
----------------------------------------------------------------------
In diva2:1837771 merged paragraphs and missing emphasis, the abstract should be:

<p>The field of <strong>artificial intelligence</strong> (AI) has grown rapidly in recent years and its applications are becoming more widespread in various fields, including politics. In particular, <em>presidential debates</em> have become a crucial aspect of election campaigns and it is important to analyze the information exchanged in these debates in an objective way to let voters choose without being influenced by biased data. The objective of this project was to create an <strong>automatic analysis tool</strong> for presidential debates using AI.</p><p>The main challenge of the final system was to determine the <em>speaking time</em> of each candidate and to analyze <em>what each candidate said</em>, to detect the <em>topics discussed</em> and to calculate the time spent on each topic. This thesis focus mainly on the <strong>speaker detection</strong> part of this system. In addition, the high <strong>overlap rate</strong> in the debates, where candidates cut each other off, posed a significant challenge for speaker diarization, which aims to determine who speaks when.</p><p>This problem was considered appropriate for a Master’s thesis project, as it involves a combination of advanced techniques in AI and speech processing, making it an important and difficult task. The application to political debates and the accompanying <em>overlapping pathways</em> makes this task both challenging and innovative.</p><p>There are <strong>several ways</strong> to solve the problem of speaker detection. We have implemented <em>classical</em> approaches that involve <strong>segmentation</strong> techniques, <strong>speaker representation</strong> using embeddings such as i-vectors or x-vectors, and <strong>clustering</strong>. Yet, due to speech overlaps, the <em>End-to-end</em> solution was implemented using pyannote-audio (an open-source toolkit written in Python for speaker diarization) and the <strong>diarization error rate</strong> was significantly reduced after refining the model using our own labeled data.</p><p>The results of this project showed that it was possible to create an <strong>automated presidential debate analysis tool</strong> using AI. Specifically, this thesis has established a state of the art of <strong>speaker detection</strong> taking into account the particularities of the politics such as the high speaker <em>overlap rate</em>.</p>

Similiary for the Swedish abstract:

<p><strong>AI-området</strong> (artificiell intelligens) har vuxit snabbt de senaste åren och dess tillämpningar blir alltmer utbredda inom olika områden, inklusive politik. Särskilt <em>presidentdebatter</em> har blivit en viktig aspekt av valkampanjerna och det är viktigt att analysera den information som utbyts i dessa debatter på ett objektivt sätt så att väljarna kan välja utan att påverkas av partiska uppgifter. Målet med detta projekt var att skapa ett <strong>automatiskt analysverktyg</strong> för presidentdebatter med hjälp av AI.</p><p>Den största utmaningen för det slutliga systemet var att bestämma <em>taltid</em> för <em>varje kandidat</em> och att analysera vad varje kandidat sa, att upptäcka <em>diskuterade ämnen</em> och att beräkna den tid som spenderades på varje ämne. Denna avhandling fokuserar huvudsakligen på <strong>detektering av talare</strong> i detta system. Dessutom innebar den höga <strong>överlappningsgraden</strong> i debatterna, där kandidaterna avbröt varandra, en stor utmaning för talardarization, som syftar till att fastställa vem som talar när.</p><p>Detta problem ansågs lämpligt för ett examensarbete, eftersom det omfattar en kombination av avancerade tekniker inom AI och talbehandling, vilket gör det till en viktig och svår uppgift. Tillämpningen på politiska debatter och den åtföljande <em>överlappande vägar</em> gör denna uppgift både utmanande och innovativ.</p><p>Det finns <strong>flera sätt</strong> att lösa problemet med att upptäcka talare. Vi har genomfört <em>klassiska</em> metoder som innefattar <strong>segmentering</strong> tekniker, <strong>representation av talare</strong> med hjälp av inbäddningar som i-vektorer eller x-vektorer och <strong>klustering</strong>. På grund av talöverlappningar implementerades dock <em>Endto-end-lösningen</em> med pyannote-audio (en verktygslåda med öppen källkod skriven i Python för diarisering av talare) och <strong>diariseringsfelprocenten</strong> reducerades avsevärt efter att modellen förfinats med hjälp av våra egna märkta data.</p><p>Resultaten av detta projekt visade att det var möjligt att skapa ett <strong>automatiserat verktyg för analys av presidentdebatten</strong> med hjälp av AI. Specifikt har denna avhandling etablerat en state of the art av <strong>talardetektion</strong> med hänsyn till politikens särdrag såsom den höga <em>överlappningsfrekvensen</em> av talare.</p>

----------------------------------------------------------------------

In diva2:1463843 missing emphasis, the abstract should be:

<p>Creating cloze sentences—contextual fill-in-the-blanks questions—can be time consuming and challenging. While research has been done on automatic question generation, a problem within this area is the high complexity of processing text and constructing relevant questions. <em>word2vec</em> is a new word embedding toolkit, which allows for vectorisation of words. For instance, by creating a vector space from a paragraph, distances between words may be found. This report investigates the use of <em>word2vec</em> in cloze sentence generation by constructing a bare-bones program and comparing the results to MEK questions of the SweSAT. Of 20 survey respondents, 39.6% identified computer generated questions as SweSAT questions. In contrast, 56.8% correctly identified the computer generated questions, and 70% correctly identified the control SweSAT questions. However, due to the low number of candidates partaking in the survey, the results were inconclusive.</p>
----------------------------------------------------------------------
In diva2:1332039 merged words and paragraphs, the abstract should be:

<p>Voltage Impasse Region (VIR) is a phenomenon in power systems whose dynamics are describe by a set of Differential Algebraic Equations (DAE). VIR denotes a state-space area where voltage causality is lost, i.e. the Jacobian of the algebraic part of DAE is singular. In a Time Domain Simulation (TDS) once system trajectories enter VIR, TDS experiences non-convergence of the solution. Then, there is no reason to continue with the simulation. This is why it is important to understand the mechanisms that introduce VIR. It is known that VIR appears in relation to static, non-linear load models. However, it remained unknown what the cumulative effect of several static, non-linear loads would be.</p><p>This master thesis has further expanded the concept of VIR by carrying out a structured study on how the load modelling affects VIR. For this purpose, this thesis proposes a quasi-dynamicm ethodology to map VIR in the relative rotor angle space. The methodology introduces a new discrete index called Voltage Impasse Region Flag (VIR<sub>flag</sub>), which allows to determine if the algebraic equations of DAE are solvable or not and, thus, to locate VIR. A test system is used to test the proposed quasi-dynamic approach.The VIR<sub>flag</sub> was first used to map VIR for various load combinations. Then, the relationship between TDS non-convergence issues and the intersection of a trajectory with VIR is examined to verify the proposed methodology.</p><p>The proposed method has been proved to be efficient in the determination of VIR regardless of the number of non-linear loads in the power system. Among the static exponential load models, the Constant Power (CP) load component has been identified as the one with the largest influence on VIR appearance and shape. The Constant Current (CC) loads induce ”smaller" VIR areas and the Constant Impedance (CI) load can only alter the shape of VIR in the presence of non-linear load models.</p>

Similar changes needed for Swedish abstract:

<p>VIR (Voltage Impasse Regions) är ett fenomen i kraftsystem vars dynamiska förlöp beskrivs av differential-algebraiska ekvationer (DAE). VIR betecknar ett område i tillståndsrummet där går förlorad,dvs Jakobianen av den algebraiska delen av DAE är singulärI tidsdomän-simuleringar (TDS) när en trajektoria träffar VIR, konvergerar TDS inte till en lösning. Då finns ingen anledning att fortsätta med simuleringen. Därför är det viktigt att förstå mekanismerna som introducerar VIR. Det är känt att VIR är relaterade till statiska, icke-linjära lastmodeller. Det var dock okänt vad den kumulativa effekten av flera statiska, icke-linjära belastningar skulle vara.</p><p>Denna uppsats har vidareutvecklat begreppet VIR genom att genomföra en strukturerad studie om hur lastmodellering påverkar VIR. För detta ändamål föreslår denna avhandling en kvasidynamiskmetod för att kartlägga VIR i det relativa rotorvinkelrummet. Metoden introducerar ett nytt diskret index som heter Voltage Impasse Region Flag (VIR<sub>flag</sub>), vilket gör det möjligt att bestämma om den algebraiska delen av DAE är lösbar eller inte och därmed lokalisera VIR. Ett används för att testa det föreslagna kvasi-dynamiska tillvägagångssättet. VIR<sub>flag</sub> användes först för att kartlägga VIR för olika belastningskombinationer. Därefter granskas förhållandet mellan konvergensproblem i TDS och korsningen mellan en trajektoria och VIR för att verifiera den föreslagna metoden.</p><p>Den föreslagna metoden har visat sig vara effektiv vid bestämning av VIR, oberoende av antalet icke-linjära belastningar. Bland de statiska exponentiella belastningsmodellerna har konstanteffekt last(CP) har identifierats som den som har störst inflytande på VIR;s form. Den konstantströmlasten (CC) inducerar mindre "VIR-områden och konstantimpedanslasten (CI) kan endast ändra formen av VIR i närvaro av icke-linjära belastningsmodeller.</p>
----------------------------------------------------------------------
In diva2:820944
   'questionnarie'
should be:
   'questionnaire'

......................................................................
In diva2:958222 merged parapgraphs and missing <li>, the abstract should be:

<p>The older generation now have plenty of time for daily use of their computers. (1) Nordicom´s Mediabarometer from March 2014 shows that among daily users of the Internet, the age group 65-79 look for more information and digital companionship. On the average day in 2014 they spend 101 minutes on the Net, to be compared with 69 minutes in 2011, an increase of 68 %. Statistics available on the web in 2011 showed the 2009 figures relating to the interest of older adults in using (2) Facebook. Only 4 % of the Swedish users were in the interest group 65 plus, and there was no maximum age.</p><p>The study intends to find out how potential Facebook users over 65 on sociala media perceive and use Facebook.<ol>
<li>How, in wich way, does the older generation use Facebook?</li>
<li>With whom and why?</li>
<li>How use they their social interaction?</li>
<li>Covers Facebook user´s daily needs for information and communication on social media?</li>
</ol></p>
<p>To have my questions about Facebook answered, I had a questionnare compiled and laid it out on the Web, hoping for replies from potential users. SeniorNet in Sweden is an association of about 50 clubs across the country with 8 500 members. Through contact with SeniorNet in Stockholm area, who mediated my link to their area members, I received answers to my questions. The link to the questionnarie was available for about a month. Of the 90 induviduals who responded, 60 already used Facebook and 30 were not interested in using Facebook. Of the respondents most were woman; 58 woman (64 %) and 32 men (36 %) between 65 and 90. The most frequent use was watching on pictures and status resports of children and grandchildren, and also chatting with old friends and the family. Being able to get quick information about close relatives abroad in the event of natural disasters and not having to worry about them was very important. Being able to follow development of family members with pictures and status reports for those who were living abroad was also important. Availability and having quick answers from Facebook users was the most positive aspect, according to the study.</p>

If one is to correct the spelling errors in the original, then:
   'questionnarie'
should be:
   'questionnaire'

   'sociala'
should be:
   'social'

   'wich'
should be:
   'which'

----------------------------------------------------------------------
In diva2:1142713 many missing ligatures, missing subscripts, and merged words, the abstract should be:

<p>Huge-scale optimization problems appear in several applications ranging from machine learning over large data sets to distributed model predictive control. Classical optimization algorithms struggle to handle these large-scale computations, and recently, a number of randomized first-order methods that are simple to implement and have small per-iteration cost have been proposed. However, optimal step size selections and corresponding convergence rates of many randomized first-order methods were still unknown. In this thesis, we hence derive convergence rate results for several randomized first-order methods for convex and strongly convex optimization problem, both with and without convex constraints. Furthermore, we have implemented these randomized first-order methods in MATLAB and evaluated their performance on l<sub>2</sub>-regularized least-squares support vector machine (SVM) classication problems. In addition, we have implemented randomized first-order projection methods for constrained convex optimization, derived associated convergence rate bounds, and evaluate the methods on l<sub>2</sub>-regularized least-squares SVM classication problems with Euclidean ball constraints of the weight vector. Based on the implementation experience, we finally discuss how data scaling/normalization and conditioning affect the convergence rates of randomized first-order methods.</p>
----------------------------------------------------------------------
In diva2:806746 the title is misssing a space:
    Environment for Industrial RobotPerformance Evaluation
should be:
    Environment for Industrial Robot Performance Evaluation

The abstract has merged words and paragraphs, the abstract should be:

<p>Industrial robots and robotic systems have already become key components in various industry sectors. After a new industrial robot model is developed, robot manufactures have the responsibility to test it before the new model hits market. In this thesis, several technologies regarding industrial robot test are analysed, and a test environment for industrial robots is developed.</p><p>ISO 9283 is the latest international standard for industrial robot tests. Robot test processes and test principles are described in this thesis. Questions and doubts about the standard are discussed. Laser tracker is one of the most advanced measuring instruments for industrial robot tests. The accuracy of laser tracker is essentially important for test results. Accuracy of laser trackers is analyzed in this thesis. Coordinate system calibration problem is discussed and solutions are shown. Two calibration methods are applied in this thesis for connecting different coordinate systems. Results of different calibration methods are compared. In data analysis, an accuracy verification test of the laser tracker is carried out. A data selection strategy is introduced by analysing raw data sequences.</p><p>To verify functionalities of the test environment, an industrial robot is tested with the test environment. Pose accuracy and pose repeatability test results from the test environment are shown and analysed. Result error sources are discussed. The example test shows that the test environment is able to provide scientific results, which reflect robot performance characteristics.</p>
----------------------------------------------------------------------
In diva2:1078389 mered words, the abstract should be:

<p>FPGA based systems have been heavily used to prototype and test Application Specic Integrated Circuit (ASIC) designs with much lower costs and development time compared to hardwired prototypes. In recent years, thanks to both the latest technology nodes and a change in the architecture of reconfigurable integrated circuits (from traditional Complex Programmable Logic Device (CPLD) to full-CMOS FPGA), FPGAs have become more popular in embedded systems, both as main computation resources and as hardware accelerators. A new era is beginning for FPGA based systems: the partial run-time reconguration of a FPGA is a feature now available in products already on the market and hardware designers and software developers have to exploit this capability. Previous works show that, when designed properly, a system can improve both its power efficiency and its performance taking advantage of a partial run-time reconfigurable architecture. Unfortunately, taking advantage of run-time reconfigurable hardware is very challenging and there are several problems to face: the reconfiguration overhead is not negligible compared to nowadays CPUs performance, the reconfiguration time is not easily predictable, and the software has to be re-though to work with a time-evolving platform.</p><p>This thesis project aims to investigate the performance of a modern run-time reconfigurable SoC (a Xilinx Zynq 7020), focusing on the reconfiguration overhead and its predictability, on the achievable speedup, and the trade-off and limits of this kind of platform. Since it is not always obvious when an application (especially a real-time one) is really able to use at its own advantage a partial run-time reconfigurable platform, the data collected during this project could be a valid help for hardware designers that use reconfigurable computing.</p>

Note that the manuscript has
    're-though'
which should have been
    're-thought'
----------------------------------------------------------------------
In diva2:1779224 merged paragraphs, the abstract should be:

<p>Today, face recognition is becoming increasingly accurate and faster with deep learning methods such as convolutional neural networks (CNNs), and is now widely used in areas such as security and entertainment. Typically, these CNNs are trained using real-face datasets like CASIA-WebFace, which was put together using web-crawling of IMDB. This can, however, lead to privacy and bias issues. Synthetic datasets made up of computer generated pictures, such as DigiFace-1M, created by Microsoft and the University of Cambridge, provide alternatives, offering large volumes of unbiased training data that respect privacy.</p><p>Despite these advances, there’s been little research comparing the performance of models pre-trained on synthetic datasets versus traditional ones. In this study, we address this gap. We tested a ResNet-18 model pre-trained on a subset of real-face images from CASIA-WebFace against one trained on a subset of the synthetic DigiFace-1M dataset. We also compared these results to a model pre-trained on ImageNet, a large, general-purpose, mixed object dataset, and a model without any pre-training. The models were later evaluated on the first 100 identities in CASIA-WebFace.</p><p>The findings showed that while the best performance came from pre-training on real-face datasets, the synthetic dataset also offered a viable option for multiclass face recognition. The synthetic dataset showed slightly better performance than ImageNet and significantly better performance than the model without pre-training, all while avoiding privacy issues linked to web-crawled images. More research is needed to further explore whether classification models pre-trained on larger synthetic datasets like DigiFace-1M can significantly outperform broader datasets like ImageNet, or even improve upon real-face pre-training.</p>
----------------------------------------------------------------------
In diva2:1067587
    'real-time-capability'
should be:
    'real-time capability'

It is correct in the original abstract.
----------------------------------------------------------------------
In diva2:1193593 merged paragraphs and words, the abstract should be:

<p>Over the last years, the growth and development of video on demand (VOD) services has given new possibilities of performing machine learning on large amounts of video history data. A common usage of machine learning for businesses is market segmentation, which is usually addressed with cluster analysis.</p><p>Market segmentation with cluster analysis has been performed for the video streaming service company Viaplay. It was found that K-means with cosine measure performed best of the attempted methods and has been shown to facilitate a useful and interpretable market segmentation based on a set of segment criteria: <em>understandability</em>, <em>homogeneity independence</em>, <em>stability</em> and <em>actionability<em>.</p><p>The thesis also shows an example of how to evaluate clustering of video streaming users. A version of term frequency-inverse document frequency (tf-idf) was introduced, which is called video importance score (VIS). VIS is used to find videos specifically important to a cluster, and has proven to be helpful in interpreting the resulting clusters.</p><p>The results were evaluated within a commonly used market segmentation evaluation framework, which was adapted to the problem at hand. Although the market segmentation strongly indicates to be useful, it still has to be in real-word scenario evaluated by the company before any definitive conclusions can be drawn.</p>

Note that 'real-word' should be 'real-world', but 'real-word' appears in the original abstract.
----------------------------------------------------------------------
In diva2:1039132 and diva2:1597929
Note that 'real-word' should be 'real-world', but 'real-word' appears in the original abstract.
----------------------------------------------------------------------
In diva2:1108935 merged paragraphs, the abstract should be:

<p>In this thesis, the correlation of the code metrics Cyclomatic Complexity, Number of Methods in Class and Depth of Inheritance Tree with the real reasons a codebase got heavily refactored is investigated. Code metrics have long been used in a attempts to indicate the amount of effort needed to change a codebase when new requirements emerge. In the case study we look at a codebase that has been refactored due to the low maintainability, to see to which degree the code metrics imply the problems on which the decision to refactor was based. The goal is to find out whether code metrics can be used as indicators of what should be refactored when you as a consultant join a project with existing quality problems.</p><p>The method used to identify the major problems in the codebase is qualitative interviews and the code metrics values are measured with the tool NDepend. To improve the reproducability and validity of the analysis, the interviews are built around the definition of maintainability presented in ISO/IEC 25010.</p><p>The conclusion is that metrics showing very high values for certain classes or methods often indicate objects involved in major problems in the codebase. In this case study though, the measures seldom indicated objects for the ”right reason”, by which we mean the reason given in the literature. Because of this, the code metric values should not be used as a direct indication of what should be refactored but rather as a direction in which to look while doing a professional assessment. The results are not entirely in line with previous research, where common correlations between code metrics values and maintainability problems have been compiled. This is assumed to be due to the high degree of loose coupling in the studied code.</p>

Note that 'reproducability' should be 'reproducibility', but 'reproducability' has been used the the original abstract.
----------------------------------------------------------------------
In diva2:1259247 merged paragraphs, the abstract should be:

<p>CERN Scalable Analytics Section currently offers shared YARN clusters to its users as monitoring, security and experiment operations. YARN clusters with data in HDFS are difficult to provision, complex to manage and resize. This imposes new data and operational challenges to satisfy future physics data processing requirements. As of 2018, there were over 250 PB of physics data stored in CERN’s mass storage called EOS. Hadoop-XRootD Connector allows to read over network data stored in CERN EOS. CERN’s on-premise private cloud based on OpenStack allows to provision on-demand compute resources. Emergence of technologies as Containers-as-a-Service in Openstack Magnum and support for Kubernetes as native resource scheduler for Apache Spark, give opportunity to increase workflow reproducability on different compute infrastructures with use of containers, reduce operational effort of maintaining computing cluster and increase resource utilization via cloud elastic resource provisioning. This trades-off the operational features with datalocality known from traditional systems as Spark/YARN with data in HDFS.</p><zp>In the proposed architecture of cloud-managed Spark/Kubernetes with data stored in external storage systems as EOS, Ceph S3 or Kafka, physicists and other CERN communities can on-demand spawn and resize Spark/Kubernetes cluster, having fine-grained control of Spark Applications. This work focuses on Kubernetes CRD Operator for idiomatically defining and running Apache Spark applications on Kubernetes, with automated scheduling and on-failure resubmission of long-running applications. Spark Operator was introduced with design principle to allow Spark on Kubernetes to be easy to deploy, scale and maintain with similar usability of Spark/YARN.</p><zp>The analysis of concerns related to non-cluster local persistent storage and memory handling has been performed. The architecture scalability has been evaluated on the use case of sustained workload as physics data reduction, with files in ROOT format being stored in CERN mass-storage called EOS. The series of microbenchmarks has been performed to evaluate the architecture properties compared to state-of-the-art Spark/YARN cluster at CERN. Finally, Spark on Kubernetes workload use-cases have been classified, and possible bottlenecks and requirements identified.</p>

Note that 'reproducability' should be 'reproducibility', but 'reproducability' has been used the the original abstract.
----------------------------------------------------------------------
In diva2:1612469 merged paragraphs, the abstract should be:

<p>The companies’ support phase, as all of business’ functional areas and components, went through a heavy and rapid digitalization which has unlocked the availability of an unprecedented amount of data. Unlike other relevant business areas and components, the support phase seems to have experienced fewer improvements attributable to Data Science and machine learning.</p><p>By focusing on two well-known problems of these two fields, Time Series Analysis and Regression Analysis, this project aims at understanding which techniques are applicable within the support phase and how these can improve the effectiveness and pro-activeness of this area. The goal within this project is to apply them to improve the handling of support tickets, the digital entity used to track issues and requests within support systems. Through the use of Time Series Analysis, we aim at forecasting the volume of tickets to be expected in a near-future time frame. Using Regression Analysis we intend to estimate the resolution time of a newly submitted ticket.</p><p>The results produced by the two tasks were satisfactory. On one hand, the Time Series task produced accurate results and the models could be directly employed and bring some added value to help Elvenite’s support team. On the other hand, while the Regression Analysis results were not as good, they nonetheless proved that the task’s aim is achievable through improvements on both the data used and the models applied. Finally, both tasks successfully showcased how to investigate and evaluate the application of such techniques within the support phase of a business. </p>
----------------------------------------------------------------------
In diva2:1421309 merged paragrpahs and unnecessary hyphens, the abstract should be:

<p>In accordance to the insertion of the regulation "(EU) nr 1227/2011"  (REMIT), responsibilities and duties for participants as well as a People Professionally Arranging Transactions (PPAT) comes to maintain a competitive and fair market.  After recent clarifications of the term PPAT, directives from the Agency for the Cooperation of Energy Regulators (ACER) specify that system operators with responsibilities over a balancing power market should also be considered as PPAT’s. Thus, Svenska kraftnät (Svk) has an obligation to introduce an weekly automated ex post monitoring of the balancing power market to identify potential violations.</p><p>The study proposes a method to detect potential violations on the weekly data. The method is divided into two steps where the first step contains a comparison of two approaches for a screening analysis and the second step is a regression analysis. The two approaches compared in the first step is one where the data are approximated to normal distribution and one non-parametric approach called bootstrap.  The purpose of the first step is to find anomalies in the weekly bids based on the price. The flagged bids are then used in a regression analysis where the actual bids are compared to predicted prices which are generated by using external variables.</p><p>It was concluded that the bidding data that was used could not be approx- imated to normal distribution. The results were more promising for the boot-strap approach in the screening analysis. Further, it was concluded that such a statistical analysis can be applied more efficiently on a market where the submitted bids reflect the companies bidding strategy, that is, by placing bids per regulation object (RO). The market design also had an impact on the regression analysis results.  The R2 value showed that the regression explained the price variations better on a market where the bids were connected to a RO.</p>
----------------------------------------------------------------------
In diva2:919024 unnecessary hyphens, the abstract should be:

<p>The aim of this thesis work is to improve the performance of an already existing information retrieval system that uses relevance feedback for performing query expansion. It is a constant goal to improve this system because the documents that are retrieved are a base for various data analysis tasks. It is therefore important that the precision and recall are high. A user can choose to give relevance feedback when executing a query, meaning the user can mark documents in the search result as relevant or irrelevant and redo the search based on this feedback. The original query will then be expanded based on the user’s feedback. The approach presented in this thesis uses the documents marked as relevant or irrelevant to train a classifier that can classify unknown documents from the search result as either relevant, irrelevant or unknown. The aim is to classify unknown documents and add them to the set of feedback documents that are used for the query expansion. The assumption that this thesis is based on is that the more feedback a user gives, the better the query expansion will perform. The system developed in this thesis is evaluated for the English language. The results in this thesis show that integrating the classifier in the existing system improved the performance in three out of four use cases. The existing system already has a good performance, but small improvements are important. It would therefore be beneficial to integrate it into the existing system. </p>
----------------------------------------------------------------------
In diva2:1247859 merged paragraphs and words, the abstract should be:

<p>The purpose of the thesis work is to investigate methods for closed loop control of hydraulic pressure in transmissions to make them be more precise. This is desirable since it decreases the fuel consumption as well as emissions, and improves the driving performance.</p><p>To be able to study the behaviour of the transmission, a Simulink model is designed with the parts relevant to the problem, and from this a linear model is obtained. Three different controllers are designed and implemented in the Simulink model, to compare and analyze different solutions. The controllers implemented are a PI controller, a PID controller and a LQR controller.</p><p>The results from the simulation with the different controllers show step responses to be able to evaluate their individual performance. The results show that all of the controllers meet the requirements for a step reponse under better conditions, but under worse ones the LQR controller performs best of the three. The LQR controller is therefore the most suitable of the three controllers for this particular problem.</p>
----------------------------------------------------------------------
In diva2:1247868 merged paragraphs and words, the abstract should be:

<p>The consumption has increased drastically over the years, where consumers have high demands on the quality of the products, the time it takes to receive the products and the personalization options. Factories try to scale with the consumers demands by removing human labour and deploying automation devices that can produce products more rapidly and with higher precision.</p><p>Wireless communication in the factories would help to achieve this goal, by enabling mobility as well as reducing cable reconfiguration/troubleshooting and increasing the utilization of the factories resources.</p><p>This report is investigating if it is possible to achieve beneficial wireless communication in a production line, where the evolved Node B scheduler can prioritize important cyclic Real-Time and alarm packets by using machine learning based classification models. This new prioritization technique would allow important factory applications to have high priority and it would make sure that important packets gets served. We found several useful application classification models for factory environments, but demonstrated that the best model may depend on the factory setup. Therefore, the report introduces as well the idea of automated deep learning model construction, which allows for model improvements by time.</p>
----------------------------------------------------------------------
In diva2:852897 merged words, the abstract should be:

<p>Healthcare practices are changing as focus shifts from treating acute illnesses to chronic diseases. The responsibility of managing the treatment has shifted from healthcare providers to the individual in a higher degree. To achieve good treatment the patients need to be empowered so that they understand their condition and can make informed choices throughout their self-care. A research through design approach was used to investigate how to design a personalized empowering application for heart failure patients. Aside from information relating to the condition, the themes of physical activity, dieting and social connectedness were identified as central to address for the empowerment of this group. Patients, partners and healthcare providers contributed with different perspectives throughout the design process. As a result five personas, representing potential users, were developed. Based on the personas and knowledge of the domain, user scenarios in current- and preferred state were constructed in order to guide the design of the empowering application called ‘The Heart Companion’. It is a tablet application catering to the different needs of the personas that also addresses the three themes relevant for empowerment. The purpose of the application is to facilitate better understanding, a feeling of safety and a more active empowered life for the patient. The application enables personalization of the content by providing bookmarking and addresses empowerment of physical activity by enabling various guided exercise sessions, personalized feedback, the possibility of reflection and construction of personalized exercise sessions.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-12
======================================================================
In diva2:1140173 merged paragraphs. merged words, and missing ligatures, the abstract should be:

<p>Learning to control an uncertain system is a problem with a plethora of applications in various engineering fields. In the majority of practical scenarios, one wishes that the learning process terminates quickly and does not violate safety limits on key variables. It is particularly appealing to learn the control policy directly from experiments, since this eliminates the need to first derive an accurate physical model of the system. The main challenge when using such an approach is to ensure safety constraints during the learning process.</p><p>This thesis investigates an approach to safe learning that relies on a partly known state-space model of the system and regards the unknown dynamics as an additive bounded disturbance. Based on an initial conservative disturbance estimate, a safe set and the corresponding safe control are calculated using a Hamilton-Jacobi-Isaacs reachability analysis. Within the computed safe set a variant of the celebrated Q-learning algorithm, which systematically explores the uncertain areas of the state space, is employed to learn a control policy. Whenever the system state hits the boundary of the safe set, a safety-preserving control is applied to bring the system back to safety. The initial disturbance range is updated on-line using Gaussian Process regression based on the measured data. This less conservative disturbance estimate is used to increase the size of the safe set. To the best of our knowledge, this thesis provides the first attempt towards combining these theoretical tools from reinforcement learning and reachability analysis to safe learning.</p><p>We evaluate our approach on an inverted pendulum system. The proposedvalgorithm manages to learn a policy that does not violate the pre-specied safety constraints. We observe that performance is signicantly improved when we incorporate systematic exploration to make sure that an optimal policy is learned everywhere in the safe set. Finally, we outline some promising directions for future research beyond the scope of this thesis.</p>
----------------------------------------------------------------------
In diva2:1884899 missing subscripts and superscripts, the abstract should be:

<p>The formation of hydrogen sulfide (H<sub>2</sub>S) in wastewater and sewage systems poses significant challenges to wastewater infrastructure and public health. A deeper understanding of H<sub>2</sub>S prediction could be beneficial for implementing preventive measures, ultimately reducing the impact of hydrogen sulfide. This paper compares the predictive capabilities of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models for predicting H<sub>2</sub>S levels using operational data from pump stations, including temperature, concentration, water age, and flow. Using statistical measures such as R<sup>2</sup> and various custom metrics, the study found that the CNN model outperformed the LSTM model. The CNN achieved an accuracy of 94.2%, compared to LSTM’s 89.8%, within a defined margin of error. While LSTM effectively captured peak characteristics, it struggled with generalization. Despite CNN’s superior performance, both models showed strong predictive capabilities based solely on operational data, without incorporating chemical features such as oxygen levels, nitrates, or pH. Additionally, the study examined strategic factors essential for integrating H<sub>2</sub>S predictions into enterprise offerings. The findings highlight considerations such as financial, capability, market strategy, and implementation risks.</p>
----------------------------------------------------------------------
In diva2:1524918 merged paragraphs and missing ligratures, the abstract should be:

<p>The space electronics sector is shifting towards the New-Space paradigm, in which traditional space-qualified and expensive components and payloads are replaced by commercial off-the-shelf (COTS) alternatives. This change in mentality is accompanied by the development of inexpensive cubesats, lowering the entry barrier in terms of cost, enabling an increase in scientific research in space. However, also well-established and resourceful spacecraft manufacturers are adopting this trend that allows them to become more competitive in the market.</p><p>Following this trend, Thales Alenia Space is developing R&amp;D activities using COTS components. One example is the SpaceFibre In-Orbit Demonstrator, a digital board integrated in a cubesat payload that aims to test two Intellectual Property blocks implementing the new ECSS standard for high-speed onboard communication.</p><p>This thesis presents the necessary steps that were taken to integrate the firmware for the demonstrator's Field-Programmable Gate Array (FPGA) that constitutes the main processing and control unit for the board. The activity is centered around the development of a Leon3 System-on-Chip in VHDL used to manage the components in the board and test the SpaceFibre technology.</p><p>Moreover, it also addresses the main problem of using COTS components in the space environment: their sensitivity to radiation, that, for a FPGA results in Single-Event Upsets causing the implementation to malfunction, and a potential failure of the mission if they are not addressed. To accomplish the task, a SEU-emulation methodology based in partial reconfiguration and integrating the state of the art techniques is elaborated and applied to test the reliability of the SpaceFibre technology.</p><p>Finally, results show that the mean time between failures of the SpaceFibre Intellectual Property Block using a COTS FPGA is of 170 days for Low Earth Orbit (LEO) and 2278 days for Geostationary Orbit (GEO) if configuration memory scrubbing is included in the design, enabling its usage in short LEO missions for data transmission. Moreover, tailored mitigation techniques based on the information gathered from applying the proposed methodology are presented to improve the figures. </p>
----------------------------------------------------------------------
In diva2:1548984 merged words, merged paragraphs, and some problems with hyphens, the abstract should be: 

<p>Reinforcement learning methods allows self-learning agents to play video- and board games autonomously. This project aims to study the efficiency of the reinforcement learning algorithms Q-learning and deep Q-learning for dynamical multi-agent problems. The goal is to train robots to optimally navigate through a warehouse without colliding.</p><p>A virtual environment was created, in which the learning algorithms were tested by simulating moving agents. The algorithms’ efficiency was evaluated by how fast the agents learned to perform predetermined tasks.</p><p>The results show that Q-learning excels in simple problems with few agents, quickly solving systems with two active agents. Deep Q-learning proved to be better suited for complex systems containing several agents, though cases of sub-optimal movement were still possible. Both algorithms showed great potential for their respective areas however improvements still need to be made for any real-world use.</p>
----------------------------------------------------------------------
In diva2:1635576 merged words and paragraphs, the abstract should be:

<p>Researchers within digital pathology are endeavoring to develop machine-learning tools to support dentists when making a diagnosis. The purpose of this study was to investigate how applying colour normalisation (CN) algorithms on an oral, histopathological dataset would impact both machine-learning models and ensembles of models when classifying cell types.</p><p>The dataset was run through four different CN algorithms by using a stain normalisation toolbox. The now five datasets (1 + 4) were then fed separately into a pipeline to create machine-learning models, specifically convolutional neural networks with EfficientNet architecture. Two different ensembles were studied, one that used all the models and one that used the three models with the highest test accuracy. Each model gave a cell type prediction of each cell. The ensembles super positioned their models’ predictions of the same cell and used the results as their own predictions.</p><p>The models based on datasets created by two of the CN algorithms had a weighted, average accuracy of ca. four percentage points higher than the model based on the unnormalised dataset. Unexpectedly, the models based on the colour-normalised datasets had a larger standard deviation than the model based on the unnormalised dataset. All the models were generally bad at classifying two of the four cell types. Both the ensembles had a weighted, average accuracy of ca. ten percentage points higher than the model based on the unnormalised dataset, as well as a larger standard deviation. The increase in accuracy is significant and could move forward the timeline for when machine-leaning tools can be implemented into dentists’ and pathologists’ workflow.</p>

Similarly for the Swedish abstract:
diva2:1635576: <p>Forskare inom digital patologi strävar efter att utveckla maskininlärnings-verktyg som stödjer tandläkarenär de ställer diagnoser. Syftet med denna studie är att utreda hur tillämpning av färgnormaliserande algoritmer (CN algoritmer) på ett oralt, histopatologiskt dataset påverkar hur både maskininlärningsmodeller och ensembler av modeller klassificerar celltyper.</p><p>Datasetet kördes igenom fyra olika CN algoritmer med hjälpav en färgnormaliserings-verktygslåda. De nu fem dataseten (1 + 4) matades separat in i en ”pipeline” för att skapa maskininlärningsmodeller, specifikt djupa neurala nätverk med EfficientNet arkitektur. Två olika ensembler skapades, en som använde alla modeller och en som endast använde de tre som hade högst noggrannhet på testsettet. Varje modell uppskattade celltypen för varje cell. Ensemblerna superpositionerade deras modellers uppskattningar för varje cell och använde resultaten som sina egna uppskattningar.</p><p>Modellerna som tränats på två av de färgnormaliseraden dataseten ökade i viktad, snitt-noggrannhet med fyra procentenheteri förhållande till modeller tränade på det ursprungliga datasetet. Förvånansvärt nog så ökade även standardavvikelsen hos modeller tränade på de färgnormaliserade dataseten. Alla modeller var generellt dåliga på att klassificera två av de fyra celltyperna. Ensemblen uppnådde en viktad snitt-noggrannhet på ca. tio procentenheter mer än modeller tränade på det ursprungliga datasetet. Noggrannhetens signifikanta ökning kan leda till en tidigare implementering av maskininlärnings-verktyg i tandläkares och patologers arbetsflöde.</p>
----------------------------------------------------------------------
In diva2:912685 merged words and paragraphs, as well as a missing subscript and incorrect handling of combining dieresis, the abstract should be:

<p>This thesis was aimed at studying the existing methods for origin-destination (OD) estimation problem and developing a new algorithm which provides higher promise.</p><p>The performance was evaluated on a simulated data-set for Stockholm city. Data for this study were obtained with the help of G. Flötteröd from Department of Transport Science in KTH. Information minimizing approach and entropy maximizing approach, which are the state-of-art methods in transport field were modified to implement. Several existing algorithms in signal processing field, such as BP/BPDN, OMP and SP, were implemented and analyzed. A recently proposed algorithm calld OMP + was described. Then a more effective method SP<sub>+</sub> with better reconstruction performance in sparse signal processing area was proposed in this report.</p><p>By numerical experiments, it was concluded that the methods in signal processing field could deal with OD estimation problem well. Hopefully this thesis could make a contribution to opening the door to another field and introducing methods of that universe, as well as developing a new algorithm with robust results and small computation cost.</p>
----------------------------------------------------------------------
In diva2:1142920 merged words, the abstract should be:

<p>Transmission of visual data can be quite demanding in terms of energy and bandwidth. Therefore, it is important that all the sensors in a Wireless Visual Sensor Network get a good signal quality for their transmissions when they come online for the first time. The purpose of this report is to design, simulate and evaluate an algorithm that the sensors can use to perform this initialization, i.e. bootstrapping. First the bootstrapping process is modeled as an NP-hard optimization problem. Then, by taking into account what information is directly available to a sensor, the algorithm is designed to be distributed in order to save energy and achieve scalability of the network. To avoid excessive computing times due to the NP-hardness of the problem, the algorithm is designed to approximate the theoretical solution. The algorithm is implemented and tested in a software simulation environment that was built as part of the project. The tests show that the algorithm performs close to optimal for small networks and retains a good approximation ratio for medium to large networks.</p>
----------------------------------------------------------------------
In diva2:1635585 merged words, the abstract should be:

<p>Many problems involving decision making with imperfect information can be modeled as extensive games. One family of state-of-the-art algorithms for computing optimal play in such games is Counterfactual Regret Minimization (CFR). The purpose of this paper is to explore the viability of CFR algorithms on the board game Stratego. We compare different algorithms within the family and evaluate the heuristic method “imperfect recall” for game abstraction. Our experiments show that the Monte-Carlo variant External CFR and use of game tree pruning greatly reduce training time. Further, we show that imperfect recall can reduce the memory requirements with only a minor drop in player performance. These results show that CFR is suitable for strategic decision making. However, solutions to the long computation time in high complexity games need to be explored.</p>

Similar for the Swedish abstract:

diva2:1635585: <p>Många beslutsproblem med dold information kan modelleras som spel på omfattande form. En familj av ledande algoritmer för att beräkna optimal strategi i sådana spelär Counterfactual Regret Minimization (CFR). Syftet med denna rapport är att undersöka effektiviteten för CFR-algoritmer i brädspelet Stratego. Vi jämför olika algoritmer inom familjen och utvärderar den heuristiska metoden “imperfekt minne” för spelabstraktion. Våra experiment visar att Monte-Carlo-varianten External CFR och användning av trimning av spelträd kraftigt minskar träningstiden. Vidare visar vi att imperfekt minne kan minska algoritmens lagringskrav med bara en mindre förlust i spelstyrka. Dessa resultat visar att CFR är lämplig för strategiskt beslutsfattande. Lösningar på den långa beräkningstiden i spel med hög komplexitet måste dock undersökas.</p>
----------------------------------------------------------------------
In diva2:714894 merged words, the abstract should be:

<p>With the increasing share of variable renewable generation, balancing electric power systems could become a major concern for system operators because of their variable and hardly predictable nature. However, gas technologies appear as a solution to provide this flexibility, but the impacts on the gas power system have hardly been investigated.</p><p>In this thesis, consulting reports on the subject matter, regulator suggestions and gas-electricity interaction models in scientific literature are studied and four sources are identified to be used for balancing: linepack, storage facilities, liquefied natural gas and intraday gas supply from adjacent areas. Then, a gas-electricity model for flexibility supply is designed and three case studies are simulated in order to analyze both gas and electric power systems’ behaviors. In these case studies, electricity generation, contribution of gas sources and costs are analysed.</p><p>The study concludes that critical situations on gas market that can occur, e.g. in cases of large variation in the net electricity demand and limited availability of linepack and storage facilities, the need of intraday modulation can exceed the possibilities to provide for it. Then, gas cannot be supplied to power plants during peak periods, and more gas than necessary is used during off-peak periods. The case studies also show that day-ahead forecast errors in variable renewable generation can be handled much easier than variations by the gas system but leads to higher costs.</p>
----------------------------------------------------------------------
In diva2:713473 merged words and missing ligatures, the abstract should be:

<p>Performance of many P2P systems depends on the ability to construct a random overlay network among the nodes. Current state-of-the-art techniques for constructing random overlays have an implicit requirement that any two nodes in the system should always be able to communicate and establish a link between them. However, this is not the case in some of the environments where distributed systems are required to be deployed, e.g, Decentralized Online Social Networks, Wireless networks, or networks with limited connectivity because of NATs/firewalls, etc. In such restricted networks, every node is able to communicate with only a predefined set of nodes and thus, the existing solutions for constructing random overlays are not applicable. In this thesis we propose a gossip based peer sampling service capable of running on top of such restricted networks and producing an on-the-fly random overlay. The service provides every participating node with a set of uniform random nodes from the network, as well as efficient routing paths for reaching those nodes via the restricted network. We perform extensive experiments on four real-world networks and show that the resulting overlays rapidly converge to random overlays. The results also exhibit that the constructed random overlays have self healing behaviour under churn and catastrophic failures.</p>

Note the "e.g," is an error in the original abstract.
----------------------------------------------------------------------
In diva2:1821993 merged words, the abstract should be:

<p>As the world moves away from fossil fuels, microgrids have garnered attention for their ability to aid the deployment of sustainable energy alternatives. Residential microgrid technology has proven to be a useful aid in the design of the power grid of the future. As society moves towards more renewable energy, more sophisticated energy solutions are required. This paper proposes a modeling approach for a residential hybrid AC/DC microgrid using Matlab and Simulink. The system consists of a photovoltaic (PV) system for DC generation, battery storage, and the potential for grid connection to enable reliable and efficient power. The paper discusses the modeling process and performance during various operating conditions. Simulation results showthe system's performance in terms of power generation, and storage as well as the impact of factors such as solar irradiation levels. The proposed model could be a valuable tool for further study and optimization for residential hybrid AC/DC microgrid systems.</p>
----------------------------------------------------------------------
In diva2:1248299 merged words and unnecessary hyphens, the abstract should be:

<p>This thesis attempts to model homeostatic regulation, a behavioural phenomenon ubiquitous in animals, in the domain of reinforcement learning. We specifically look at multi-objective reinforcement learning that can facilitate multi-variate regulation. When multiple objectives are to be handled, the current framework of Multi-objective Reinforcement Learning proves to be unsuitable without information on some preference over the objectives. We therefore model homeostatic regulation as a motivational process, that selectively activates some objectives over others, and implements cognitive control. In doing so, we utilize cognitive control not as behavioural principle, but as a control mechanism that arises as a natural necessity for homeostatic regulation.</p><p>We utilize a recent framework for drive reduction theory of reinforcement learning, and attempt to provide a normative account of arbitration of objectives from drives. We show that a purely reactive agent can face difficulties in achieving this regulation, and would require a persistence-flexibility mechanism. This could be handled effectively in our model by incorporating a progress metric. We attempt to build this model with the intention of acting as a natural extension to the current reinforcement learning framework, while also showing appropriate behavioural properties.</p>
----------------------------------------------------------------------
In diva2:1570300 merged words, the abstract should be:

<p>The strategy game Risk is a very popular board game, requiring little effort to learn but lots of skill to master. The aim of this project is to explore the fortification phase of the game, where the player’s troops are moved between territories. Our method is based on adapting Monte Carlo tree search (MCTS) to Risk. To improve the troop movements, we propose two techniques, hierarchical search and progressive bias. These methods, combined with other extensions of MCTS are then compared against a baseline player of the game. Our results show that hierarchical search improved the MCTS agent’s playing power and the progressive bias have potential to improve the agent but needs further investigation.</p>
----------------------------------------------------------------------
In diva2:1112609 merged words and paragraphs as well as a missing ligature, the abstract should be:

<p>With the rapid growth of data volume, data storage has attracted more and more research interests in recent years. Distributed storage systems play important roles of meeting the demand for data storage in large amounts. That is, data are stored by multiple storage nodes which are connected together with various network topologies. The main merits of such distributed storage are faster response, higher reliability and better scalability. However, due to network failure, link outage or buffer overflow, the updated data might not be received by all storage nodes, resulting in the coexistence of multiple versions of the file in the system. Thus, the major challenge is consistency, which means that the latest version of the file is accessible to any read request. We aim to study multi-version storage and code design in distributed storage systems, where the latest version of the file or a version close to the latest version is recoverable. Moreover, compared to previous studies, higher availability can be achieved in our system model, namely, at least one version of the file can be obtained.</p><p>On the other hand, both storage nodes and links are vulnerable to fail in storage systems. For the sake of reliability demand, the lost data is supposed to be reconstructed. In this thesis, additional storage nodes dedicated to repair (DR storage nodes) are introduced in the repair process. The results show that optimal repair bandwidth with minimal additional storage space can be achieved by introducing a certain number of DR storage nodes. Subsequently, linear combinations are provided to reduce the communication cost of repair where the link cost is high. Last but not the least, we show that the cooperation among surviving nodes and DR storage nodes suffices to complete the repair process successfully even with link failure.</p>
----------------------------------------------------------------------
In diva2:1587035 merged paragraphs and merged words, the abstract should be:

<p>Object detection is a research area within computer vision that consists of both localising and classifying objects in images. The applications of this kind of research in society are many, ranging from facial recognition to self driving cars. Some of these use cases requires the detection of objects in motion and are therefore considered to be in a separate category of object detection, commonly referred to as real time object detection.</p><p>The goal of this thesis is to shed further light on the area of real time object detection by investigating the effectiveness of successful object detection techniques when applied to objects of smaller sizes. More specifically, the task of detecting small objects is described by the community as a difficult problem. This is also an area that has not been extensively researched before and the results could thus be used by the research community at large and/or for real life applications. This paper is a comparative study between the effectiveness of two different deep learning techniques within real time object detection, namely RetinaNet and YOLOv3. The objects used are small characters and digits that are engraved onto ball bearings. Ball bearings have been photographed while traveling on a production line, and a collection of such images are what constitutes the dataset used in this study. The goal is to classify as many characters and digits as possible on each bearing, with as low inference time as possible.</p><p>The two deep learning models were implemented and then evaluated on their performance, measured in terms of precision and average inference time. The evaluation was performed on labeled bearings not previously seen by the two models.</p><p>The results show that RetinaNet vastly outperforms YOLOv3 when it comes to real-time object detection of small objects in terms of mAP@50. However, when it comes to average inference time YOLOv3 performed twice as fast as RetinaNet. In conclusion it can be noted that YOLOv3 struggles when it comes to smaller objects whereas RetinaNet excels in this area. It can also be concluded, from previous research, that an increase in mAP and average inference time is most likely limited by the hardware used during training. The verification of this could be a potential further investigation of this thesis </p>
----------------------------------------------------------------------
In diva2:1632729 merged words and a missing hyphen, the abstract should be:

<p>In recent years, the usage of 3D deep learning techniques has seen a surge, mainly driven by advancements in autonomous driving and medical applications. This thesis investigates the applicability of existing state-of-the-art 3D deep learning network architectures to dense voxel grids from single photon counting 3D LiDAR. This work also examine the choice of loss function as a means of dealing with extreme data imbalance, in order to segment people and vehicles in outdoor forest scenes. Due to data similarities with volumetric medical data, such as computer tomography scans, this thesis investigates if a model for 3D deep learning used for medical applications, the commonly used 3D U-Net, can be used for photon counting data. The results show that segmentation of people and vehicles is possible in this type of data but that performance depends on the segmentation task, light conditions, and the loss function. For people segmentation the final models are able to predict all targets, but with a significant amount of false positives, something that is likely caused by similar LiDAR responses between people and tree trunks. For vehicle detection, the results are more inconsistent and varies greatly between different loss functions as well as the position and orientation of the vehicles. Overall, we consider the 3D U-Net model a successful proof-of-concept regarding the applicability of 3D deep learning techniques to this kind of data.</p>
----------------------------------------------------------------------
In diva2:1142974 merged words, the abstract should be:

<p>Wave power is not an energy source taken advantage of partly due to the lack of effective generators for slow speeds. There is an ongoing project at the Royal Institute of Technology in Sweden where a transversive flux machine specialised for slow speeds is being developed. This paper aims to design the core of this machine to achieve high efficiency and low cost. The basic design is presented along with the approach to the different aspects of the manufacturing. After examining possible losses these have been by passed or minimised using various methods. If this is not done properly, the losses will be too severe forthe machine to prove useful. A study of the results show that a very high efficiency will be achieved, way superior to that of currently existing wave power generators.</p>
----------------------------------------------------------------------
In diva2:1601440 merged words, the abstract should be:

<p>A denoising autoencoder is a type of neural network which excels at removing noise from noisy input data. In this project, a denoising autoencoder is optimized for removing noise from mobile positioning data. The mobile positioning data with noise is generated specifically for this project. In order to generate realistic noise, a study in how real world noise looks like is carried out. The project aims to answer the question: can a denoising autoencoder be used to remove noise from mobile positioning data? The results show that using this method can effectively cut the noise in half. In this report it is mainly analyzed how the amount of hidden layers and respective sizes affected the performance. It was concluded that the most optimal design forthe autoencoder was a single hidden layer model with multiple more nodes in the hidden layer than the input and output layer.</p>
----------------------------------------------------------------------
In diva2:1470639  'single-viewDI'
should be:
'single-view DI'
----------------------------------------------------------------------
In diva2:1871789  'singlestain'
should be:
'single-stain'
----------------------------------------------------------------------
In diva2:874367 merged paragraphs and words, the abstract should be:

<p>Articulated Funiculator is a new and innovative concept developed by Tyréns for achieving a more efficient vertical transportation with a higher space utilization. Having a variety of merits, i.e.: simple construction, direct electromagnetic thrust propulsion, and high safety and reliability in contrast to rotary induction motor, linear induction motor (LIM) is considered to be one of the cases as the propulsion system for Articulated Funiculator. The thesis is then carried out with the purpose of determining the feasibility of this study case by designing the LIMs meeting some specific requirements. The detailed requirements include: a set of identical LIMs are required to jointly produce the thrust that is sufficient to vertically raise the moving system up to 2 m/s<sup>2</sup>; the size of the LIMs cannot exceed the specification of the funiculator; the maximum flux density in the air gap for each LIM is kept slightly below 0.6 T; no iron saturation of any part of the LIMs is allowed.</p><p>In this thesis report, an introduction of LIM is firstly presented. Following the introduction, relevant literature has been reviewed for a strengthened theoretical fundamentals and a better understanding of LIM’s history and applications. A general classification of LIMs is subsequently introduced. In addtion, an analytical model of the single-sided linear induction motor (SLIM) has been built based on an approximate equivalent circuit, and the preliminary geometry of the SLIM is thereby obtained. In order to acquire a more comprehensive understanding of the machine characteristics and a more precise SLIM design, a two-dimensional finite element method (2D-FEM) analysis is performed initially according to the preliminary geometry. The results, unfortunately, turn out to be iron severely saturated in the teeth and yoke, and a excessive maximum value of air-gap flux density. Specific to the problems, different parameters of the SLIM are marginally adjusted and a series of design scenarios are run in Flux2D for 8-pole and 6-pole SLIM. The comparisons between the results are conducted and the final solution is lastly chosen among them.</p>
----------------------------------------------------------------------
In diva2:1458904 unnecessary "- ", the abstract should be:

<p>Contract management is a sensitive and crucial task for companies. Contracts and other legal documents have to be fully searched and understood to define the obligations of the companies towards other actors in the company business. It is common for such companies to hire lawyers to analyze their contracts for them. These lawyers spend tremendous amounts of time doing tedious and repetitive tasks in order to extract all of the relevant information from the documents such as relationships between different entities. To address these issues, Hyperlex offers a software solution based on Natural Language Processing (NLP).</p><p>Relation Extraction is an active area in NLP research which aims to extract and classify the relationship between two entities. Legal documents contain many such entities and relationships which are specific to the judicial domain and provide capital information about obligation definitions. This master thesis proposes novel approaches to deal with these issues and improve Hyperlex’s relation extraction and classification tools.</p>
----------------------------------------------------------------------
In diva2:1214412 unnecessary "- ", the abstract should be:

<p>Sentiment analysis is the process of letting a computer guess the sentiment of someone towards something based on a text. This can among other things be useful in marketing, for example in the case of the computer figuring out that a certain person likes a certain product it can present ads for similar products to the person. Sentiment analysis in social media is when the texts analyzed are from a social media context like comments or posts on Twitter, Facebook, etc. One problematic aspect of these texts is sarcasm. People tend to be sarcastic very often in social media, with sarcasm being something that can be hard to detect even for a human this does cause problems for the computer. This study was conducted with the intention of investigating how sarcasm detection can be performed in social media texts with the help of machine learning. For this purpose Google’s machine learning framework for Python, TensorFlow, was utilized. The machine learning model created was a deep neural network with two hidden layers containing ten nodes each. As for the input a dataset of 4692 texts were used with a 80/20 training/testing split. For preprocessing the texts into a more suitable form for TensorFlow the methods Bag of Words, Bigrams and a naive method here refered to as Char for Char were considered. However due to time constraints proper results from the more advanced approaches (Bigrams and Bag of Words) were not achieved. It was at least found that the rather simple approach was better than expected, with results notably better than 50% that would be highly unlikely to achieve through sheer luck.</p>
----------------------------------------------------------------------
In diva2:1499098 unnecessary "- ", the abstract should be:

<p>One of the biggest challenges of the automotive industry at the moment is the idea of autonomous vehicles and the huge amount of data that they require due to the main technology they use, Deep Learning. Often, collecting enough data is very expensive and time-consuming, causing the industry to start adopting technologies such as Scenario Cloning, where previously recorded sequences are used to digitally reconstruct the scenario. At its time, within this field, one of the most relevant tasks is Simultaneous Localization and Mapping. This thesis presents a series of improvements based on Deep Learning that can be introduced in current feature-based Visual Simultaneous Localization and Mapping systems to overcome some of the most recurrent problems, such as dealing with highly dynamic environments. The main focus of the thesis is to take an existing state-of-the-art Visual Simultaneous Localization and Mapping method and combine it with Deep Learning-based semantic segmentation. The resulting system successfully avoids placing features on dynamic objects and other regions that tend to decrease the performance of the system, thus improving substantially the overall performance on dynamic environments. Additionally, the system uses the information provided by the Deep Learning model to assign semantic information to each of the points forming the sparse map, resulting in a more complete tool and opening the door for new opportunities in tasks such as obstacle avoidance or planning.</p>
----------------------------------------------------------------------
In diva2:1699284 missing superscripts, the abstract should be:

<p>Unique layered structure with excellent electrical, mechanical, thermal, and optical properties gives graphene widespread application. Graphene based materials are extensively studied in the field of energy storage such as batteries, hydrogen storage and supercapacitors (SC’s). High surface area, electrical conductivity and mechanical flexibility are notable properties for the materials used in energy conversion systems. Porous spaced graphene oxide (PGO) structures were synthesized by hydrothermal and solvothermal reaction between GO and various pillaring molecules include Tetrakis (4-aminophenyl) methane (TKAm), Ethylenediamine (EDA), 2-Amino-5-diethylaminopentane (ADAP) and 2-Aminoethyl trimethylammonium chloride hydrochloride (ATA). Pristine GO shows interlayer distance of 7.2 Å. Characterisation techniques such as XRD, SEM, FTIR, BET and TGA were used understand the properties of these PGO. In contrast, these pillared structures show interlayer distance greater than of the pristine GO. Notably, GO/TKAm show interlayer distance of 14.30 Å. These pillared structures are considered to solve the restacking and aggregation issues found in 2D porous structures. Since these pillaring molecules help to achieve 3D porous network. Pristine GO shows only surface area of 14 m<sup>2</sup>/g whereas these materials also show excellent surface area as well. GO/TKAm shows high surface area of 450 m<sup>2</sup>/g. Followed it GO/ATA shows surface area of 106 m<sup>2</sup>/g. GO/pillared structures show low sheet resistance which means good electrical conductivity. Ultimately, these pillared structures not only solve the issues in 2D porous systems but also improve the surface area, mechanical stability, and electrical conductivity of those systems by means of 3D porous interconnected structures. All these excellent properties make them a great candidate for the energy conversion systems.</p>
----------------------------------------------------------------------
In diva2:742569
   'start-upsoperating'
should be:
   'start-ups operating'
----------------------------------------------------------------------
In diva2:1479310 merged paragraphs and words, the abstract should be:

<p>Massive multiple-input and multiple-output (MIMO) is a method to improve the performance of wireless communication systems by having a large number of antennas at both the transmitter and the receiver. In the fifth-generation (5G) mobile communication system, Massive MIMO is a key technology to face the increasing number of mobile users and satisfy user demands. At the same time, recovering the transmitted information in a massive MIMO uplink receiver requires more computational complexity when the number of transmitters increases. Indeed, the optimal maximum likelihood (ML) detector has a complexity exponentially increasing with the number of transmitters. Therefore, one of the main challenges in the field is to find the best sub-optimal MIMO detection algorithm according to the performance/complexity tradeoff. In this work, all the algorithms are empirically evaluated for large MIMO systems and higher-order modulations.</p><p>Firstly, we show how MIMO detection can be represented by a Markov Random Field (MRF) and addressed by the loopy belief propagation (LBP) algorithm to approximately solve the equivalent MAP (maximum a posteriori) inference problem. Then, we propose a novel algorithm (BP-MMSE) that starts from the minimum mean square error (MMSE) solution and updates the prior in each iteration with the LBP belief. To avoid the complexity of computing MMSE, we use Graph Neural Networks (GNNs) to learn a message-passing algorithm that solves the inference task on the same graph.</p><p>To further reduce the complexity of message-passing algorithms, we recall how in the large system limit, approximate message passing (AMP), a low complexity iterative algorithm, can be derived from LBP to solve MIMO detection for i.i.d. Gaussian channels. Then, we show numerically how AMP with damping (DAMP) can be robust to low/medium correlation among the channels. To conclude, we propose a low complexity deep neural iterative scheme (Pseudo-MMNet) for solving MIMO detection in the presence of highly correlated channels at the expense of online training for each channel realization. Pseudo-MMNet is based on MMNet algorithm presented in [24] (in turn based on AMP) and it significantly reduces the online training complexity that makes MMNet far from realistic implementations.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-16
======================================================================
In diva2:824967 merged words, the abstract should be:

<p>In this report we investigate the technique of stereoscopic 3D. This report investigates the steps needed to create a game adapted for an improved stereoscopic 3D effect. Furthermore we investigate what improvements one should make to avoid the beholder to experience any discomfort due to the effect. The report talks about technical aspects one needs to consider when using stereoscopic 3D, as well as performance issues we might need to take into consideration. The process of developing the prototype of the game Kodo using anaglyph stereoscopic 3D and OpenGL is described in this report. The prototype was then used for testing and analyzing the stereoscopic 3D effects.</p>
----------------------------------------------------------------------
In diva2:1548792 merged words, the abstract should be:

<p>Many machine learning approaches have been used for financial forecasting to estimate stock trends in the future. The focus of this project is to implement a Support Vector Machine with price and news analysis for companies within the technology sector as inputs to predict if the price of the stock is going to rise or fall in the coming days and to observe the impact on the prediction accuracy by adding news to the technical analysis. The price analysis is compiled of 9 different financial indicators used to indicate changes in price, and the news analysis uses the bag-of-words method to rate headlines as positive or negative. There is a slight indication of the news improving the results if the validation data is randomly sampled the testing accuracy increases. When testing on the last fifth of the data of each company, there was only a small difference in the results when adding news to the calculation and such no clear correlation can be seen. The resulting program has a mean and median testing accuracy over 50 % for almost all settings. Complications when using SVM for the purpose of price forecasting in the stock market is also discussed.</p>
----------------------------------------------------------------------
In diva2:753928 the title is missing a space:
"Mobile traffic dataset comparisons throughcluster analysis of radio network event sequences"
should be
"Mobile traffic dataset comparisons through cluster analysis of radio network event sequences"

----------------------------------------------------------------------
In diva2:1586062 the title is missing a space:
"Penetration Testinga Saia Unit: A Control System for Water, Ventilation, and Heating in Smart Buildings"
should be
"Penetration Testing a Saia Unit: A Control System for Water, Ventilation, and Heating in Smart Buildings"
----------------------------------------------------------------------
In diva2:677075 space missing in title:
"social´sschedule"
should be
"social´s schedule"
----------------------------------------------------------------------
In diva2:883329 unnecessary "- ", the abstract should be:

<p>In this thesis, the possibility to use event correlation techniques in the C2 monitor systems is analyzed. C2 is a monitoring software built by Ericsson Broadcast &amp; Media Services for their television broadcasting center in Stockholm. C2 enables the operators in playout to easily monitor events and alarms generated by devices responsible for producing each channel.</p><p>Previously archived events that are saved in the database was analyzed to achieve an understanding regarding what events that can occur within the system. C2 utilizes a compression correlation technique for events to limit the number of records in the database. This means however that information regarding the time aspect for each event often is lost which made the problematic to draw any conclusions from the data.</p><p>The most common correlations techniques today are compared and analyzed in comparison to the requirements set up for an implementation into C2. The dependency graph technique was chosen as the most promising technique. A prototype with a depth first search and a limit on depth level was then tested. The results indicate that this technique was not efficient enough to correlate all events in real time. The main reason for this is because C2 is built on Node.js, which is an single threaded framework. When events can occur within a few milliseconds between each other, extensive computations would make it impossible to handle new events quickly enough.</p><p>After discussions with operators a none real-time correlation implementation was done. This makes it possible for them to quickly find devices with corresponding events that are masked out and hidden from the current view. </p>
----------------------------------------------------------------------
In diva2:643020 an unnecessary "- " and an empty paragraph, the abstract should be:

<p>This report presents a layer-optimized streaming technique for delivering video content over the Internet using quality-scalable motion-compensated orthogonal video. We use Motion-Compensated Orthogonal Transforms (MCOT) to remove temporal and spatial redundancy. The resulting subbands are quantized and entropy coded by Embedded Block Coding with Optimized Truncations (EBCOT). Therefore, we are able to encode the input video into multiple quality layers with sequential decoding dependency. A layer-distortion model is constructed to measure the trade-off between expected streaming layer and expected distortion. Due to the sequential dependency among streaming layers, we build a cost function with concave properties. With that, we develop a fast algorithm to find the optimal transmission policy at low computational complexity. The experiments demonstrate the advantages of expected distortion and computational complexity for challenging streaming scenarios.</p>
----------------------------------------------------------------------
In diva2:1094877 unnecessary "-", the abstract should be:

<p>This Master's Thesis focuses on the recent Cortical Learning Algorithm (CLA), designed for temporal anomaly detection. It is here applied to the problem of anomaly detection in user behavior of web services, which is getting more and more important in a network security context.</p><p>CLA is here compared to more traditional state-of-the-art algorithms of anomaly detection: Hidden Markov Models (HMMs) and t-stide (an N-gram-based anomaly detector), which are among the few algorithms compatible with the online processing constraint of this problem.</p><p>It is observed that on the synthetic dataset used for this comparison, CLA performs significantly better than the other two algorithms in terms of precision of the detection. The two other algorithms don't seem to be able to handle this task at all. It appears that this anomaly detection problem (outlier detection in short sequences over a large alphabet) is considerably different from what has been extensively studied up to now.</p>
----------------------------------------------------------------------
In diva2:1548799 merged words and an unnecessary hyphen, the abstract should be:

<p>Using the powerful methods developed in the field of reinforcement learning requires an understanding of the advantages and drawbacks of different methods as well as the effects of the different adjustable parameters. This paper highlights the differences in performance and applicability between three different Q-learning methods: Q-table, deep Q-network and double deep Q-network where Q refers to the value assigned to a given state-action pair. The performance of these algorithms is evaluated on the two OpenAI gym environments MountainCar-v0 and CartPole-v0. The implementations are done in Python using the Tensorflow toolkit with Keras. The results show that the Q-table was the best to use in the Mountain car environment because it was the easiest to implement and was much faster to compute, but it was also shown that the network methods required far less training data. No significant difference in performance was found between the deep Q-network and the double deep Q-network. In the end, there is a trade-off between the number of episodes required and the computation time for each episode. The network parameters were also harder to tune since much more time was needed to compute and visualize the result.</p>

In the Swedish abstract "Q- network" should be "Q-network".
----------------------------------------------------------------------
In diva2:1129343 many missing ligatures, the abstract should be:

<p>The aim of this thesis was to systematically develop a tool used for the visualization of real-time flight data during suborbital mission, on behalf of the Swedish Space Corporation. By focusing on requirements, restrictions, and trade-offs regarding functionality, communications technologies, geospatial visualization platforms, and many other aspects, an interactive product visualizing the position, attitude, speed, G-loads and angular rates of the rockets could be developed, ensuring a visualization as close to real-time as possible. The resulting product was tested during the flight of the MAXUS 9 rocket, Europe's largest sounding rocket, and was subsequently refined based on the results of that flight.</p>
----------------------------------------------------------------------
In diva2:815221 missing ligratures and unnecessary paragraph divisions, the abstract should be:

<p>The aim of this thesis is to investigate the possibility to improve the separation of <em>HCl</em> and <em>SO<sub>2</sub></em> in the dry flue-gas treatment for boiler #3 at Fortum's thermal power plant in Hogdalen; by using a model predictive control instead of a PID controller to govern the slaked lime injection.</p><p>To achieve this an ARMAX model was derived using MATLAB's System Identification Toolbox and measurements of the incoming and outgoing levels of <em>HCl</em>, <em>SO<sub>2</sub></em> and the speed of the injection motor. The ARMAX model was then converted to a state space model which will be used as the internal model for the MPC predictions. The cost function was a quadratic problem which included the error between the output and the set points, the change rate of the input and the inputs deviation from a default value. The MPC uses both a feedforward and a feedback loop to estimate the error over the prediction horizon. The controller also utilizes the ability to set constraints and tuning of the cost function weights.</p><p>In conclusion, the thesis shows that a MPC controlled lime injection is possible and would offer some unique possibilities such as: natural constraints handling, more intuitive live tuning for the operator and prioritized input control. However the dry scrubber still struggles to suppress high amounts of incoming <em>SO<sub>2</sub></em> and since the project lacked a measuring unit for incoming <em>HCl<z/em> concentration the results showing an improvements in pollution separation was not conclusive.</p>
----------------------------------------------------------------------
In diva2:1046438 some unnecessary hyphen, the abstract should be:

<p>End-to-end encryption is becoming a standard feature in popular mobile chat applications (apps) with millions of users. In the two years a number of leading chat apps have added end-end encryption features including LINE, KakaoTalk, Viber, Facebook Messenger, and WhatsApp.</p><p>However, most of these apps are closed-source and there is little to no independent veriﬁcation of their end-to-end encryption system design. These implementations may be a major concern as proprietary chat apps may make use of non-standard cryptographic algorithms that may not follow cryptography and security best practices. In addition, governments authorities may force chat app providers to add easily decryptable export-grade cryptography to their products. Further, mainstream apps have a large attack surface as they oﬀer a variety of features. As a result, there may be software vulnerabilities that could be exploited by an attacker in order to compromise user’s end-to-end privacy. Another problem is that, despite being closed-source software, providers often market their apps as being so secure that even the provider is not able to decrypt messages. These marketing claims may be potentially misleading as most users do not have the technical knowledge to verify them.</p><p>In this Master’s thesis we use KakaoTalk – the most popular chat app in South Korea – as a case study to perform a security and privacy assessment and audit of its “Secure Chat” opt-in end-to-end encryption feature. Also, we examine KakaoTalk’s Terms of Service policies to verify claims such as “[. . . ] Kakao’s server is unable to decrypt the encryption [. . . ]” from a technical perspective.</p><p>The main goal of this work is to show how various issues in a product can add up to the potential for serious attack vectors against end-to-end privacy despite there being multiple layers of security. In particular, we show how a central public-key directory server makes the end-to-end encryption system vulnerable to well-known operator-site man-in-the-middle attacks. While this naive attack may seem obvious, we argue that (KakaoTalk) users should know about the strength and weaknesses of a particular design in order to make an informed decision whether to trust the security of a chat app or not.</p>

----------------------------------------------------------------------
*** correction there was also a space missing before "e.g."

In diva2:1368334 there was an "ff" ligature, a missing italics. The abstract should be:

 <p>In the last years, the demands on different models using deep learning to generate textual data conditionally have increased, where one would like to control what textual data to generate from a deep learning model. For this purpose, a couple of models have been developed and achieved state-of-art performance in the ﬁeld of generating textual data conditionally. Therefore, the purpose of this study was to develop a new model that could outperform the relevant baseline models with respect to the BLEU metric. The alternative model combined some of the properties from the state-of-art models and was given the name the <em>Variational Attribute-to-Sequence decoder model</em> (shortened to the V-Att2Seq model) that paraphrases the name of one of the state-of-art models and "variational" refers to its application of variational recurrent autoencoders (VRAE). The data set used in this study contained drug reviews that were written by patients to express their opinion about the drug that they have used to treat a certain condition. The drug review texts were accompanied by the following attributes: the (name of the) drug, the condition, and the rating that the patient has given to the drug. The results in this study show that the V-Att2Seq model did not outperform all the baseline models, which concluded that the V-Att2Seq model did not satisfy the requirements imposed on the model itself. However, there are some future work that is suggested by this study to hopefully improve the performance of the V-Att2Seq model in the future such as including other mechanisms that are present in the state-of-art models, testing with e.g. other sizes and settings of the V-Att2Seq model, and testing different strategies forgenerating sequences since there is still potential that has been observed in the model that should be further investigated to improve its performance.</p>
----------------------------------------------------------------------
In diva2:1381341 merged words and paragraphs, the abstract should be:

<p>Combined Cycle Power Plant (CCPP)s play a key role in modern power system due to their lesser investment cost, lower project execution time, and higher operational flexibility compared to other conventional generating assets. The nature of generation system is changing with ever increasing penetration of the renewable energy resources. What was once a clearly defined generation, transmission, and distribution flow is shifting towards fluctuating distribution generation. Because of variation in energy production from the renewable energy resources, CCPP are increasingly required to vary their load levels to keep balance between supply and demand within the system. CCPP are facing more number of start cycles. This induces more stress on the gas turbine and as a result, maintenance intervals are affected.</p><p>The aim of this master thesis project is to develop a dispatch algorithm for the short-term operation planning for a combined cycle power plant which also includes the long-term constraints. The long-term constraints govern the maintenance interval of the gas turbines. These long-term constraints are defined over number of Equivalent Operating Hours (EOH) and Equivalent Operating Cycles (EOC) for the Gas Turbine (GT) under consideration. CCPP is operating in the open electricity market. It consists of two SGT-800 GT and one SST-600 Steam Turbine (ST). The primary goal of this thesis is to maximize the overall profit of CCPP under consideration. The secondary goal of this thesis it to develop the meta models to estimate consumed EOH and EOC during the planning period.</p><p>Siemens Industrial Turbo-machinery AB (SIT AB) has installed sensors that collects the data from the GT. Machine learning techniques have been applied to sensor data from the plant to construct Input-Output (I/O) curves to estimate heat input and exhaust heat. Results show potential saving in the fuel consumption for the limit on Cumulative Equivalent Operating Hours (CEOH) and Cumulative Equivalent Operating Cycles (CEOC) for the planning period. However, it also highlighted some crucial areas of improvement before this economic dispatch algorithm can be commercialized.</p>
----------------------------------------------------------------------
In diva2:1249005 merged words and paragraphs, the abstract should be:

<p>This paper focuses on improving the accuracy of detecting on-road objects, including cars, trucks, pedestrians, and cyclists. To meet the requirements of the embedded vision system and maintain a high speed of detection in the advanced driving assistance system (ADAS) domain, the neural network model is designed based on single channel images as input from a monocular camera.</p><p>In the past few decades, forward collision avoidance system, a sub-system of ADAS, has been widely adopted in vehicular safety systems for its great contribution in reducing accidents. Deep neural networks, as the the-state-of-art object detection techniques, can be achieved in this embedded vision system with efficient computation on FPGA and high inference speed. Aimed at detecting on-road objects at a high accuracy, this paper applies an advanced end-to-end neural network, single-shot multi-box detector (SSD).</p><p>In this thesis work, several experiments are carried out on how to enhance the accuracy performance of SSD models with grayscale input. By adding proper extra default boxes in high-layer feature maps and adjust the entire scale range, the detection AP over all classes has been efficiently improved around 20%, with the mAP of SSD300 model increased from 45.1% to initially 76.8% and the mAPof SSD512 model increased from 58.5% to 78.8% on KITTI dataset. Besides, it has been verified that without color information, the model performance will not degrade in both speed and accuracy. Experimental results were evaluated using Nvidia Tesla P100 GPU on KITTI Vision Benchmark Suite, Udacity annotated dataset and a short video recorded on one street in Stockholm.</p>
----------------------------------------------------------------------
In diva2:1609020 an unnecessary hyphen, the abstract should be:

<p>This thesis investigates whether Computer Vision can be a useful tool in interpreting the behaviors of monitored horses. In recent years, research in the field of Computer Vision has primarily focused on people, where pose estimation and action recognition are popular research areas. The thesis presents a pose classification network, where input features are described by estimated 2D keypoints of horse body parts. The network output classifies three poses: ’Head above the wither’, ’Head aligned with the wither’ and ’Head below the wither’. The 2D reconstructions of keypoints are obtained using DeepLabCut applied to raw video surveillance data of a single horse. The estimated keypoints are then fed into a Multi-layer preceptron, which is trained to classify the mentioned classes. The network shows promising results with good performance. We found label noise when we spot-checked random samples of predicted poses and comparing them to the ground truth, as some of the labeled data consisted of false ground truth samples. Despite this fact, the conclusion is that satisfactory results are achieved with our method. Particularly, the keypoint estimates were sufficient enough for these poses for the model to succeed to classify a hold-out set of poses. </p>
----------------------------------------------------------------------
In diva2:1723072 merged words, the abstract should be:

<p>This report presents an application of reinforcement learning to the problem of controlling multiple robots performing the task of moving boxes in a warehouse environment. The robots make autonomous decisions individually and avoid colliding with each other and the walls of the warehouse. The problem is defined as a dynamical multi-agent system and a solution is reached by applying the DQN algorithm. The solution is designed for achieving scalability, meaning that the trained robots are flexible enough to be deployed in simulated environments of different sizes and alongside a different number of robots. This was successfully achieved by feature engineering.</p>
----------------------------------------------------------------------
In diva2:860599 missing space in title:
"System Integration Testing ofAdvanced Driver Assistance Systems"
should be
"System Integration Testing of Advanced Driver Assistance Systems"

There were merged words and missing ligatures, the abstract should be:

<p>A key factor to further improve road safety is the development and implementation of Advanced Driver Assistance Systems (ADAS) in vehicles. Common aspects of the investigated ADAS' are their abilities of detecting and avoiding hazardous traffic situations by using sensor data and vehicle states in order to control the movement. As more complex and safety critical ADAS are developed, new test methods have to be considered. This thesis investigate how to test new ADAS from a complete vehicle level by considering aspects such as suitable test environments and traffic scenarios, and thereafter compare the results with existing testing methods. Different classifications of ADAS have been investigated and combined with own classifications considering complexity and traffic safety aspects, have made it possible to conclude and propose general test strategies for different ADAS.</p>
----------------------------------------------------------------------
In diva2:1477486 merged paragraphs & words and missing ligatures, the abstract should be:

<p>In this work, a methodology to visualize and mitigate antenna coupling in a systematic way is proposed, which can be used to increase the isolation between different antenna systems installed on a common platform. Formulations for visualizing the coupling in space, based on reciprocity are derived from the reaction theorem and the different formulations are compared with each other through numerical simulations in CST Microwave Studio and by post-processing in Matlab. The derived formulations are also compared witha more established method based on the Poynting vector.</p><p>The ability of the derived formulations to predict appropriate positions for placing absorbers is compared by introducing surface wave absorbing materials (SWAM), based on the visualization results. This is done for two simplified platforms with simple antennas to compare the different methods. A realistic model consisting of a ground plane with cavities for integrating the antennas that could represent an integrated mast for naval ships is studied with the target to reduce the mutual coupling using the visualization methods.A cavity-backed spiral antenna and a body-of-revolution (BoR) array antenna are used as sources.</p><p>One of the formulations derived from the reaction theorem performs well as a predictor for absorber placement and at several points better than existing methods for the simplified platforms. This formulation can predict areas where placing an absorber will increase or decrease the coupling respectively. Furthermore, it is possible to estimate the impact of introducing an absorber in one region, relative to another. In the realistic case, it is demonstrated how this formulation can be used to reduce the mutual coupling in a systematic and efficient way. It is shown that the isolation is increased with 4-5 dB by placing SWAM at positions based on the visualization results, compared to covering the entire surface with SWAM.</p>
----------------------------------------------------------------------
In diva2:1847107 merged paragraphs and a missing dash after "y" in "y-dimensions", the abstract should be:

<p>Cancer is one of the most common causes of death worldwide. When given optimal treatment, however, the risk of severe illness may greatly be reduced. Determining optimal treatment in turn requires evaluation of disease progression and response to potential, previous treatment.</p><p>Analysis of perfusion, a physiological property that describes how well different tissues are supplied with blood, has been shown useful for revealing important tumor characteristics. By performing a contrast agent-enhanced, non-invasive medical imaging procedure, quantitative parameters of perfusion can be obtained by fitting the image data to mathematical models. These parameters may then provide valuable insights into tumor properties, useful for purposes such as diagnostics and treatment response evaluation. Varieties of parameter calculation frameworks and perfusion models may however lead to a wide range of possible parameter values, which negatively impacts reproducibility and confidence in results.</p><p>The aim of this thesis project was to explore how different implementation choices in a perfusion parameter calculations framework, as well as image data noise and filtering, affected the parameter estimations. Image data of nine brain-tumor patients and a physical phantom was used for calculating perfusion parameters after systematically applying changes to the default calculations framework. The results showed that the choice of optimization method for parameter estimations could provide a significant difference in parameter estimations. A semi-automated method for obtaining a venous input function was evaluated and shown to be robust with respect to simulated user inputs. Generation of a T1 map, used when performing the parameter calculations, was explored for the variable flip-angle method and from this investigation it was concluded that a few combinations of flip-angles generated unrealistic T1 maps. Finally, a Gaussian image filter applied in the x- and y-dimensions of the image data was found to provide a noticeable reduction of applied noise. The outcome of the experiments exemplified how calculation framework setup affected parameter estimations, which was discussed to be of importance for other areas of research as well. Future work could encompass exploration of other, more complex perfusion models, and performing similar analysis for tumors in other body-parts.</p>
----------------------------------------------------------------------
In diva2:1216324 merged words and unnecessary hyphens, the abstract should be:

<p>Voxel based cone tracing is a promising approach to approximate global illumination for real-time applications. This technique utilizes a voxel field approximating the original scene to retrieve the necessary radiance information during sampling. The simplest approach to creating a voxel field is to use a 3D texture. Since this requires too much GPU memory for larger scenes alternative data structures are necessary. This thesis compares two seemingly suitable data structures <em>3D-Clipmaps</em> and <em>Sparse voxel octrees</em>. To compare the two structures we implement them using OpenGL and C++. We then use the improved Sponza model with additional dynamic objects to benchmark the differences between the two approaches. Both data structures has its pros and cons. Our conclusion is that Clipmaps seems to be the most practical approach for real-world purposes.</p>
----------------------------------------------------------------------
In diva2:927736 unnecessary hyphens, the abstract should be:

<p>Unsupervised pre-training has recently emerged as a method for initializing supervised machine learning methods. Foremost it has been applied to artificial neural networks (ANN). Previous work has found unsupervised pre-training to increase accuracy and be an effective method of initialization for ANNs[2].</p><p>This report studies the effect of unsupervised pre-training when detecting Twitter trends. A Twitter trend is defined as a topic gaining popularity.</p><p>Previous work has studied several machine learning methods to analyse Twitter trends. However, this thesis studies the efficiency of using a multi-layer perceptron classifier (MLPC) with and without Bernoulli restricted Boltzmann machine (BRBM) as an unsupervised pre-training method. Two relevant factors studied are the number of hidden layers in the MLPC and the size of the available dataset for training the methods.</p><p>This thesis has implemented a MLPC that can detect trends at an accuracy of 85%. However, the experiments conducted to test the effect of unsupervised pre-training were inconclusive. No benefit could be concluded when using BRBM pre-training for the Twitter time series data. </p>
----------------------------------------------------------------------
In diva2:1556731 merged words, the abstract should be:

<p>Field observations has shown that wind turbines are especially exposed to lightning strikes. The probability for lightning strikes to offshore wind turbines has been analysed in a previous article. In this project the probability for upward self-initiated lightning strikes to onshore wind turbines and a lightning protection tower was analysed. This was done by collecting elevation data and recreating the site topography in COMSOL Mutliphysics 5.5, and also by collecting weather data which were analysed in MATLAB. The probability for the critical electrostatic field was then calculated and analysed. The result shows that the risk of lightning strike is correlated to the topography and cloud height.</p>
----------------------------------------------------------------------
In diva2:1453588 merged words, the abstract should be:

<p>Non-dispersive infrared (NDIR) gas sensing is one of the best gas measurement method for air quality monitor. The major advantage of NDIR gas sensors is their long lifespan. However, all sensors drift over time, NDIR sensors are not excluded due to sensor aging and environmental factors. Due to this, research on calibration for long-term accuracy becomes necessary. In particular, self-calibration methods have been subject of many studies but the problem still remains a challenge. Today, the most common used self-calibration method for NDIR sensors is the ABC technology (Automatic Baseline Correction). It has been shown that the ABC technology is efficient, easy to implement and has been widely used in commercial gas sensors. However, this method cannot work well in an environment where the sensors are never exposed to fresh air. In order to build smart networks of infrared gas sensors for continuous self-calibration, accurately quantifying the relationship between the sensor’s drift and external factors becomes more and more important. In this work, we aim to model the sensor’s drift by taking advantage of statistical learning algorithms. Firstly, we investigate the provided dataset and apply the ABC technology for estimating the sensor drift. Secondly, the random walk model and Gaussian process models are adopted to describe the drift with respect to sensor aging and ambient temperature changes. Both the random walk and Gaussian processes can be used to provide a predictive probability distribution on the drift for each measurement of the sensors. Experiments have been carried out on different inputs and kernels for modeling the drift as Gaussian processes. Comparisons of the test prediction performance have also been made between different models. The results demonstrate that Gaussian processes, with the advantage of providing an estimate of the prediction of the drift together with an estimate of the uncertainty in the prediction, can be used to model the sensor drift and provide good prediction performance.</p>
----------------------------------------------------------------------
In diva2:1793576 merged words, the abstract should be:

<p>Text mining has gained considerable attention due to the extensive usage of electronic documents. The significant increase in electronic document usage has created a necessity to process and analyze them effectively. Rule-based systems have traditionally been used to evaluate short pieces of text, but they have limitations, including the need for significant manual effort to create and maintain rules and a high risk of complex bugs. As a result, text classification has emerged as a promising solution for extracting meaning from short texts, which are defined as texts limited by a specific character count or word count. This study investigates the feasibility and effectiveness of text classification in classifying short pieces of text according to their appropriate text properties, based on users’ intentions in the text. The study focuses on comparing two transformer models, GPT-2 and BERT, in their ability to classify short texts. While other studies have compared these models in intention classification of text, this study is unique in its examination of their performance on short pieces of text in this specific context. This study uses user-labelled data to fine-tune the models, which are then tested on a test dataset from the same source. The comparative analysis of the models indicates that BERT generally outperforms GPT-2 in classifying users’ intentions based on the appropriate text properties, with an F1-score of 0.68 compared to GPT-2’s F1-score of 0.51. However, GPT-2 performed better on certain closely related classes, suggesting that both models capture interesting features of these classes. Furthermore, the results demonstrated that some classes were accurately classified despite being context-dependent and positioned within longer sentences, indicating that the models likely capture features of these classes and facilitate their classification. Both models show promising potential as classification models for short texts based on users’ intentions and their associated text properties. However, further research may be necessary to improve their accuracy. Suggestions for enhancing their performance include utilizing more recent versions of GPT, such as GPT-3 or GPT-4, optimizing hyperparameters, adjusting preprocessing methods, and adopting alternative approaches to handle data imbalance. Additionally, testing the models on datasets from diverse domains with more intricate contexts could provide greater insight into their limitations.</p>
----------------------------------------------------------------------
In diva2:1331871 merged paragraphs and words, the abstract should be:

<p>The nature of the power system is changing; what was once a clearly defined generation-transmission-distribution-consumer power flow is now shifting towards a distributed infrastructure, with great amounts of variable renewable sources in the system. The penetration of alternative energies like solar and wind have represented a game changer for the electric power industry, diminishing the traditional dominance of fossil fuel based sources, and moving towards a more renewable mixture. All this was made possible due to severe climate regulations, both in the European and global framework, as well as ever decreasing installation, operation and maintenance costs for these novel technologies.</p><p>However, the penetration of renewable energies brings challenges to the reliability and stability of the power system, which must be tackled accordingly. Fortunately, the tools are there, standards like the IEC61850 and IEC61499 were independently designed, and for different purposes. The IEC61850 standard strives for inter-operability and vendor-independence within the substation design field, specifically in regards to the data objects exchanged between the devices. On the other hand, the IEC61499 standard is used for the design of industrial distributed systems.</p><p>This report aims to answer the question: is it possible to generate an IEC61499 description which complements an IEC61850 substation specification? To this end, an IEC61850-IEC61499 interface was created, which takes an IEC61850 substation specification description, as well as a series of configuration files (rules, connections, allocation, parameters) and generates extended descriptions of the substation, as well as a fully operational IEC61499 system specification. This can be directly imported in the 4DIAC tool, and executed to evaluate the performance of the designed protection systems, in a network of distributed or centralized devices.</p><p>A series of test cases were evaluated, and the obtained results demonstrate that it is possible to bridge the gap between IEC61850 and IEC61499, thus enabling power system engineers to assess the performance of a certain protection scheme, before actually implementing it in the substation. This potentially reduces human errors and development times. The interface was implemented in Java p.l. and is distributed as an open source project.</p>
----------------------------------------------------------------------
In diva2:933512 there were unnecessary "- " occurrences, the abstract should be:

<p>A study was performed on Naive-Bayes and Label Spreading methods applied in a spam filter as classifiers. In the testing procedure their ability to predict was observed and the results were compared in a McNemar test; leading to the discovery of the strengths and weaknesses of the chosen methods in a environment of varying training data. Though the results were inconclusive due to resource restrictions, the theory is discussed from various angles in order to provide a better understanding of the conditions that can lead to potentially different results between the chosen methods; opening up for improvement and further studies. The conclusion made of this study is that a significant difference exists in terms of ability to predict labels between the two classifiers. On a secondary note it is recommended to choose a classifier depending on available training data and computational power. </p>
----------------------------------------------------------------------
In diva2:724013 there were unnecessary "- " occurrences, the abstract should be:

<p>Speech synthesis is an area of computer science with many practical uses, such as enabling people with visual impairments to take part of text and to provide more human-like feedback from information systems. A similar area of research is text-to-song, where systems comparable to those used in text-to-speech provide mappings from text to melodic units of song. This paper discusses how a text-to-song algorithm can be developed and what parameters affect what emotion is communicated. Fifty participants listened to music generated with our algorithm. Results show that tempo and mode both heavily account for what emotion is communicated; a melody performed with a tempo of 250 bpm was perceived as significantly more happy than a performance with a tempo of 120 bpm, and a melody in major tonality was perceived as significantly more happy than a melody in minor tonality. Combined, these parameters gave even more significant results. A fast tempo combined with major tonality produced a performance that was perceived as even more happy. The opposite was observed when a slow tempo was combined with minor tonality. When a fast tempo was combined with a minor tonality the average answer was neutral with answers distributed over the whole spectrum from sad to happy. A slow tempo combined with a major tonality gave almost identical results. We concluded that generating emotionally expressive song with the use of an algorithm is definitely possible, but that the methodology can be improved in order to convey emotions even more clearly.</p>
----------------------------------------------------------------------
In diva2:618745
   'webuser-interface'
should be:
   'web user-interface'
----------------------------------------------------------------------
In diva2:470725 merged paragraphs and an space and 0x2010 dash instead of "-", the abstract should be:
<p>The netback value is a price mechanism approach for determining prices in long-term contracts. This method consists in ensuring that gas remains competitive with competing fuels by setting the border in each long-term sale contract below the weighted-average price of the cheapest alternative fuels across all sectors, including industry, residential tertiary, transportation and electricity generation. This price mechanism is used in Continental Europe because the gas does not have captive uses and it can be replaced by other energy in all sectors such as the coal in the electricity generation sector.</p><p>This price is then indexed under long-term contracts to the main competing fuel. This pattern of indexation allows avoiding volatility and the use of market power but some actors really want to change this method of indexation. Indeed, the power sector represents the main source of incremental demand growth for gas in the recent years and therefore, indexation to coal is conceivable. Moreover, the economic crisis contributes to lower the gas market price, opening a gap between the spot and the long-term contract prices. The spot price and the long-term price are currently at nearly the same level but during the winter, the long-term contract price should be normally lower. More and more Combine Cycle Gas and Turbines (CCGT) are built in Europe and the investors would like to include the gas value of the electricity generation in the calculation of the weighted Netback value. Some key factors such as the construction of the coal plant cost, the fuel cost and the CO2 cost have a significant impact on this Netback value for electricity generation.</p><p>The CO2 Capture and Storage (CCS) is also a promising technology regarding the share of fossil fuel in the energy mix. However, the netback value found with the CCS coal plants must be viewed with some caution regarding the uncertainties around the capture costs, the transport costs, the distribution of storage sites and the learning rate.</p>


Oddly, there is only an abstract!

----------------------------------------------------------------------
In diva2:1232315 missing the footnote and missing the emphasis, the abstract should be:

<p>On the internet today, information to expose books is generated manually. That includes information such as genre, author, places, and summary. The full text of books are not publicly available on the Internet due to copyright law, and for this reason it is not possible to generate this type of information automatically. One solution is to construct a prototype that processes the original book and automatically generates information that can be exposed to the Internet, without exposing the entire book. In this report, three different algorithms that deal with processing books are compared: <em>stemming</em>, <em>filtering of stop words</em> and <em>scrambling of sentences within paragraphs</em>. The algorithms are compared by generating relevant information to the services: search engines, automatic metadata, smart ads and text analysis. Search engines allows a user to search for e.g. the title or a sentence from the book. Automatic metadata automatically breaks out descriptive information from the book. Smart ads can use descriptive information to recommend and promote books. Text analysis can be used to automatically create a brief descriptive summary. The information stored from the books should only be relevant information for the services and the information should not have any literal value<sup>2</sup> for a human to read. The result of the work shows that the combinations <em>scrambling of sentences→filtering of stop words</em> and <em>filtering of stop words→scramlbing of sentences</em> are optimal in terms of searchability. It is also recommended to add stemming as an additional step in the processing of the original book, as it generates more relevant automatic metadata to the book.</p>

2 is a footnote:
<p>No literal value means that the content of the book is revised so that it is no longer possible for a person to understand the context and action. Revising the contents of a book can be done in different ways, for example, by changing the structure of the sentences and deleting conjunctions in the text.</p>
----------------------------------------------------------------------
In diva2:1248819 "<7p>" should be "</p>", the abstract should be:

<p>Vatten är en av de viktigaste resurserna i världen. Det har direkt inverkan på mänsklighetens dagliga liv och samhällets hällbara utveckling. Vattenkvaliteten påverkar det biologiska livet och måste följa strikta föreskrifter. Traditionella metoder för vattenkvalitetssäkring, som används idag, innefattar manuell provtagning foljt av laboratorieanalys. Denna process är dyr på grund av höga arbetskostnader för provtagning och laboratoriearbete. Dessutom saknar den realtidsanalys som är väsentlig för att minimera förorening.</p><p>Avhandlingen syftar till att hitta en lösning på detta problem med hjälp av IoT-sensorer och maskinlärningsteknik för att upptäcka avvikelser i vattenkvaliteten. Den spatiala skalbarheten är ett viktigt krav vid val av överföringsprotokoll, eftersom sensorer kan spridas runt vattennätverket. Vi diskuterar lösningar som är lättillgängliga eller snart ska vara på marknaden. De viktigaste LPWAN-teknikerna som studerats är: SigFox, LoRaWAN och NB-IoT. Generellt har dessa protokoll många egenskaper som är nödvändiga för övervakning av färskvatten, som lång batterilivslängd och lång räckvidd, men de har många begränsningar vad gäller överföringshastighet och arbetscykel. Det är därför viktigt att hitta en lösning som skulle hitta anomalier vid högt säkerhet men samtidigt överensstämmer med begränsade överförings- och bearbetningskapaciteter hos sensorerna och de ovan nämnda protokoll.</p><p>En försökssensor finns redan på plats i Lake Mälaren och dess avläsningar används för denna studie. Övervakade maskininlärningsalgoritmer, såsom Logistic Regression, Artificial Neural Network, Decision Tree, One Class K-NN and Support Vector Machine (SVM) studeras och diskuteras beträffande tillgängliga data. SVM väljs sedan, implementeras och optimeras för att uppfylla IoTs begränsningarna. Balansen mellan falska avvikelser och falska normala avläsningar diskuteras också.</p>
----------------------------------------------------------------------
In diva2:706748 merged words and paragraphs, the abstract should be:

<p>Genome-wide association studies (GWAS) has been in the heart of medical research for the last 5 years. These studies seek for common variants in the genome that are linked to risk for common complex diseases (CCDs). Although GWAS has defined a number of interesting genetic loci for a range of CCDs, the current GWAS analysis has limitation such as investigating the DNA variants one-by-one focusing on the most significant DNA variants. As a consequence, most risk variants for CCDs are, in my belief, still hidden in the GWAS data. Herein, I use a method of GWAS analysis that considers risk-enrichment for groups of functionally associated genes defined by for example gene networks, believed to play a role in CCDs.</p><p>In this method, a set of expression SNP (single nucleotide polymorphism) was selected from genes which are known to be related to coronary artery disease (CAD) in a way that a singlee SNP was chosen for each gene. Then using the data available from the International HapMap Project and a GWAS data available, it is possible to find SNPs which are in strong linkage with the initial set, which we call it expanded set. Depending on the association of the initial set to the CAD, expanded set can show an enrichment score greater or smaller compared to the null distribution set of SNPs with same properties of the expanded set.</p><p>In conclusions, CCDs are not a consequence of isolated genetic variants/genes in isolated pathways but instead sets of genetic variants/genes acting in conjunction, cause CAD. Genetic risk enrichment analysis is a fairly simple and straightforward method to determine to what extent a group of functionally associated genetic variants/genes are enriched for a given CCD. In addition, this analysis can perhaps help to decipher some of the 90-85% of risk variation in populations that remains unaccounted.</p>
----------------------------------------------------------------------
In diva2:808731 unnecessary paragraphs, the abstract should be:

<p>With the rapid development of new technologies during the last decades, the demand of complex algorithms to work in real-time applications has increased considerably. To achieve the real time expectations and to assure the stability and accuracy of the systems, the application of numerical methods and matrix decompositions have been studied as a trade-off between complexity, stability and accuracy. In the first part of this thesis, a survey of state-of-the-art QR Decomposition methods applied to matrix inversion is done. Stability and accuracy of these methods are analyzed analytically and the complexity is studied in terms of operations and level of parallelism. Besides, a new method called Modified Gaussian Elimination (MGE) is proposed. This method is shown to have better accuracy and less complexity than the previous methods while keeping good stability in real time applications. In the second part of this thesis, different techniques of extended Kalman Filter implementations are discussed. The EKF is known to be numerically unstable and various methods have been proposed in the literature to improve the performance of the filter. These methods include square-root and unscented versions of the filter that make use of numerical methods such as QR, LDL and Cholesky Decomposition. At the end of the analysis, the audience/reader will get some idea about best implementation of the filter given some specifications.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-08-19
======================================================================
In diva2:511132 a "c" was inserted at the end of the first line and the paragraphs were merged, the abstract should be:

<p>The Spinning QUad Ionospheric Deployer (SQUID) is a sounding rocket experiment developed to test and verify a novel mechanism to deploy wire booms. The SQUID consists of the Rocket Mounted Unit (RMU) and the Free Flying Unit (FFU), the former is attached to the rocket and the latter is ejected. The FFU carries the electronics box (eBox) that controls the system and the boom deployment system known as SCALE. The FFU needs to be independent when has been ejected from the rocket.</p><p>This thesis work covers the design and manufacture of the SQUID electronics system to control the functionality of the experiment. The control is implemented in a Field Programmable Gate Array (FPGA) using the VHDL language. The integration, testing and validation of software and hardware also is presented here.</p><p>The SQUID experiment was launched onboard the REXUS-10 rocket from ESRANGE the 23rd February 2011.</p>
----------------------------------------------------------------------
In diva2:1040724 merged paragraphs, missing ligatures, and unnecessaru hyphens, the abstract should be:

<p>Autonomous vehicles have been the subject of intense research, resulting in many of the latest cars being at least partly self driving. Cooperative driving extends this to a group of vehicles called a platoon, relying on communication between the vehicles in order to increase safety and improve the flow of traffic. This thesis is partly done in context of Grand Cooperative Driving Challenge (GCDC) 2016 where KTH has participated with a Scania truck and the Research Concept Vehicle (RCV), an electric prototype car.</p>p>Trajectory planning is investigated for the longitudinal control of both the truck and the RCV. This planner is to ensure that the vehicles reached a position in a given time and a desired velocity. This is done using Pontryagin's minimum principle and interpolation.</p>p>A more advanced planner based on Model Predictive Control (MPC) is used to avoid collisions in two different scenarios. One considers obstacle avoidance in the form of an overtake and the other a lane change scenario were the vehicle needs to decide how to position itself relative to the other vehicles.</p>p>Simulations of the longitudinal control and planning of the truck did show that it could time the position and speed with a position error of less than 2m and speed error less than 0.2 m/s, assuming a distance of 120-200 m, a time interval of 40s and goal speed of 7m/s. The same simulation for the RCV had a distance error of less than 0.3m and a speed error below 0.2m.</p>p>Simulations of the RCV using MPC planners showed that overtaking and lane changes could be performed. When performing the lane change the RCV managed to maintain a longitudinal distance of at least 1m, even if the other vehicles are slowing down or increasing their speed. The overtaking could also be successfully performed although with small margins, having a lateral distance of 0.5 m to the vehicle being overtaken.</p>
----------------------------------------------------------------------
In diva2:706726 the "*" marks a footnote:

For the English abstract:
This master thesis also considers and describes other security specifications such as British Standard Institution (BSI) 10012, and International Standards Organization (ISO) and International Electrotechnical Commission (IEC) standard (ISO/IEC 27001) to provide a complete picture of the rules and regulations concerning personal data protection.


For the Swedish:
Examensarbetet tar också hänsyn till och beskriver andra säkerhetspecifikationer som British Standard Institution (BSI) 10012, och International Standards Organization (ISO) och International Electrotechnical Commission (IEC) standard (ISO/IEC 27001) för att ge en fullständig bild av regler och förordningar angående säkrandet av personlig data.
----------------------------------------------------------------------
In diva2:789372 merged paragraphs and words, the abstract should be:

<p>Video conferencing services are dependent on many other underlying devices, network services and infrastructure and TCP/IP services before they can provide seamless, reliable and good quality video meeting services to end users. Providing fully automated video conferencing services at Skiptrip AB requires engagement of even more variant and complex set of TCP/IP services and devices that has made its network a heterogeneous one consisting of hundreds of modern and legacy systems along with the high definition and bandwidth sensitive video conferencing systems. In this thesis the process of designing and implementing a secure network module forseparating and transferring non-production (management) network traffic flow of all network equipment via establishing and fine-tuning virtual IP-sec tunnels among edge routers or firewalls of each video station in this enterprise-scale network has been conducted in order to make sure that the network traffic flow belonging to the management module is treated separately and securely thanks to the encryption mechanisms of IPsec protocol on the header and payload of IP packets.</p><p>After getting inspired by studying some well-known network design and architecture methodologies and industry best practices like Cisco SAFE, characterizing the existing network is done in the early stages of this thesis with a focus on security measures such as the utilization of Access Control Lists on different router interfaces which were utilized to provide perimeter network security to some extent. Afterwards, a new network design is proposed where the management flow is separated from the production traffic flow and is transferred through the secure IPsec tunnels in a semi-mesh topology which form a virtual network module for the management traffic of the whole internetwork. The new network module is then given a new IP addressing scheme based on the private range of IPv4 addresses and, after relevant discussions, a certain way of implementation of static routing in combination with classless interdomain routing and variable length subnetmasking is introduced to provide, implemented and tested in order to provide route-redundancy in IP connectivity level of management network module in a similar-to dynamic routing protocol manner.</p><p>Innate sensitivity of high definition video conferencing protocols like H.323 and SIP to quality of the underlying network infrastructure which is usually defined in terms of packet loss and jitter as well as the bandwidth limitation of costly Internet links in each video station and the heterogeneity of the internetwork were amongst the main technical challenges of this thesis and shaped the outcome of proposed design and also the evaluation mechanisms which are done at the end of this project.</p>
----------------------------------------------------------------------
In diva2:1453627 merged paragraphs and merged words, the abstract should be:

<p>The most common fault type in electric power systems is the line to ground fault. In this type of faults, an electrical arc is usually developed. The thesis presents a mathematical model that describes the behavior of the arc during a fault. The arc model has been verified based on real and simulated tests that were conducted on a system that has resonant earthing coil.</p><p>In addition, two studies have been conducted on the same verified system. The first studied was implemented to see the effect of detuning the resonant earthing coil at different levels. It was noted that detuning the coil affected AC and the DC components in the arc faults. Also, the detuning affected the arc extinction.</p><p>The second study has been looking at the effects of implementing a parallel resistor to the resonant earthing coil. The tests have been conducted using different set values of the resistor. In some of the studied cases and during the testing period, the resistor has affected the self-extinguish behavior of the arc.</p>
----------------------------------------------------------------------
In diva2:1148538 merged paragraphs and words, the abstract should be:

<p>Unmanned Aerial Vehicles (UAV), in particular the four-rotor quadrotor, are gaining wide popularity in research as well as in commercial and hobbyist applications. Maneuverability, low cost, and small size make quadrotors an attractive option to full-scale, manned helicopters while opening up new possibilities. These include applications where full-scale helicopters are unsuitable, such as cooperative tasks or operating indoors.</p><p>Many UAV systems use the Global Positioning System (GPS), IMU (Inertial Measurement Unit) sensors, and camera sensors to observe the UAV’s state. Depending on the application, different methods for observing the states are suitable. Outdoors, GPS is available and widely used and in cluttered environments on-board cameras can be the best choice. Controlled lab environments often use external cameras to track the quadrotor. Most applications make use of the IMU in their implementations, most commonly the gyroscope for attitude estimation.</p><p>In this thesis, several external ultra-wideband (UWB) radio sensors are used to measure the distance between individual sensors and a quadrotor. The range measurements are fused with acceleration and angular velocity measurements from an Inertial Measurement Unit to estimate the quadrotors position and attitude. An ultra-wideband sensor is cheap and does not require line-of-sight or heavy equipment mounted on the quadrotor. The drawback of UWB-based positioning is that it requires the assumption of known sensor locations in order to calculate the distance between sensor and UAV. This thesis aims to remove this assumption by estimating the quadrotor’s and the sensors’ position simultaneosly using the Smoothing and Mapping (SAM) technique.</p><p>The Georgia Tech Smoothing and Mapping (GTSAM) framework provides the incremental Smoothing and Mapping implementation, used for estimation of both the quadrotor’s position and attitude, and the sensors’ position. The Inertial Measurement Unit is handled by the state of the art IMU factor, included in GTSAM.</p><p>The system is evaluated with and without prior knowledge of the sensor positions, using recorded flight data from a Crazyflie quadrotor and six Loco Positioning Node sensors. The results show that the system is able to track the UAV’s position and attitude with acceptable errors. The error in estimated sensor position is too large to be satisfactory, Based on the results several topics for future work are proposed.</p>
----------------------------------------------------------------------
In diva2:881082 merged paragraphs & words, and unnecessary hyphen, the abstract should be:

<p>Both data traffic and number of subscriptions have enormously increased in mobile network in recent years. Moreover, there will be an even faster growth in the future. A promising way to satisfy a significantly increasing demand in future radio access network is by using so called Ultra Dense Networks (UDNs) which deploy a large number of base stations compared to the number of active users.</p><p>Radio spectrum is a finite resource and therefore has to be shared by multiple users. This sharing of radio spectrum inevitably causes interference between the users. In this study, the interference management performance of different resource allocation schemes in different network density is studied, which is from a traditional network density to ultra dense network.</p><p>Except for traditional frequency reuse scheme and reuse partitioning scheme, Coordinated Multi Point (CoMP) schemes have been chosen in the work. Different CoMP techniques such as the universal frequency reuse (UFR) and cooperative frequency reuse (CFR) are tested to find the best network performance in terms of average users data throughput and cell rate.</p><p>Besides, after measuring these CoMP schemes which are designed for high base station density, the optimal scheme is found to be a potential method adopted by ultra-dense network.</p>
----------------------------------------------------------------------
In diva2:1556753 merged paragraphs & words, the abstract should be:

<p>Internet of Things devices, such as smartphones and smartwatches, are currently becoming widely accessible and progressively advanced. As the use of these devices steadily increases, so does the access to large amounts of sensory data. In this project, we developed a system that recognizes certain activities by applying a linear classifier machine learning model to a data set consisting of examples extracted from accelerometer sensor data. We obtained the data set by collecting data from a mobile device while performing commonplace everyday activities.These activities include walking, standing, driving, and riding the subway. The raw accelerometer data was then aggregated into data points, consisting of several informative features. The complete data set was subsequently split into 80% training data and 20% test data. A machine learning algorithm, in this case, a support vector machine, was presented with the training dataset and finally classified all test data with a precision higher than 90%. Hence, meeting our set objective to build a service with a correct classification score of over 90%.</p><p>Human activity recognition has a large area of application, including improved health-related recommendations and a more efficiently engineered system for public transportation.</p>
----------------------------------------------------------------------
In diva2:680540 merged paragraphs & words, the abstract should be:

<p>In the context of military field, more and more international coalitions among allied forces have taken place. Information from heterogeneous systems needs to be exchanged without misinterpretation so the involved participating actors can share a common situational awareness regarding certain data and/or messages. This, in turn, requires the preservation of the intended meaning not only on the syntax, language, and representation level, but on a semantic level as well.</p><p>The application domain of the Business Object Reference Ontology Program (BORO) method focuses on the development of ontological or semantic models for large complex operational applications, especially in the military context. It is chosen by FOI, the Swedish Defense Research Agency in the field of Information Systems, to apply to their Semantic Interoperability (SI) project.</p><p>The goal of this thesis is to investigate how BORO method can be implemented for aligning the data and/or messages between the Swedish Armed Forces and other military organizations on a semantic level for the FOI SI project. To achieve this goal the design science research methodology is conducted through a series of steps. The analysis regarding the usability of BORO method for FOI to obtain semantic interoperability in its project will be demonstrated as the result of this thesis, which can also be utilized as a reference for other military organizations when conducting activities of information exchange.</p>
----------------------------------------------------------------------
In diva2:678507 merged words and an unnecessary space, the abstract should be:

<p>The abundance of high-dimensional datasets provides scientists with a strong foundation in their research. With high-performance computing platforms becoming increasingly available and more powerful, large-scale data processing represents an important step toward modeling and understanding the underlying processes behind such data.</p><p>In this thesis, we propose a general cortex-inspired information processing network architecture capable of capturing spatio-temporal correlations in data and forming distributed representations as cortical activation patterns. The proposed architecture has a modular and multi-layered organization which is efficiently parallelized to allow large-scale computations. The network allows unsupervised processing of multivariate stochastic time series, irregardless of the data source, producing a sparse de-correlated representation of the input features expanded by time delays.</p><p>The features extracted by the architecture are then used for supervised learning with Bayesian confidence propagation neural networks and evaluated on speech classification and recognition tasks. Due to their rich temporal dynamics, we exploited auditory signals for speech recognition as an use case for performance evaluation. In terms of classification performance, the proposed architecture outperforms modern machine-learning methods such as support vector machines and obtains results comparable to other state-of-the-art speech recognition methods. The potential of the proposed scalable cortex-inspired approach to capture meaningful multivariate temporal correlations and provide insight into the model-free high-dimensional data decomposition basis is expected to be of particular use in the analysis of large brain signal datasets such as EEG or MEG.</p>
----------------------------------------------------------------------
In diva2:789371 merged paragraphs & words, the abstract should be:

<p>There are different ways to store and process large amount of data. Hadoop is widely used, one of the most popular platforms to store huge amount of data and process them in parallel. While storing sensitive data, security plays an important role to keep it safe. Security was not that much considered when Hadoop was initially designed. The initial use of Hadoop was managing large amount of public web data so confidentiality of the stored data was not an issue. Initially users and services in Hadoop were not authenticated; Hadoop is designed to run code on a distributed cluster of machines so without proper authentication anyone could submit code and it would be executed. Different projects have started to improve the security of Hadoop. Two of these projects are called project Rhino and Project Sentry [1].</p><p>Project Rhino implements splittable crypto codec to provide encryption for the data that is stored in Hadoop distributed file system. It also develops the centralized authentication by implementing Hadoop single sign on which prevents repeated authentication of the users accessing the same services many times. From the authorization point of view Project Rhino provides cell-based authorization for Hbase [2].</p><p>Project Sentry provides fine-grained access control by supporting role-based authorization which different services can be bound to it to provide authorization for their users [3].</p><p>It is possible to combine security enhancements which have been done in the Project Rhino and Project Sentry to further improve the performance and provide better mechanisms to secure Hadoop.</p><p>In this thesis, the security of the system in Hadoop version 1 and Hadoop version 2 is evaluated and different security enhancements are proposed, considering security improvements made by the two aforementioned projects, Project Rhino and Project Sentry, in terms of encryption, authentication, and authorization. This thesis suggests some high-level security improvements on the Centralized authentication system (Hadoop Single Sign on) implementation made by Project Rhino.</p>
----------------------------------------------------------------------
In diva2:1634378 merged words, the abstract should be:

<p>This project investigates the applicability of the original version of Markowitz’s mean-variance model for portfolio optimization to real-world modern actively managed portfolios. The method measures the mean-variance model’s capability to accurately capture the riskiness of given portfolios, by inverting the mathematical formulation of the model. The inversion of the model is carried out both for fabricated data and real-world data and shows that in the cases of real-world data the model lacks certain accuracy for estimating risk averseness. The method has certain errors which both originate from the proposed estimation methods of input variables and invalid assumptions of investors.</p>
----------------------------------------------------------------------
In diva2:762845 merged words, the abstract should be:

<p>This report details our research done on web sandboxes with a focus on two different implementations, Google Caja and ADsafe. Detailing their differences, their soundness, and their suitability for isolation of untrusted JavaScript in a specific multi-module web platform. The report also contains our results from implementing a prototype of a tool to automatically test an implementation of an ADsafesandbox. We present our motivation for this research as the security issues with running non-isolated and unchecked JavaScript, and the specific risks related to Multisoft’s Softadmin platform</p>

I think that the last sentence should end with a period, but there is no full text in DiVA.
----------------------------------------------------------------------
In diva2:1085518 merged words, the abstract should be:

<p>The load response to voltage and frequency changes has a considerable impact on the behaviour of the power system. Thus, the selection of a load model structure and its corresponding parameters is an important task in order to study and predict the system behaviour. Currently, the Nordic Transmission System Operators (TSO) use the ZIP load model, as it provides an easy and flexible way of representing the load. The main goal of the thesis has been to test two approaches for deriving ZIP model parameters, namely the component-based and measurement-based approaches. The former approach uses predefined parameter values, and information on the loads electricity consumption, whereas the latter uses measurement data and curve-fitting techniques. In order to evaluate the methodology, a case study has been performed, where the two aggregation approaches were applied on an evaluation point. It was found that the aggregation by means of the component-based approach may result in ZIP parameters lacking physical significance. ZIP parameters without physical significance pose a challenge for system planners, who may have difficulties in accepting these values as they are less intuitive than physically significant ones. Furthermore, the results of the measurement-based approach indicate that the ZIP model has some limitation when it comes to the sudden load changes that it can accommodate. This has been the case with the measured reactive power in the case study. Based on the results of applying the methodology, it can be concluded that the component-based and measurement-based approaches provide useful information when understanding power system loads.</p>
----------------------------------------------------------------------
In diva2:1453320 merged paragraphs & words, the abstract should be:

<p>Sustainable transport has lead to recent technological advancements for electric vehicles. A weak component of the electric vehicles is the energy storage units and their efficient operation. A battery management system is usually employed to ensure the safe and efficient operation of the batteries. State of charge (SoC), parameter and capacity estimation are vital functions of such a device. However, the development and performance evaluation of these processes is difficult.</p><p>In this thesis, capacity estimation algorithms are developed and tested under various scenarios, like parameter initialisation and SoC error compensation. The investigated algorithms are based on recursive version of the least squares method. The validation of the algorithms is performed on data, provided by <em>Scania AB</em>.</p><p>The experimental results proved that the errors-in-variables (EIV) solution performs overall better than the ordinary recursive least squares in terms of bias compensation and convergence. However, an improved identification of the battery model will eliminate considerable inaccuracies.The proposed estimator achieves similar accuracy to the EIV method in terms of bias and outlier removal. However, its convergence speed is undoubtedly moderate. The validation and testing are substantial obstacles in the development of such algorithms due to huge amount of data and long simulations. Further investigation is required in terms of different battery temperatures.</p>
----------------------------------------------------------------------
In diva2:1272232 merged words, the abstract should be:

<p>Sensorless-controlled drives represent a solution drawing an increasingly important attention for the benefits they entail. At the cost ofa slightly lower dynamics compared with the traditional drives, they involve a reduction of the system costs and complexity and improved noise immunity and reliability. Besides the traditional signal injection methods, involving a limitation of the machine voltage margin, higher iron losses, torque ripple and acoustic noise, a new method has been proposed in 2010 by professors Bolognani, Faggion and Sgarbossa. This algorithm, which has been defined "intrinsic injection" method, makes used of the harmonic content deriving from the PWM modulation.</p><p>In this work, the intrinsic injection sensorless algorithm and its implementation in a MATLAB/Simulink environment is the object of study. Its theoretical foundation is deeply analysed together with the phenomena and the operating conditions that might affect its performance.The drive model is described and three different alternatives for the estimator have been proposed. Simulations have been run with the estimator operating both in open-loop and in closed-loop. The influences of the sampling frequency, of the motor speed, of the load torque, of the implemented modulation strategy and of the DC-link voltage amplitude have been analysed. Lastly, the drive has been simulated with regard to a fan or pump application case.</p>
----------------------------------------------------------------------
IN diva2:888124 merged paragraphs & words:

<p>The established use of IT systems has increased the use of information in modern enterprises. From this information use, the concept of Business Intelligence has developed to enable more efficient and informed decision-making. As the business’ requirements of Business Intelligence reports changes rapidly due to changes of the business’ needs and more analytical organisations, traditional Business Intelligence development faces problems of ad-hoc analyses due to the inefficient adaption to changing needs.</p><p>This Master Thesis serves the purpose of deepen the understanding of the establishment of an agile development program of Self-service BI, addressing the concerns of more effectively meeting the changing requirements of traditional Business Intelligence development. This study explores enablers through a qualitative case study, conducted at a Swedish bank, consisting of four group interviews discussing the establishment of such program in Organisational, Processes, Technical and External dimensions, respectively. The qualitative case study was then followed by a discussion of governance of such program for alignment to enablers.</p><p>The qualitative case study resulted in 15 enablers of an agile development program of Self-Service BI, considering further enablers compared to more general literature of BI success factors, addressing the perspective of both an agile development program and Self-Service BI applications. The discussion of governance of the program then identified eight governance mechanisms, which might align the program to the enablers, for successful establishment and development of applications.</p><p>The findings of the study can be considered to culminate into a structure of an agile development program of Self-Service BI. The Thesis presents, from the findings, a framework for structuring such program, consisting of three development phases; Ordering process, Agile development, and Maintenance/Support and Training, and with the discussed governance for steering the development.</p>
----------------------------------------------------------------------
In diva2:484516 merged words and a typo, the abstract should be:

<p>UWB is a promising technology for short-range high-rate wireless applications. It is able to provide maximal 480Mbps data-rate at a distance of 2 meters in realistic indoor multi-path environments. UWB technology is widely applied to the next generation WPAN as well as the wireless access of consumer electronics at home. Recently, Multi-Band OFDM based UWB technology proposed by WiMedia has been selected as the international standard by ISO. In China, a new transmission architecture based on Dual-Carrier OFDM technology is adopted as UWB standard draft. Comparing to MB-OFDM based UWB system, DC-OFDM based UWB system has multiple advantages, like more spectrum resource, lower requirements on devices, etc. Besides, it is compatible with existing MB-OFDM based UWB technology. Therefore, DC-OFDM based UWB is more flexible.</p>
<p>Synchronization is the first step at the receiver digital baseband, which is of tremendous importance in any wireless communication systems. The performance of synchronization directly determines whether the receiver can pick up radio signals correctly or not, whether the baseband modules can fulfill the digital signal processing effectively or not. The synchronization process in OFDM system can be briefly divided into two parts: symbol timing and frequency synchronization. Symbol timing serves to judge the starting position of OFDM symbols after considering the impact of multi-path fading channel. While the frequency synchronization estimates the multiple imperfections in analog front-end signal processing and make proper compensation.</p>
<p>This thesis puts the emphasis on synchronization issues in DC-OFDM based UWB systems. We are the first to analyze the synchronization algorithm as well as the hardware implementation method tailored for DC-OFDM based UWB system. We also present the VLSI implementation result for synchronization module. The thesis consists of symbol timing and frequency synchronization.</p>
<p>Regarding on the symbol timing, we analyze the impact of several synchronization errors in OFDM system. After that, we divide the synchronization process into four modules by functionality: packet detection, coarse timing, TFC detection and fine timing. The internal parameters in each module are determined by system simulations. In the aspect of algorithm development, we adopt the joint auto-correlation and cross-correlation method to meet the requirements of UWB system in different indoor multi-path environments, and therefore achieve the robustness. In the aspect of hardware implementation, we put the attention on the structure of some key modules in symbol timing and their VLSI implementation result, such as auto-correlator, cross-correlator, real-number divider, etc.</p>
<p>Regarding on the frequency synchronization, we first investigate the multiple analog front-end imperfections in OFDM system, like CFO, SFO and I/Q imbalance, and present their mathematics models respectively in DC-OFDM based UWB system. After that, we analyze the performance degradation in OFDM system due to these non-ideal effects by the metric of EVM. RF designer can build the connection between mismatching parameters and performance degradation by referring to the analysis. Hence, the RF designer is able to traceout the outline of system design. In the aspect of algorithm development, we explore the intrinsic character of I/Q imbalance which causes the image interference. Then, we design a set of new training sequences based on phase rotation and give the corresponding estimation algorithm. The simulation result shows that the new training sequence is able to obtain the diversity message introduced by I/Q imbalance and therefore achieve the diversity gain during demodulation process. In order to deal with the challenging situation where multiple analog front-end imperfections co-exist, we propose a joint estimation and compensation scheme. In the aspect of hardware implementation, we present the hardware structure of CFO estimation and compensation module catered for DC-OFDM based UWB system, with the emphasis on CORDIC unit that is responsible for triangle calculations. The VLSI implementation result shows that the proposed CFO estimation and compensation module satisfies the timing and resource requirements in DC-OFDM based UWB system.</p>
<p>In the last, we present the prospective research area in 60-GHz applications. It includes multiple non-ideal impairments, like phase noise, non-linear power amplification, DC offset, ADCs mismatch, etc. It is even more challenging to develop joint estimation and compensation scheme for these non-ideal effects.</p>
----------------------------------------------------------------------
In diva2:576280 merged paragraphs & words, the abstract should be:

<p>This diploma work is mainly focused on developing the control strategy for a  variable speed drive as an alternative solution to a micro-hydro power plant. The detailed mathematical model for a micro-hydro system including a Kaplan turbine, mechanical shaft and electrical machines is presented and validated through simulations. A control strategy for an autonomous operation of a doubly-fed induction machine-based drive is developed for a wide range of speed. The drive can operate at a unity power factor.</p><p>The possible applications of the analyzed system are also presented. As a positive side of the system, it is found that the direct interaction between the power electronic converters and the utility grid can be avoided by exploiting the proposed topology, which might lead to a better quality of the produced power in terms of harmonics. This could also lead to removal or reduction of the size of the harmonic filters that are being used in conventional doubly-fed induction generator installations.</p><p>As regards to the drawbacks of the system, a comparison of converter and generator ratings between the analyzed solution and the conventional solution was performed. While the converters rating remain the same, there is one more electrical machine and the doubly-fed generator rating is slightly increased. Losses are also slightly larger due to the presence of the second machine.</p>
----------------------------------------------------------------------
In diva2:1071824 merged paragraphs & words, and missing ligatures, the abstract should be:

<p>The emerging field of mobile microrobotics has been suggestive of a plethora of applications, from intelligent micromanipulation to diversifying the prospects of minimally invasive surgeries. Several designs of microrobots have been proposed and are being notoriously studied to enhance the knowledge of their behavior in different environments to cater such applications. At present, control and surveillance of these microrobots have been aided by the convergence of various technologies like magnetic actuation, microscopy and computer vision. The quintessential knowledge of their maneuverability could provide interesting implications of these microrobots in their targeted applications.</p><p>In context of widening this understanding of their behavior, several dexterous methods have been proposed to study and control these microrobots, employing microscopy in stereo vision for visual surveillance. The intuitive drawback of microscopy, jeopardizing its focusing accuracy against field of imaging, has limited these studies to smaller observable volumes. Addressing limitations of conventional microscopy, holography has been explored as a potential candidate for imaging and spatial tracking of these microrobots by means of reconstructing 2-D information in the images. However, the resolution of depth estimation and processing cost incurred in reconstruction posed drawbacks on its applicability.</p><p>The novel method proposed in this report, employed the holographic imaging in stereo vision, overcoming the limitations of both conventional microscopy and holographic reconstruction. The cost of processing this image information at a much lower processing speeds further benchmarked its candidature for spatial tracking of microrobots. The inventive setup design proposed in the report was inspired by conventional stereo vision. It also entails the tracking procedure of a class of flagellated microrobots called Artificial Bacterial Flagella (ABF) based on retrieval of its two diffraction based holograms, and estimating its 3-D position based on lateral positions in the two stereo projections. It further suggests feasible design metrics for fabrication of these ABFs based on Fresnel dirffaction. The central idea behind this 3-D estimation proposed here is based on processing diffraction pattern produced by the ABFs using an image segmentation algorithm and further projecting the lateral coordinates so obtained to real space, achieving much more finesse over the depth resolution in either direction.</p>

----------------------------------------------------------------------
In diva2:1531638 merged words, the abstract should be:

<p>The thesis tackles the problem of data association for monocular object-based SLAM, which gets often omitted in related works. A method for estimating ellipsoid object landmark representations is implemented. This method uses bounding box multi-view object detections from 2D images with the help of YOLOv3 object detector and ORB-SLAM2 for camera pose estimation. The online data association uses SIFT image feature matching and landmark backprojection matching against bounding box detections to associate these object detections. This combination and its evaluation is the main contribution of the thesis. The overall algorithm is tested on several datasets, both real-world and computer rendered. The association algorithm manages well on the tested sequences and it is shown that matching with the backprojections of the ellipsoid landmarks improves the robustness of the approach. It is shown that with some implementation changes, the algorithm can run at real-time. The landmark estimation part works satisfactory for landmark initialization. Based on the findings future work is proposed.</p>
----------------------------------------------------------------------
In diva2:1413113 some missing spaces, the abstract should be:

<p>Automation of the forest industry has for over 30 years been an important subject of research, which could reduce the human workload and costs significantly. However, there are still many problems to be solved, such as enabling the communication between the heavy machinery in a forest and a remote base. High speed and reliable communication is the key to automated operations and remote control of machinery. This thesis investigates the feasibility and performance of IEEE 802.11n/ac WiFi hardware to provide high-bandwidth connection in a forest. In this project, the propagation of WiFi signals in the 2.4 GHz and 5 GHz bands in a typical Nordic forest environment has been simulated using specialized radio propagation software employing ray-tracing and different diffraction models to evaluate the path loss and signal strength. The simulations show that the idea is feasible if high-gain directional antennas are employed, as connections of sufficiently high speed (400+ Mbps for the 5 GHz band) can potentially be established for typical working distances, i.e. 300m. We then designed a directional antenna system and evaluated it in a real Nordic forest environment. We found that by manually aligning the antennas in a forest, reliable connections could be achieved up to 50 m without line-of-sight, however higher distances result in significantly lower speeds (13.3 Mbps at 80 m and 1.21 Mbps at 100 m) due to antenna misalignment. It is however possible to construct a more accurate, automated alignment system, which could replicate the simulation results and fully solve the problem of communication.</p>
----------------------------------------------------------------------
In diva2:618115 missing missing some spaces, missing ligatures, and missing emphasis, the abstract should be:

<p>Tangible Interface Objects underpin the interactions between users and a SAR environment. When utilizing SAR for rapid-prototyping work flows, particularly when the subject of the prototyping is a user-input centric design, the role of the Tangible Interface Objects is crucial. A Tangible Interface Object with form or functionality that does not reflect that of its real-world counterpart is detrimental to the prototyping workflow, where realism in prototypes is highly sought after. Moving from the use of `dumb' input controls with SAR-emulated functionality to `intelligent', state-aware input controls can greatly aid the rapid-prototyping work flow, and SAR environments generally. This research examines two areas: integrating sensors into input controls to enhance both the <em>self-awareness</em> and the <em>local environmental-awareness</em> of the input control, and increasing state-awareness of traditional input controls such as switches and radial dials. This second area has a focus on input controls which do not require a traditional power source. The results from both these areas demonstrate that `intelligent' Tangible Interface Objects are viable, providing numerous benefits to SAR scenes, particularly in the realm of rapid-prototyping.</p>
----------------------------------------------------------------------
In diva2:1249024 merged paragraphs & words and missing ligatures, the abstract should be:

<p>In the near future, the decommissioning of large power plants is planned in the Nordic electric power system, due to environmental and market reasons. This will be countered by an increase in the wind power installed capacity, as well as by significant investments in the transmission system. In such a context, characterized by several changes, the Nordic power system might face reliability challenges.</p><p>This thesis aims to calculate the risk of power shortage in the different price areas which constitute the Nordic power system, for three different scenarios: a base scenario 2015, scenario 2020, and scenario 2025. Different case studies, focusing on the Nordic power system and on some of its subsystems, are investigated. The reliability evaluation which is carried out follows a probabilistic approach, by means of Monte Carlo simulations. Crude Monte Carlo, as well as an advanced variance reduction technique – namely Cross-Entropy based Importance Sampling (CEIS) – are applied and compared. An alternative sampling method based on stratified sampling is presented too.</p><p>The starting point of this thesis is Viktor Terrier's 2017 Master thesis, “North European Power Systems Reliability" [1]. Model-wise, among the other improvements, load and wind power are sampled in a different way to account for the correlation between them. Data-wise, more realistic assumptions are made and more accurate data are used, thanks also to the collaboration with Sweco Energuide AB, Department of Energy Markets.</p><p>From the model perspective, it is concluded that CEIS outperforms crude Monte Carlo when simulating small to medium size systems, but it cannot be successfully applied when simulating large and very reliable systems like the Nordic system as a whole. The presented alternative sampling method can however be used for such cases. From the numerical-results perspective, the drawn conclusion is that the Nordic power system is estimated to become more reliable by years 2020 and 2025. Even if partly intermittent, more generation capacity is expected to be available, and thanks to the significant investments which are planned in the transmission system, it will be possible to effectively transmit more power where needed, regardless of the area where it has been generated.</p><p>The thesis is carried out at KTH Royal Institute of Technology, Department of Electric Power and Energy Systems, in collaboration with Sweco Energuide AB, Department of Energy Markets, within the frame of the North European Energy Perspectives Project (NEPP).</p>
----------------------------------------------------------------------
In diva2:1498908 merged words, the abstract should be:

<p>The ion acceleration process in Bipolar High Power Impulse Magnetron Sputtering is investigated for use in a novel space propulsion system - the BP-HiPIMS thruster. The interest for BP-HiPIMS has recently been growing within the area of thin film deposition due to the theorised acceleration of target ions caused by the reversed pulse following the regular HiPIMS pulse. This same acceleration could be used to produce thrust in a space propulsion system, where the lack of physical grids and temporal separation of ionisation and acceleration are attractive benefits of the suggested system. In this paper the physical processes and parameters of importance are experimentally investigated to gain understanding of the ion acceleration process with the goal of verifying the theory of BP-HiPIMS thusters. Through plasma potential measurements a beneficial potential structure between the magnetic trap and bulk of the plasma which could potentially accelerate ions is found at certain discharge conditions and some acceleration of ions is confirmed in mass spectrometer measurements. The results are promising for a thruster application but further research is needed to evaluate the viability of the proposed system.</p>

Note: 'thuster' should be 'thruster' but the error is in the original thesis
----------------------------------------------------------------------
In diva2:872371 merged paragraphs & words and missing ligatures, the abstract should be:

<p>Shaft current protection in hydro and turbo generators is an important generator protection issue. Currents flowing in the generator shaft might damage generator bearings which, in turn, could reduce operating time and cause large financial losses. Therefore, it is important to prevent operation of the generator under conditions of high shaft currents.</p><p>In this project, task was to develop measurement and protection system that is able to operate under certain conditions. Measurement device has to be able to accurately measure currents lower than 1 A in a generator shaft that can vary in diameter from 16 cm up to 3 m. Also, those currents might appear in frequencies equal to multiples of line frequency. Device is to be located in a limited space and in a proximity of the generator. Thus, stray flux is expected which might influence measurements. Furthermore, since currents that have to be measured are low, output of a measurement device is usually a low level signal. Such signal had to be catered for and adapted in a way that it can be used with numerical relay.</p><p>After literature review and overview of possible solutions, Rogowski coil was chosen as the measurement device which will be further analysed. Two other current transformers were considered which served as a good comparison with Rogowski coil. Several different tests and measurements were made on mentioned measurement devices. Also, it was investigated how IEC61850-9-2 and Merging Unit (MU) could be used in this application. Upon this investigation, complete protection systems were assembled in the laboratory and they were tested. To asses the behaviour of different systems in the real environment, test installation was built in the hydro power plant, Hallstahammar. This installation included traditional systems, with measurement signals connected to the relaya, and the one which utilized concepts of Process Bus and Merging Unit. Measurements and tests that were made there served as a final proof of successfulness of protection systems. Results showed that Rogowski coil was a suitable choice for a measurement device due to its beneficial mechanical and electrical properties. Also, tests made with actual shaft current showed advantages of using Rogowski coil in pair with Merging Unit and process bus over traditional protection systems. Nevertheless,it was confirmed that both types of systems satisfy project requirements.</p>


Note "relayå" is in the original abstract.
----------------------------------------------------------------------
In diva2:1537890 many unnecessary spaces and merged words, the abstract should be:

<p>N̈ar automatiserade fordon introduceras i trafiken och beḧover interagera med m̈anskliga f̈orare ̈ar det viktigt att kunna f̈orutsp̊a m̈anskligt beteende. Detta f̈or att kunna erh̊alla en s̈akrare trafiksituation. I denna studie har en modellsom estimerar m̈anskligt beteende utvecklats. Estimeringarna ̈ar baserade p̊a en Hidden Markov Model d̈ar observationer anv̈ands f̈or att besẗamma k̈orstil hos omgivande fordon i trafiken. Modellen tr̈anas med tv̊a olika metoder: Baum Welch tr̈aning och Viterbi tr̈aning f̈or att f̈orb̈attra modellens prestanda. Tr̈aningsmetoderna utv̈arderas sedan genom att analysera deras tidskomplexitet och konvergens. Modellen ̈ar implementerad medoch utan tr̈aning och testad f̈or olika k̈orstilar. Erh̊allna resultat visar att tr̈aning ̈ar viktigt f̈or att kunna f̈orutsp̊a m̈anskligt beteende korrekt. Viterbi tr̈aning ̈ar snabbare men mer k̈anslig f̈or brus i j̈amf̈orelse med Baum Welch tr̈aning. Viterbi tr̈aningger ̈aven en bra estimering i de fall d̊a observerad tr̈aningsdata avspeglar f̈orarens k̈orstil, vilket inte alltid ̈ar fallet. Baum Welch tr̈aning ̈ar mer robust i s̊adana situationer. Slutligen rekommenderas en estimeringsmodell implementerad med Baum Welch tr̈aning f̈or att erh̊alla en s̈aker k̈orning d̊a automatiserade fordon introduceras i trafiken</p>
----------------------------------------------------------------------

The above were all sent on or before 2024-08-23
======================================================================
In diva2:800428 merged paragraphs & words and missing hyphen, the abstract should be:

<p>Moore’s law is the observation that over the years, the transistor density will increase, allowing billions of transistors to be integrated on a single chip. Over the last two decades, Moore’s law has enabled the implementation of complex systems on a single chip (SoCs). The challenge of the System-on-Chip (SoC) era was the demand of an efficient communication mechanism between the growing number of processing cores on the chip. The outcome established an new interconnection scheme (among others, like crossbars, rings, buses) based on the telecommunication networks and the Network-on-Chip (NoC) appeared on the scene.</p><p>The NoC has been developed not only to support systems embedded into a single processor, but also to support a set of processors embedded on a single chip. Therefore, the Multi-Processors System on Chip (MPSoC) has arisen, which incorporate processing elements, memories and I/O with a fixed interconnection infrastructure in a complete integrated system. In such systems, the NoC constitutes the backbone of the communication architecture that targets future SoC composed by hundred of processing elements. Besides that, together with the deep sub-micron technology progress, some drawbacks have arisen. The communication efficiency and the reliability of the systems rely on the proper functionality of NoC for on-chip data communication. A NoC must deal with the susceptibility of transistors to failure that indicates the demand for a fault tolerant communication infrastructure. A mechanism that can deal with the existence of different classes of faults (transient,intermittent and permanent [11]) which can occur in the communication network.I</p><p>n this thesis, different algorithms are investigated that implement fault tolerant techniques for permanent faults in the NoC. The outcome would be to deliver a fault tolerant mechanism for the NoC System Generator Tool [29] which is a research in Network-on-Chip carried out at the Royal Institute of Technology. It will be explicitly described the fault tolerant algorithm that is implemented in the switch in order to achieve packet rerouting around the faulty communication links.</p>
----------------------------------------------------------------------
Correction in:
In diva2:814393. merged words and missing paragraph separation, abstract should be:

<p>Stock trading is increasingly done pseudo-automatically or fully automatically, using algorithms which make day-to-day or even moment-to-moment decisions.</p><p>This report investigates the possibility of creating a virtual stock trader, using a method used in Artificial Intelligence, called Neural Networks, to make intelligent decisions on when to buy and sell stocks on the stock market.</p><p>We found that it might be possible to earn money over a longer period of time, although the profit is less than the average stock index. However, the method also performed well in situations where the stock index is going down.</p>


I missed the hyphen in "day-to-day"
----------------------------------------------------------------------
In diva2:1223867 merged paragraphs and a missing ligature, the abstract should be:

<p>Alzheimer’s disease (AD) was discovered 111 years ago by Alois Alzheimer. Today, it is the leading cause of dementia in elderly, and incidence is expected to increase with life expectancy. By 2050, the number of affected individuals is predicted to reach 10 million [1]. There have been numerous attempts to describe AD by its primary hallmarks, including amyloid plaques, amyloid beta (Aβ) oligomers, and tau tangles. However, despite several decades of intense research, the cause of AD remains unknown.</p><p>Recently, there has been a focus on the inflammatory components of AD. There is an extensive activation of the immune system within the CNS of AD patients, but neither its cause nor its role in AD is known. However, there are strong indications that the inflammation has an autoimmune character. Considering this, there is an imperative need to examine autoimmunity within AD. In the present study, a proteomic approach was used to determine the autoantibody profiles within plasma and cerebrospinal fluid (CSF) within AD patients and healthy controls.</p><p>Paired plasma and CSF samples from 23 healthy controls and 49 patients were included in the present study. In addition, 2 plasma samples and 18 CSF samples from patients were included (not paired). One 380-plex and one 314-plex targeted suspension bead array (SBA), each consisting of color-coded magnetic microspheres with immobilized antigens, were used to analyze autoantibody profiles in all samples. The resulting data revealed an increased autoantibody response towards anti-gens SLC17A6 (Solute Carrier Family 17 Member 6), MAP1A (Microtubule Associated Protein 1A), and MAP2 (Microtubule Associated Protein 2) in patients compared to healthy controls. However, as these antigens have displayed wide reactivities in previous, unpublished studies, they require further investigation to determine their role in AD.</p><p>Furthermore, the paired CSF and plasma samples were used to investigate the correlation of autoantibody profiles within patients. The correlation was found to follow a normal distribution, with correlation being higher in antigens displaying stronger autoantibody reactivity. This work represents one of the first large-scale studies on the correlation of autoantibody profiles in plasma and CSF.</p>
----------------------------------------------------------------------
The abstract for diva2:1147592 does not match the thesis! This abstract is for
some other document.
----------------------------------------------------------------------
The above were all sent on or before 2024-08-31
======================================================================
diva2:778828 contains equations that use the Unicode Supplementary Private Use Area-B, for example the character U+100D46. However, as there is no full text in DiVA, I cannot make sense of these equations.

Additionally, the abstract seems to have had each line of text entered as a paragraph.
----------------------------------------------------------------------
In diva2:952512 missing "E" in "measurement" and merged paragraphs, the abstract should be:

<p>Wireless microphone systems are set up in schools to improve the sound quality for students with hearing difficulties. Karolinska Universitetssjukhuset is responsible for which resources to use and they purchase systems from different providers. The clients at Karolinska want the ability to check that the specifications of the systems are correct. Aims and objectives of the controls are to: 1. create a basis for questioning the providers’ specifications, 2. create a basis for improving communication with providers, 3. examining and comparing different microphone system for future investments and 4. detect individual defects. The object of this project is to find a suitable measurement equipment and to develop a measurement method to fulfill the aims 1-4.</p><p> In order to satisfy these aims, a number of interviews with experts in audio and microphone systems has been done. Based on the interviews CLIO Pocket was purchased and a measurement method has been developed. The measurement method describes how to use CLIO Pocket in order to check the systems’ bandwidth, noise, dynamic range, and total harmonic distorsion. Aims 2, 3 and 4 have been fulfilled, however Aim 1 was not fulfilled due to inadequate funding which lead to deficiencies in equipment.</p>
----------------------------------------------------------------------
Duplicates: diva2:635950 has no full text, while the version at diva2:646620 does
----------------------------------------------------------------------
In diva2:460449
"<p>[1]Förekomst av industriellt spillvärme vid låga temperaturer,  Ingrid Nyström, Per-Åke Franck, Industriell Energianalys AB, 2002-04-15</p>"

should be

"<p lang='sv'>[1]Förekomst av industriellt spillvärme vid låga temperaturer,  Ingrid Nyström, Per-Åke Franck, Industriell Energianalys AB, 2002-04-15</p>"
----------------------------------------------------------------------
The above were all sent on or before 2024-09-04
======================================================================
possible duplicates: diva2:751100 and diva2:751697
	 	     diva2:721862 and diva2:776949 - note diva2:721862 does not have full text, where as the other does
----------------------------------------------------------------------
The full text to diva2:1770888 does not currespond to the thesis. It actually
points to the text for diva2:1770883
----------------------------------------------------------------------
diva2:893748 missing space after "utav" in title
----------------------------------------------------------------------
In diva2:1229795, the title is missing a space:
"Integrating membrane filtration forwater reuse in tissue mill"
should be:
"Integrating membrane filtration for water reuse in tissue mill"
----------------------------------------------------------------------
In diva2:1219371 should 'ClickMucins' be 'Click Mucins'?
There is not full text in DiVA. But the two words seems to be used in the scientific literature.
----------------------------------------------------------------------
In diva2:756805 the word "Hope" should be "Hops" - the error is in the original thesis
----------------------------------------------------------------------
In diva2:1352888
There title is "Performance monitoring of systems for airpuricationAuthor:Anders"
should be:     "Performance monitoring of systems for air purification"
----------------------------------------------------------------------
In diva2:1159711, there are no fonts - it seems to just be a bit scan of the document.
PDF is images of the pages
----------------------------------------------------------------------
In diva2:1741440, there are no fonts - it seems to just be a bit scan of the document.
----------------------------------------------------------------------
In diva2:802843 'WU' is likely to be 'EU' - no full text in DiVA
----------------------------------------------------------------------
In diva2:722717 each line set as a paragraph!
----------------------------------------------------------------------
In diva2:730268 there is a strange character coding of text, so one cannot easily search for a string
----------------------------------------------------------------------
The above were all sent on or before 2024-09-19
======================================================================
In diva2:1154619 "triglycer ides" shouold be "triglycerides"
this will make it match the Swedish abstract.
----------------------------------------------------------------------
In diva2:1741633 - the PDF is just images of the pages
----------------------------------------------------------------------
The equation n page 28 of https://kth.diva-portal.org/smash/get/diva2:848284/FULLTEXT01.pdf seems to be odd.
It looks like a CAPTCHA test!
-------------------------------------------------------------------------------
In https://kth.diva-portal.org/smash/get/diva2:757292/FULLTEXT01.pdf there are only the two abstracts, there is no additional text.
----------------------------------------------------------------------
In diva2:1577129 the full text has a strange character mapping.
----------------------------------------------------------------------
In diva2:1412560 - as space missing in title: "Automated Intro Detection ForTV Series"
                                    should be "Automated Intro Detection For TV Series"
----------------------------------------------------------------------
In diva2:1785530 - the PDF is just pictures of pages - it is not searchable.
----------------------------------------------------------------------
----------------------------------------------------------------------
Abstract corrections - starting with the one that has the most merged words
----------------------------------------------------------------------
In diva2:1219373 abstract is: <p>The prevalence of antibiotic resistantmicroorganisms is rising and has become one ofthe biggest threats to global health. As a result ofthe decrease of antibiotic effectiveness it hasbecome desirable to develop new technologiesto circumvent or reduce antibiotics usage.Particularly, in surgeries where implants andprosthetic devices are incorporated into the bodyantibiotics are critical to prevent infections. Analternative strategy to prevent extensive use ofantibiotics in this field is to create antimicrobialsurfaces. In this report, antimicrobial surfaceswere created by coupling antibacterial and antibiofilmenzymes to the recombinant spider silk4RepCT. The coupled enzyme were theendolysins Sal-1 and PlysS2, the catalyticdomain of Sal-1 called CHAP and Dispersin B.The coupling was obtained by Sortase Amediated protein conjugation, and investigatedwith SDS-PAGE analysis for coupling reactionin solution and with Octet-analysis for couplingreaction with 4RepCT in coating format.Furthermore, the enzymatic activity of theenzyme coupled spider silk was investigatedwith Turbidity Reduction Assay, where thereduction in OD600 of Staphylococcus aureusbacteria was measured. For Sal-1 and DispersinB an activity test with a substrate addition assaywas also performed. The coupling reaction insolution showed successful coupling for CHAPand Sal-1. Coupling reactions in solution withDispersin B and PlySs2 did not clearly indicatesuccessful coupling. However, the Octetanalysisindicated successful coupling with allenzymes. Enzymatic activity could bedemonstrated for Sal-1 coupled spider silk in theTRA and a substrate addition assay withFluorescein-Di-b-D-Galactopyranoside (FDG).In the TRA, Sal-1 coupled spider silk coatings,prepared by using 17.3 μM of 4RepCT forcoatings and 17.3 μM of Sal-1 for coupling3(17.3/17.3-coating), were shown to reduceOD600 with 62 % after 100 minutes. For Sal-1coupled spider silk coatings, prepared by using17.3 μM Sal-1 and 4.3 μM of 4RepCT(17.3/4.3-coating), the OD600 reduction was 48% after 100 minutes. The OD600 reduction of17.3/17.3-coatings and 17.3/4.3-coatings wereconsidered comparable with 25 nM soluble Sal-1 and 12.5 nM soluble Sal-1 respectively.</p>

corrected abstract: 
<p>The prevalence of antibiotic resistant microorganisms is rising and has become one of the biggest threats to global health. As a result of the decrease of antibiotic effectiveness it has become desirable to develop new technologies to circumvent or reduce antibiotics usage. Particularly, in surgeries where implants and prosthetic devices are incorporated into the body antibiotics are critical to prevent infections. An alternative strategy to prevent extensive use of antibiotics in this field is to create antimicrobialsurfaces. In this report, antimicrobial surfaces were created by coupling antibacterial and antibiofilmenzymes to the recombin ant spider silk 4RepCT. The coupled enzyme were the endolysins Sal-1 and PlySs2, the catalytic domain of Sal-1 called CHAP and Dispersin B. The coupling was obtained by Sortase A mediated protein conjugation, and investigated with SDS-PAGE analysis for coupling reaction in solution and with Octet-analysis for coupling reaction with 4RepCT in coating format. Furthermore, the enzymatic activity of the enzyme coupled spider silk was investigated with Turbidity Reduction Assay, where the reduction in OD600 of Staphylococcus aureusbacteria was measured. For Sal-1 and DispersinB an activity test with a substrate addition assay was also performed. The coupling reaction in solution showed successful coupling for CHAP and Sal-1. Coupling reactions in solution with Dispersin B and PlySs2 did not clearly indicate successful coupling. However, the Octet-analysis is indicated successful coupling with all enzymes. Enzymatic activity could be demonstrated for Sal-1 coupled spider silk in the TRA and a substrate addition assay with Fluorescein-Di-b-D-Galactopyranoside (FDG). In the TRA, Sal-1 coupled spider silk coatings, prepared by using 17.3 μM of 4RepCT for coatings and 17.3 μM of Sal-1 for coupling 3(17.3/17.3-coating), were shown to reduce OD600 with 62 % after 100 minutes. For Sal-1 coupled spider silk coatings, prepared by using 17.3 μM Sal-1 and 4.3 μM of 4RepCT(17.3/4.3-coating), the OD600 reduction was 48% after 100 minutes. The OD600 reduction of 17.3/17.3-coatings and 17.3/4.3-coatings were considered comparable with 25 nM soluble Sal-1 and 12.5 nM soluble Sal-1 respectively.</p>
----------------------------------------------------------------------
In diva2:826712 abstract is: <p>Telge Nät is the current owner and operator for Södertäljes district heating system. In this project a part of this system which includes Scania industial area as well as the residential areas of Pershagenand Värdsholmen will be investigated. The heating for this system is provided partly by thecogeneration plant of Igelsta as well as from heat production and recovery within the area of Scania.The main purpose of this project is to investigate whether or not these unusual operationcircumstances result in any temperature or pressure fluctuations which could lead to advancedfatigue on the system. Temperature fluctuations have been analyzed for main pipe as well as forB006, B210 and Clab which are all larger complexes within the area.Pressure fluctuations have been analyzed for the complex of B210 where hot water as well assuperheated water is used. This leads to intense pump and valve adjustments which is likely to causea lot of pressure fluctuations. The pressure is also measured at the main pipe to investigate if thefluctuations from B210 spread to other parts of the system.Analysis of the temperature fluctuations data from 2012 showed a correlation between the outdoorstemperature and the temperature within the district heating system. Following investigationsmeasured the amount of temperature cycles at the different complexes at several differentmagnitudes. The Temperature cycles were thereafter converted into full temperature cycleequivalents at 110°C using Palmgren-Miners hypothesis. These results were used to maketemperature fatigue estimations for the different complexes. The estimations showed that Clab wasa relatively stable system and that the fluctuations were kept within an acceptable range. Thetemperature of the feed pipe at B006 was proven to be quite unstable; this could be a direct result ofthe overall instabilities on the main pipe caused by heat production and heat recovery within Clabsand B210.The temperature at B210 was very unstable for the superheated water return pipe and extremelyunstable for the hot water return pipe. This instability could in the long run lead to a severelydecreased lifetime for the whole system. The instability is likely to be caused by a constant feed ofsuperheated water for heating at a paint shop which is located within B210, water that is fedregardless if there is need for heating in the paint shop or not. Excess superheated water which is notused for heating the paint shop is redirected to heat areas and pipes where hot water is normally theheat source, something which leads to large fluctuations on the system. By reducing the excess feedof superheated water to the paint shop a large portion of the problems with the systems could besolved.The pressure fluctuations at B210 were analyzed with a measure interval of 0.1 seconds and 30seconds between the measurements. This was done to determine whether or not the intervalbetween the measurements had a big influence on the registered pressure transient. Analysisindicated that pressure transients where registered as slightly bigger and relatively faster when themeasure interval of 0.1 seconds was used. It is however known that even a measure interval of 0.1seconds is far too slow to provide any results on the actual magnitude of the pressure transient. Theresult of this analyze should therefore not be considered as definitive.The largest pressure transients at B210 were registered at the startup of the paint shop. These werehowever still at a level where no damage is expected to occur on the system. Measurements at themain pipe showed no traces of the pressure transients from B210. This leads to the concussion thatthe analyzed pressure fluctuations are kept within an acceptable range.</p>

w='industial' val={'c': 'industrial', 's': 'diva2:826712', 'n': 'error in original'}

corrected abstract:
<p>Telge Nät is the current owner and operator for Södertäljes district heating system. In this project apart of this system which includes Scania industial area as well as the residential areas of Pershagen and Värdsholmen will be investigated. The heating for this system is provided partly by the cogeneration plant of Igelsta as well as from heat production and recovery within the area of Scania. The main purpose of this project is to investigate whether or not these unusual operation circumstances result in any temperature or pressure fluctuations which could lead to advanced fatigue on the system. Temperature fluctuations have been analyzed for main pipe as well as for B006, B210 and Clab which are all larger complexes within the area. Pressure fluctuations have been analyzed for the complex of B210 where hot water as well as superheated water is used. This leads to intense pump and valve adjustments which is likely to cause a lot of pressure fluctuations. The pressure is also measured at the main pipe to investigate if the fluctuations from B210 spread to other parts of the system. Analysis of the temperature fluctuations data from 2012 showed a correlation between the outdoors temperature and the temperature within the district heating system. Following investigations measured the amount of temperature cycles at the different complexes at several different magnitudes. The Temperature cycles were thereafter converted into full temperature cycle equivalents at 110°C using Palmgren-Miners hypothesis. These results were used to make temperature fatigue estimations for the different complexes. The estimations showed that Clab wasa relatively stable system and that the fluctuations were kept within an acceptable range. The temperature of the feed pipe at B006 was proven to be quite unstable; this could be a direct result of the overall instabilities on the main pipe caused by heat production and heat recovery within Clabs and B210. The temperature at B210 was very unstable for the superheated water return pipe and extremely unstable for the hot water return pipe. This instability could in the long run lead to a severely decreased lifetime for the whole system. The instability is likely to be caused by a constant feed of superheated water for heating at a paint shop which is located within B210, water that is fed regardless if there is need for heating in the paint shop or not. Excess superheated water which is not used for heating the paint shop is redirected to heat areas and pipes where hot water is normally the heat source, something which leads to large fluctuations on the system. By reducing the excess feed of superheated water to the paint shop a large portion of the problems with the systems could be solved. The pressure fluctuations at B210 were analyzed with a measure interval of 0.1 seconds and 30 seconds between the measurements. This was done to determine whether or not the interval between the measurements had a big influence on the registered pressure transient. Analysis indicated that pressure transients where registered as slightly bigger and relatively faster when the measure interval of 0.1 seconds was used. It is however known that even a measure interval of 0.1 seconds is far too slow to provide any results on the actual magnitude of the pressure transient. The result of this analyze should therefore not be considered as definitive. The largest pressure transients at B210 were registered at the startup of the paint shop. These were however still at a level where no damage is expected to occur on the system. Measurements at the main pipe showed no traces of the pressure transients from B210. This leads to the concussion that the analyzed pressure fluctuations are kept within an acceptable range.</p>
----------------------------------------------------------------------
In diva2:1447055 abstract is: <p>In the recent years microplastics in the marine environment has been recognized as a potentiallyimportant environmental issue. Today there are microplastics spread in the waterbodies all overthe world, from the equator to the poles in south and north. In 2016 artificial turf was labeled thesecond largest source of microplastics to the marine environment in Sweden [1]. Football is thenational sport of Sweden and accounts for the majority of the activity hours among the youth inSweden. The artificial turf has made it possible for more children to play football and for them toget more hours on the field. Today about 90 % of the football players play on artificial turf [2].The microplastics pathways to the nature and the marine environment were studied andtreatment methods were developed. One of these methods is the so called granule trap, a filterbag which is placed in a stormwater drainage well to catch the rubber granulates and the artificialturf fibers which can be spread from the artificial field to the drainage system. The aim of thisstudy was to optimize the granule trap for possible waterflows to the stormwater drainage welland its efficiency to catch microplastics. This was researched through field studies of the efficiencyof the granule trap at two artificial turfs in Stockholm and the development of a waterflow modelof an artificial turf with varying construction.The rainfall which was used in the waterflow model was the 10-year storm with a duration of 10minutes. This to find the maximum waterflow the granuletraps must manage. The waterflows tothe stormwater drainage well were dependent on the number of wells placed around the artificialturf, in which area of Sweden the football field was placed, in other words the amount of rain thatfell, and the infiltration capacity of the artificial turf. The waterflow model works as a templatefor possible waterflows at an artificial turf with a certain construction and at a certain location inSweden.The artificial turfs which were examined in the field studies were Skytteholms IP in Solna andSpånga IP in Stockholm. At each football field 6 granuletraps were placed, each loaded with twofilter bags, the inner with larger sized mesh and the outer with smaller sized mesh. The mesh sizecombinations were 200 μm with 100 μm, 200 μm with 50 μm and 100 μm with 50 μm. atSkytteholms IP a total amount of 10.3 kg microplastics were caught and at Spånga IP a total of 1.5kg microplastics were caught during the 49 days the granuletraps were placed at the footballfields. Out of the total amount of microplastics in each granuletrap at least 99 % by mass was inthe inner filter bag and maximum 1 % by mass was in the outer filter bag, in the size fractionbetween the outer and the inner filter bag..In conclusion this study shows that the waterflow to the stormwater drainage wells placed aroundthe artificial turfs vary a lot depending on the construction of the artificial turf. Foremost itdepends on the infiltration capacity of the artificial turf and the number of stormwater drainagewells around the field. With regards to the waterflows from the waterflow model and the resultsfrom the field studies the recommended mesh size for the filter bags is 200 μm. This since at least99 % by mass of the microplastics, which were larger than 50 μm, that reached the granule trapsIVwere trapped in the inner filter bag and the elevated risk of clogging and biofilm growth on thefilter bags with smaller mesh size. Further studies should be conducted on the waterflow throughthe granuletraps over time, microplastics smaller than 50 μm, other pathways for themicroplastics away from the artificial turf, improved constructions of artificial turfs and improvedmaintenance on the artificial turfs to reduce the risk of spreading of microplastics from artificialturfs.</p>

w='trapsIVwere' val={'c': 'traps were', 's': 'diva2:1447055', 'n': 'The "IV" was from the page number!'}

corrected abstract:
<p>In the recent years microplastics in the marine environment has been recognized as a potentially important environmental issue. Today there are microplastics spread in the waterbodies all over the world, from the equator to the poles in south and north. In 2016 artificial turf was labeled the second largest source of microplastics to the marine environment in Sweden [1]. Football is the national sport of Sweden and accounts for the majority of the activity hours among the youth in Sweden. The artificial turf has made it possible for more children to play football and for them to get more hours on the field. Today about 90 % of the football players play on artificial turf [2].</p><p>The microplastics pathways to the nature and the marine environment were studied and treatment methods were developed. One of these methods is the so called granule trap, a filter bag which is placed in a stormwater drainage well to catch the rubber granulates and the artificial turf fibers which can be spread from the artificial field to the drainage system. The aim of this study was to optimize the granule trap for possible waterflows to the stormwater drainage well and its efficiency to catch microplastics. This was researched through field studies of the efficiency of the granule trap at two artificial turfs in Stockholm and the development of a waterflow model of an artificial turf with varying construction.</p><p>The rainfall which was used in the waterflow model was the 10-year storm with a duration of 10 minutes. This to find the maximum waterflow the granuletraps must manage. The waterflows to the stormwater drainage well were dependent on the number of wells placed around the artificial turf, in which area of Sweden the football field was placed, in other words the amount of rain that fell, and the infiltration capacity of the artificial turf. The waterflow model works as a template for possible waterflows at an artificial turf with a certain construction and at a certain location in Sweden.</p><p>The artificial turfs which were examined in the field studies were Skytteholms IP in Solna and Spånga IP in Stockholm. At each football field 6 granuletraps were placed, each loaded with two filter bags, the inner with larger sized mesh and the outer with smaller sized mesh. The mesh size combinations were 200 μm with 100 μm, 200 μm with 50 μm and 100 μm with 50 μm. at Skytteholms IP a total amount of 10.3 kg microplastics were caught and at Spånga IP a total of 1.5 kg microplastics were caught during the 49 days the granuletraps were placed at the football fields. Out of the total amount of microplastics in each granuletrap at least 99 % by mass was in the inner filter bag and maximum 1 % by mass was in the outer filter bag, in the size fraction between the outer and the inner filter bag..</p><p>In conclusion this study shows that the waterflow to the stormwater drainage wells placed around the artificial turfs vary a lot depending on the construction of the artificial turf. Foremost it depends on the infiltration capacity of the artificial turf and the number of stormwater drainage wells around the field. With regards to the waterflows from the waterflow model and the results from the field studies the recommended mesh size for the filter bags is 200 μm. This since at least 99 % by mass of the microplastics, which were larger than 50 μm, that reached the granuletraps were trapped in the inner filter bag and the elevated risk of clogging and biofilm growth on the filter bags with smaller mesh size. Further studies should be conducted on the waterflow through the granuletraps over time, microplastics smaller than 50 μm, other pathways for the microplastics away from the artificial turf, improved constructions of artificial turfs and improved maintenance on the artificial turfs to reduce the risk of spreading of microplastics from artificial turfs.</p>
----------------------------------------------------------------------
In diva2:1770583 abstract is: <p>One of the biggest challenges in healthcare is Emergency Department (ED)crowding which creates high constraints on the whole healthcare system aswell as the resources within and can be the cause of many adverse events.Is is a well known problem were a lot of research has been done and a lotof solutions has been proposed, yet the problem still stands unsolved. Byanalysing Real-World Data (RWD), complex problems like ED crowding couldbe better understood. Currently very few applications of survival analysis hasbeen adopted for the use of production data in order to analyze the complexityof logistical problems. The aims for this thesis was to apply survival analysisthrough advanced Machine Learning (ML) models to RWD collected at aSwedish hospital too see how the Length Of Stay (LOS) until admission ordischarge were affected by different factors. This was done by formulating thecrowding in the ED for survival analysis through the use of the LOS as thetime and the decision regarding admission or discharge as the event in order tounfold the clinical complexity of the system and help impact clinical practiceand decision making.By formulating the research as time-to-event in combination with ML, thecomplexity and non linearity of the logistics in the ED is viewed from a timeperspective with the LOS acting as a Key Performance Indicator (KPI). Thisenables the researcher to look at the problem from a system perspective andshows how different features affect the time that the patient are processedin the ED, highlighting eventual problems and can therefore be useful forimproving clinical decision making.</p><p>Five models: Cox Proportional Hazards(CPH), Random Survival Forests (RSF), Gradient Boosting (GB), ExtremeGradient Boosting (XGB) and DeepSurv were used and evaluated using theConcordance index (C-index) were GB were the best performing model witha C-index of 0.7825 showing that the ML models can perform better than thecommonly used CPH model. The models were then explained using SHapleyAdaptive exPlanations (SHAP) values were the importance of the featureswere shown together with how the different features impacted the LOS. TheSHAP also showed how the GB handled the non linearity of the features betterthan the CPH model. The five most important features impacting the LOS wereif the patient received a scan at the ED, if the visited and emergency room,age, triage level and the label indicating what type of medical team seemsmost fit for the patient. This is clinical information that could be implementedto reduce the crowding through correct decision making. These results show that ML based survival analysis models can be used for further investigationregarding the logistic challenges that healthcare faces and could be furtherused for data analysis with production data in similar cases. The ML survivalanalysis pipeline can also be used for further analysis and can act as a first stepin order to pinpoint important information in the data that could be interestingfor deeper data analysis, making the process more efficient.</p>

corrected abstract:
<p>One of the biggest challenges in healthcare is Emergency Department (ED) crowding which creates high constraints on the whole healthcare system as well as the resources within and can be the cause of many adverse events. Is is a well known problem were a lot of research has been done and a lot of solutions has been proposed, yet the problem still stands unsolved. By analysing Real-World Data (RWD), complex problems like ED crowding could be better understood. Currently very few applications of survival analysis has been adopted for the use of production data in order to analyze the complexity of logistical problems. The aims for this thesis was to apply survival analysis through advanced Machine Learning (ML) models to RWD collected at a Swedish hospital too see how the Length Of Stay (LOS) until admission or discharge were affected by different factors. This was done by formulating the crowding in the ED for survival analysis through the use of the LOS as the time and the decision regarding admission or discharge as the event in order to unfold the clinical complexity of the system and help impact clinical practice and decision making.</p><p>By formulating the research as time-to-event in combination with ML, the complexity and non linearity of the logistics in the ED is viewed from a time perspective with the LOS acting as a Key Performance Indicator (KPI). This enables the researcher to look at the problem from a system perspective and shows how different features affect the time that the patient are processed in the ED, highlighting eventual problems and can therefore be useful for improving clinical decision making. Five models: Cox Proportional Hazards (CPH), Random Survival Forests (RSF), Gradient Boosting (GB), Extreme Gradient Boosting (XGB) and DeepSurv were used and evaluated using the Concordance index (C-index) were GB were the best performing model with a C-index of 0.7825 showing that the ML models can perform better than the commonly used CPH model. The models were then explained using SHapley Adaptive exPlanations (SHAP) values were the importance of the features were shown together with how the different features impacted the LOS. The SHAP also showed how the GB handled the non linearity of the features better than the CPH model. The five most important features impacting the LOS were if the patient received a scan at the ED, if the visited and emergency room, age, triage level and the label indicating what type of medical team seems most fit for the patient. This is clinical information that could be implemented to reduce the crowding through correct decision making. These results show that ML based survival analysis models can be used for further investigation regarding the logistic challenges that healthcare faces and could be further used for data analysis with production data in similar cases. The ML survival analysis pipeline can also be used for further analysis and can act as a first step in order to pinpoint important information in the data that could be interesting for deeper data analysis, making the process more efficient.</p>
----------------------------------------------------------------------
In diva2:573478 abstract is: <p><strong>Background: </strong>Since1994, the EU NickelDirective is limiting the release of nickel from objects that are in prolongedcontact with skin. Despite this, nickel is still the most common cause of contactallergy in industrialized countries. This could implicate that other productswith short contact to the skin, which are not restricted by EU´s legislation inREACH (Registration, Evaluation, Authorisation and Restriction of Chemicalsubstances), could be a source of nickel or cobalt exposure. There is no limitfor cobalt release within REACH. To determine if there is cobalt or nickelpresent on the surface of a material and if metal can be released in contactwith the skin, there are quantitative and qualitative methods, such as the DMG(dimethylglyoxime)-test, the cobalt spot-test and a method for releasedetermination according to EN1811.<strong>Objectives: </strong>Laptops are not included in the REACH legislation. The purpose of thisstudy was to investigate whether nickel or cobalt can be found on computersurfaces and if nickel and cobalt can be released from surfaces that are incontact with skin, during work with laptops. Are there any variations betweendifferent models or different brands of laptops (two years old or less) andwhat levels of nickel and cobalt are released from different surfaces on a specificHP computer, that gave positive results for nickel and cobalt in the spot- tests? <strong>Methodand materials: </strong>In this study 30laptops (7 Hewlett Packard (HP), 18 Dell, 3 Sony Vaio, 1Fujitsu and 1 Macbook), weretested by using DMG-test and cobalt spot-test. To measure the release of nickeland cobalt on one specific laptop (HP2560p), the standard method EN1811 and amodified version of the method, were used. <strong>Results: </strong>Laptopsof different models and manufacturers were tested for nickel by using theDMG-test. The test turned out positive for all tested laptops from HP and 4 of18 of the laptops from Dell. For cobalt, onlyone laptop (HP) of the total 30 computers, gave a positive result. The resultsfrom the release test of a specific laptop, was lower than the limit thresholdvalue within REACH (0.5µg/cm<sup>2</sup>/week). The highest amounts of nickel,0.1μg/cm<sup>2</sup>/week, were measured for the computer components thatwere derived from the palm rest. All the cobalt levels, except one, were belowthe detection limit for the analysis. <strong>Conclusions: </strong>Our study indicatesthat laptops are a potential source for nickel deposition onto skin. In this study, computers from five differentmanufacturers have been tested, and it turned out that the tested models from HPand Dell released nickel. Cobalt was only released in very low concentrations,at a level near the LOD for the analysis.</p>

corrected abstract:
<p><strong>Background: </strong>Since 1994, the EU Nickel Directive is limiting the release of nickel from objects that are in prolonged contact with skin. Despite this, nickel is still the most common cause of contact allergy in industrialized countries. This could implicate that other products with short contact to the skin, which are not restricted by EU´s legislation in REACH (Registration, Evaluation, Authorisation and Restriction of Chemical substances), could be a source of nickel or cobalt exposure. There is no limit for cobalt release within REACH. To determine if there is cobalt or nickel present on the surface of a material and if metal can be released in contact with the skin, there are quantitative and qualitative methods, such as the DMG (dimethylglyoxime)-test, the cobalt spot-test and a method for release determination according to EN1811.<strong>Objectives: </strong>Laptops are not included in the REACH legislation. The purpose of this study was to investigate whether nickel or cobalt can be found on computer surfaces and if nickel and cobalt can be released from surfaces that are in contact with skin, during work with laptops. Are there any variations between different models or different brands of laptops (two years old or less) and what levels of nickel and cobalt are released from different surfaces on a specific HP computer, that gave positive results for nickel and cobalt in the spot- tests? <strong>Method and materials: </strong>In this study 30 laptops (7 Hewlett Packard (HP), 18 Dell, 3 Sony Vaio, 1 Fujitsu and 1 Macbook), were tested by using DMG-test and cobalt spot-test. To measure the release of nickel and cobalt on one specific laptop (HP2560p), the standard method EN1811 and a modified version of the method, were used. <strong>Results: </strong>Laptops of different models and manufacturers were tested for nickel by using the DMG-test. The test turned out positive for all tested laptops from HP and 4 of 18 of the laptops from Dell. For cobalt, only one laptop (HP) of the total 30 computers, gave a positive result. The results from the release test of a specific laptop, was lower than the limit threshold value within REACH (0.5µg/cm<sup>2</sup>/week). The highest amounts of nickel, 0.1μg/cm<sup>2</sup>/week, were measured for the computer components that were derived from the palm rest. All the cobalt levels, except one, were below the detection limit for the analysis. <strong>Conclusions: </strong>Our study indicates that laptops are a potential source for nickel deposition onto skin. In this study, computers from five different manufacturers have been tested, and it turned out that the tested models from HP and Dell released nickel. Cobalt was only released in very low concentrations, at a level near the LOD for the analysis.</p>
----------------------------------------------------------------------
In diva2:1170378 abstract is: <p>Nitrogen reduction is an important process that many of the larger treatment plants in Swedenuse since nitrogen affects the environment and the global warming. An efficient method to getrid of nitrogen is to use a process called denitrification. In this process a carbon source is usuallyused and has a main purpose to reduce the nitrogen to more stable and less reactive molecules.There are many parameters to take into consideration when choosing the most suitable carbonsource for the specific treatment plant. The reason for this is that the carbon sources that areavailable are more or less effective in different environments which depends on parameters suchas pH levels and temperatures.</p><p>SÖRAB use Brenntaplus as carbon source in their denitrification process to achieve a reductionof nitrogren. They have recently looked at alternative carbon sources that could replace parts ortheir current one. The primary aim with this project was to investigate if the glycol that SÖRABreceive as waste from the public could be used as carbon source for the leachate, or if it containstoo much dangerous chemicals and heavy metals that will affect the wastewater. In addition tothe glycol, there was some focus was on comparing some other carbon sources to each other inan economic and environmental perspective.</p><p>To determine wheter the incoming glycol is suitable to use as a carbon source or not, severalsamplings were made on different batches of the incoming glycol. The purpose of this was tosee if there was a variety in the composition of the glycol from batch to batch. To determine ifthe content of the glycol would affect the wastewater by raising the concentration of dangerouschemicals to such a high values that the wastewater would not meet the emission requirementsthat SÖRAB has set, a calculation were made to see how much the glycol would affect theleachate.</p><p>When reducing nitrogen in the presence of a carbon source it is important to acknowledge thatthe amount of COD in the carbon source determines how much nitrogen that can be reduced.The problem that occurred is the lack of practical testing in the process makes it hard to give aprecise number of how much carbon source that is needed to give the desired nitrogen reduction.</p><p>The carbon sources that were investigated were ethanol, methanol, glycerol, acetic acid and thecurrent one, Brenntaplus. From an economical perspective the price for respective carbon sourcewere the same if looking one the amount needed in consideration to the price per kilo for eachone. The carbon sources work in different conditions and need different requirements to workfunctionally. Some of the carbon sources are an explosive risk and need special handling andsome sources have a very high freezing temperature which requires special storage.</p><p>The conclusion that can be drawn from this master thesis is that every individual treatment plantreacts different to different carbon sources and it is really hard to theoretically estimate whatamount that needs to be dosed. Theoretical a reference value can be determined and in regard tothis the dosage can be adjusted with several tests until an amount of carbon source that give thewanted nitrogen reduction is found. The deposited glycol that SÖRAB receives from theirrecyclingstations is considered to not be suitable for dosage because the composition differsfrom batch to batch.</p>

w='nitrogren' val={'c': 'nitrogen', 's': 'diva2:1170378', 'n': 'error in original'}
note "recyclingstations" set as one word in the original


corrected abstract: 
<p>Nitrogen reduction is an important process that many of the larger treatment plants in Sweden use since nitrogen affects the environment and the global warming. An efficient method to get rid of nitrogen is to use a process called denitrification. In this process a carbon source is usually used and has a main purpose to reduce the nitrogen to more stable and less reactive molecules. There are many parameters to take into consideration when choosing the most suitable carbon source for the specific treatment plant. The reason for this is that the carbon sources that are available are more or less effective in different environments which depends on parameters such as pH levels and temperatures.</p><p>SÖRAB use Brenntaplus as carbon source in their denitrification process to achieve a reduction of nitrogren. They have recently looked at alternative carbon sources that could replace parts or their current one. The primary aim with this project was to investigate if the glycol that SÖRAB receive as waste from the public could be used as carbon source for the leachate, or if it contains too much dangerous chemicals and heavy metals that will affect the wastewater. In addition to the glycol, there was some focus was on comparing some other carbon sources to each other in an economic and environmental perspective.</p><p>To determine wheter the incoming glycol is suitable to use as a carbon source or not, several samplings were made on different batches of the incoming glycol. The purpose of this was to see if there was a variety in the composition of the glycol from batch to batch. To determine if the content of the glycol would affect the wastewater by raising the concentration of dangerous chemicals to such a high values that the wastewater would not meet the emission requirements that SÖRAB has set, a calculation were made to see how much the glycol would affect the leachate.</p><p>When reducing nitrogen in the presence of a carbon source it is important to acknowledge that the amount of COD in the carbon source determines how much nitrogen that can be reduced. The problem that occurred is the lack of practical testing in the process makes it hard to give a precise number of how much carbon source that is needed to give the desired nitrogen reduction.</p><p>The carbon sources that were investigated were ethanol, methanol, glycerol, acetic acid and the current one, Brenntaplus. From an economical perspective the price for respective carbon source were the same if looking one the amount needed in consideration to the price per kilo for each one. The carbon sources work in different conditions and need different requirements to work functionally. Some of the carbon sources are an explosive risk and need special handling and some sources have a very high freezing temperature which requires special storage.</p><p>The conclusion that can be drawn from this master thesis is that every individual treatment plant reacts different to different carbon sources and it is really hard to theoretically estimate what amount that needs to be dosed. Theoretical a reference value can be determined and in regard to this the dosage can be adjusted with several tests until an amount of carbon source that give the wanted nitrogen reduction is found. The deposited glycol that SÖRAB receives from their recyclingstations is considered to not be suitable for dosage because the composition differs from batch to batch.</p>
----------------------------------------------------------------------
In diva2:1217827 abstract is: <p>In kraft pulping, one of the main issues is the extensive wood losses. With increasing prices ofwoody biomass an incentive towards minimizing the wood losses exists. Amongst the variousprocess steps, the impregnation of wood chips has shown to enhance the cooking by providinga homogeneous distribution of chemicals inside the chips. It is proven that a more proficientimpregnation phase can improve the overall yield in kraft pulping. However, there is a lack ofscientific research comparing different impregnation techniques for hardwood. Hence, thisthesis will attempt to clarify the impregnation of hardwood.The impregnation efficiency was studied by comparing three different impregnation methods:High Alkali Impregnation (HAI), Extended Impregnation (EI) using a low alkali level and aReference Impregnation (REF) to enable a comparison to the industrially establishedconditions. The cases were compared by analysing the yield, selectivity and homogeneity. Thecomparison was also made under cooking conditions with the objective to understand theimpact of impregnation on the subsequent cooking phase. The cooking procedure was assessedby analysing the degree of delignification, yield and reject content.In impregnation, most chemical consuming reactions occurred within the first 10-30 minutes,mainly contributed by deacetylation. HAI obtained the fastest homogeneous distribution of OH-(~60 min), but the fastest dissolution of wood. The effect was contributed by the high [OH-],providing fast diffusion of ions and rapid dissolution of xylan. In the contrary, EI attained thehighest impregnation yield after a given impregnation time but required a prolonged durationto obtain a chemical equilibrium between the free and bound liquor (~120 min). REF showeda higher yield than HAI and similar chemical equilibrium as EI. The hydrosulphide sorption inimpregnation was highest for EI due to the high initial sulphidity charge and similar for REFand HAI. For impregnations at 115°C, the HS- sorption was significantly increased for all cases,resulting from delignification. In the subsequent cooking phase, it was prevalent that impregnation of chips under EIconditions were easier delignified, leading to a reduced cooking time to reach the defibrationpoint. Birch was more prone to delignification than eucalyptus. In turn, eucalyptus also obtaineda higher defibration point. Highest total cooking yield at similar kappa numbers was achievedwith REF conditions, followed by HAI and lastly the EI conditions. The high yield of REF incontrast to HAI could be explained by an improved xylan yield due to an alleviated hydroxidelevel. The low yield of EI can be assigned to continues peeling due to the prolongedimpregnation and loss of xylan when removing black liquor after impregnation. In terms ofproduction rate, yield, energy and chemical consumption the REF is the most efficientimpregnation condition for birch kraft cooking in this batchwise laboratory kraft cookingprocedure.</p>

w='OH- ],providing' val={'c': 'OH<sup>-</sup> ], providing', 's': 'diva2:1217827', 'n': 'correct in original'}
w='OH~60' val={'c': 'OH<sup>-</sup> (~60 min)', 's': 'diva2:1217827', 'n': 'correct in original - there was a newline after the superscript'}
w='(~60' val={'c': '~60', 's': 'diva2:1217827'}
w='min)' val={'c': 'minutes', 's': 'diva2:1217827'}

corrected abstract:
<p>In kraft pulping, one of the main issues is the extensive wood losses. With increasing prices of woody biomass an incentive towards minimizing the wood losses exists. Amongst the various process steps, the impregnation of wood chips has shown to enhance the cooking by providing a homogeneous distribution of chemicals inside the chips. It is proven that a more proficient impregnation phase can improve the overall yield in kraft pulping. However, there is a lack of scientific research comparing different impregnation techniques for hardwood. Hence, this thesis will attempt to clarify the impregnation of hardwood.</p><p>The impregnation efficiency was studied by comparing three different impregnation methods: High Alkali Impregnation (HAI), Extended Impregnation (EI) using a low alkali level and a Reference Impregnation (REF) to enable a comparison to the industrially established conditions. The cases were compared by analysing the yield, selectivity and homogeneity. The comparison was also made under cooking conditions with the objective to understand the impact of impregnation on the subsequent cooking phase. The cooking procedure was assessed by analysing the degree of delignification, yield and reject content.</p><p>In impregnation, most chemical consuming reactions occurred within the first 10-30 minutes, mainly contributed by deacetylation. HAI obtained the fastest homogeneous distribution of <sup>-</sup> (~60 min), but the fastest dissolution of wood. The effect was contributed by the high [OH<sup>-</sup>], providing fast diffusion of ions and rapid dissolution of xylan. In the contrary, EI attained the highest impregnation yield after a given impregnation time but required a prolonged duration to obtain a chemical equilibrium between the free and bound liquor (~120 min). REF showed a higher yield than HAI and similar chemical equilibrium as EI. The hydrosulphide sorption in impregnation was highest for EI due to the high initial sulphidity charge and similar for REF and HAI. For impregnations at 115°C, the HS<sup>-</sup> sorption was significantly increased for all cases, resulting from delignification.</p><p>In the subsequent cooking phase, it was prevalent that impregnation of chips under EI conditions were easier delignified, leading to a reduced cooking time to reach the defibration point. Birch was more prone to delignification than eucalyptus. In turn, eucalyptus also obtained a higher defibration point. Highest total cooking yield at similar kappa numbers was achieved with REF conditions, followed by HAI and lastly the EI conditions. The high yield of REF in contrast to HAI could be explained by an improved xylan yield due to an alleviated hydroxide level. The low yield of EI can be assigned to continues peeling due to the prolonged impregnation and loss of xylan when removing black liquor after impregnation. In terms of production rate, yield, energy and chemical consumption the REF is the most efficient impregnation condition for birch kraft cooking in this batchwise laboratory kraft cooking procedure.</p>
----------------------------------------------------------------------
In diva2:1745941 abstract is: <p>In Sweden almost three persons over the age of 65 years dies every daybecause of fall injuries. The overall societal costs of elderly fall accidentswere estimated to to be 14 billion SEK, and if no action is taken this cost isestimated to increase to 22 billion SEK until 2050. The individual decreasein life of quality due to pain, decrease of independence and, for those stillworking, a decrease in income is of course also well worth considering.It is well known that multitasking while walking will decrease attentionon the surroundings and gait behaviour which increases the risk of falling. Itis known that walking uses both sensory input and visual inputs to guide themotion. The visual input prepares the body to adjust itself before a step istaken to optimize the outcome.This study aimed to investigate the effect of multitasking on gaze strategiesand gait performance. Five healthy adults walked over a setup of ramps and astep while performing three different levels of cognitive loading: just walking,walking and performing mental arithmetic’s and walking and scrolling on amobile cell phone.The eye tracking device Pupil Core (Pupil Labs, Berlin, Germany) wasused to capture the gaze points of the participants and Vicon Nexus togetherwith force plates were used to capture data to compute the kinematics of theparticipants during the walking.The results revealed that four out of four participants had a lower ratio ofgaze fixations on objects of interest when scrolling on the phone comparedto just walking, and three out of four participants had a lower ratio of gazefixations on objects of interest when doing mental arithmetic’s compared tojust walking. Simultaneously the gait parameters and kinematics changed in away that might increase the risk of falling. Four out of four participants had adecrease in average stride length and average stride velocity when walkingwhile scrolling on a phone and a decrease in average stride velocity whenperforming mental arithmetic’s compared to just walking. Three out of fourparticipants had a decrease in average stride length when performing mentalarithmetic’s compared to just walking.Since the participant number was low more studies are needed to confirmthese results. The experimental design would benefit from adjustments to tryto separate the effect on gaze behaviour between altered cognitive loading andaltered gait pattern, but are a good base to use for further studies.</p>

corrected abstract:
<p>In Sweden almost three persons over the age of 65 years dies every day because of fall injuries. The overall societal costs of elderly fall accidents were estimated to to be 14 billion SEK, and if no action is taken this cost is estimated to increase to 22 billion SEK until 2050. The individual decrease in life of quality due to pain, decrease of independence and, for those still working, a decrease in income is of course also well worth considering.</p><p>It is well known that multitasking while walking will decrease attention on the surroundings and gait behaviour which increases the risk of falling. It is known that walking uses both sensory input and visual inputs to guide the motion. The visual input prepares the body to adjust itself before a step is taken to optimize the outcome.</p><p>This study aimed to investigate the effect of multitasking on gaze strategies and gait performance. Five healthy adults walked over a setup of ramps and a step while performing three different levels of cognitive loading: just walking, walking and performing mental arithmetic’s and walking and scrolling on a mobile cell phone.</p><p>The eye tracking device Pupil Core (Pupil Labs, Berlin, Germany) was used to capture the gaze points of the participants and Vicon Nexus together with force plates were used to capture data to compute the kinematics of the participants during the walking.</p><p>The results revealed that four out of four participants had a lower ratio of gaze fixations on objects of interest when scrolling on the phone compared to just walking, and three out of four participants had a lower ratio of gaze fixations on objects of interest when doing mental arithmetic’s compared to just walking. Simultaneously the gait parameters and kinematics changed in a way that might increase the risk of falling. Four out of four participants had a decrease in average stride length and average stride velocity when walking while scrolling on a phone and a decrease in average stride velocity when performing mental arithmetic’s compared to just walking. Three out of four participants had a decrease in average stride length when performing mental arithmetic’s compared to just walking.</p><p>Since the participant number was low more studies are needed to confirm these results. The experimental design would benefit from adjustments to try to separate the effect on gaze behaviour between altered cognitive loading and altered gait pattern, but are a good base to use for further studies.</p>
----------------------------------------------------------------------
In diva2:1463440 abstract is: <p>The automobile industry is one of the largest contributors to carbon emissions through vehicularemissions when in use. To make the transport sector more sustainable EU policies dictate toinclude a higher share of renewables in total energy consumption and one way to achieve that isto incorporate biofuels in the traditional fossil fuels. This would reduce carbon footprintgenerated by the automobile industry in an economical manner without making majortechnological modifications in existing engines. Scania AB has optimized their truck fuel systemto comply with the 10% blend of biodiesel in normal diesel fuel.Scania AB is a truck manufacturer in Sweden, with their research and development centresituated in Södertälje. They seek a solution to make their fuel filters in truck engines efficient tofilter out soft particles generated due to biofuel degradation. The aim of this project can bedivided in two major phases; the first phase is to develop a method to simulate fuel ageing anddegradation of biodiesel leading to formation of soft particles at laboratory conditions. The laterphase would be to use results from the first phase to generate a mock degraded fuel that would besubsequently used for testing in a full size filtration rig to assess the efficacy of Scania fuelfilters. These tests would give an insight about existing filters performance regarding degradedfuel and thus better filters could be designed to efficiently handle soft particles.In this thesis project, different iterations and methods of formation of soft particles arediscussed. An important assumption based on real deposit formations from trucks across theworld is that calcium is one of major causes of soft particles formation. Results and evidencefrom previous thesis projects have been used, modified and extended in this project. Newmethods have been developed based on empirical evidence from the experiments conductedduring this project which would be further improved as Scania progresses with this project. Amajor requirement for a mock degraded fuel was to make it stable in terms of suspension of softparticles in the biodiesel without a use of constant agitation in order to simulate the conditions ofthe fuel tank in a truck.The results presented in this thesis project are the optimized method to successfullyproduce soft particles and a working method to prepare test fuel concentrate. Analysis has beenperformed on test fuel concentrate in this project to check for viability of test fluid forconducting experiments on filtration rig. It has been concluded that 10 folds dilution test fuel isthe most promising test fuel sample that can be prepared with the given conditions and timerestrictions</p>

corrected abstract:
<p>The automobile industry is one of the largest contributors to carbon emissions through vehicular emissions when in use. To make the transport sector more sustainable EU policies dictate to include a higher share of renewables in total energy consumption and one way to achieve that is to incorporate biofuels in the traditional fossil fuels. This would reduce carbon footprint generated by the automobile industry in an economical manner without making major technological modifications in existing engines.</p><p>Scania AB has optimized their truck fuel system to comply with the 10% blend of biodiesel in normal diesel fuel. Scania AB is a truck manufacturer in Sweden, with their research and development centre situated in Södertälje. They seek a solution to make their fuel filters in truck engines efficient to filter out soft particles generated due to biofuel degradation. The aim of this project can be divided in two major phases; the first phase is to develop a method to simulate fuel ageing and degradation of biodiesel leading to formation of soft particles at laboratory conditions. The later phase would be to use results from the first phase to generate a mock degraded fuel that would be subsequently used for testing in a full size filtration rig to assess the efficacy of Scania fuel filters. These tests would give an insight about existing filters performance regarding degraded fuel and thus better filters could be designed to efficiently handle soft particles.</p><p>In this thesis project, different iterations and methods of formation of soft particles are discussed. An important assumption based on real deposit formations from trucks across the world is that calcium is one of major causes of soft particles formation. Results and evidence from previous thesis projects have been used, modified and extended in this project. New methods have been developed based on empirical evidence from the experiments conducted during this project which would be further improved as Scania progresses with this project. A major requirement for a mock degraded fuel was to make it stable in terms of suspension of soft particles in the biodiesel without a use of constant agitation in order to simulate the conditions of the fuel tank in a truck.</p><p>The results presented in this thesis project are the optimized method to successfully produce soft particles and a working method to prepare test fuel concentrate. Analysis has been performed on test fuel concentrate in this project to check for viability of test fluid for conducting experiments on filtration rig. It has been concluded that 10 folds dilution test fuel is the most promising test fuel sample that can be prepared with the given conditions and time restrictions.</p>

----------------------------------------------------------------------
In diva2:1228545 abstract is: <p>As the mission to the decrease global warming and phase out highly pollutingenvironmental practices globally, regulations including Euro 6 and policies generated by theUnited Nations Framework Convention on Climate Change (UNFCCC) are pushing companiesto be more innovative when it comes to their energy sources. These regulations involve manyfactors related to the cleanliness of the fuel and produced emissions, for example, propertiesof the fuels such as sulfur content, ash content, water content, and resulting emission valuesof Carbon dioxide (CO2) and Nitrogen Oxides (NOx). Furthermore, Sweden has set achallenging target of a fossil-fuel-independent vehicle fleet by 2030 and no net greenhousegasemissions by 2050.One way to cut down on the polluting properties in the fuel, as well as weakening thedependence on fossil fuel based fuel includes utilizing higher blending ratios of biofuels in thetransport sector. This transition to biofuels comes with many challenges to the transportindustry due to higher concentrations of these new fuels leads to clogging of the filters in theengine, as well as, internal diesel injector deposits (IDIDs) that produce injector fouling. Thisclogging of the filters leads to lower performance by the engines which leads to higher repairtimes (uptime) and less time on the road to transport goods. The formation of these softparticles at the root of the clogging issue is a pivotal issue because the precise mechanismsbehind their formation are highly unknown. Scania, a leader in the Swedish automotiveindustry, is very interested in figuring out what mechanisms are the most influential in theformation of these particles in the engine. Understanding the key mechanisms would allowScania to make appropriate adjustments to the fuel or the engines to ensure more time onthe road and less maintenance.There are many conditions known to be possible causes of the formation of softparticles in engines such as water content, ash content, and temperature. After generatingsoft particles using a modified accelerated method, particles were analyzed using infraredtechnology (RTX-FTIR) and a Scanning Electric Microscope (SEM-EDX). Many differentexperiments were performed to be able to make a conclusion as to which mechanisms weremost influential including temperature, time, water, air, and oil. The combination of agingbiofuels (B100, B10, HVO) with metals, and water produced the largest amount of particlesfollowed by aging the biofuels with aged oil, metals, and water. Aging the fuels with aged oilincreased particles, meanwhile the addition of water prevented particle production possiblydue to additives. B100 produced the highest amount of particles when aged with Copper, B10with Brass, and HVO with Iron.</p>

corrected abstract:
<p>As the mission to the decrease global warming and phase out highly polluting environmental practices globally, regulations including Euro 6 and policies generated by the United Nations Framework Convention on Climate Change (UNFCCC) are pushing companies to be more innovative when it comes to their energy sources. These regulations involve many factors related to the cleanliness of the fuel and produced emissions, for example, properties of the fuels such as sulfur content, ash content, water content, and resulting emission values of Carbon dioxide (CO<sub>2</sub>) and Nitrogen Oxides (NO<sub>x</sub>). Furthermore, Sweden has set a challenging target of a fossil-fuel-independent vehicle fleet by 2030 and no net greenhouse-gas emissions by 2050.</p><p>One way to cut down on the polluting properties in the fuel, as well as weakening the dependence on fossil fuel based fuel includes utilizing higher blending ratios of biofuels in the transport sector. This transition to biofuels comes with many challenges to the transport industry due to higher concentrations of these new fuels leads to clogging of the filters in the engine, as well as, internal diesel injector deposits (IDIDs) that produce injector fouling. This clogging of the filters leads to lower performance by the engines which leads to higher repair times (uptime) and less time on the road to transport goods. The formation of these soft particles at the root of the clogging issue is a pivotal issue because the precise mechanisms behind their formation are highly unknown. Scania, a leader in the Swedish automotive industry, is very interested in figuring out what mechanisms are the most influential in the formation of these particles in the engine. Understanding the key mechanisms would allow Scania to make appropriate adjustments to the fuel or the engines to ensure more time on the road and less maintenance.</p><p>There are many conditions known to be possible causes of the formation of soft particles in engines such as water content, ash content, and temperature. After generating soft particles using a modified accelerated method, particles were analyzed using infrared technology (RTX-FTIR) and a Scanning Electric Microscope (SEM-EDX). Many different experiments were performed to be able to make a conclusion as to which mechanisms were most influential including temperature, time, water, air, and oil. The combination of aging biofuels (B100, B10, HVO) with metals, and water produced the largest amount of particles followed by aging the biofuels with aged oil, metals, and water. Aging the fuels with aged oil increased particles, meanwhile the addition of water prevented particle production possibly due to additives. B100 produced the highest amount of particles when aged with Copper, B10 with Brass, and HVO with Iron.</p>
----------------------------------------------------------------------
In diva2:1676098 abstract is: <p>Introduction: The brain can change its structure and functionality as a result ofexternal factors. The working memory (WM) of the brain is where informationcan be held and manipulated during a short period of time, with the purpose ofachieving higher cognitive functions such as reasoning and learning. The WMimproves in capacity during the development from childhood into adulthood,and variation of improvement is possible as an effect of situational factors andstimuli.Goal: The main goal of this project was to examine the effects of a WMtraining program on power distribution, connectivity and synchronicity withinbrain networks, using an intra-individual analysis approach.Method: A series of magnetoencephalography (MEG) measurements wasacquired for four subjects while they were performing WM and control tasks,during a WM training program, along with an MRI image of the brain for eachof the participants. The data was preprocessed for noise and artifact removaland a source reconstruction was performed. Time-frequency representationsof the data were created and the frequencies were categories into alpha,beta and gamma bands. The power difference between the WM and controltask was calculated as a function of cognitive load of each frequency band,and its variation over load was calculated as a constructed metric called’area under power difference curve’ (AUPDC), and visualised using colourscale representation upon the brain MRI of each subject. Brain parcels thatsignificantly deviated from a random distribution of AUPDC values wereidentified using a Gaussian distribution fit.Results and discussion: All subjects showed a clear improvement inperformance accuracy of the tasks, but as the effect on the power distributionsvaried considerably for each subject and frequency band, other aspects besidepower need to be investigated in order to understand the mechanisms behindthe improvement. However, the overall results indicate that many significantAUPDC values seem to have decreased during the WM training, both forthe positive and negative significant AUPDC values, suggesting a strongerdecreasing trend in power difference over cognitive load and a weaker increasingtrend. This could suggest an improved brain activation efficiency as an effectof the WM training.</p>

corrected abstract:
<p><em>Introduction</em>: The brain can change its structure and functionality as a result of external factors. The working memory (WM) of the brain is where information can be held and manipulated during a short period of time, with the purpose of achieving higher cognitive functions such as reasoning and learning. The WM improves in capacity during the development from childhood into adulthood, and variation of improvement is possible as an effect of situational factors and stimuli.</p><p><em>Goal</em>: The main goal of this project was to examine the effects of a WM training program on power distribution, connectivity and synchronicity within brain networks, using an intra-individual analysis approach.</p><p><em>Method</em>: A series of magnetoencephalography (MEG) measurements was acquired for four subjects while they were performing WM and control tasks, during a WM training program, along with an MRI image of the brain for each of the participants. The data was preprocessed for noise and artifact removal and a source reconstruction was performed. Time-frequency representations of the data were created and the frequencies were categories into alpha, beta and gamma bands. The power difference between the WM and control task was calculated as a function of cognitive load of each frequency band, and its variation over load was calculated as a constructed metric called ’area under power difference curve’ (AUPDC), and visualised using colour scale representation upon the brain MRI of each subject. Brain parcels that significantly deviated from a random distribution of AUPDC values were identified using a Gaussian distribution fit.</p><p><em>Results and discussion</em>: All subjects showed a clear improvement in performance accuracy of the tasks, but as the effect on the power distributions varied considerably for each subject and frequency band, other aspects beside power need to be investigated in order to understand the mechanisms behind the improvement. However, the overall results indicate that many significant AUPDC values seem to have decreased during the WM training, both for the positive and negative significant AUPDC values, suggesting a stronger decreasing trend in power difference over cognitive load and a weaker increasing trend. This could suggest an improved brain activation efficiency as an effect of the WM training.</p>
----------------------------------------------------------------------
diva2:1455145 seems to be a duplicate of this


In diva2:1421531 abstract is: <p>Lignocellulosic biomass has potential to chip in the chemical and biofuels supplies in future societies,even though lignocellulose is a recalcitrant structure that has to be treated in several steps. After theirproper life cycle, wood-derived materials such as particleboards have few outcomes today apart fromenergy recovery for heat production. Then, they may be used as lignocellulosic biomass sources in theproduction of molecules of interest. Fermentation from wood-derived monosaccharides imposespreliminary sugar retrieval, for instance through pre-treatment and enzymatic hydrolysis. This studyfocuses on the potential of particleboards waste for chemical and biofuel production by comparingsaccharification through simulated steam explosion pre-treatment and enzymatic hydrolysis betweennative and particleboard-derived wood, with an insight in subsequent fermentation by Saccharomycescerevisiae. Urea-Formaldehyde bound particleboard was investigated, as well as some aspects ofMelamine-Urea-Formaldehyde bound particleboard.Pre-treatment resulted in apparition of lignocellulosic degraded compounds in a much larger extent innative wood than in particleboard, which seemed to be only superficially impacted. Formation ofdegraded compounds from sugars – furfural and 5-hydroxymethylfurfural – was enhanced when pretreatmentwas prolonged. Removal of a substantial fraction of the adhesive contained in theparticleboards was observed, leading to comparable concentrations in free urea, its degradedproducts, and formaldehyde between native wood and particleboards during enzymatic hydrolysis.Enzymatic hydrolysis with cellulases and hemicellulases highlighted a critical role of pre-treatment toenhance final yields, both in native wood and in Urea-Formaldehyde particleboard. Adding 20 minutessteam-explosion type pre-treatment at 160 °C resulted in glucose yields increase from 18.5 % to 32.8% for native wood and from 15.6 % to 37.4 % for particleboard. Prolonging pre-treatment residencetime to 35 minutes resulted in much better glucose extraction for native wood but only slight progressfor the particleboard, as glucose yields reached 64.5 % and 41.1 % respectively. Maximalconcentrations achieved were 277 and 184 mg/gbiomass respectively.Fermentation brought to light high inhibition from both native wood and particleboard sources ofmedia, which were attributed to components or degraded products of lignocellulose that were notanalysed in this project. Ethanol was formed during fermentation, with reduced productivity butincreased yields as compared with the control sample. Inhibition was so strong that no difference couldbe given between native and particleboard wood. In this situation, no inhibition potential of resin orits degradation products could be proved.</p>


corrected abstract:
<p>Lignocellulosic biomass has potential to chip in the chemical and biofuels supplies in future societies, even though lignocellulose is a recalcitrant structure that has to be treated in several steps. After their proper life cycle, wood-derived materials such as particleboards have few outcomes today apart from energy recovery for heat production. Then, they may be used as lignocellulosic biomass sources in the production of molecules of interest. Fermentation from wood-derived monosaccharides imposes preliminary sugar retrieval, for instance through pre-treatment and enzymatic hydrolysis. This study focuses on the potential of particleboards waste for chemical and biofuel production by comparing saccharification through simulated steam explosion pre-treatment and enzymatic hydrolysis between native and particleboard-derived wood, with an insight in subsequent fermentation by <em>Saccharomyces cerevisiae</em>. Urea-Formaldehyde bound particleboard was investigated, as well as some aspects of Melamine-Urea-Formaldehyde bound particleboard.</p><p>Pre-treatment resulted in apparition of lignocellulosic degraded compounds in a much larger extent in native wood than in particleboard, which seemed to be only superficially impacted. Formation of degraded compounds from sugars – furfural and 5-hydroxymethylfurfural – was enhanced when pretreatment was prolonged. Removal of a substantial fraction of the adhesive contained in the particleboards was observed, leading to comparable concentrations in free urea, its degraded products, and formaldehyde between native wood and particleboards during enzymatic hydrolysis. Enzymatic hydrolysis with cellulases and hemicellulases highlighted a critical role of pre-treatment to enhance final yields, both in native wood and in Urea-Formaldehyde particleboard. Adding 20 minutes steam-explosion type pre-treatment at 160 °C resulted in glucose yields increase from 18.5 % to 32.8 % for native wood and from 15.6 % to 37.4 % for particleboard. Prolonging pre-treatment residence time to 35 minutes resulted in much better glucose extraction for native wood but only slight progress for the particleboard, as glucose yields reached 64.5 % and 41.1 % respectively. Maximal concentrations achieved were 277 and 184 mg/g<sub>biomass</sub> respectively.</p><p>Fermentation brought to light high inhibition from both native wood and particleboard sources of media, which were attributed to components or degraded products of lignocellulose that were not analysed in this project. Ethanol was formed during fermentation, with reduced productivity but increased yields as compared with the control sample. Inhibition was so strong that no difference could be given between native and particleboard wood. In this situation, no inhibition potential of resin or its degradation products could be proved.</p>
----------------------------------------------------------------------
In diva2:1878490 abstract is: <p>Stroke is an enormous global burden, six and a half-million people die fromstroke annually [1]. Effectively monitoring blood hemodynamic parameters suchas blood velocity and volume flow permits to help and cure people. This projectaimed to calibrate a custom-made wearable system for measuring cerebral bloodflow (CBF) using a photoplethysmography (PPG) sensor. The measurementswere validated using Doppler ultrasound as a reference method. Five (N=5)subjects (age = 24±1.41 years) were selected for the project. The PPG and Dopplerultrasound probe were placed above the left and right common carotid arteries(CCA), respectively. Measurements were taken simultaneously for 12 secondseach, with six consecutive measurements per subject and 2 time-synchronizedECG recordings. Subsequently, using an extraction algorithm the velocityenvelope (TAMEAN) was extracted from the Doppler image to obtain the bloodvolume flow (ml/min). After synchronization, the PPG signal output expressedin volts was calibrated to the corresponding volume, and a calibration curve wascreated.The extraction algorithm achieved remarkable results, with almost perfectcorrelation with the Doppler image reference, rT AM EAN =0.951 and rvolume=0.975demonstrating its reliability. Challenges encountered during postprocessingand synchronization highlighted the need for careful refinement in the projectframework. Despite successful signal processing and alignment techniques,calibration results were suboptimal due to synchronization difficulties andmotion artifacts. Limitations included impractical measurement locations andsusceptibility to movement artifacts. The calibration process did not yield theexpected outcomes and the project aim was not achieved. All the linear regressionmodels for each subject failed to accurately predict the volume flow based on themeasured voltages. Future work could focus on refining calibration procedures,improving synchronization methods, and expanding studies to include largercohorts. Although the wearable device was tested, the project’s goal was onlypartially achieved, underscoring the complexity of accurately measuring cerebralblood flow using PPG sensors.</p>


corrected abstract:
<p>Stroke is an enormous global burden, six and a half-million people die from stroke annually [1]. Effectively monitoring blood hemodynamic parameters such as blood velocity and volume flow permits to help and cure people. This project aimed to calibrate a custom-made wearable system for measuring cerebral blood flow (CBF) using a photoplethysmography (PPG) sensor. The measurements were validated using Doppler ultrasound as a reference method. Five (N=5) subjects (age = 24±1.41 years) were selected for the project. The PPG and Doppler ultrasound probe were placed above the left and right common carotid arteries (CCA), respectively. Measurements were taken simultaneously for 12 seconds each, with six consecutive measurements per subject and 2 time-synchronized ECG recordings. Subsequently, using an extraction algorithm the velocity envelope (TAMEAN) was extracted from the Doppler image to obtain the blood volume flow (ml/min). After synchronization, the PPG signal output expressed in volts was calibrated to the corresponding volume, and a calibration curve was created. The extraction algorithm achieved remarkable results, with almost perfect correlation with the Doppler image reference, r<sub>TAMEAN</sub>=0.951 and r<sub>volume</sub>=0.975 demonstrating its reliability. Challenges encountered during postprocessing and synchronization highlighted the need for careful refinement in the project framework. Despite successful signal processing and alignment techniques, calibration results were suboptimal due to synchronization difficulties and motion artifacts. Limitations included impractical measurement locations and susceptibility to movement artifacts. The calibration process did not yield the expected outcomes and the project aim was not achieved. All the linear regression models for each subject failed to accurately predict the volume flow based on the measured voltages. Future work could focus on refining calibration procedures, improving synchronization methods, and expanding studies to include larger cohorts. Although the wearable device was tested, the project’s goal was only partially achieved, underscoring the complexity of accurately measuring cerebral blood flow using PPG sensors.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-23
======================================================================
In diva2:1688672 abstract is: <p>When the Covid-19 pandemic hit, many organizations had to adapt to new waysof working. For many, this meant that the work previously done in offices wasnow allowed to move home instead. This meant a major change, not least forauthorities that had not been able to carry out their work from home before. Theaim with the study was to investigate positive and negative effects in the workenvironment during a transition from office work to work from home.Authorities and other organizations may benefit from this new knowledge inanticipating risk factors and preventing them. A case study was conducted on aunit of an authority in a larger city in Sweden. A questionnaire was sent out tothe unit and two in-depth interviews were conducted. The results presentconnections that have emerged in the organizational and social workenvironment as well as the physical work environment. The results of the studyare interpreted as saying that most people were positive about working fromhome. All static relationships investigated regarding the organizational, socialand physical working environment showed significant correlations (correlationvalues ranged from 0.387-0.754). These results showed that a well-functioningdigital platform is a prerequisite for good communication, that undisturbed workaffects the experience of concentration, that productivity was affected by sittingundisturbed, that job satisfaction was affected by the support of management,that perception of their working position affected pain and discomfort in thebody and that what they thought about the physical work environment regardingtables and chair matters. Both the answers in the survey and in the interviewsturned out to be positive. All the relationships regarding the organizational andsocial work environment showed correlation that was significant. Also, all therelationships regarding the physical work environment showed correlation thatwas significant. Since the study showed such positive results, authorities shouldconsider about how to approach employees doing work from home even aftersociety has gained better control of the Covid-19 pandemic. This study hasgenerated new knowledge that can change any position regarding work fromhome for employees at authorities.</p>


corrected abstract:
<p>When the Covid-19 pandemic hit, many organizations had to adapt to new ways of working. For many, this meant that the work previously done in offices was now allowed to move home instead. This meant a major change, not least for authorities that had not been able to carry out their work from home before. The aim with the study was to investigate positive and negative effects in the work environment during a transition from office work to work from home. Authorities and other organizations may benefit from this new knowledge in anticipating risk factors and preventing them. A case study was conducted on a unit of an authority in a larger city in Sweden. A questionnaire was sent out to the unit and two in-depth interviews were conducted. The results present connections that have emerged in the organizational and social work environment as well as the physical work environment. The results of the study are interpreted as saying that most people were positive about working from home. All static relationships investigated regarding the organizational, social and physical working environment showed significant correlations (correlation values ranged from 0.387-0.754). These results showed that a well-functioning digital platform is a prerequisite for good communication, that undisturbed work affects the experience of concentration, that productivity was affected by sitting undisturbed, that job satisfaction was affected by the support of management, that perception of their working position affected pain and discomfort in the body and that what they thought about the physical work environment regarding tables and chair matters. Both the answers in the survey and in the interviews turned out to be positive. All the relationships regarding the organizational and social work environment showed correlation that was significant. Also, all the relationships regarding the physical work environment showed correlation that was significant. Since the study showed such positive results, authorities should consider about how to approach employees doing work from home even after society has gained better control of the Covid-19 pandemic. This study has generated new knowledge that can change any position regarding work from home for employees at authorities.</p>
----------------------------------------------------------------------
In diva2:1751618 abstract is: <p>The improvement of data acquisition and computer heavy methods in recentyears has paved the way for completely digital healthcare solutions. Digitaltherapeutics (DTx) are such solutions and are often provided as mobileapplications that must undergo clinical trials. A common method for suchapplications is to utilize cognitive behavioral-therapy (CBT), in order toprovide their patients with tools for self-improvement. The Swedish-basedcompany Alex Therapeutics is such a provider. They develop state-of-theartapplications that utilize CBT to help patients. Among their applications,they have one that aims to help users quit smoking. From this app, they havecollected user data with the goal of continuously improving their servicesthrough machine learning (ML). In their current application, they utilizemultiple ML methods to personalize the care, but have opened up possibilitiesfor the usage of reinforcement learning (RL). Often the wanted behavior isknown, such as to quitting smoking, but the optimal path, within the app, forhow to reach such a goal is not. By formalizing the problem as a Markovdecision process, where the transition probabilities have to be inferred fromuser data, such an optimal policy can be found. Standard methods of RL arereliant on direct access of an environment for sampling of data, whereas theuser data sampled from the application are to be treated as such. This thesisthus explores the possibilities of using RL on a static dataset in order to inferan optimal policy.</p><p>A double deep Q-network (DDQN) was chosen as the reinforcement learningagent. The agent was trained on two different datasets and showed goodconvergence for both, using a custom metric for the task. Using SHAPvaluesthe strategy of the agent is visualized and discussed, together with themethodological challenges. Lastly, future work for the proposed methods arediscussed.</p>

corrected abstract:
<p>The improvement of data acquisition and computer heavy methods in recent years has paved the way for completely digital healthcare solutions. Digital therapeutics (DTx) are such solutions and are often provided as mobile applications that must undergo clinical trials. A common method for such applications is to utilize cognitive behavioral-therapy (CBT), in order to provide their patients with tools for self-improvement. The Swedish-based company Alex Therapeutics is such a provider. They develop state-of-the-art applications that utilize CBT to help patients. Among their applications, they have one that aims to help users quit smoking. From this app, they have collected user data with the goal of continuously improving their services through machine learning (ML). In their current application, they utilize multiple ML methods to personalize the care, but have opened up possibilities for the usage of reinforcement learning (RL). Often the wanted behavior is known, such as to quitting smoking, but the optimal path, within the app, for how to reach such a goal is not. By formalizing the problem as a Markov decision process, where the transition probabilities have to be inferred from user data, such an optimal policy can be found. Standard methods of RL are reliant on direct access of an environment for sampling of data, whereas the user data sampled from the application are to be treated as such. This thesis thus explores the possibilities of using RL on a static dataset in order to infer an optimal policy.</p><p>A double deep Q-network (DDQN) was chosen as the reinforcement learning agent. The agent was trained on two different datasets and showed good convergence for both, using a custom metric for the task. Using SHAP-values the strategy of the agent is visualized and discussed, together with the methodological challenges. Lastly, future work for the proposed methods are discussed.</p>
----------------------------------------------------------------------
In diva2:1880445 abstract is: <p>This study aims to analyze factors and individual characteristics that affect the outcomes:work-life balance, work engagement and sense of coherence for white-collar workers whenworking remotely.</p><p>In today’s society where technology is constantly evolving, hybrid and remote work optionsare becoming more common. Research has shown that work-life balance, work engagement,and sense of coherence affect employee well-being and organizational performance. Theresearch on the extent to which remote work can contribute to sustainable work andemployee well-being is mixed, and most of the research on remote work has been conductedeither before or during the COVID-19 pandemic. Further post-pandemic research cancontribute to more sustainable work and society as a whole.</p><p>To analyze what factors affect the outcomes in a remote work setting, a mixed-methodapproach was used to gain a comprehensive understanding, including a quantitative and aqualitative study. The quantitative method included bivariate correlation tests, multiple linearregression and group difference testing on cross-sectional survey data collected from twolarge Swedish companies. The qualitative method included six interviews with experiencedwhite-collar workers, which helped us interpret the findings and gain a more in depthunderstanding of quantitative results. To analyze the study findings the theoretical model ofhuman, technology, and organization (HTO) was applied.</p><p>The quantitative study showed that social support from superiors, remote leadership quality,functionality of digital management systems and digital learning climate were identified asfactors that were associated with work-life balance, work engagement and sense ofcoherence. However, digital resources and social support from superiors were found to beinfluencing work engagement and sense of coherence to a greater extent. For work-lifebalance, social support from superiors and remote leadership quality was found to besignificant. The qualitative study showed that the interviewees' perception of work-lifebalance, work engagement, and sense of coherence was affected by flexible workingarrangements, functionality and use of digital tools, social interaction, collaboration,communication, inspirational relationships at the workplace, supportive colleagues, andsupportive and responsive superiors. Better understanding of what factors affect employees'personal and working life when working remotely can help organizations to promoteoccupational well-being and performance, which can contribute to more sustainable work.</p>

corrected abstract:
<p>This study aims to analyze factors and individual characteristics that affect the outcomes: work-life balance, work engagement and sense of coherence for white-collar workers when working remotely.</p><p>In today’s society where technology is constantly evolving, hybrid and remote work options are becoming more common. Research has shown that work-life balance, work engagement, and sense of coherence affect employee well-being and organizational performance. The research on the extent to which remote work can contribute to sustainable work and employee well-being is mixed, and most of the research on remote work has been conducted either before or during the COVID-19 pandemic. Further post-pandemic research can contribute to more sustainable work and society as a whole.</p><p>To analyze what factors affect the outcomes in a remote work setting, a mixed-method approach was used to gain a comprehensive understanding, including a quantitative and a qualitative study. The quantitative method included bivariate correlation tests, multiple linear regression and group difference testing on cross-sectional survey data collected from two large Swedish companies. The qualitative method included six interviews with experienced white-collar workers, which helped us interpret the findings and gain a more in depth understanding of quantitative results. To analyze the study findings the theoretical model of human, technology, and organization (HTO) was applied.</p><p>The quantitative study showed that social support from superiors, remote leadership quality, functionality of digital management systems and digital learning climate were identified as factors that were associated with work-life balance, work engagement and sense of coherence. However, digital resources and social support from superiors were found to be influencing work engagement and sense of coherence to a greater extent. For work-life balance, social support from superiors and remote leadership quality was found to be significant. The qualitative study showed that the interviewees' perception of work-life balance, work engagement, and sense of coherence was affected by flexible working arrangements, functionality and use of digital tools, social interaction, collaboration, communication, inspirational relationships at the workplace, supportive colleagues, and supportive and responsive superiors. Better understanding of what factors affect employees' personal and working life when working remotely can help organizations to promote occupational well-being and performance, which can contribute to more sustainable work.</p>
----------------------------------------------------------------------
In diva2:1451583 abstract is: <p>AbstractDevelopment of products and services has historically been preceded by a clearrequirements specification of the desired features, a fixed margin of expenditureand a launch time frame. Nowadays, agile methods have gradually becomestandard within software development and do not require an explicit requirementsspecification. However, the need for such a specification, or an alternative to it,may remain to assist in the management of overall planning and thereby functionas a bridge between old and new requirements management. A literature study wasconducted, reviewing three agile frameworks, including a mapping of whereproduct requirements exist in modern structures, which may correspond to atraditional requirements specification. The study demonstrated that overallrequirements are produced prior to the start of a project and later specified duringthe development process, to match individual user scenarios. By aggregating these,a requirements specification, corresponding to the traditional format, can beobtained.The Swedish Tax Agency is undergoing an organizational transformation to theagile framework named Scaled Agile Framework (SAFe), whereupon a need formanaging requirements and its documentation has arisen. The literature study,together with modeling of their current working methods in the software tools Jiraand Confluence, has been compared with "best practice". The comparison showedthat the agency follows the SAFe framework structure, but that discrepancy occurregarding the documentation structure used in the tools, whereupon amendmentsare presented.KeywordsAgile methods, software development, requirements specification, SAFe, agileframeworks.</p>


'whereupon' is set as one word in the original

corrected abstract:
<p>Abstract Development of products and services has historically been preceded by a clear requirements specification of the desired features, a fixed margin of expenditure and a launch time frame. Nowadays, agile methods have gradually become standard within software development and do not require an explicit requirements specification. However, the need for such a specification, or an alternative to it, may remain to assist in the management of overall planning and thereby function as a bridge between old and new requirements management. A literature study was conducted, reviewing three agile frameworks, including a mapping of where product requirements exist in modern structures, which may correspond to a traditional requirements specification. The study demonstrated that overall requirements are produced prior to the start of a project and later specified during the development process, to match individual user scenarios. By aggregating these, a requirements specification, corresponding to the traditional format, can be obtained.</p><p>The Swedish Tax Agency is undergoing an organizational transformation to the agile framework named Scaled Agile Framework (SAFe), whereupon a need for managing requirements and its documentation has arisen. The literature study, together with modeling of their current working methods in the software tools Jira and Confluence, has been compared with "best practice". The comparison showed that the agency follows the SAFe framework structure, but that discrepancy occur regarding the documentation structure used in the tools, where upon amendments are presented.</p>
----------------------------------------------------------------------
title: "Development of cell assay for cellbasedinteraction studies with Attana’s 3rd generation biosensor"
==>    "Development of cell assay for cellbased interaction studies with Attana’s 3rd generation biosensor"


In diva2:1454421 abstract is: <p>Optimization of experimental assay design is crucial in all areas of biomedical research.Assay development and experimental optimization were carried out in an effort toproduce a cell-based demonstration assay for Attana AB. The demonstration assaywould provide practical training for cell-based experiments in a comprehensive,practical, and effective manner when operating Attana CellTM 200 biosensor. The AttanaCellTM 200 system is a label-free, dual-channel, temperature-controlled biosensor basedon Quartz Crystal Microbalance (QCM) technology. Attana's QCM biosensors facilitatereal-time interactions and enable direct evaluation of quantitative and qualitativeparameters. The findings generated in this project were applied to help optimize aprotocol for the cell-based assay. These included determining optimal experimentalconditions and variables, including flow rate, running buffer, cell coverage, andconcentrations. Other desired aspects that were examined for optimization includedsustainability, stability, and reproducibility. Attana CellTM 200 system was used in thisstudy to determine specificity, kinetics, and affinity of cell-lectin interactions. Twometastatic colorectal cell lines, HT29 and SW480, representing different metastasispotential, were chosen to study lectin interactions with cell surface glycans. The cellswere immobilized on polystyrene-coated sensor chips, and interactions were studiedusing Helix promatia agglutinin (HPA) and Ricinus communis agglutinin (RCA) lectins.HPA and RCA have demonstrated to interact with cancer cells to the degree that isproportional to their metastatic capacity by displaying specificity for glycans withvarying degrees of N and O-linked glycosylation. The study demonstrated cleardifferences in the interaction profiles and kinetics between the two colorectal cancer cellswhen interacting with HPA and RCA, respectively. Furthermore, due to the two lectinsdifferent protein properties, variations in the interaction profiles between the lectinswere detected. The results indicated that the different interaction profiles could be usedas tools to demonstrate cell-based interactions when using the cell demonstration assay.</p>


w='AttanaCellTM' val={'c': 'Attana Cell™', 's': 'diva2:1454421'}

corrected abstract:
<p>Optimization of experimental assay design is crucial in all areas of biomedical research. Assay development and experimental optimization were carried out in an effort to produce a cell-based demonstration assay for Attana AB. The demonstration assay would provide practical training for cell-based experiments in a comprehensive, practical, and effective manner when operating Attana CellTM 200 biosensor. The Attana Cell™ 200 system is a label-free, dual-channel, temperature-controlled biosensor based on Quartz Crystal Microbalance (QCM) technology. Attana's QCM biosensors facilitate real-time interactions and enable direct evaluation of quantitative and qualitative parameters. The findings generated in this project were applied to help optimize a protocol for the cell-based assay. These included determining optimal experimental conditions and variables, including flow rate, running buffer, cell coverage, and concentrations. Other desired aspects that were examined for optimization included sustainability, stability, and reproducibility. Attana CellTM 200 system was used in this study to determine specificity, kinetics, and affinity of cell-lectin interactions. Two metastatic colorectal cell lines, HT29 and SW480, representing different metastasis potential, were chosen to study lectin interactions with cell surface glycans. The cells were immobilized on polystyrene-coated sensor chips, and interactions were studied using Helix promatia agglutinin (HPA) and Ricinus communis agglutinin (RCA) lectins. HPA and RCA have demonstrated to interact with cancer cells to the degree that is proportional to their metastatic capacity by displaying specificity for glycans with varying degrees of N and O-linked glycosylation. The study demonstrated clear differences in the interaction profiles and kinetics between the two colorectal cancer cells when interacting with HPA and RCA, respectively. Furthermore, due to the two lectins different protein properties, variations in the interaction profiles between the lectins were detected. The results indicated that the different interaction profiles could be used as tools to demonstrate cell-based interactions when using the cell demonstration assay.</p>
----------------------------------------------------------------------
In diva2:1438250 abstract is: <p>Abstract</p><p>With the current challenges for the healthcare such as increased demand for care, financial andresource constraints along with rapid changes and complexity there is high believe in digitalinnovation and digitalisation to efficacy resources and aid in delivering a safer, more accessibleand patient centred valuable care.</p><p>There is a digitalisation that is ongoing, being used and implemented over several differentareas of healthcare. Since healthcare can be seen as a complex adaptive system, there is a needto understand several agents. The aim is to gather more knowledge about perceptions withinthe physiotherapy staff and give recommendations and directions for improvements regardingdigital innovation.</p><p>Opinions about digital innovation have been gathered with open interviews and a semisystematicliterature review with focus on physiotherapy. Too find subjective data the mixedmethod Q methodology was applied.</p><p>The open interviews resulted in eight categories: digital innovation, digital innovation beingused, digital innovation not used, management, obstacles, education, wishful thinking,applications and systems and associated opinions. The semi-systematic literature reviewshowed on a rapid scientifically development, 25 articles was found and thematically analysed.140 cited viewpoints and facts was merged with the results from the open interviews. Tenphysiotherapists performed the q-sort consisting of 25 statements. Three factors were found.Interpreted as digital innovation optimism &amp; patient oriented, digital innovation scepticism &amp;management oriented and digital innovation sceptical optimism. Video-call technique isstrongly encouraged by factor one contrary to factor two. Integrity is the major conflictingviewpoint between the factors. The result shows that gender can affect if a physiotherapist iseither optimistic or sceptical to digital innovation. Using existing models such as UTAUT couldimprove acceptance about digital innovation. Education is perceived as important among allfactors. Nine participants responded on baseline questions showing low knowledge of the termmHealth and little communication with IT departments.</p><p>Keywords: Digital innovation, Digitalisation, eHealth, mHealth, Healthcare,Physiotherapists, Q methodology</p>


corrected abstract
<p>With the current challenges for the healthcare such as increased demand for care, financial and resource constraints along with rapid changes and complexity there is high believe in digital innovation and digitalisation to efficacy resources and aid in delivering a safer, more accessible and patient centred valuable care.</p><p>There is a digitalisation that is ongoing, being used and implemented over several different areas of healthcare. Since healthcare can be seen as a complex adaptive system, there is a need to understand several agents. The aim is to gather more knowledge about perceptions within the physiotherapy staff and give recommendations and directions for improvements regarding digital innovation.</p><p>Opinions about digital innovation have been gathered with open interviews and a semisystematic literature review with focus on physiotherapy. Too find subjective data the mixed method Q methodology was applied.</p><p>The open interviews resulted in eight categories: digital innovation, digital innovation being used, digital innovation not used, management, obstacles, education, wishful thinking, applications and systems and associated opinions. The semi-systematic literature review showed on a rapid scientifically development, 25 articles was found and thematically analysed. 140 cited viewpoints and facts was merged with the results from the open interviews. Ten physiotherapists performed the q-sort consisting of 25 statements. Three factors were found. Interpreted as digital innovation optimism &amp; patient oriented, digital innovation scepticism &amp; management oriented and digital innovation sceptical optimism. Video-call technique is strongly encouraged by factor one contrary to factor two. Integrity is the major conflicting viewpoint between the factors. The result shows that gender can affect if a physiotherapist is either optimistic or sceptical to digital innovation. Using existing models such as UTAUT could improve acceptance about digital innovation. Education is perceived as important among all factors. Nine participants responded on baseline questions showing low knowledge of the term mHealth and little communication with IT departments.</p>
----------------------------------------------------------------------
In diva2:1214017 abstract is: <p>Like in any modern civilization, roads in Iceland have an important role in thedaily lives of inhabitants. Consequently, road quality is of equal importance, butIcelandic roads have shown problems when surface dressing is used where itlooks decent after being paved during summer but then deforming pretty rapidlyafter being hit by elements of winter. Roads in Sweden however, do not seem tohave the same problem.The aim of this study is to minimize this road deformation by examining surfacedressing and aggregates. The Icelandic climate is also a factor to this problemsince the humidity is comparatively high, summers are cool, winters are mild andthe climate is overall challenging. Furthermore, winter thaws are distinctivecharacteristic of the Icelandic weather, which increases strain on the asphalt.An experiment was conducted where the adhesion of surface dressing that iscommon in Sweden was tested with two different aggregates by Vialit plateshock test method. First it was tested with Swedish granite and then withIcelandic basalt. The results from the aggregates were compared where theadhesion with the granite was stronger than with the basalt.Previous study have found that by choosing binder and aggregate that have highadhesivity at low temperature reduces the risk of surface dressing defects,especially when paving takes place in the early and late summer season. Whenchoosing aggregates for road construction the main criteria is cost, thereforeaggregates that are used usually reflect the local geology because transportingaggregates for significant distances is expensive. Concluding from theexperiment, it is not recommended to use the basalt with the Swedish surfacedressing in practice now due to the lesser adhesion compared to the granite.However further research on the asphalt mix with the Swedish surface dressingand the basalt should be conducted.</p>


corrected abstract:
<p>Like in any modern civilization, roads in Iceland have an important role in the daily lives of inhabitants. Consequently, road quality is of equal importance, but Icelandic roads have shown problems when surface dressing is used where it looks decent after being paved during summer but then deforming pretty rapidly after being hit by elements of winter. Roads in Sweden however, do not seem to have the same problem.</p><p>The aim of this study is to minimize this road deformation by examining surface dressing and aggregates. The Icelandic climate is also a factor to this problem since the humidity is comparatively high, summers are cool, winters are mild and the climate is overall challenging. Furthermore, winter thaws are distinctive characteristic of the Icelandic weather, which increases strain on the asphalt.</p><p>An experiment was conducted where the adhesion of surface dressing that is common in Sweden was tested with two different aggregates by Vialit plate shock test method. First it was tested with Swedish granite and then with Icelandic basalt. The results from the aggregates were compared where the adhesion with the granite was stronger than with the basalt.</p><p>Previous study have found that by choosing binder and aggregate that have high adhesivity at low temperature reduces the risk of surface dressing defects, especially when paving takes place in the early and late summer season. When choosing aggregates for road construction the main criteria is cost, therefore aggregates that are used usually reflect the local geology because transporting aggregates for significant distances is expensive. Concluding from the experiment, it is not recommended to use the basalt with the Swedish surface dressing in practice now due to the lesser adhesion compared to the granite. However further research on the asphalt mix with the Swedish surface dressing and the basalt should be conducted.</p>
----------------------------------------------------------------------
In diva2:1451748 abstract is: <p>AbstractThe degree project was carried out at the request of the Swedish Tax Agency's IntegrationCompetency Center (ICC), or Skatteverket’s ICC, which works with buildingsystem integrations and open APIs, as well as providing competence support in integrationdevelopment for other organizations within Skatteverket. Developers atSkatteverket’s ICC need local environments that correspond to the production environmentsexisting for services currently running. This is necessary to allow developersto investigate problems and further develop the system. The process for doingthis is currently time-consuming, manual and complex. The purpose of the degreeproject is to investigate whether it is possible to, through automation and standardization,reduce the number of steps required to set up the local development environment.To investigate this, a pre-study has been conducted, in which both previouswork in the subject area and the Swedish Tax Agency's system have been evaluated.Thereafter, a prototype has been developed that analyses the production environmentof the current production environment for necessary information, to then createa complete development environment using the previously retrieved information.In order to validate the results, tests have been conducted using the prototypeset against the manual approach.The result shows that it is possible to reduce the number of steps required to set upa local development environment corresponding to the current production environment.It also shows that most of the steps that are saved depend on the number ofdependencies that a service holds when its environment is replicated.KeywordsIntegration development, EAI, Skatteverket, Eclipse, automation, development environment,MuleSoft, Anypoint, dependencies.</p>


w='prototypeset' val={'c': 'prototype set', 's': 'diva2:1451748', 'n': 'correct in original'}

corrected abstract:
<p>The degree project was carried out at the request of the Swedish Tax Agency's Integration Competency Center (ICC), or Skatteverket’s ICC, which works with building system integrations and open APIs, as well as providing competence support in integration development for other organizations within Skatteverket. Developers at Skatteverket’s ICC need local environments that correspond to the production environments existing for services currently running. This is necessary to allow developers to investigate problems and further develop the system. The process for doing this is currently time-consuming, manual and complex. The purpose of the degree project is to investigate whether it is possible to, through automation and standardization, reduce the number of steps required to set up the local development environment. To investigate this, a pre-study has been conducted, in which both previous work in the subject area and the Swedish Tax Agency's system have been evaluated. Thereafter, a prototype has been developed that analyses the production environment of the current production environment for necessary information, to then create a complete development environment using the previously retrieved information. In order to validate the results, tests have been conducted using the prototype set against the manual approach.</p><p>The result shows that it is possible to reduce the number of steps required to set up a local development environment corresponding to the current production environment. It also shows that most of the steps that are saved depend on the number of dependencies that a service holds when its environment is replicated.</p>
----------------------------------------------------------------------
title: "Workplace Interventions forPromoting Physical Health: A Qualitative Study of Health-Promoting Initiatives in a GlobalPharmaceutical Company"
==> "Workplace Interventions for Promoting Physical Health: A Qualitative Study of Health-Promoting Initiatives in a Global Pharmaceutical Company"

In diva2:1887795 abstract is: <p>Workplaces provide promising prospects for enhanced public health through related efforts,intended to preserve, enhance and ensure safety and health. A Health and Wellness departmentat a pharmaceutical company conveyed curiosity regarding the reach of the health-promotinginitiatives they provided to their front-line workers. The aim of this study was thus to investigatehow the front-line workers perceive health-promoting initiatives and to identify promoting andhindering aspects for increased engagement. Applied methods included semi-structuredinterviews, document review, and a synthesis of collected empirical evidence to enable anoverview and formulation of proposals. The interviews contributed insight into how the healthpromotingefforts are perceived by the target group, which also highlighted nuanced aspects ofcommitment. The approach included 18 respondents, with nine front-line representatives, andthe rest with a reputable connection to health promotion efforts and the work of the target group.The document review included web-based material on health promotion initiatives, termedHealth-Toolbox, focusing on three of its domains which include physical health, competencedevelopment and Health Ambassadors. The synthesis was constructed via an applicableframework (HTO), systematic review and comparison of collected data. The results showedthat a variety of health-promoting efforts exist at the company, but that perceptions about itsavailability and applicability often vary from person to person. Promoting aspects for increasedengagement were found to include motivational work, support, communication andaccessibility as well as the Health Ambassador role. While hindering aspects included lack oftime and resources, format and availability. A recurring phenomenon in the results was to findaspects that sometimes highlighted two sides of the same coin, where, for example, accessibilitywas shown as a promoting aspect that required a separate interpretation from availability thathighlighted other hindering aspects. The comparison of the empirical evidence generatedrecommendations with a particular focus on making available information about healthpromotingefforts and benefits through focused channels adapted to the target group.Organisational climates with possibilities for motivational work are concluded as important forhealth-promoting activities.</p>


corrected abstract:
<p>Workplaces provide promising prospects for enhanced public health through related efforts, intended to preserve, enhance and ensure safety and health. A Health and Wellness department at a pharmaceutical company conveyed curiosity regarding the reach of the health-promoting initiatives they provided to their front-line workers. The aim of this study was thus to investigate how the front-line workers perceive health-promoting initiatives and to identify promoting and hindering aspects for increased engagement. Applied methods included semi-structured interviews, document review, and a synthesis of collected empirical evidence to enable an overview and formulation of proposals. The interviews contributed insight into how the health promoting efforts are perceived by the target group, which also highlighted nuanced aspects of commitment. The approach included 18 respondents, with nine front-line representatives, and the rest with a reputable connection to health promotion efforts and the work of the target group. The document review included web-based material on health promotion initiatives, termed Health-Toolbox, focusing on three of its domains which include physical health, competence development and Health Ambassadors. The synthesis was constructed via an applicable framework (HTO), systematic review and comparison of collected data. The results showed that a variety of health-promoting efforts exist at the company, but that perceptions about its availability and applicability often vary from person to person. Promoting aspects for increased engagement were found to include motivational work, support, communication and accessibility as well as the Health Ambassador role. While hindering aspects included lack of time and resources, format and availability. A recurring phenomenon in the results was to find aspects that sometimes highlighted two sides of the same coin, where, for example, accessibility was shown as a promoting aspect that required a separate interpretation from availability that highlighted other hindering aspects. The comparison of the empirical evidence generated recommendations with a particular focus on making available information about health promoting efforts and benefits through focused channels adapted to the target group. Organisational climates with possibilities for motivational work are concluded as important for health-promoting activities.</p>
----------------------------------------------------------------------
In diva2:1454856 abstract is: <p>Measurement of protein levels, one of the essential criteria to assess the health state ofpatients, is widely used in clinical settings nowadays. Various advanced proteomics assays forclinical use have been approved by the Food and Drug Administration (FDA) and cleared forlaboratory use in US, including immunoassays (IAs) and liquid chromatography tandem massspectrometry (LC-MS/MS)2. However, both of them have the limitation of sensitivity, specificity,and multiplexing ability despite their strengths3-11. In this project, a combination of these twotechnologies called immunocapture mass spectrometry (MS) will be, thus, developed to overcomethe concerns about the precise quantification of multiple targets in complex samples, multiplexingability, time, and cost for studies of large cohorts11. The final aim is to establish a refined protocolfor robust sample handling and to optimize the enrichment conditions for precise quantification oflow abundant proteins over time as well as for highly-parallel workflows required in clinicalcontext. This project used plasma, binders, and stable-isotope internal standards (SIS), availableresources provided by the research group and the Human Protein Atlas (HPA) project. Theexperiments demonstrated a great binding ability of agarose beads as solid phase support towardsbinders, monoclonal antibodies (mAbs), with 10- to 1000-fold increase in the amount of singlechainvariable fragment (scFv) found on beads without significant non-specific binding to theQuantification Tag (QTag) compared to other types of beads. The analysis also revealed that notall mAbs could capture their corresponding PrEST during validation: only seven out of 60 mAbsshowed the signals of their target but one showed off-target interaction. Besides, limit of detection(LOD), limit of quantification (LOQ), and the endogenous level of each peptide determined fromits equivalent protein in plasma were examined through constructing its standard curves bySelecting Reaction Monitoring (SRM) development. Nevertheless, those seven mAbs did notsuccessfully capture their target protein in diluted plasma to ensure successful enrichment pastLOD and LOQ. Those results suggest a further enhancement of the conditions for binder-targetincubation. Additionally, alternative for PrEST should be proposed to completely eliminateunwanted binding towards solid phase support and to increase the binding rate towards binders.Finally, mAbs should be thoroughly validated their right-target-capturing ability.</p>


w='cohorts11' val={'c': 'cohorts', 's': 'diva2:1454856', 'n': 'the 11 was a superscript citation'}
w='strengths3-11' val={'c': 'strengths', 's': 'diva2:1454856', 'n': 'the 3-11 was a superscript citation'}

corrected abstract:
<p>Measurement of protein levels, one of the essential criteria to assess the health state of patients, is widely used in clinical settings nowadays. Various advanced proteomics assays for clinical use have been approved by the Food and Drug Administration (FDA) and cleared for laboratory use in US, including immunoassays (IAs) and liquid chromatography tandem mass spectrometry (LC-MS/MS)<sup>2</sup>. However, both of them have the limitation of sensitivity, specificity, and multiplexing ability despite their strengths<sup>3-11</sup>. In this project, a combination of these two technologies called immunocapture mass spectrometry (MS) will be, thus, developed to overcome the concerns about the precise quantification of multiple targets in complex samples, multiplexing ability, time, and cost for studies of large cohorts<sup>11</sup>. The final aim is to establish a refined protocol for robust sample handling and to optimize the enrichment conditions for precise quantification of low abundant proteins over time as well as for highly-parallel workflows required in clinical context. This project used plasma, binders, and stable-isotope internal standards (SIS), available resources provided by the research group and the Human Protein Atlas (HPA) project. The experiments demonstrated a great binding ability of agarose beads as solid phase support towards binders, monoclonal antibodies (mAbs), with 10- to 1000-fold increase in the amount of single-chain variable fragment (scFv) found on beads without significant non-specific binding to the Quantification Tag (QTag) compared to other types of beads. The analysis also revealed that not all mAbs could capture their corresponding PrEST during validation: only seven out of 60 mAbs showed the signals of their target but one showed off-target interaction. Besides, limit of detection (LOD), limit of quantification (LOQ), and the endogenous level of each peptide determined from its equivalent protein in plasma were examined through constructing its standard curves by Selecting Reaction Monitoring (SRM) development. Nevertheless, those seven mAbs did not successfully capture their target protein in diluted plasma to ensure successful enrichment past LOD and LOQ. Those results suggest a further enhancement of the conditions for binder-target incubation. Additionally, alternative for PrEST should be proposed to completely eliminate unwanted binding towards solid phase support and to increase the binding rate towards binders. Finally, mAbs should be thoroughly validated their right-target-capturing ability.</p>
----------------------------------------------------------------------
In diva2:1843155 abstract is: <p>It is very expensive to develop ground-based infrastructure to supply the entire earth with internet.This is especially the case in loosely populated areas where the economic incentive is very low for acompany. Sweden which is a large country with a small population has many areas in this category.At the same time, Sweden is a technologically advanced nation where most people use the internetand mobile telephony daily.The Swedish government cooperates with teleoperators. These teleoperators buy licenses on auctionto be able to use radio spectrum within Sweden which is a limited resource. In exchange they need tofulfill certain quotas on reception and wireless service. The operators are in this way obligated by theSwedish government to supply the country with the potential for reception .However, this is not very economic in rural areas where few people live, and the usage of individualground stations is low. This leads to ground stations being very unprofitable and is also a waste ofresources such as land, materials and capital. Companies are willing to spend large sums of money toavoid obligation to do this.At the same time satellite internet is becoming increasingly sophisticated and more and more peopleworldwide are starting to adopt satellite internet.A lot of wireless technology uses specific different radio frequencies to transmit information. The newtype of satellite internet generally uses frequencies within the Ku - and Ka-bands which are both af-fected by water. This means that for example, rain or snow could affect the quality of the wirelessconnection.A question that should be discussed is how satellite internet compares to existing technologies like4G and 5G. If this is done, it will be easier for involved parties to estimate its usefulness.This bachelor’s thesis focuses on gathering information to help resolve the question of how usefulsatellite internet can be. To accomplish this, performance measurements have been performed on aservice that provides satellite internet. Starlink, which is the most deployed satellite internet service,has been the subject of these measurements. The result is measurements on the performance of Star-link and comparisons between 4G/5G and Starlinks that can hopefully help shed some light on Star-links usefulness in Scandinavia.The measurements indicate that Starlink delivers a reliable connection that can be compared to mo-bile service but that in Sweden there is an added delay of circa 20 milliseconds due to the groundstations being located in central Europe.</p>


corrected abstract:
<p>It is very expensive to develop ground-based infrastructure to supply the entire earth with internet. This is especially the case in loosely populated areas where the economic incentive is very low for a company. Sweden which is a large country with a small population has many areas in this category. At the same time, Sweden is a technologically advanced nation where most people use the internet and mobile telephony daily.</p><p>The Swedish government cooperates with teleoperators. These teleoperators buy licenses on auction to be able to use radio spectrum within Sweden which is a limited resource. In exchange they need to fulfill certain quotas on reception and wireless service. The operators are in this way obligated by the Swedish government to supply the country with the potential for reception.</p><p>However, this is not very economic in rural areas where few people live, and the usage of individual ground stations is low. This leads to ground stations being very unprofitable and is also a waste of resources such as land, materials and capital. Companies are willing to spend large sums of money to avoid obligation to do this.</p><p>At the same time satellite internet is becoming increasingly sophisticated and more and more people worldwide are starting to adopt satellite internet.</p><p>A lot of wireless technology uses specific different radio frequencies to transmit information. The new type of satellite internet generally uses frequencies within the Ku- and Ka-bands which are both affected by water. This means that for example, rain or snow could affect the quality of the wireless connection.</p><p>A question that should be discussed is how satellite internet compares to existing technologies like 4G and 5G. If this is done, it will be easier for involved parties to estimate its usefulness.</p><p>This bachelor’s thesis focuses on gathering information to help resolve the question of how useful satellite internet can be. To accomplish this, performance measurements have been performed on a service that provides satellite internet. Starlink, which is the most deployed satellite internet service, has been the subject of these measurements. The result is measurements on the performance of Starlink and comparisons between 4G/5G and Starlinks that can hopefully help shed some light on Starlinks usefulness in Scandinavia.</p><p>The measurements indicate that Starlink delivers a reliable connection that can be compared to mobile service but that in Sweden there is an added delay of circa 20 milliseconds due to the ground stations being located in central Europe.</p>
----------------------------------------------------------------------
In diva2:1272715 abstract is: <p>Henriksdals sewage treatment plant is undergoing a major reconstruction in order to handleincreased load as well as the stricter cleaning requirements expected in the future. Due to theplanned shutdown of Bromma treatment plant, additional wastewater will be led to Henriksdalssewage treatment plant. When a significant increase in population in the catchment area is alsoexpected, the treatment plant will need a doubling of its capacity compared to today. Today'snitrogen treatment requirements in the effluent wastewater are 10 mg/L and this is expected tobe 6 mg/L in future. To achieve these requirements, the biological treatment step of the currentactive sludge process will be combined with membrane filtration in a so-called membranebioreactor (MBR). In the biological treatment, the nitrogen removal is a two-step process usingbacteria. These steps are nitrification and denitrification.</p><p>In the pilot plant at Hammarby Sjöstadsverk, in 2018, it is of interest to study the presence ofnitrogen fractions in the process, since the first of seven MBR treatment lines will be started atHenriksdals WWTP early 2020. At the start of this line there are some limitations regardingdosage of chemicals for the removal of phosphorus and nitrogen. No carbon source orprecipitation chemicals will be possible to add, since the storage for these will not be constructedin time for the startup.</p><p>To examine how the nitrogen acts in the different zones, and if it is possible to achieve thedesired nitrogen removal under these conditions, 12 test points have been selected in thebiological process line. The total concentration of nitrogen, N-tot, and the concentration ofnitrogen in the form of NO<sub>2</sub>, NO<sub>3</sub>, NH<sub>4</sub>, as well as the COD have been measured in these pointson four separate occasions. The samples have been analyzed and mass balances over thedifferent zones in the process line have been set up. The results vary between the different tests,performed in the morning and after lunch, but some trends can be observed. The predenitrificationtakes place in two zones, of which the second zone, BR2, exhibits very low ornon-existent changes in the mass flow of nitrate nitrogen (NO<sub>3</sub>-N), indicating that one zone withpre-denitrification would be sufficient. The first zone of nitrification also shows poor results,which may indicate that the zone has a low oxygen content and needs better airflow. The zonewith after-denitrification still has large amounts of nitrate in the outflow, which means that twozones with post-denitrification could produce better results. At all samplings a total nitrogen (Ntot)content of less than 10 mg/L was measured in the purified permeate flow. The average ofthe four occasions was also below the future requirement of 6 mg/L.</p>


w='Ntot' val={'c': 'N-tot', 's': 'diva2:1272715', 'n': 'hyphen at end of line'}

corrected abstract:
<p>Henriksdals sewage treatment plant is undergoing a major reconstruction in order to handle increased load as well as the stricter cleaning requirements expected in the future. Due to the planned shutdown of Bromma treatment plant, additional wastewater will be led to Henriksdals sewage treatment plant. When a significant increase in population in the catchment area is also expected, the treatment plant will need a doubling of its capacity compared to today. Today's nitrogen treatment requirements in the effluent wastewater are 10 mg/L and this is expected to be 6 mg/L in future. To achieve these requirements, the biological treatment step of the current active sludge process will be combined with membrane filtration in a so-called membrane bioreactor (MBR). In the biological treatment, the nitrogen removal is a two-step process using bacteria. These steps are nitrification and denitrification.</p><p>In the pilot plant at Hammarby Sjöstadsverk, in 2018, it is of interest to study the presence of nitrogen fractions in the process, since the first of seven MBR treatment lines will be started at Henriksdals WWTP early 2020. At the start of this line there are some limitations regarding dosage of chemicals for the removal of phosphorus and nitrogen. No carbon source or precipitation chemicals will be possible to add, since the storage for these will not be constructed in time for the startup.</p><p>To examine how the nitrogen acts in the different zones, and if it is possible to achieve the desired nitrogen removal under these conditions, 12 test points have been selected in the biological process line. The total concentration of nitrogen, N-tot, and the concentration of nitrogen in the form of NO<sub>2</sub>, NO<sub>3</sub>, NH<sub>4</sub>, as well as the COD have been measured in these points on four separate occasions. The samples have been analyzed and mass balances over the different zones in the process line have been set up. The results vary between the different tests, performed in the morning and after lunch, but some trends can be observed. The predenitrification takes place in two zones, of which the second zone, BR2, exhibits very low or non-existent changes in the mass flow of nitrate nitrogen (NO<sub>3</sub>-N), indicating that one zone with pre-denitrification would be sufficient. The first zone of nitrification also shows poor results, which may indicate that the zone has a low oxygen content and needs better airflow. The zone with after-denitrification still has large amounts of nitrate in the outflow, which means that two zones with post-denitrification could produce better results. At all samplings a total nitrogen (N-tot)content of less than 10 mg/L was measured in the purified permeate flow. The average of the four occasions was also below the future requirement of 6 mg/L.</p>
----------------------------------------------------------------------
In diva2:853075 abstract is: <p>The purpose of this thesis was to examine the risk of hazardous air pollutantsthat employees at the harbor may be exposed to and assess whether anyexposure limits might be exceeded, and to review if the security measures taken,can be improved. The results of the study show that there is some risk ofexposure for those working in the harbor from both the cargo, diesel exhaustand residues from fumigants in the containers. The operations when the truckdriver is inside the forklift is considered relatively protected from both particlesand gases, but there are other tasks where the worker is not as protected.Measurements of nitrogen dioxide showed that exposure to diesel exhaust forthe employee who opened the containers during the days that the measurementswere performed did not exceed some critical values, but was slightly higherthan for the driver who was sitting in the forklift. Whether the exposure to dust,from the cargo, constitutes a risk for health effects is impossible to determinewithout a thorough risk assessment, where measurements of dust can give anindication of how risky the situation is for the exposed workers.The organization should implement safer practices and implementing measuresthat motivates employees to use existing protective equipment. This applies notonly for exposure to diesel exhaust and dust from loading and unloading ofgoods, but also at the opening the doors on the containers in which residues offumigants, in which some are suspected to be carcinogenic, can remain in thecontainer.Conclusions: Conduct a thorough risk assessment and identify potential health riskswith air pollution. Establish written work instructions where potential hazards exist. Install ventilation in the warehouses, which is controlled by the levels ofnitrogen dioxide, to ensure that high levels of diesel exhaust does notoccur. Need to increase motivation for safety and the use of protectiveequipment. Implement procedures to comply with the legislation for the managementof dust and gases that applies to carcinogens substances.</p>

corrected abstract:
<p>The purpose of this thesis was to examine the risk of hazardous air pollutants that employees at the harbor may be exposed to and assess whether any exposure limits might be exceeded, and to review if the security measures taken, can be improved. The results of the study show that there is some risk of exposure for those working in the harbor from both the cargo, diesel exhaust and residues from fumigants in the containers. The operations when the truck driver is inside the forklift is considered relatively protected from both particles and gases, but there are other tasks where the worker is not as protected. Measurements of nitrogen dioxide showed that exposure to diesel exhaust for the employee who opened the containers during the days that the measurements were performed did not exceed some critical values, but was slightly higher than for the driver who was sitting in the forklift. Whether the exposure to dust, from the cargo, constitutes a risk for health effects is impossible to determine without a thorough risk assessment, where measurements of dust can give an indication of how risky the situation is for the exposed workers.</p><p>The organization should implement safer practices and implementing measures that motivates employees to use existing protective equipment. This applies not only for exposure to diesel exhaust and dust from loading and unloading of goods, but also at the opening the doors on the containers in which residues of fumigants, in which some are suspected to be carcinogenic, can remain in the container.</p><p>Conclusions:<ul><li>Conduct a thorough risk assessment and identify potential health risks with air pollution.</li><li>Establish written work instructions where potential hazards exist.</li><li>Install ventilation in the warehouses, which is controlled by the levels of nitrogen dioxide, to ensure that high levels of diesel exhaust does not occur.</li><li>Need to increase motivation for safety and the use of protective equipment.</li><li>Implement procedures to comply with the legislation for the management of dust and gases that applies to carcinogens substances.</li> </ul></p>
----------------------------------------------------------------------
In diva2:1229284 abstract is: <p>The objective of this study is to investigate the eect of dierent fuels on two uidized bed boiler systemsat the energy company Soderenergi's site in Igelsta, called IKV and IGV P3. Today, recovered wastewood (RWW) is the major fuel share fed into the boilers. However, with an insecure fuel supply in thefuture, other fuel types must be considered. Based on knowledge from previous fuel usage in the boilers,an evaluation of how other potential fuel mixtures may eect the operation is conducted. The additionalfuels considered in the fuel blends are; stem wood chips, cutter shavings, solid recycled fuel (SRF) andrubber.With elemental analysis of the fuels and established key numbers, the previous fuel mixtures are evaluated.The indications by the guiding parameters are compared with experienced problems and the formercondition of the boilers, and the risk limits for the key numbers are adjusted to a suitable level. Thepotential mixtures are evaluated with the key numbers and the updated limits. In addition to the keynumbers, the heavy metal concentration, the heating value, the moisture content and the ash content ofthe fuel blends are included in the evaluation. The considered damages in the boilers caused by the fuelblends are corrosion, sintering and fouling.The damage level from the current fuel usage for IKV and IGV P3 is fairly low. The results from theanalyzed fuel mixtures show an increased damage risk in the boilers. Additionally, adjustments of theboiler systems are required by some of the analyzed fuel mixtures. In general, the corrosion risk andthe heavy metal content will increase in comparison with today's fuel. The fouling and slagging are aswell expected to increase for the assessed fuel mixtures. Moreover, the result illustrates an increased ashgeneration, which demands a reconstruction of the ash cooling system for IKV. Furthermore, the increaseof LHV in the assessed fuel mixtures to IGV P3, is likely to require an increased capacity of the ue gasrecirculation pump.In the analysis of the potential fuel mixtures it is found that the corrosion risk expressed by the keynumbers is reduced with a higher share of rubber. The heavy metal content is, however, increased,leading to e.g. an enhanced risk for formation of eutectic salts, which as well are corrosive. On thecontrary, the fuel mixtures with a high risk expressed by the key numbers, have the lowest concentrationsof heavy metals. Due to the results are con icting, a balance between the risk indicated by the keynumbers and the heavy metal concentration must be considered in the evaluation. The fuel mixturesconsidered causing least damage to IKV are a mixture of 42% RWW, 48% wood fuel and 15% SRF, and amixture of 70% wood fuel, 20% SRF and 10% rubber. The fuel mixtures considered causing least damageto IGV P3 are a mixture of 85% RWW and 15% rubber and a mixture of 70% RWW and 30% SRF.</p>

w='eect' val={'c': 'effect', 's': ['diva2:1344757', 'diva2:1229284'], 'n': 'missing ligature'}
w='uidized' val={'c': 'fluidized', 's': 'diva2:1229284', 'n': 'missing ligature'}
w='ue' val={'c': 'flue', 's': 'diva2:1229284', 'n': 'missing ligature'}

corrected abstract:
<p>The objective of this study is to investigate the effect of different fuels on two fluidized bed boiler systems at the energy company Söderenergi’s site in Igelsta, called IKV and IGV P3. Today, recovered waste wood (RWW) is the major fuel share fed into the boilers. However, with an insecure fuel supply in the future, other fuel types must be considered. Based on knowledge from previous fuel usage in the boilers, an evaluation of how other potential fuel mixtures may effect the operation is conducted. The additional fuels considered in the fuel blends are; stem wood chips, cutter shavings, solid recycled fuel (SRF) and rubber.</p><p>With elemental analysis of the fuels and established key numbers, the previous fuel mixtures are evaluated. The indications by the guiding parameters are compared with experienced problems and the former condition of the boilers, and the risk limits for the key numbers are adjusted to a suitable level. The potential mixtures are evaluated with the key numbers and the updated limits. In addition to the key numbers, the heavy metal concentration, the heating value, the moisture content and the ash content of the fuel blends are included in the evaluation. The considered damages in the boilers caused by the fuel blends are corrosion, sintering and fouling.</p><p>The damage level from the current fuel usage for IKV and IGV P3 is fairly low. The results from the analyzed fuel mixtures show an increased damage risk in the boilers. Additionally, adjustments of the boiler systems are required by some of the analyzed fuel mixtures. In general, the corrosion risk and the heavy metal content will increase in comparison with today’s fuel. The fouling and slagging are as well expected to increase for the assessed fuel mixtures. Moreover, the result illustrates an increased ash generation, which demands a reconstruction of the ash cooling system for IKV. Furthermore, the increase of LHV in the assessed fuel mixtures to IGV P3, is likely to require an increased capacity of the flue gas recirculation pump.</p><p>In the analysis of the potential fuel mixtures it is found that the corrosion risk expressed by the key numbers is reduced with a higher share of rubber. The heavy metal content is, however, increased, leading to e.g. an enhanced risk for formation of eutectic salts, which as well are corrosive. On the contrary, the fuel mixtures with a high risk expressed by the key numbers, have the lowest concentrations of heavy metals. Due to the results are conflicting, a balance between the risk indicated by the key numbers and the heavy metal concentration must be considered in the evaluation. The fuel mixtures considered causing least damage to IKV are a mixture of 42% RWW, 48% wood fuel and 15% SRF, and a mixture of 70% wood fuel, 20% SRF and 10% rubber. The fuel mixtures considered causing least damage to IGV P3 are a mixture of 85% RWW and 15% rubber and a mixture of 70% RWW and 30% SRF.</p>
----------------------------------------------------------------------
In diva2:1468986 abstract is: <p>Background:Approximately 15% of the world’s population are affected bysome kind of disability where over 150 conditions may affect the human gaitpattern. The ability to ambulate with ease is important for overall well-being.Various assistive devices have been developed to improve mobility of theirusers. A lot of research is currently focused on ankle exoskeletons, showingpromising results in providing important assistance during stance phase of gait.</p><p>Objective:To investigate how different combinations of active and passiveelements in an ankle exoskeleton affects the metabolic cost of walking.</p><p>Methods:Musculoskeletal simulations were carried out in OpenSim Moco.Different assistive configurations were tested over one gait cycle using a pas-sive element, an active element, and a parallel connection of the both. Parame-ter values were modified to find the most optimal setup for reducing metaboliccost.</p><p>Results:All assistive configurations were found successful in reducing bothwhole-body metabolic cost and the metabolic cost of the plantarflexors whencompared to the unassisted gait. Most whole-body metabolic cost reductionwas found when using a passive spring with resting length of 0.28 m and stiff-ness of 6 kN/m in parallel with an active motor capable of providing forceequal to 150% of body weight. The most reduction in metabolic cost of theplantarflexors was also found for a parallel connection of elements, but herewith a 100% body weight motor and spring with rest length of 0.19 m andstiffness of 10 kN/m. With higher assistance, more reduction in ankle mo-ment generated by the muscles was observed.</p><p>Conclusion:Powered ankle exoskeletons are promising in terms of minimiz-ing metabolic cost during walking due to assistance during late stance phaseof gait for ambulators requiring plantarflexor assistance.</p><p>Keywords:Simulation, exoskeleton, ankle, moco.</p>

w='mo-ment' val={'c': 'moment', 's': 'diva2:1468986', 'n': 'correct in original'}

corrected abstract:
<p><strong>Background</strong>: Approximately 15% of the world’s population are affected by some kind of disability where over 150 conditions may affect the human gait pattern. The ability to ambulate with ease is important for overall well-being. Various assistive devices have been developed to improve mobility of their users. A lot of research is currently focused on ankle exoskeletons, showing promising results in providing important assistance during stance phase of gait.</p><p><strong>Objective</strong>: To investigate how different combinations of active and passive elements in an ankle exoskeleton affects the metabolic cost of walking.</p><p><strong>Methods</strong>: Musculoskeletal simulations were carried out in OpenSim Moco. Different assistive configurations were tested over one gait cycle using a passive element, an active element, and a parallel connection of the both. Parameter values were modified to find the most optimal setup for reducing metabolic cost.</p><p><strong>Results</strong>: All assistive configurations were found successful in reducing both whole-body metabolic cost and the metabolic cost of the plantarflexors when compared to the unassisted gait. Most whole-body metabolic cost reduction was found when using a passive spring with resting length of 0.28 m and stiffness of 6 kN/m in parallel with an active motor capable of providing force equal to 150% of body weight. The most reduction in metabolic cost of the plantarflexors was also found for a parallel connection of elements, but here with a 100% body weight motor and spring with rest length of 0.19 m and stiffness of 10 kN/m. With higher assistance, more reduction in ankle moment generated by the muscles was observed.</p><p><strong>Conclusion</strong>: Powered ankle exoskeletons are promising in terms of minimizing metabolic cost during walking due to assistance during late stance phase of gait for ambulators requiring plantarflexor assistance.</p>
----------------------------------------------------------------------
In diva2:1739120 abstract is: <p>Automatically detecting events for people with diabetes mellitus using continuousglucose monitors is an important step in allowing insulin pumps to automaticallycorrect the blood glucose levels and for a more hands-off approach to thedisease. The automatic detection of events could also aid physicians whenassisting their patients when referring to their continuous glucose monitordata. A range of different deep learning algorithms has been applied forpredictions of different events for continuous glucose monitor data, such asthe onset for hyperglycemia, hypoglycemia or mealtime events. This thesisfocused on constructing sequences labelled from an unbalanced and assumedmisslabelled dataset to classify them as such using four different deep learningnetworks using convoluted neural networks and recurrent neural networks.Manual correction of the dataset allowed for only clear events starting witha high positive gradient to be labelled as positive. The classification wasperformed on exact timepoints and in time windows to allow the classificationto to be done around the beginning of an event instead of the exact timepoint.The results from using the unbalanced and assumed misslabelled datasetshowed the networks performing similarly, with high Recall and Precisionbelow 0.5, thus not found to be of use in a for automatic event detection.Further testing by using another dataset or further configurations is neededto clarify the capabilities of automatically detecting events. DDAnalytics willnot use any of the developed networks in any of their products.</p>

corrected abstract:
<p>Automatically detecting events for people with diabetes mellitus using continuous glucose monitors is an important step in allowing insulin pumps to automatically correct the blood glucose levels and for a more hands-off approach to the disease. The automatic detection of events could also aid physicians when assisting their patients when referring to their continuous glucose monitor data. A range of different deep learning algorithms has been applied for predictions of different events for continuous glucose monitor data, such as the onset for hyperglycemia, hypoglycemia or mealtime events. This thesis focused on constructing sequences labelled from an unbalanced and assumed misslabelled dataset to classify them as such using four different deep learning networks using convoluted neural networks and recurrent neural networks. Manual correction of the dataset allowed for only clear events starting with a high positive gradient to be labelled as positive. The classification was performed on exact timepoints and in time windows to allow the classification to to be done around the beginning of an event instead of the exact timepoint. The results from using the unbalanced and assumed misslabelled dataset showed the networks performing similarly, with high <em>Recall</em> and <em>Precision</em> below 0.5, thus not found to be of use in a for automatic event detection. Further testing by using another dataset or further configurations is needed to clarify the capabilities of automatically detecting events. DDAnalytics will not use any of the developed networks in any of their products.</p>
----------------------------------------------------------------------
In diva2:1864417 abstract is: <p>This thesis explores which transfer methods young adults prefer among Swish,Klarna, and Tink, as well as the technical implementation concerning code com-plexity for these transfer and payment methods. This is important to offer transferand payment methods that young adults feel comfortable with and to ensure thistarget group continues to move towards a cashless society. The study aims to fill theresearch gap regarding a direct comparison of these payment methods.Research shows that young adults prefer methods that are user-friendly and have astrong brand reputation. The research also highlights the need to specifically ad-dress young adults as they lack financial knowledge in the area.A prototype was developed to compare code complexity with the most popular pay-ment methods Swish, Klarna, and Tink. The code complexity for the implementa-tion of these methods was measured using Cyclomatic Complexity, and the resultsshowed that Swish, Klarna, and Tink all have similar implementations via API callsin their simplest form, which means they do not increase code complexity for devel-opers.Based on the comparison of code complexity, a complete prototype was chosen tobe developed for functional testing, implemented with Tink, using Quarkus for thebackend logic and SwiftUI for the frontend interface. SwiftUI was chosen to createa dynamic and user-friendly interface experience for Apple devices, while Quarkuswas chosen for its robustness and efficiency in server-side processing.The prototype was functionally tested and evaluated by the product owner Peak AMSecurities AB. The result was that the prototype's codebase can be used in futureproduction deployment.This study contributes to the understanding of young adults' preferences for digitalpayment methods and provides a technical foundation for further research in thearea. By comparing Swish, Klarna, and Tink, the work offers important insightsthat can help developers create better financial tools that meet users' needs and ex-pectations.</p>

w='devel-opers' val={'c': 'developers', 's': 'diva2:1864417'}
w='ad-dress' val={'c': 'address', 's': 'diva2:1864417'}
w='com-plex' val={'c': 'complex', 's': ['diva2:1864417', 'diva2:1802066']}

corrected abstract:
<p>This thesis explores which transfer methods young adults prefer among Swish, Klarna, and Tink, as well as the technical implementation concerning code complexity for these transfer and payment methods. This is important to offer transfer and payment methods that young adults feel comfortable with and to ensure this target group continues to move towards a cashless society. The study aims to fill the research gap regarding a direct comparison of these payment methods.</p><p>Research shows that young adults prefer methods that are user-friendly and have a strong brand reputation. The research also highlights the need to specifically address young adults as they lack financial knowledge in the area.</p><p>A prototype was developed to compare code complexity with the most popular payment methods Swish, Klarna, and Tink. The code complexity for the implementation of these methods was measured using Cyclomatic Complexity, and the results showed that Swish, Klarna, and Tink all have similar implementations via API calls in their simplest form, which means they do not increase code complexity for developers.</p><p>Based on the comparison of code complexity, a complete prototype was chosen to be developed for functional testing, implemented with Tink, using Quarkus for the backend logic and SwiftUI for the frontend interface. SwiftUI was chosen to create a dynamic and user-friendly interface experience for Apple devices, while Quarkus was chosen for its robustness and efficiency in server-side processing.</p><p>The prototype was functionally tested and evaluated by the product owner Peak AM Securities AB. The result was that the prototype's codebase can be used in future production deployment.</p><p>This study contributes to the understanding of young adults' preferences for digital payment methods and provides a technical foundation for further research in the area. By comparing Swish, Klarna, and Tink, the work offers important insights that can help developers create better financial tools that meet users' needs and expectations.</p>
----------------------------------------------------------------------
In diva2:1636873 abstract is: <p>Summary</p><p>Autonomous vehicles are a growing trend in society and are anticipated to change the wholetransportation system as we know it today. Many vehicle manufacturers are focusing ondevelopment of different kinds of self-driving vehicles. The variety of automation in vehiclescan be divided into six levels of automation, according to the Society for Automotive Engineers(SAE) International’s division. As the level of automation varies between vehicles, the level ofneeded driver action also varies. As the automation level gets higher the drivers role changesmore and more towards being a supervisory controller for the automation. This leads to adifferent need of information presented to the driver in an autonomous vehicle compared towhat is provided in a manual vehicle. The driver must be able to understand what the automationis doing and predict how it will behave in different situations. Hence, the design of the Human-Machine Interface (HMI) is utterly important for the safety of autonomous driving.</p><p>The main objective for the thesis work was to evaluate the driving compartment, from acognitive ergonomics point of view, for a special type of professional autonomous road vehicle.</p><p>A literature study focusing on evaluation and testing of HMIs in autonomous vehicles wereperformed and the evaluation was done through user tests in a simulator with 22 professionaldrivers. The tests were followed by questionnaires and interviews about the test participants’experience of the designed HMI.</p><p>In the literature study, only a few publications, were found addressing the validation of thewhole human machine interaction in autonomous vehicles. Most of the references had focus ontesting of limited subparts of the HMI, such as trust in automation, situational awareness andtransitions between manual and automatic driving mode.</p><p>The designed HMI worked as intended, since the test participants noticed and acted upon allHMI signals and were able to takeover and handover the control from/to the automation. Somerecommendations were, however, provided for the future development work of the evaluatedautonomous vehicle. For example, since there were some limitations in the simulator used andin the test design, a next iteration of the test was recommended for future development.</p><p>This evaluation was done as a first exploratory test and the aim of the thesis work has beenfulfilled. The design of the HMI, the manoeuvring devices for the autonomous vehicle and theconcept of two driver roles are, at this stage, considered adequate for continuing the project.</p><p>It was also concluded that the driving behaviour of the automation is a crucial aspect forfacilitation of user-trust in the autonomous vehicle and that clear procedures for communicationare needed to ensure traffic safety.</p><p>Further, it was concluded that standardization of frameworks and methods for evaluatingautonomous vehicles is beneficial for developers, authorities as well as end-users.</p>


corrected abstract:
<p>Autonomous vehicles are a growing trend in society and are anticipated to change the whole transportation system as we know it today. Many vehicle manufacturers are focusing on development of different kinds of self-driving vehicles. The variety of automation in vehicles can be divided into six levels of automation, according to the Society for Automotive Engineers (SAE) International’s division. As the level of automation varies between vehicles, the level of needed driver action also varies. As the automation level gets higher the drivers role changes more and more towards being a supervisory controller for the automation. This leads to a different need of information presented to the driver in an autonomous vehicle compared to what is provided in a manual vehicle. The driver must be able to understand what the automation is doing and predict how it will behave in different situations. Hence, the design of the Human-Machine Interface (HMI) is utterly important for the safety of autonomous driving.</p><p>The main objective for the thesis work was to evaluate the driving compartment, from a cognitive ergonomics point of view, for a special type of professional autonomous road vehicle.</p><p>A literature study focusing on evaluation and testing of HMIs in autonomous vehicles were performed and the evaluation was done through user tests in a simulator with 22 professional drivers. The tests were followed by questionnaires and interviews about the test participants’ experience of the designed HMI.</p><p>In the literature study, only a few publications, were found addressing the validation of the whole human machine interaction in autonomous vehicles. Most of the references had focus on testing of limited subparts of the HMI, such as trust in automation, situational awareness and transitions between manual and automatic driving mode.</p><p>The designed HMI worked as intended, since the test participants noticed and acted upon all HMI signals and were able to takeover and handover the control from/to the automation. Some recommendations were, however, provided for the future development work of the evaluated autonomous vehicle. For example, since there were some limitations in the simulator used and in the test design, a next iteration of the test was recommended for future development.</p><p>This evaluation was done as a first exploratory test and the aim of the thesis work has been fulfilled. The design of the HMI, the manoeuvring devices for the autonomous vehicle and the concept of two driver roles are, at this stage, considered adequate for continuing the project.</p><p>It was also concluded that the driving behaviour of the automation is a crucial aspect for facilitation of user-trust in the autonomous vehicle and that clear procedures for communication are needed to ensure traffic safety.</p><p>Further, it was concluded that standardization of frameworks and methods for evaluating autonomous vehicles is beneficial for developers, authorities as well as end-users.</p>
----------------------------------------------------------------------
In diva2:1568044 abstract is: <p>Finite Element (FE) head models are very convenient tools forthe study of Traumatic Brain Injuries (TBIs) but lack significantanatomical details for the investigation of morphology or age-dependantinjury mechanisms. In this context, the use of deformable registrationalgorithms for the generation of personalized head models is veryconsistent for the development of improved protection systems likehelmets. This thesis presents the performances of the registrationpipeline Demons combined to the Difformable Registration via AttributesMatching and Mutual-SaliencyWeighting (DRAMMS) for the generationof FE head models. Twelve subject-specific models are formed bymorphing the baseline mesh with the displacement fields resultingfrom the registration methods. The obtained models are assessedand compared through the evaluation of elements’ quality by analysisof the distortion index distribution. The Dice similarity coefficientis also calculated to estimate the personalization accuracy of theapplied pipeline. The Demons+DRAMMS registration pipeline showssatisfactory personalization accuracy for cranial mask and internalbrain structures. No significant degradation of mesh quality dueto the morphing process or specific subject morphology is observed.The present work corroborates previous study regarding the use ofDemons+DRAMMS registration pipeline for generating subject-specifichead models and validates the performances of the registration methodsand the repeatability of the morphing process for this purpose.</p>

w='age-dependant' val={'c': 'age-dependent', 's': 'diva2:1568044'}

corrected abstract:
<p>Finite Element (FE) head models are very convenient tools for the study of Traumatic Brain Injuries (TBIs) but lack significant anatomical details for the investigation of morphology or age-dependant injury mechanisms. In this context, the use of deformable registration algorithms for the generation of personalized head models is very consistent for the development of improved protection systems like helmets. This thesis presents the performances of the registration pipeline Demons combined to the Difformable Registration via Attributes Matching and Mutual-Saliency Weighting (DRAMMS) for the generation of FE head models. Twelve subject-specific models are formed by morphing the baseline mesh with the displacement fields resulting from the registration methods. The obtained models are assessed and compared through the evaluation of elements’ quality by analysis of the distortion index distribution. The Dice similarity coefficient is also calculated to estimate the personalization accuracy of the applied pipeline. The Demons+DRAMMS registration pipeline shows satisfactory personalization accuracy for cranial mask and internal brain structures. No significant degradation of mesh quality due to the morphing process or specific subject morphology is observed. The present work corroborates previous study regarding the use of Demons+DRAMMS registration pipeline for generating subject-specific head models and validates the performances of the registration methods and the repeatability of the morphing process for this purpose.</p>

----------------------------------------------------------------------
In diva2:730247 abstract is: <p>Thermalageing of the commercial selective catalytic reduction catalysts used inScania’s trucks was investigated using catalyst characterization techniques.Catalyst samples were oven-aged at 550 °C for up to 990 hours and investigatedwith nitrogen adsorption, oxygen chemisorption, X-ray fluorescence, X-raydiffraction, X-ray photoelectron spectroscopy and temperature-programmeddesorption of ammonia. The two latter methods are new to Scania and wereevaluated in depth. Furthermore, field-aged samples, which had had theircatalytic performance tested in another study, were investigated, in an attemptto find some link between characterization results and catalytic activity. Theinvestigation of oven-aged samples yielded information about the timescales ofcarrier sintering and sintering of catalytically active material, showing theformer to be much slower than the latter. It was also noted that the rate withwhich the catalyst’s ability to store ammonia decreases during thermal ageingwas similar to the rate of sintering of catalytically active material,suggesting that the loss of ammonia storage capability due to thermal ageing isrelated to sintering of the catalytically active material. X-ray photoelectronspectroscopy revealed that the fraction of vanadium in the outermost surfacelayer of the catalysts had increased during ageing. At the same time, thischaracterization technique appeared to have a low repeatability, possibly dueto the investigated catalyst having a high surface inhomogeneity. Whether ornot 500 ppm of NOx was present in the ageing atmosphere did notappear to affect the deactivation of the catalyst. Finally, no clear link couldbe found between characterization results and catalytic activity for field-agedsamples.</p>

w='Thermalageing' val={'c': 'Thermal ageing ', 's': 'diva2:730247', 'n': 'correct in original'}

corrected abstract:
<p>Thermal ageing of the commercial selective catalytic reduction catalysts used in Scania’s trucks was investigated using catalyst characterization techniques. Catalyst samples were oven-aged at 550 °C for up to 990 hours and investigated with nitrogen adsorption, oxygen chemisorption, X-ray fluorescence, X-ray diffraction, X-ray photoelectron spectroscopy and temperature-programmed desorption of ammonia. The two latter methods are new to Scania and were evaluated in depth. Furthermore, field-aged samples, which had had their catalytic performance tested in another study, were investigated, in an attempt to find some link between characterization results and catalytic activity. The investigation of oven-aged samples yielded information about the timescales of carrier sintering and sintering of catalytically active material, showing the former to be much slower than the latter. It was also noted that the rate with which the catalyst’s ability to store ammonia decreases during thermal ageing was similar to the rate of sintering of catalytically active material, suggesting that the loss of ammonia storage capability due to thermal ageing is related to sintering of the catalytically active material. X-ray photoelectron spectroscopy revealed that the fraction of vanadium in the outermost surface layer of the catalysts had increased during ageing. At the same time, this characterization technique appeared to have a low repeatability, possibly due to the investigated catalyst having a high surface inhomogeneity. Whether or not 500 ppm of NO<sub>x</sub> was present in the ageing atmosphere did not appear to affect the deactivation of the catalyst. Finally, no clear link could be found between characterization results and catalytic activity for field-aged samples.</p>
----------------------------------------------------------------------
In diva2:1352060 abstract is: <p><strong>Introduction:</strong> Clostridium thermocellum is considered a model organism forconsolidated bioprocessing, due to its ability to hydrolyze lignocellulosicbiomass more efficiently than many other organisms and to produce ethanol.In order to meet the industrial requirements of ethanol yield and titer, metabolicengineering efforts have been made resulting in a strain that successfullydisplays increased ethanol yield with reduced amount of some byproducts.However, the ethanol yield in this engineered strain still does not meet theindustrial requirements and significant amounts of amino acids are stillproduced. To attempt to decrease the level of amino acid excretion intended toimprove the ethanol yield in C. thermocellum, it is essential to understand itsmetabolism and how it is affected by different cultivation conditions and mediumcompositions. This study aimed to gain an insight in how carbon- and nitrogenlimitation affect amino acid excretion in C. thermocellum, with the hypothesisthat excess of carbon and nitrogen yields more amino acid excretion.</p><p><strong>Methods:</strong> Mass-balance based calculations of rates and yields were used toanalyze the metabolism of a wild-type of C. thermocellum (DSM 1313) grownanaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbonmedium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as thelimiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0and 400 RPM shaking at a dilution rate of 0.1 h-1.</p><p><strong>Conclusion:</strong>  Considering yields of total amino acids excreted in bothlimitations, it was hypothesized that C. thermocellum exploited the amino acidexcretion to maintain carbon balance around the pyruvate node caused byexcess of the carbon. Based on yield of valine excreted in particular, it washypothesized that amino acid excretion was used to maintain redox balance inthe metabolism of C. thermocellum, where malate shunt could play a major role.However, results of the Carbon-limitation did not allow any conclusion ofnitrogen excess having an effect on amino acid excretion in C. thermocellum.</p>


corrected abstract:
<p><strong>Introduction:</strong> <em>Clostridium thermocellum</em> is considered a model organism for consolidated bioprocessing, due to its ability to hydrolyze lignocellulosic biomass more efficiently than many other organisms and to produce ethanol. In order to meet the industrial requirements of ethanol yield and titer, metabolic engineering efforts have been made resulting in a strain that successfully displays increased ethanol yield with reduced amount of some byproducts. However, the ethanol yield in this engineered strain still does not meet the industrial requirements and significant amounts of amino acids are still produced. To attempt to decrease the level of amino acid excretion intended to improve the ethanol yield in <em>C. thermocellum</em>, it is essential to understand its metabolism and how it is affected by different cultivation conditions and medium compositions. This study aimed to gain an insight in how carbon- and nitrogen limitation affect amino acid excretion in <em>C. thermocellum</em>, with the hypothesis that excess of carbon and nitrogen yields more amino acid excretion.</p><p><strong>Methods:</strong> Mass-balance based calculations of rates and yields were used to analyze the metabolism of a wild-type of <em>C. thermocellum</em> (DSM 1313) grown anaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbon medium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as the limiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0 and 400 RPM shaking at a dilution rate of 0.1 h-1.</p><p><strong>Conclusion:</strong>  Considering yields of total amino acids excreted in both limitations, it was hypothesized that <em>C. thermocellum</em> exploited the amino acid excretion to maintain carbon balance around the pyruvate node caused by excess of the carbon. Based on yield of valine excreted in particular, it was hypothesized that amino acid excretion was used to maintain redox balance in the metabolism of <em>C. thermocellum</em>, where malate shunt could play a major role. However, results of the Carbon-limitation did not allow any conclusion of nitrogen excess having an effect on amino acid excretion in <em>C. thermocellum</em>.</p>
----------------------------------------------------------------------
In diva2:1038977 abstract is: <p>The cost of developing new pharmaceuticals has increased, while the number ofpharmaceuticals approved has declined. This highlights the importance for newpharmaceuticals to quickly become successful. The aim of this thesis is to explore thefactors of importance when launching new pharmaceuticals. Initially a literature reviewhas been conducted to explore general factors of importance for a pharmaceutical tobecome a commercial success. Furthermore, eleven in-depth interviews have beenperformed with stakeholders from the Swedish healthcare system to identify significantfactors on a national and regional level in Sweden. A thematic analysis was used tocategorize the data collected in the interviews.</p><p>The result of the literature review showed that the value creating process is of utmostimportance for a pharmaceutical to become a success. This is affected by a customeroriented focus, the design of the pharmaceutical study and the outcome from the healtheconomic analysis. Additionally, a number of challenges in the pharmaceutical supplychain were identified, which could cause a bottleneck during the launch of newpharmaceuticals.</p><p>In the empirical part a main theme ‘Trust’ with a total of seven subthemes was identified.The seven subthemes are factors that are essential to gain the trust and create the value,they are: ‘Guidelines and Regulations’, ‘Clinical Efficacy and Clinical Evidence’,‘Marketing’, ‘Information’, ‘Adherence and Compliance’, ‘Health Economics’ and‘Financial Aspect’. The subtheme ‘Guidelines and Regulations’ highlights the connectionbetween guidelines and utilization of pharmaceuticals. The ‘Clinical Efficacy and ClinicalEvidence’ was identified as the utmost important success factor. Furthermore the theme‘Value of Money’, including the subthemes ‘Health Economics’ and ‘Financial Aspect’,highlights the importance of the budget aspect and the increasing use of healtheconomics to evaluate health benefits and costs in healthcare. The subtheme ‘Adherenceand Compliance’ stresses the importance of information to the end user, whereas thesubtheme ‘Information’ highlights the information exchange among differentstakeholders. The subtheme ‘Marketing’ describes the effect of personal relationshipbetween industry and prescribers, and the effect on the pharmaceutical use it can have.</p>


corrected abstract:
<p>The cost of developing new pharmaceuticals has increased, while the number of pharmaceuticals approved has declined. This highlights the importance for new pharmaceuticals to quickly become successful. The aim of this thesis is to explore the factors of importance when launching new pharmaceuticals. Initially a literature review has been conducted to explore general factors of importance for a pharmaceutical to become a commercial success. Furthermore, eleven in-depth interviews have been performed with stakeholders from the Swedish healthcare system to identify significant factors on a national and regional level in Sweden. A thematic analysis was used to categorize the data collected in the interviews.</p><p>The result of the literature review showed that the value creating process is of utmost importance for a pharmaceutical to become a success. This is affected by a customer oriented focus, the design of the pharmaceutical study and the outcome from the health economic analysis. Additionally, a number of challenges in the pharmaceutical supply chain were identified, which could cause a bottleneck during the launch of new pharmaceuticals.</p><p>In the empirical part a main theme ‘Trust’ with a total of seven subthemes was identified. The seven subthemes are factors that are essential to gain the trust and create the value, they are: ‘Guidelines and Regulations’, ‘Clinical Efficacy and Clinical Evidence’, ‘Marketing’, ‘Information’, ‘Adherence and Compliance’, ‘Health Economics’ and ‘Financial Aspect’. The subtheme ‘Guidelines and Regulations’ highlights the connection between guidelines and utilization of pharmaceuticals. The ‘Clinical Efficacy and Clinical Evidence’ was identified as the utmost important success factor. Furthermore the theme ‘Value of Money’, including the subthemes ‘Health Economics’ and ‘Financial Aspect’, highlights the importance of the budget aspect and the increasing use of health economics to evaluate health benefits and costs in healthcare. The subtheme ‘Adherence and Compliance’ stresses the importance of information to the end user, whereas the subtheme ‘Information’ highlights the information exchange among different stakeholders. The subtheme ‘Marketing’ describes the effect of personal relationship between industry and prescribers, and the effect on the pharmaceutical use it can have.</p>
----------------------------------------------------------------------
In diva2:1228146 abstract is: <p>Using and finding applications from biomass is and will continue to be an important subject forresearch, and biomass from trees, has shown several outstanding aspects other than just for the pulpand paper applications. It is now, more than ever, time to find efficient uses for all the woodcomponents, in particular, the hemicelluloses. The hemicelluloses account for approximately onethirdof a dry composition of lignocellulosic wood biomasses. Of these hemicelluloses, xylan is themost abundant in many plants, particularly in hardwood. As for the Swedish forestry, xylan frombirch is considered as one of the most promising resources for the future.This thesis investigates the impact of acetylation of xylan on some properties such as solubility,thermal stability and film formation. Films were prepared using the non- and acetylated xylan withaddition of different plasticizers (glycerol, sorbitol and xylitol).Alkali-soluble birch xylan (ASX), obtained by ethanol/toluene extraction and sodium chloritedelignification of the wood sawdust followed by potassium hydroxide extraction of the obtainedholocellulose, and commercial xylan (CX) were acetylated to different degree of substitution withacetyl groups (DSAc), using acetic anhydride in dimethyl sulfoxide (DMSO) and 1-methylimidazole(NMI). Films were prepared by suspending non-acetylated xylan in water (H2O) and adding differentpercentages of plasticizers (20 and 40%) or by suspending acetylated xylan in chloroform (CHCl3).Characterizations of the non- and acetylated polymer (AcASX and AcCX) and films were conducted inorder to determine thermal and mechanical properties.CX and ASX presented different reactivity leading to different behaviour during acetylation and sodifferent DSAc. The thermal stability has been improved for both ASX and CX following the increase ofthe DSAc. Concerning film formation, ASX showed a great ability to form films through casting with orwithout plasticizers while it was impossible to obtain any films using only CX. For AcASX and AcCX thefilm formation using chloroform was depending on the DSAc and the dispersability in the solvent. Allthe films obtained have been mechanically and thermally tested. Best results for the mechanicaltests were obtained with 40% plasticizers with creation of a plastic behaviour and improvement ofthe flexibility. Thermally speaking, the thermal stability gained through acetylation of the samples islost by film casting, and use of plasticizers reduced the thermal stability as a new component wasadded to the composition.</p>

corrected abstract:
<p>Using and finding applications from biomass is and will continue to be an important subject for research, and biomass from trees, has shown several outstanding aspects other than just for the pulp and paper applications. It is now, more than ever, time to find efficient uses for all the wood components, in particular, the hemicelluloses. The hemicelluloses account for approximately one-third of a dry composition of lignocellulosic wood biomasses. Of these hemicelluloses, xylan is the most abundant in many plants, particularly in hardwood. As for the Swedish forestry, xylan from birch is considered as one of the most promising resources for the future.</p><p>This thesis investigates the impact of acetylation of xylan on some properties such as solubility, thermal stability and film formation. Films were prepared using the non- and acetylated xylan with addition of different plasticizers (glycerol, sorbitol and xylitol).<br>Alkali-soluble birch xylan (ASX), obtained by ethanol/toluene extraction and sodium chlorite delignification of the wood sawdust followed by potassium hydroxide extraction of the obtained holocellulose, and commercial xylan (CX) were acetylated to different degree of substitution with acetyl groups (DS<sub>Ac</sub>), using acetic anhydride in dimethyl sulfoxide (DMSO) and 1-methylimidazole (NMI). Films were prepared by suspending non-acetylated xylan in water (H<sub>2</sub>O) and adding different percentages of plasticizers (20 and 40%) or by suspending acetylated xylan in chloroform (CHCl<sub>3</sub>). Characterizations of the non- and acetylated polymer (AcASX and AcCX) and films were conducted in order to determine thermal and mechanical properties.</p><p>CX and ASX presented different reactivity leading to different behaviour during acetylation and so different DS<sub>Ac</sub>. The thermal stability has been improved for both ASX and CX following the increase of the DS<sub>Ac</sub>. Concerning film formation, ASX showed a great ability to form films through casting with or without plasticizers while it was impossible to obtain any films using only CX. For AcASX and AcCX the film formation using chloroform was depending on the DS<sub>Ac</sub> and the dispersability in the solvent. All the films obtained have been mechanically and thermally tested. Best results for the mechanical tests were obtained with 40% plasticizers with creation of a plastic behaviour and improvement of the flexibility. Thermally speaking, the thermal stability gained through acetylation of the samples is lost by film casting, and use of plasticizers reduced the thermal stability as a new component was added to the composition.</p>
----------------------------------------------------------------------
In diva2:1451603 abstract is: <p>AbstractThe organized crime and its need to launder money is growing all over theworld. A great responsibility to work with anti money laundering and counterterrorism financing lies on the financial institutions, and a big part of thatwork is transaction monitoring. The purpose of monitoring is to detect devianttransaction behavior that could indicate money laundering among customers.This thesis aimed to investigate what the Swedish law demand of a newlyfounded financial institution regarding the monitoring of transactions, andlater on develop an automated model to meet those demands. Sweden’s financialsupervisory authority demands a customer's transactions should bemonitored to find deviations from the expected behavior. Due to this demandthe model developed takes customers’ transactional history in account.The model makes use of a binary Bayesian network based on a number ofrules defined by well known money laundering transaction patterns.Through validation, against a manually evaluated transaction set, the developedmodel managed to find a number of potential cases of money laundering.On top of that around 90% of the planted known money launderingcases were found.KeywordsMoney laundering, terror financing, AML, CFT, transaction monitoring,Bayesian network</p>


corrected abstract:
<p>The organized crime and its need to launder money is growing all over the world. A great responsibility to work with anti money laundering and counter terrorism financing lies on the financial institutions, and a big part of that work is transaction monitoring. The purpose of monitoring is to detect deviant transaction behavior that could indicate money laundering among customers.</p><p>This thesis aimed to investigate what the Swedish law demand of a newly founded financial institution regarding the monitoring of transactions, and later on develop an automated model to meet those demands. Sweden’s financial supervisory authority demands a customer's transactions should be monitored to find deviations from the expected behavior. Due to this demand the model developed takes customers’ transactional history in account. The model makes use of a binary Bayesian network based on a number of rules defined by well known money laundering transaction patterns.</p><p>Through validation, against a manually evaluated transaction set, the developed model managed to find a number of potential cases of money laundering. On top of that around 90% of the planted known money laundering cases were found.</p>
----------------------------------------------------------------------
In diva2:1315697 abstract is: <p>Skin creams are one of Sky Resources key products, they are produced as an oil-in-water(O/W) emulsion. In order to form an emulsion the oil and water needs to be able to mixtogether, for that to happen the oil and water droplets have to be broken up into very smalldroplets (colloids).</p><p>There is a certain quality difference between the products from the research and developmentdepartment and the production department.</p><p>The skin creams have been made through a given recipe, which contains a number of chemicalformulas. Tests have been preformed and the results have been examined. The creams werefirst made in the research and development department’s laboratory and then that small scaleproduction was taken to big scale production in the production department. The results havebeen documented and the parameters that have been examined are speed, temperature and timeto see how they affect the viscosity of the creams.</p><p>A factorial experiment with three factors has been made. The factors are the time the skincream is homogenized, at what speed the cream is homogenized and at what temperature thephases are when the homogenizing is started. That gives a total of 8 creams from thelaboratory level.</p><p>The viscosity of the skin creams have been measured after 10 minutes, 24 hours, 48 hours and1 week to see how it is increasing with time and if it is increasing at all or perhaps decreasing.After 1 week the creams were also studied under a microscope to see how successful theemulsions were with different factors.</p><p>The fourth cream was the only cream from the laboratory that had a successful emulsion andstabilized viscosity. So the factors that are brought from laboratory to production is highhomogenization time, high homogenization speed and low temperature. Two differenthomogenizers were tried in production.</p><p>The factor that affects the viscosity the most seems to be the temperature. After these tests,there is still a difference between the products from the research and development departmentand the production department even with the lower temperatures on the oil and water phases.The second cream from production and cream 4 from the laboratory show the smallest qualitydifference. More tests need to be done in the production department with differenthomogenization speed and time with the lower temperature to establish the result.</p>

corrected abstract:
<p>Skin creams are one of Sky Resources key products, they are produced as an oil-in-water (O/W) emulsion. In order to form an emulsion the oil and water needs to be able to mix together, for that to happen the oil and water droplets have to be broken up into very small droplets (colloids).</p><p>There is a certain quality difference between the products from the research and development department and the production department.</p><p>The skin creams have been made through a given recipe, which contains a number of chemical formulas. Tests have been preformed and the results have been examined. The creams were first made in the research and development department’s laboratory and then that small scale production was taken to big scale production in the production department. The results have been documented and the parameters that have been examined are speed, temperature and time to see how they affect the viscosity of the creams.</p><p>A factorial experiment with three factors has been made. The factors are the time the skin cream is homogenized, at what speed the cream is homogenized and at what temperature the phases are when the homogenizing is started. That gives a total of 8 creams from the laboratory level.</p><p>The viscosity of the skin creams have been measured after 10 minutes, 24 hours, 48 hours and 1 week to see how it is increasing with time and if it is increasing at all or perhaps decreasing. After 1 week the creams were also studied under a microscope to see how successful the emulsions were with different factors.</p><p>The fourth cream was the only cream from the laboratory that had a successful emulsion and stabilized viscosity. So the factors that are brought from laboratory to production is high homogenization time, high homogenization speed and low temperature. Two different homogenizers were tried in production.</p><p>The factor that affects the viscosity the most seems to be the temperature. After these tests, there is still a difference between the products from the research and development department and the production department even with the lower temperatures on the oil and water phases. The second cream from production and cream 4 from the laboratory show the smallest quality difference. More tests need to be done in the production department with different homogenization speed and time with the lower temperature to establish the result.</p>
----------------------------------------------------------------------
In diva2:1229795 abstract is: <p>Water is an essential and indispensable component is the pulp- and paper production industry.The increase in energy costs, stricter environmental regulations and water resource shortageshave caused a reduction of the water footprint in the industry as well as an increase in waterrecycling and water circuit closure. Reducing water usage requires an understanding of wherecontaminants originate, as well as which streams are critical to the process and how they impactmill operation. The recirculation of water can cause contaminant accumulation; therefore millsemploy technologies for water treatment in the internal water cycles, the so-called ‘kidneys’.Application of membrane technology is one such option which can improve the recycled waterquality and reduce contaminant buildup.The present study was carried out on a lab-scale for the treatment of a tissue mill effluent usingmembrane separation. A combination of pretreatment methods and various membranes werecompared with regards to separation, flux and fouling. The AlfaLaval M20 device was to treatwastewater samples sent from the mill, where the permeate was recirculated to the feed tank.COD and TOC levels are compared with regards to determining the separation efficiency. Thepermeate flux was measured over the two-hour filtration period, as well as flux recovery todetermine fouling levels. Additionally, some economic aspects of the process are discussed.This study suggests the potential application of a combination of flocculation or centrifugationpretreatment, with reverse osmosis membranes for recycling water to replace freshwater intake.The results also indicate the possibility of using ultrafiltration as kidneys to decreasecontamination buildup for further water loop closure.</p>


corrected abstract:
<p>Water is an essential and indispensable component is the pulp- and paper production industry. The increase in energy costs, stricter environmental regulations and water resource shortages have caused a reduction of the water footprint in the industry as well as an increase in water recycling and water circuit closure. Reducing water usage requires an understanding of where contaminants originate, as well as which streams are critical to the process and how they impact mill operation. The recirculation of water can cause contaminant accumulation; therefore mills employ technologies for water treatment in the internal water cycles, the so-called ‘kidneys’. Application of membrane technology is one such option which can improve the recycled water quality and reduce contaminant buildup.</p><p>The present study was carried out on a lab-scale for the treatment of a tissue mill effluent using membrane separation. A combination of pretreatment methods and various membranes were compared with regards to separation, flux and fouling. The AlfaLaval M20 device was to treat wastewater samples sent from the mill, where the permeate was recirculated to the feed tank. COD and TOC levels are compared with regards to determining the separation efficiency. The permeate flux was measured over the two-hour filtration period, as well as flux recovery to determine fouling levels. Additionally, some economic aspects of the process are discussed.</p><p>This study suggests the potential application of a combination of flocculation or centrifugation pretreatment, with reverse osmosis membranes for recycling water to replace freshwater intake. The results also indicate the possibility of using ultrafiltration as kidneys to decrease contamination buildup for further water loop closure.</p>
----------------------------------------------------------------------
In diva2:745566 abstract is: <p>Tape samples of medium density polyethylene withthickness of 0.4 mm were supplied containing 0.1% and 0.3 % of four differentphenolic stabilizers. These samples exposed continuously and interrupted (6hour/day) to 10 ppm chlorine dioxide water solution with PH: 6.8 at 70˚C. The antioxidants life times were followed bydifferential scanning calorimetry through oxidation induction time. Sample with0.3% antioxidants exhibit an unstable and longer OIT results in the interruptedexposure system in comparison to o.3% samples in continues system. Solubilitydifficulties and precipitation of antioxidants with 0.3% concentration inpolyethylene matrix supposed to be reason for this non reliable data. Sampleswith 0.1% antioxidants approximately followed the same OIT profile as afunction of exposure time in both continues and interrupted system. Surface degradationsof tapes (with 0.1% stabilizer) were examined by IR spectroscopy and carbonylbond formation on sample after depletion time of stabilizers confirmed for allsamples. Carbonyl index rate corresponding to chemical degradation of samplesdepends on the antioxidant type even after depletion time (OIT: 0.0) ofantioxidants. Micrographs of exposed sample after necking were used tocalculation of chemical assisted crack growth as function of exposure time. Theonset for crack initiation times for matrix polymer and disintegration ofdegraded surface from fresh polymer occurs in longer time than depletion timeof antioxidants. It depends on antioxidants type in which the crack growth ratefor studied antioxidants including Irganox 1330 and Irganox 3114 were 5 µm and3.5 µm per 1000 min exposure time after crack initiation onset.      </p>


w='o.3' val={'c': '0.3', 's': 'diva2:745566', 'n': 'no full text'}

corrected abstract:
<p>Tape samples of medium density polyethylene with thickness of 0.4 mm were supplied containing 0.1% and 0.3 % of four different phenolic stabilizers. These samples exposed continuously and interrupted (6 hour/day) to 10 ppm chlorine dioxide water solution with PH: 6.8 at 70˚C. The antioxidants life times were followed by differential scanning calorimetry through oxidation induction time. Sample with 0.3% antioxidants exhibit an unstable and longer OIT results in the interrupted exposure system in comparison to 0.3% samples in continues system. Solubility difficulties and precipitation of antioxidants with 0.3% concentration in polyethylene matrix supposed to be reason for this non reliable data. Samples with 0.1% antioxidants approximately followed the same OIT profile as a function of exposure time in both continues and interrupted system. Surface degradations of tapes (with 0.1% stabilizer) were examined by IR spectroscopy and carbonyl bond formation on sample after depletion time of stabilizers confirmed for all samples. Carbonyl index rate corresponding to chemical degradation of samples depends on the antioxidant type even after depletion time (OIT: 0.0) of antioxidants. Micrographs of exposed sample after necking were used to calculation of chemical assisted crack growth as function of exposure time. The onset for crack initiation times for matrix polymer and disintegration of degraded surface from fresh polymer occurs in longer time than depletion time of antioxidants. It depends on antioxidants type in which the crack growth rate for studied antioxidants including Irganox 1330 and Irganox 3114 were 5 µm and 3.5 µm per 1000 min exposure time after crack initiation onset.</p>
----------------------------------------------------------------------
In diva2:1454832 abstract is: <p>A fundamental tool in anatomical pathology for disease diagnosis is preserving tissues in theform of formalin-fixed paraffin-embedded (FFPE) samples. A major advantage of this type ofsamples is its ability to maintain the morphology and structure of the cells, which is the basisof disease diagnosis and biomarker detection. This advantage has rendered FFPE specimensas the most popular approach for long-term preservation of tissues. However, since thecrosslinks introduced through the fixation of the tissues significantly affect the integrity of thenucleic acids within, their use is limited, especially in studies that involve gene expressionanalysis. Therefore, developing a workflow that enables determination of RNA quality in FFPEsamples will have a positive impact on both the research community and pathologydepartments. Results obtained from such quality control workflow can be used to guidedecision making regarding deeper levels of analysis.</p><p>Due to the high availability and usage of FFPE specimens, a number of studies has been doneto investigate and evaluate the integrity of their genomic content. Nevertheless, no studieshave been performed to provide estimations of FFPE nucleic acids integrity as a function oftheir spatial distribution. Being able to spatially determine RNA integrity in all sub-areas ofthe tissue is expected to facilitate the examination of FFPE specimens and ensure that allareas of the section have a good-enough quality to provide data from more expensive spatialtranscriptomics experiments. For this purpose, and building on the previous spatial RIN assaydeveloped by the spatial transcriptomics (ST) group at SciLifelab for fresh frozen tissues, wehave developed a quality control assay that is compatible with FFPE tissue specimens. Thedesigned assay combines being specialized in detection of short-length fragments;particularly in the size range of 50bp to 200bp, and providing the spatial localizations of thesefragments. This combination eventually enables an integrity estimation in the form of aspatial heat map.</p>

corrected abstract:
<p>A fundamental tool in anatomical pathology for disease diagnosis is preserving tissues in the form of formalin-fixed paraffin-embedded (FFPE) samples. A major advantage of this type of samples is its ability to maintain the morphology and structure of the cells, which is the basis of disease diagnosis and biomarker detection. This advantage has rendered FFPE specimens as the most popular approach for long-term preservation of tissues. However, since the crosslinks introduced through the fixation of the tissues significantly affect the integrity of the nucleic acids within, their use is limited, especially in studies that involve gene expression analysis. Therefore, developing a workflow that enables determination of RNA quality in FFPE samples will have a positive impact on both the research community and pathology departments. Results obtained from such quality control workflow can be used to guide decision making regarding deeper levels of analysis.</p><p>Due to the high availability and usage of FFPE specimens, a number of studies has been done to investigate and evaluate the integrity of their genomic content. Nevertheless, no studies have been performed to provide estimations of FFPE nucleic acids integrity as a function of their spatial distribution. Being able to spatially determine RNA integrity in all sub-areas of the tissue is expected to facilitate the examination of FFPE specimens and ensure that all areas of the section have a good-enough quality to provide data from more expensive spatial transcriptomics experiments. For this purpose, and building on the previous spatial RIN assay developed by the spatial transcriptomics (ST) group at SciLifelab for fresh frozen tissues, we have developed a quality control assay that is compatible with FFPE tissue specimens. The designed assay combines being specialized in detection of short-length fragments; particularly in the size range of 50bp to 200bp, and providing the spatial localizations of these fragments. This combination eventually enables an integrity estimation in the form of a spatial heat map.</p>
----------------------------------------------------------------------
In diva2:1796606 abstract is: <p>The analysis of human movement is important for diagnosis of as wellas planning and evaluating treatments of disorders or injuries affectingmovement. Optical motion capture combined with force plates provideaccurate measurements, but are confined to laboratory settings limiting theirpotential usefulness in clinical applications. Efforts are made to movemeasurements out of the laboratory making them more accessible, cheaperand easier to use for healthcare providers. This work aimed to assess thefeasibility of doing motion analysis with a wearable system consisting ofIMUs and pressure insole sensors, while also developing a methodology thatcould be used for subsequent validation. Six subjects performed walking, sideskipping, squats, chair stands and a balance exercise, while data was collectedsimultaneously from the wearable system and optical motion capture withforce plates. For demonstration, data from one example subject was analysedand included in this work. The wearable system showed promising results formeasuring ground reaction force. Center of pressure errors were relativelyhigh, likely influenced by the choice of method for coordinate transformationbetween the systems. Joint angle errors varied from low to very high fordifferent trials. Ankle dorsiflexion angle showed low errors and pelvis tiltangle high errors for all motion types. There is a need to investigate thecause for these high errors before more measurements are conducted. Themethodology presented in this work can, with a few recommended changes,be used for future validation of the wearable motion analysis system.</p>


corrected abstract:
<p>The analysis of human movement is important for diagnosis of as well as planning and evaluating treatments of disorders or injuries affecting movement. Optical motion capture combined with force plates provide accurate measurements, but are confined to laboratory settings limiting their potential usefulness in clinical applications. Efforts are made to move measurements out of the laboratory making them more accessible, cheaper and easier to use for healthcare providers. This work aimed to assess the feasibility of doing motion analysis with a wearable system consisting of IMUs and pressure insole sensors, while also developing a methodology that could be used for subsequent validation. Six subjects performed walking, side skipping, squats, chair stands and a balance exercise, while data was collected simultaneously from the wearable system and optical motion capture with force plates. For demonstration, data from one example subject was analysed and included in this work. The wearable system showed promising results for measuring ground reaction force. Center of pressure errors were relatively high, likely influenced by the choice of method for coordinate transformation between the systems. Joint angle errors varied from low to very high for different trials. Ankle dorsiflexion angle showed low errors and pelvis tilt angle high errors for all motion types. There is a need to investigate the cause for these high errors before more measurements are conducted. The methodology presented in this work can, with a few recommended changes, be used for future validation of the wearable motion analysis system.</p>
----------------------------------------------------------------------
In diva2:1237160 abstract is: <p>The induced rupturing of Poly Vinyl Alcohol (PVA) microbubbles with high mechanical index (MI)ultrasound beam is used in multiple medical application such as drug delivery, image contrastenhancement and perfusion imaging.In this work, Triggered imaging technique with subtraction algorithm is used to enhance themicrobubble’s (MB) contrast over tissue (CTR). The technique is performed by rupturing MBwith one destruction wave sequence followed by 100 B-mode imaging pulse sequences. Theimages obtained are then subtracted by a base image that is selected after the destruction pulse[1].The result of this technique depends mainly on the effectiveness of destruction pulse inrupturing highest number of MB. This has been tested through tissue mimicking phantomwithout replenishing the MB. The evaluation of the methods is done through the CTR and CNRcalculation for each of the 100 frames.The contrast enhancement technique used has also been tested with similar setup but withcontinuous replenishment of MB. The evaluation is done by comparing CNR and CTR results forthe 100 frames obtained by B-mode imaging with the ones resulted from the subtractionalgorithm.The contrast values obtained from both experiments are used in driving the characterization ofPVA response to high MI.The result for the destruction pulse effectiveness shows that the pulse indeed managed toreduce number of MB, but not to the lowest. This is because of leaked gas from cracked shell,the shell acoustic enhancement effect, and large bubbles which managed to survive.The Triggered imaging has shown large improvement in CTR value with use of the subtractionalgorithm when compared to B-mode results. In addition, it has provided an experimental wayfor perfusion imaging and quantification by monitoring CTR value after the destructive pulse[2]. This sets the bases for experimental research relevant to tissue perfusion at ultrasound labof KTH.</p>

corrected abstract:
<p>The induced rupturing of Poly Vinyl Alcohol (PVA) microbubbles with high mechanical index (MI) ultrasound beam is used in multiple medical application such as drug delivery, image contrast enhancement and perfusion imaging.</p><p>In this work, Triggered imaging technique with subtraction algorithm is used to enhance the microbubble’s (MB) contrast over tissue (CTR). The technique is performed by rupturing MB with one destruction wave sequence followed by 100 B-mode imaging pulse sequences. The images obtained are then subtracted by a base image that is selected after the destruction pulse [1].</p><p>The result of this technique depends mainly on the effectiveness of destruction pulse in rupturing highest number of MB. This has been tested through tissue mimicking phantom without replenishing the MB. The evaluation of the methods is done through the CTR and CNR calculation for each of the 100 frames.</p><p>The contrast enhancement technique used has also been tested with similar setup but with continuous replenishment of MB. The evaluation is done by comparing CNR and CTR results for the 100 frames obtained by B-mode imaging with the ones resulted from the subtraction algorithm.</p><p>The contrast values obtained from both experiments are used in driving the characterization of PVA response to high MI.</p><p>The result for the destruction pulse effectiveness shows that the pulse indeed managed to reduce number of MB, but not to the lowest. This is because of leaked gas from cracked shell, the shell acoustic enhancement effect, and large bubbles which managed to survive.</p><p>The Triggered imaging has shown large improvement in CTR value with use of the subtraction algorithm when compared to B-mode results. In addition, it has provided an experimental way for perfusion imaging and quantification by monitoring CTR value after the destructive pulse [2]. This sets the bases for experimental research relevant to tissue perfusion at ultrasound lab of KTH.</p>
----------------------------------------------------------------------
In diva2:1879784 abstract is: <p>With the growing demand for an increase in food production, whilst 30% of thecrop production is lost due to plant diseases, there is a need for plant diseasedetection that do not require laboratories and trained personnel. This can besolved by the usage of nanotechnology and the integration of microneedles.Microneedles can extract biofluids from the plants, and with the integrationwith a sensor, allow for on-site disease detection.The most recently developed type of microneedle is the porous microneedle,which extract biofluids by capillary action. Previous papers have proposeddifferent materials and processes for the fabrication of porous microneedlesbut state that further research has to be done. This Thesis aims to createprotocols for fabrication of porous microneedles from different cellulosicmaterials as well as evaluate their characteristics.Before the protocols were established, two master molds were created forthe casting of the porous microneedles. One by engraving a PMMA sheetusing a CO2 laser, and the other by 3D printing a resin mold. The twomaterials that were used were cellulose acetate and chromatography paper.The materials were used both separately and combined in the fabricationprocess. To characterize the porous microneedles, three properties wereevaluated, porosity, mechanical stability and extraction capability.The results from the fabrication of porous microneedles and the propertytests has provided information for further development of the project as wellas future work regarding porous microneedles.</p>

corrected abstract:
<p>With the growing demand for an increase in food production, whilst 30% of the crop production is lost due to plant diseases, there is a need for plant disease detection that do not require laboratories and trained personnel. This can be solved by the usage of nanotechnology and the integration of microneedles. Microneedles can extract biofluids from the plants, and with the integration with a sensor, allow for on-site disease detection.</p><p>The most recently developed type of microneedle is the porous microneedle, which extract biofluids by capillary action. Previous papers have proposed different materials and processes for the fabrication of porous microneedles but state that further research has to be done. This Thesis aims to create protocols for fabrication of porous microneedles from different cellulosic materials as well as evaluate their characteristics.</p><p>Before the protocols were established, two master molds were created for the casting of the porous microneedles. One by engraving a PMMA sheet using a CO<sub>2</sub> laser, and the other by 3D printing a resin mold. The two materials that were used were cellulose acetate and chromatography paper. The materials were used both separately and combined in the fabrication process. To characterize the porous microneedles, three properties were evaluated, porosity, mechanical stability and extraction capability.</p><p>The results from the fabrication of porous microneedles and the property tests has provided information for further development of the project as well as future work regarding porous microneedles.</p>
----------------------------------------------------------------------
In diva2:1269990 abstract is: <p>The pelvic floor muscle is a series of muscle plates in the bottom of the abdominal cavity that supportthe internal organs. These organs are slightly different in men and women. A group of muscles formthe pelvic floor muscles and can be seen as a hammock. The pelvic floor muscles have many importantfunctions, such as control of the intestines and urinary bladder, stabilization and support of the spineby keeping the internal organs in place, but also, support during pregnancy.If the pelvic floor tissues are weakened and exposed to stress, one or more organs may descend intothe pelvic floor. This condition is called Prolapse and three types are presented: Anal prolapse, Rectalprolapse, and uterine prolapse. These conditions are examined with a chair, that called strain chair(kryststol). A strain chair consists basically of a hygiene chair with a mirror mounted under the seat.Today a strain chair is used at the surgery clinic at Södersjukhuset. This chair has someflaws andneeds some improvement. The chair is old and the ergonomics are not the best, which causesunpleasant and uncomfortable conditions for both healthcare professionals and patients.This report describes the development of a new strain chair (kryststol) for use in the surgery clinic atSödersjukhuset.The result of this work has shown that a new strain chair (kryststol) should be height adjustable.This solution result in a reduction of the poor working positions for the healthcare professionals thatare forced to cope with at present. An adjustable height function also means that patients can easilyget on and off the chair. Mounting a lamp on/under the seat is a solution to facilitate examination anddiagnosis. Even, mounting a mirror under the seat is also essential for examination. Regarding theabove information, some solutions have been developed. This report investigated the solutionscarefully. To develop these</p>


corrected abstract:
<p>The pelvic floor muscle is a series of muscle plates in the bottom of the abdominal cavity that support the internal organs. These organs are slightly different in men and women. A group of muscles form the pelvic floor muscles and can be seen as a hammock. The pelvic floor muscles have many important functions, such as control of the intestines and urinary bladder, stabilization and support of the spine by keeping the internal organs in place, but also, support during pregnancy. If the pelvic floor tissues are weakened and exposed to stress, one or more organs may descend into the pelvic floor. This condition is called Prolapse and three types are presented: Anal prolapse, Rectal prolapse, and uterine prolapse. These conditions are examined with a chair, that called strain chair (kryststol). A strain chair consists basically of a hygiene chair with a mirror mounted under the seat.</p><p>Today a strain chair is used at the surgery clinic at Södersjukhuset. This chair has some flaws and needs some improvement. The chair is old and the ergonomics are not the best, which causes unpleasant and uncomfortable conditions for both healthcare professionals and patients. This report describes the development of a new strain chair (kryststol) for use in the surgery clinic at Södersjukhuset.</p><p>The result of this work has shown that a new strain chair (kryststol) should be height adjustable. This solution result in a reduction of the poor working positions for the healthcare professionals that are forced to cope with at present. An adjustable height function also means that patients can easily get on and off the chair. Mounting a lamp on/under the seat is a solution to facilitate examination and diagnosis. Even, mounting a mirror under the seat is also essential for examination. Regarding the above information, some solutions have been developed. This report investigated the solutions carefully. To develop these solutions, literature surveys and interviews with healthcare professionals have been very helpful.</p>
----------------------------------------------------------------------
title: "Configuration and device identification on networkgateways"
==>    "Configuration and device identification on network gateways"

In diva2:661292 abstract is: <p>To set up port forwarding rules on network gateways, certain technical skills are requiredfrom end-users. These assumptions in the gateway software stack, can lead to an increasein support calls to network operators and resellers of customer premises equipment. Theuser interface itself is also an important part of the product and a complicated interfacewill contribute to a lessened user experience. Other issues with an overwhelming userinterface include the risk of faulty configuration by the user, potentially leaving the networkvulnerable to attacks.We present an enhancement of the current port forwarding configuration in the gatewaysoftware, with an extensible library of presets along with usability improvements. To helpusers with detecting available services, a wrapper for a network scanner is implemented, fordetecting devices and services on the local network. These parts combined relieves end-usersof looking up forwarding rules for ports and protocols to configure their gateway, basingtheir decisions on data collected by the network scanner or by using an applications nameinstead of looking up its ports. Another usability improvement is an internal DNS service,which enables access to the gateway interface through a human-memorable domain name,instead of using the LAN IP address.Using the Nmap utility for identifying services on the network, could be consideredharmful activity by network admins and intrusion detection systems. The preset libraryis extensible and generic enough to be included in the default software suite shipping withthe network equipment. Working within the unified configuration system of OpenWrt, thepreset design will add value and allow resellers to easily customize it to their services. Thisproposal could reduce support costs for the service operators and improve user experiencein configuring network gateways.</p>

corrected abstract:
<p>To set up port forwarding rules on network gateways, certain technical skills are required from end-users. These assumptions in the gateway software stack, can lead to an increase in support calls to network operators and resellers of customer premises equipment. The user interface itself is also an important part of the product and a complicated interface will contribute to a lessened user experience. Other issues with an overwhelming user interface include the risk of faulty configuration by the user, potentially leaving the network vulnerable to attacks.</p><p>We present an enhancement of the current port forwarding configuration in the gateway software, with an extensible library of presets along with usability improvements. To help users with detecting available services, a wrapper for a network scanner is implemented, for detecting devices and services on the local network. These parts combined relieves end-users of looking up forwarding rules for ports and protocols to configure their gateway, basing their decisions on data collected by the network scanner or by using an applications name instead of looking up its ports. Another usability improvement is an internal DNS service, which enables access to the gateway interface through a human-memorable domain name, instead of using the LAN IP address.</p><p>Using the Nmap utility for identifying services on the network, could be considered harmful activity by network admins and intrusion detection systems. The preset library is extensible and generic enough to be included in the default software suite shipping with the network equipment. Working within the unified configuration system of OpenWrt, the preset design will add value and allow resellers to easily customize it to their services. This proposal could reduce support costs for the service operators and improve user experience in configuring network gateways.</p>
----------------------------------------------------------------------
In diva2:1272696 abstract is: <p>The solar cell industry is one of the fastest growing industries in the world. This is due to thedeclining prices of solar cells and that many countries now try to reduce their greenhouse gasemissions. The growing industry leads to an increased range of variants and suppliers of solarcells on the market. The environmental problems of solar cells occur during the production ofthe various components, as well as in the recovery of the used solar cells.</p><p>The studied solar cells are mono- and multi-crystalline silicon cells, cadmium telluride (CdTe)and CIGS/CIS. The mono- and multi-crystalline solar cells are produced from purified siliconwhich achieves a purity of 6N (SG-Si), where silicon is doped with phosphorus to produce nsemiconductorand the p-semiconductor is doped with boron.</p><p>For thin film solar cell CdTe, CdS is used as the n-semiconductor and the p-semiconductorconsists of cadmium and tellurium. For thin film solar cell CIGS/CIS, copper, indium, galliumand selenium are used as p-semiconductors and CdS as n-semiconductors.</p><p>For the monocrystalline solar cells, a recovery rate of 96% can be achieved, which is doneeconomically and environmentally. For CdTe, a 95% recycled material is obtained for glass,90% for CdTe and 90% for CdS. While for CIGS/CIS, glass, EVA, selenium, aluminium,indium and gallium materials can be recycled.</p><p>The chemicals used during the processes have been classified within a risk categorization,where the majority of the chemicals used are classified as high- and very high risk. The greaterimpact on the environment at the production location is due to the energy supply used sinceonly transport accounts for 1,6 to 2,8 % of carbondioxide emissions from solar cells.</p><p>The parameters that were considered to have a major impact on the environment are the loadfrom critical material extraction, the power supply used during production, the hazardouschemicals used during production and recycling, and the air and waterborne emissions thatarise during production and recycling.</p><p>For all solar cells, non-virgin aluminium should be used as construction material for the frame,or it should be without frames. Manufacturers of solar cells should clean the waterconsumption that occurs and recycle water to the their utmost ability. The factories should alsouse a recycling center for their trash and residues, or recycle at the factory. A clear follow-upand residual product plan should exist for the produced solar cells, which can be done throughPV CYCLE. Companies should work actively in matters relating to health, safety, humanrights, labour law and comply with the rules prevailing in the current country.</p>

w='nsemiconductor' val={'c': 'n-semiconductor', 's': 'diva2:1272696', 'n': 'correct in original'}

corrected abstract:
<p>The solar cell industry is one of the fastest growing industries in the world. This is due to the declining prices of solar cells and that many countries now try to reduce their greenhouse gas emissions. The growing industry leads to an increased range of variants and suppliers of solar cells on the market. The environmental problems of solar cells occur during the production of the various components, as well as in the recovery of the used solar cells.</p><p>The studied solar cells are mono- and multi-crystalline silicon cells, cadmium telluride (CdTe) and CIGS/CIS. The mono- and multi-crystalline solar cells are produced from purified silicon which achieves a purity of 6N (SG-Si), where silicon is doped with phosphorus to produce n-semiconductor and the p-semiconductor is doped with boron.</p><p>For thin film solar cell CdTe, CdS is used as the n-semiconductor and the p-semiconductor consists of cadmium and tellurium. For thin film solar cell CIGS/CIS, copper, indium, gallium and selenium are used as p-semiconductors and CdS as n-semiconductors.</p><p>For the monocrystalline solar cells, a recovery rate of 96% can be achieved, which is done economically and environmentally. For CdTe, a 95% recycled material is obtained for glass, 90% for CdTe and 90% for CdS. While for CIGS/CIS, glass, EVA, selenium, aluminium, indium and gallium materials can be recycled.</p><p>The chemicals used during the processes have been classified within a risk categorization, where the majority of the chemicals used are classified as high- and very high risk. The greater impact on the environment at the production location is due to the energy supply used since only transport accounts for 1,6 to 2,8 % of carbondioxide emissions from solar cells.</p><p>The parameters that were considered to have a major impact on the environment are the load from critical material extraction, the power supply used during production, the hazardous chemicals used during production and recycling, and the air and waterborne emissions that arise during production and recycling.</p><p>For all solar cells, non-virgin aluminium should be used as construction material for the frame, or it should be without frames. Manufacturers of solar cells should clean the water consumption that occurs and recycle water to the their utmost ability. The factories should also use a recycling center for their trash and residues, or recycle at the factory. A clear follow-up and residual product plan should exist for the produced solar cells, which can be done through PV CYCLE. Companies should work actively in matters relating to health, safety, human rights, labour law and comply with the rules prevailing in the current country.</p>
----------------------------------------------------------------------
In diva2:1762551 abstract is: <p>Matched frequency responses are a fundamental starting point for a variety ofimplementations for microphone arrays. In this report, two methods for frequencyresponse-calibration of a pre-assembled microphone array are presented andevaluated. This is done by extracting the deviation in frequency responses of themicrophones in relation to a selected reference microphone, using a swept sine asa stimulus signal and an inverse filter. The swept sine includes all frequencieswithin the bandwidth of human speech. This allows for a full frequency responsemeasurements from all microphones using a single recording.Using the swept sine, the deviation in frequency response between the microphonescan be obtained. This deviation represents the scaling factor that all microphonesmust be calibrated with to match the reference microphone. Applying the scalingfactors on the recorded stimulus signal shows an improvement for both implementedmethods, and where one method matches the frequency response of the microphoneswith high accuracy.Once the scaling factors of the various microphones is obtained, it can be usedto calibrate other recorded signals. This leads to an minor improvement formatching the frequency responses, as it has been shown that the differencesin frequency response between the microphones is signal-dependent and variesbetween recordings. The response differences between the microphones dependson the design of the array, speaker, room and the acoustic frequency dispersionthat occurs with sound waves. This makes it difficult to calibrate the frequencyresponses of the microphones without appropriate equipment because the responseof the microphones is noticeably affected by these other factors. Proposals to addressthese problems are discussed in the report as future work.</p>

corrected abstract:
<p>Matched frequency responses are a fundamental starting point for a variety of implementations for microphone arrays. In this report, two methods for frequency response-calibration of a pre-assembled microphone array are presented and evaluated. This is done by extracting the deviation in frequency responses of the microphones in relation to a selected reference microphone, using a swept sine as a stimulus signal and an inverse filter. The swept sine includes all frequencies within the bandwidth of human speech. This allows for a full frequency response measurements from all microphones using a single recording.</p><p>Using the swept sine, the deviation in frequency response between the microphones can be obtained. This deviation represents the scaling factor that all microphones must be calibrated with to match the reference microphone. Applying the scaling factors on the recorded stimulus signal shows an improvement for both implemented methods, and where one method matches the frequency response of the microphones with high accuracy.</p><p>Once the scaling factors of the various microphones is obtained, it can be used to calibrate other recorded signals. This leads to an minor improvement for matching the frequency responses, as it has been shown that the differences in frequency response between the microphones is signal-dependent and varies between recordings. The response differences between the microphones depends on the design of the array, speaker, room and the acoustic frequency dispersion that occurs with sound waves. This makes it difficult to calibrate the frequency responses of the microphones without appropriate equipment because the response of the microphones is noticeably affected by these other factors. Proposals to address these problems are discussed in the report as future work.</p>
----------------------------------------------------------------------
In diva2:1446988 abstract is: <p>Lignin is one of the most common biopolymers in the world. Together with cellulose andhemicellulose it constitutes the fibers in the wood. It has a high molecular weight due to its complexstructure consisting of crossed-linked phenolic monomers and is concatenated with different types ofcarbon and ether bonds.In pulping processes, lignin is extracted in large quantities and used on site to produce energy for milloperations but is also removed as a waste product. This enables a product with high resources andaccessibility due to lignin's diverse properties. Therefore, lignin has the potential to be utilized inhigher value applications such as polymer materials, as well as a source of platform chemicals. Atpresent, the value applications of lignin are promising as additives for different kinds of productssuch as emulsifiers and especially as biofuel due to lignin's high carbon content.New technologies for development for utilization lignin are emerging for different kinds ofapplications due to lignin’s biocompatibility. The possibilities of lignin combined with existingresearch of nanotechnology gives opportunities to improve biomedical applications. By designinglignin derived nanoparticles with incorporated magnetic materials, the NPs obtainsuperparamagnetic properties which can be utilized for target drug delivery. This could be promisingagainst intractable cancer such as pancreatic cancer.This report presents a protocol for developing magnetic lignin nanoparticles from the lowestmolecular weight kraft lignin fractions of eucalyptus (hardwood) and spruce (softwood). By a methodof self-assembly, particles with a doughnut and core-shell morphology, as indicated by SEM and TEM,were yielded with a 10-50μL content of water-stabilized magnetite. The particle size distribution andzeta potential were determined by DLS and the possibility of the particles being suitable forbiomedical applications was discussed.</p>


Note that the corrected abstract is based on the actual abstract in the thesis and not the abstract above.
corrected abstract:
<p>Lignin is one of the most common biopolymers in the world, together with cellulose and hemicellulose it constitutes the fibers in the wood. It has a high molecular weight due to its complex structure consisting of crossed-linked phenolic monomers and is concatenated with different types of carbon and ether bonds.</p><p>In pulping processes, lignin is extracted in large quantities and used on site to produce energy for mill operations, and is considered a waste product. Lignin has the potential to be utilized in higher value applications such as polymer materials, as well as a source of platform chemicals. At present, the value applications of lignin are promising as additives for different kinds of products such as emulsifiers and especially as biofuel due to lignin's high carbon content.</p><p>New technologies utilizing lignin are emerging in different kinds of applications, mainly due to lignin’s biocompatibility. The possibilities of lignin combined with existing research of nanotechnology gives opportunities to improve biomedical applications. By designing lignin derived nanoparticles with incorporated magnetic materials, the NPs obtain superparamagnetic properties which can be utilized for target drug delivery. This could be promising against intractable cancer such as pancreatic cancer.</p><p>The purpose of this project was to develop a method for creating magnetic lignin nanoparticles. This was achieved using the lowest molecular weight kraft lignin, after four-step solvent fractionation of both eucalyptus (hardwood) and spruce (softwood). By a method of self-assembly, particles with a doughnut and core-shell morphology, as indicated by SEM and TEM, were yielded with a 10-50µL content of water-stabilized magnetite. The particle size distribution and zeta potential were determined by DLS and the possibility of the particles being suitable for biomedical applications was discussed.</p>
----------------------------------------------------------------------
In diva2:1250709 abstract is: <p>Workers within production and assembly lines are often exposed to ergonomically unfavorable tasksand conditions. Reaction forces and reaction torques generated by industrial power tools may causenot only discomfort but also health issues and injury. The forceful tasks in combination with highlyrepetitive hand-arm motions and prolonged tool use paves the way for loss in workforce capacitywhich in turn can lead to great losses in productivity and product quality. An umbrella term for themany injuries and diseases that may arise from the use of such tools is Cumulative Trauma Disorders(CTD).This study aimed to investigate the ergonomic effect of power tool use for various tool and taskrelated conditions. The study required the setup of a test rig with a simulated handle of the tool. Theergonomic impact was assessed by measuring the torques associated with different tighteningstrategies, as well as measuring the angular displacement of the tool handle. By varying the jointstiffness and workplace orientation, the complexity of the task was varied and thus quantified.Measurements of muscle activity during each tightening procedure provided a quantification of thephysiological impact on the operator. By combining the measurements on the operator withsubjective assessment of perceived exertion and discomfort, a more holistic perspective on thetightening procedure was obtained.The results obtained from the study stressed the negative impact on the operator which the QuickStep tightening strategy on medium hard joints implies, regardless of workspace orientation. TheTurbo Tight and Tensor Pulse tightening strategies turned out to generate the lowest reactiontorques and handle deflections, regardless of joint stiffness and workspace orientation. The findingsfrom the muscle activity measurements in combination with the subjective evaluation methodsfurther confirmed the mildness of the Turbo Tight and Tensor Pulse strategies. Moreover, horizontalworkspace resulted in lower tool handle deflection compared to vertical workspace for all tighteningstrategies and joint stiffnesses.</p>


corrected abstract:
<p>Workers within production and assembly lines are often exposed to ergonomically unfavorable tasks and conditions. Reaction forces and reaction torques generated by industrial power tools may cause not only discomfort but also health issues and injury. The forceful tasks in combination with highly repetitive hand-arm motions and prolonged tool use paves the way for loss in workforce capacity which in turn can lead to great losses in productivity and product quality. An umbrella term for the many injuries and diseases that may arise from the use of such tools is Cumulative Trauma Disorders (CTD).</p><p>This study aimed to investigate the ergonomic effect of power tool use for various tool and task related conditions. The study required the setup of a test rig with a simulated handle of the tool. The ergonomic impact was assessed by measuring the torques associated with different tightening strategies, as well as measuring the angular displacement of the tool handle. By varying the joint stiffness and workplace orientation, the complexity of the task was varied and thus quantified. Measurements of muscle activity during each tightening procedure provided a quantification of the physiological impact on the operator. By combining the measurements on the operator with subjective assessment of perceived exertion and discomfort, a more holistic perspective on the tightening procedure was obtained.</p><p>The results obtained from the study stressed the negative impact on the operator which the Quick Step tightening strategy on medium hard joints implies, regardless of workspace orientation. The Turbo Tight and Tensor Pulse tightening strategies turned out to generate the lowest reaction torques and handle deflections, regardless of joint stiffness and workspace orientation. The findings from the muscle activity measurements in combination with the subjective evaluation methods further confirmed the mildness of the Turbo Tight and Tensor Pulse strategies. Moreover, horizontal workspace resulted in lower tool handle deflection compared to vertical workspace for all tightening strategies and joint stiffnesses.</p>
----------------------------------------------------------------------
 diva2:1865304 abstract is: <p>This work examines the temperature dynamics and final temperature of conductorsin electrical installation cables, for various cases of load current. A laboratory modeland a time-stepping numerical model have been made and compared. Results arecompared with the ampacity given in installation standards, and also with theoverload levels that could be sustained by fuses and circuit breakers, followingEuropean standards.Examples of use-cases for the study’s results are cable ratings with intermittentloads, and potential overload due to heavy load combined with extra infeeds intoa circuit (e.g. plug-in solar power). The studied cables were filled and non filled 3-conductor (of which 2 loaded) PEX insulated installation cables, all with 1.5 mm2copper conductors (EQLQ 3G 1.5mm2). They were tested in open air and in a sectionof insulated cavity wall. Currents were applied from a controlled dc current source.The conductor temperature was measured by logging the voltage drop across a 10 cmlength of conductor, bearing in mind the temperature coefficient of resistance. Thismethod was verified by tests in a heating chamber.Results showed that conductors reached a maximum of 92 % of the permissibleoperating temperature (90 ◦C) at 150 % load of typical fuse-size. Combined withextra infeeds the maximum temperature reached 118 % of operating temperatureat 170 % load of typical fuse-size. The results also showed a temperature differencebetween horizontal and vertical orientations, varying from 3-10 ◦C higher in thevertical scenario, depending on load and cable</p>


I have used the unicode degree-C ℃ symbol in the corrected abstract - rather than the white bullet and letter "C".
corrected abstract:
<p>This work examines the temperature dynamics and final temperature of conductors in electrical installation cables, for various cases of load current. A laboratory model and a time-stepping numerical model have been made and compared. Results are compared with the ampacity given in installation standards, and also with the overload levels that could be sustained by fuses and circuit breakers, following European standards.</p><p>Examples of use-cases for the study’s results are cable ratings with intermittent loads, and potential overload due to heavy load combined with extra infeeds into a circuit (e.g. plug-in solar power). The studied cables were filled and non filled 3-conductor (of which 2 loaded) PEX insulated installation cables, all with 1.5 mm<sup>2</sup> copper conductors (EQLQ 3G 1.5mm<sup>2</sup>). They were tested in open air and in a section of insulated cavity wall. Currents were applied from a controlled dc current source. The conductor temperature was measured by logging the voltage drop across a 10 cm length of conductor, bearing in mind the temperature coefficient of resistance. This method was verified by tests in a heating chamber.</p><p>Results showed that conductors reached a maximum of 92 % of the permissible operating temperature (90 ℃) at 150 % load of typical fuse-size. Combined with extra infeeds the maximum temperature reached 118 % of operating temperature at 170 % load of typical fuse-size. The results also showed a temperature difference between horizontal and vertical orientations, varying from 3-10 ℃ higher in the vertical scenario, depending on load and cable</p>
----------------------------------------------------------------------
In diva2:1642787 abstract is: <p><strong>Background</strong>: Duchenne muscular dystrophy is an X-linked chromosomal inheritedrecessive severe muscular dystrophy disorder, caused by a mutation in thedystrophin gene. Affected individuals undergo a progressive disease, where theyexperience a loss of muscle mass and consequently the loss of muscle function, overthe years, sorrowfully, with fatal ending.</p><p>The screening of newborns using dried blood spots (DBS) could aid early diagnoseand initiation of treatment. DBS are a minimal invasive method of collecting bloodby disposing a small volume of blood, a droplet, on specially prepared filter paperfor subsequent analysis and storage<sup>[1]</sup>.</p><p><strong>Objectives</strong>: The aim of this study is to optimize elution of proteins from DSS andexplore detection of biomarkers for DMD</p><p><strong>Methods</strong>: A total of 15 serum samples, collected at clinical sites were analyzedthrough DSS elution by BCA, SDS-PAGE electrophoresis and suspension bead arrayplatform to assess CA3 as a biomarker targeted for DMD detection.Results: The presented results show that there is a relation between theconcentration of Tween 20 detergent and the protein extract, as the higher theconcentration the higher the protein extracted was. As well as reported an evidentrift effect between 1 and 5% Tween 20. Although elution of proteins is overall highcertain proteins are prone to elute at high detergent concentration whereas othersnot. Results also showed that the bibliographic time of elution could be optimizedand reduced up to 4 hours of elution, in two 2-hour cycles.</p><p><strong>Conclusions</strong>: The developed elution method in combination with the bead-basedimmunoassay constitute a feasible method for rapid assessment of biomarkersabundance such as CA3. The optimization of the elution joint method is feasible andcustomizable for the selected biomarkers. </p>


Added <strong> for the Result section for consistency.
corrected abstract:
<p><strong>Background</strong>: Duchenne muscular dystrophy is an X-linked chromosomal inherited recessive severe muscular dystrophy disorder, caused by a mutation in the dystrophin gene. Affected individuals undergo a progressive disease, where they experience a loss of muscle mass and consequently the loss of muscle function, over the years, sorrowfully, with fatal ending.</p><p>The screening of newborns using dried blood spots (DBS) could aid early diagnose and initiation of treatment. DBS are a minimal invasive method of collecting blood by disposing a small volume of blood, a droplet, on specially prepared filter paper for subsequent analysis and storage<sup>[1]</sup>.</p><p><strong>Objectives</strong>: The aim of this study is to optimize elution of proteins from DSS and explore detection of biomarkers for DMD</p><p><strong>Methods</strong>: A total of 15 serum samples, collected at clinical sites were analyzed through DSS elution by BCA, SDS-PAGE electrophoresis and suspension be ad array platform to assess CA3 as a biomarker targeted for DMD detection.</p><p><strong>Results</strong>: The presented results show that there is a relation between the concentration of Tween 20 detergent and the protein extract, as the higher the concentration the higher the protein extracted was. As well as reported an evident rift effect between 1 and 5% Tween 20. Although elution of proteins is overall high certain proteins are prone to elute at high detergent concentration whereas others not. Results also showed that the bibliographic time of elution could be optimized and reduced up to 4 hours of elution, in two 2-hour cycles.</p><p><strong>Conclusions</strong>: The developed elution method in combination with the be ad-based immunoassay constitute a feasible method for rapid assessment of biomarkers abundance such as CA3. The optimization of the elution joint method is feasible and customizable for the selected biomarkers. </p>
----------------------------------------------------------------------
In diva2:1454456 abstract is: <p>In 2014 SÖRAB constructed a continuous biological treatment system (KBR) to handle leachate waterfrom the landfill at the facility in Löt, north of Stockholm. The KBR is mainly focused on removal ofammonium nitrogen which would otherwise be released in to the recipient and contribute toeutrophication and damage to the environment. This project has focused on replacing the currentcarbon source in the process Brenntaplus VP1 and evaluating the efficiency of denitrification andeconomy of transitioning to a new carbon source. The carbon sources glycerol and ethanol wereevaluated and compared to Brenntaplus VP1 for the denitrification efficiency and microbial profile.The experiments were performed in laboratory conditions and in pilot scale using leachate water fromLöt. The reduction of ammonia was evaluated by chemical precipitation, addition of carbon sources bymeasuring ammonia-N and nitrate-N under aerobic (nitrification) and anaerobic (denitrification)conditions. The combination of ethanol and glycerol showed an enhanced denitrification and increasedmicrobial community both in lab and pilot scale studies with reduced hydraulic retention time. Therate of nitrate reduction was 0.23 mgNO3-N 1 -1 h -1 for ethanol/glycerol compared to 0.12-0.17mgNO 3- -N 1 -1 h -1 for Brenntaplus VP1 in pilot scale. The results indicate that using ethanol, glycerolor a mix of the two as a substitute for Brenntaplus VP1 is viable. This has been based on laboratoryand pilot scale studies. Each of the carbon sources examined during this project have showed a uniqueimpact on the process and its parameters such as: denitrification rate, microbial density and microbialcomposition. The carbon sources had an impact with temperature fluctuation and faster denitrificationcompared to the conventional KBR system. This implies that the carbon sources tested in this projectcan be advantageous and beneficial for Sörab depending on the carbon source availability and theseasonal variations.</p>

Note the first mgNO3 is as set in the thesis - wihtout the 3 being a subscript.
corrected abstract:
<p>In 2014 SÖRAB constructed a continuous biological treatment system (KBR) to handle leachate water from the landfill at the facility in Löt, north of Stockholm. The KBR is mainly focused on removal of ammonium nitrogen which would otherwise be released in to the recipient and contribute to eutrophication and damage to the environment. This project has focused on replacing the current carbon source in the process Brenntaplus VP1 and evaluating the efficiency of denitrification and economy of transitioning to a new carbon source. The carbon sources glycerol and ethanol were evaluated and compared to Brenntaplus VP1 for the denitrification efficiency and microbial profile. The experiments were performed in laboratory conditions and in pilot scale using leachate water from Löt. The reduction of ammonia was evaluated by chemical precipitation, addition of carbon sources by measuring ammonia-N and nitrate-N under aerobic (nitrification) and anaerobic (denitrification) conditions. The combination of ethanol and glycerol showed an enhanced denitrification and increased microbial community both in lab and pilot scale studies with reduced hydraulic retention time. The rate of nitrate reduction was 0.23 mgNO3-N 1<sup>-1</sup> h<sup>-1</sup> for ethanol/glycerol compared to 0.12-0.17 mgNO<sub>3</sub><sup>-</sup>-N 1<sup>-1</sup> h<sup>-1</sup> for Brenntaplus VP1 in pilot scale. The results indicate that using ethanol, glycerol or a mix of the two as a substitute for Brenntaplus VP1 is viable. This has been based on laboratory and pilot scale studies. Each of the carbon sources examined during this project have showed a unique impact on the process and its parameters such as: denitrification rate, microbial density and microbial composition. The carbon sources had an impact with temperature fluctuation and faster denitrification compared to the conventional KBR system. This implies that the carbon sources tested in this project can be advantageous and beneficial for Sörab depending on the carbon source availability and the seasonal variations.</p>
----------------------------------------------------------------------
In diva2:1455012 abstract is: <p>This project has taken place over a 20 week period with the aim to find asuitable replacement for the current desalting process of large product volumes ofoligonucleotides at the company Scandinavian Gene Synthesis. In the beginning of theproject, a literature and market research was conducted to evaluate suitable options ofdesalting processes, followed by a cost analysis to choose the most cost effective andsustainable option. After selecting the most optimal solution, it was tested with variouskinds of oligonucleotides in order to validate the method. When this method validationwas proven successful, an attempt to scale up the new desalting process were executed.Key aspects that were being evaluated were product recovery, desalting performance,process time etc. Furthermore, a contamination analysis with MS was performed toinvestigate potential reuse of the new desalting process. The expected outcome ofthis project is an implementation a new desalting process that provides competitivefinancial and time advantage. In addition, it should create better sustainability andenvironmental profiles while continuing to ensure high safety and quality standards.This will enhance an increase of production and ensure that more customer demandswill be met, resulting in an expansion of their customer base and growth as a companyin general.</p>


w='masspectrometry' val={'c': 'mass spectrometry', 's': 'diva2:1455012', 'n': 'no full text'}

corrected abstract:
<p>This project has taken place over a 20 week period with the aim to find a suitable replacement for the current desalting process of large product volumes of oligonucleotides at the company Scandinavian Gene Synthesis. In the beginning of the project, a literature and market research was conducted to evaluate suitable options of desalting processes, followed by a cost analysis to choose the most cost effective and sustainable option. After selecting the most optimal solution, it was tested with various kinds of oligonucleotides in order to validate the method. When this method validation was proven successful, an attempt to scale up the new desalting process were executed. Key aspects that were being evaluated were product recovery, desalting performance, process time etc. Furthermore, a contamination analysis with MS was performed to investigate potential reuse of the new desalting process. The expected outcome of this project is an implementation a new desalting process that provides competitive financial and time advantage. In addition, it should create better sustainability and environmental profiles while continuing to ensure high safety and quality standards. This will enhance an increase of production and ensure that more customer demands will be met, resulting in an expansion of their customer base and growth as a company in general.</p>
----------------------------------------------------------------------
In diva2:1452644 abstract is: <p>Pharmacokinetic (PK)- and pharmacodynamic (PD) modeling are useful tools whenassessing treatment effect. A patient’s adherence can potentially be rate-limiting, since it isthe first process in a chain of processes that determines treatment effect. Therefore agreater system taking into consideration PKPD as well as adherence models couldpotentially unlock a greater system understanding. This study focuses on investigating thefeasibility of combining models concerning adherence, PK and PD.</p><p>An extensive mapping of previously made work on the topics of PKPD model developmentand adherence models concerning type 2 diabetes was conducted. Results concluded thatthere are gaps in research regarding adequate adherence-scoring methods that easily can belinked to dosing regimens. Furthermore, there is lacking research regarding feedback fromexposure-response to adherence. A simple model was implemented to provide a proposedlinkage inhowthe connection could be made between adherence and a PKPD-model.Sensitivity analysis showed that the adherence scoring used (Summary of DiabetesSelf-Care Activities measure, SDSCA) had a moderate correlation to the final response onfasting plasma glucose (Spearman ρ=−0.478∗∗∗). This result suggests that adherenceshould be considered as a relatively important factor to weave in to systems models ofpharmacology and future research should be made on further developing modelsimplementing both social factors, such as adherence, as well as pharmacologic response. Apossible way could be linking dose regimen to adherence scoring.</p>


corrected abstract:
<p>Pharmacokinetic (PK)- and pharmacodynamic (PD) modeling are useful tools when assessing treatment effect. A patient’s adherence can potentially be rate-limiting, since it is the first process in a chain of processes that determines treatment effect. Therefore a greater system taking into consideration PKPD as well as adherence models could potentially unlock a greater system understanding. This study focuses on investigating the feasibility of combining models concerning adherence, PK and PD.</p><p>An extensive mapping of previously made work on the topics of PKPD model development and adherence models concerning type 2 diabetes was conducted. Results concluded that there are gaps in research regarding adequate adherence-scoring methods that easily can be linked to dosing regimens. Furthermore, there is lacking research regarding feedback from exposure-response to adherence. A simple model was implemented to provide a proposed linkage in how the connection could be made between adherence and a PKPD-model. Sensitivity analysis showed that the adherence scoring used (Summary of Diabetes Self-Care Activities measure, SDSCA) had a moderate correlation to the final response on fasting plasma glucose (Spearman <em>ρ</em> = −0.478<sup>∗∗∗</sup>). This result suggests that adherence should be considered as a relatively important factor to weave in to systems models of pharmacology and future research should be made on further developing models implementing both social factors, such as adherence, as well as pharmacologic response. A possible way could be linking dose regimen to adherence scoring.</p>
----------------------------------------------------------------------
Although there is not full text, it is likely that the title "GAP43 AS A POTENTIALBIOMARKER FORNEURODEGENERATION: Division of Affinity Proteomics, SciLifeLabDepartment of Protein Science
"
==> "GAP43 AS A POTENTIAL BIOMARKER FOR NEURODEGENERATION: Division of Affinity Proteomics, SciLifeLab Department of Protein Science
"

In diva2:1228129 abstract is: <p>Alzheimer’s disease (AD) is a common neurodegenerative disorder characterized by progressivedamage and loss of neurons. Despite the efforts, the scientific community still lacks current insights intothe onset and progression of this disease. Previous multiplex analysis of cerebrospinal fluid (CSF)protein profiles on the suspension bead array (SBA) platform, applied in the context of AD, hasunderlined growth-associated protein’s 43 (GAP43 or neuromodulin) association to the disease. GAP43is a brain enriched cytoplasmic protein involved in neuronal growth and axonal regeneration. Becauseof its post translational modifications, mainly phosphorylation, GAP43 plays an important role in memoryand learning. This project aims to further explore the relation of GAP43 to AD through the identificationof antibodies that can capture this protein and their application on Western blot (WB) and brain tissuemicroarray (TMA) protocols. Our antibody selection includes eight antibodies, two of which target thephosphorylated GAP43 (ph-GAP43). All antibodies were tested on Western blots of CSF pools of ADcases and controls with elevated and decreased GAP43 levels respectively. Additionally, four of theseantibodies were tested on TMAs consisting of cortical tissue cores from ten AD patients, ten patientswith Lewy Body Dementia (DLB) and nine controls. Our results indicate that although all antibodiessuccessfully detected GAP43 both on WB and TMAs, ph-GAP43 was only captured on TMAs.Immunohistochemical analysis of temporal and frontal cortex from AD patients revealed an almostcomplementary pattern of GAP43 and ph-GAP43 higher expression in cortical tissue cores obtainedfrom AD patients compared to controls. Lastly, more intense phosphorylation was observed in temporalthan in frontal cortex.</p>


corrected abstract:
<p>Alzheimer’s disease (AD) is a common neurodegenerative disorder characterized by progressive damage and loss of neurons. Despite the efforts, the scientific community still lacks current insights into the onset and progression of this disease. Previous multiplex analysis of cerebrospin al fluid (CSF) protein profiles on the suspension bead array (SBA) platform, applied in the context of AD, has underlined growth-associated protein’s 43 (GAP43 or neuromodulin) association to the disease. GAP43 is a brain enriched cytoplasmic protein involved in neuronal growth and axonal regeneration. Because of its post translational modifications, mainly phosphorylation, GAP43 plays an important role in memory and learning. This project aims to further explore the relation of GAP43 to AD through the identification of antibodies that can capture this protein and their application on Western blot (WB) and brain tissue microarray (TMA) protocols. Our antibody selection includes eight antibodies, two of which target the phosphorylated GAP43 (ph-GAP43). All antibodies were tested on Western blots of CSF pools of AD cases and controls with elevated and decreased GAP43 levels respectively. Additionally, four of these antibodies were tested on TMAs consisting of cortical tissue cores from ten AD patients, ten patients with Lewy Body Dementia (DLB) and nine controls. Our results indicate that although all antibodies successfully detected GAP43 both on WB and TMAs, ph-GAP43 was only captured on TMAs. Immunohistochemical analysis of temporal and frontal cortex from AD patients revealed an almost complementary pattern of GAP43 and ph-GAP43 higher expression in cortical tissue cores obtained from AD patients compared to controls. Lastly, more intense phosphorylation was observed in temporal than in frontal cortex.</p>
----------------------------------------------------------------------
In diva2:1455019 abstract is: <p>Microorganisms play crucial roles in aquatic environments in determining ecosystemstability and driving the turnover of elements essential to life. Understanding thedistribution and evolution of aquatic microorganisms will help us predict how aquaticecosystems will respond to Global Change, and such understanding can be gained bystudying these processes of the past. In this project, we investigate the evolutionaryrelationship between brackish water bacteria from the Baltic Sea and Caspian Seawith freshwater and marine bacteria, with the goal of understanding how brackishwater bacteria have evolved. 11,276 bacterial metagenome-assembled genomes(MAGs) from seven metagenomic datasets were used to conduct a comparativeanalysis of freshwater, brackish and marine bacteria. When clustering the genomes bypairwise average nucleotide identity (ANI) at the approximate species level (96.5%ANI), the Baltic Sea genomes were more likely to form clusters with the Caspian Seagenomes than with Swedish lakes genomes, even though geographic distancesbetween Swedish lakes and the Baltic Sea are much smaller. Phylogenomic analysisand ancestral state reconstruction showed that approximately half of the brackishMAGs had freshwater ancestors and half had marine ancestors. Phylogeneticdistances were on average shorter to freshwater ancestors, but when subsampling thetree to the same number of freshwater and marine MAG clusters, the distances werenot significantly different. Brackish genomes belonging to Acidimicrobiia,Actinobacteria and Cyanobacteriia tended to originate from freshwater bacteria, whilethose of Alphaproteobacteria and Bacteroidia mainly had evolved from marinebacteria.</p>

corrected abstract:
<p>Microorganisms play crucial roles in aquatic environments in determining ecosystem stability and driving the turnover of elements essential to life. Understanding the distribution and evolution of aquatic microorganisms will help us predict how aquatic ecosystems will respond to Global Change, and such understanding can be gained by studying these processes of the past. In this project, we investigate the evolutionary relationship between brackish water bacteria from the Baltic Sea and Caspian Sea with freshwater and marine bacteria, with the goal of understanding how brackish water bacteria have evolved. 11,276 bacterial metagenome-assembled genomes (MAGs) from seven metagenomic datasets were used to conduct a comparative analysis of freshwater, brackish and marine bacteria. When clustering the genomes by pairwise average nucleotide identity (ANI) at the approximate species level (96.5% ANI), the Baltic Sea genomes were more likely to form clusters with the Caspian Sea genomes than with Swedish lakes genomes, even though geographic distances between Swedish lakes and the Baltic Sea are much smaller. Phylogenomic analysis and ancestral state reconstruction showed that approximately half of the brackish MAGs had freshwater ancestors and half had marine ancestors. Phylogenetic distances were on average shorter to freshwater ancestors, but when subsampling the tree to the same number of freshwater and marine MAG clusters, the distances were not significantly different. Brackish genomes belonging to Acidimicrobiia, Actinobacteria and Cyanobacteriia tended to originate from freshwater bacteria, while those of Alphaproteobacteria and Bacteroidia mainly had evolved from marine bacteria.</p>
----------------------------------------------------------------------
In diva2:1044163 abstract is: <p>The impact of carbon-dioxide emission on the environment is one of our generation’sbiggest environmental challenges, and the use of more sustainable energy sources isneeded. Bioethanol can act as a more sustainable resource instead of petroleum basedfuels. The biggest obstacle during production of second-generation bioethanol is thepre-treatment step, where the lignin is removed from the cellulolytic material.Laccases are enzymes that have the ability to oxidise lignin and thus remove ligninfrom the material. Recent articles reporting on enzymes belonging to the proteinfamily DUF152 have shown that they possess a laccase-like activity. Therefor,members of the DUF152 family are interesting targets for possible future applicationsthat involve pre-treatment of cellulolytic materials. In this report structural andpreliminary biochemical studies are presented for several novel DUF152 membersreferred to as B05, B10 and Dv152.</p><p>The proteins were produced in Escherichia coli cells and purified by immobilisedmetal ion affinity chromatography and size exclusion chromatography. Crystallisationexperiments were successful and yielded high-resolution diffraction data for all threeenzymes. The crystal structures were determined at 2.10, 2.30 and 1.60 Å resolutionfor B05 was 2.1 Å, B10 2.3 Å and for Dv152 1.6 Å respectively. The amino acids inthe active site were shown to be conserved and included two catalytic 2 histidineresidues and one cysteine residue. To further evauate similarities and differences ofstructural features, homology models were also generated for DUF152 members thathave been biochemically characterised but where no crystal structure is available.</p>

w='evauate' val={'c': 'evaluate', 's': 'diva2:1044163', 'n': 'no full text'}
w='ligninfrom' val={'c': 'lignin from', 's': 'diva2:1044163', 'n': 'no full text'}

corrected abstract:
<p>The impact of carbon-dioxide emission on the environment is one of our generation’s biggest environmental challenges, and the use of more sustainable energy sources is needed. Bioethanol can act as a more sustainable resource instead of petroleum based fuels. The biggest obstacle during production of second-generation bioethanol is the pre-treatment step, where the lignin is removed from the cellulolytic material. Laccases are enzymes that have the ability to oxidise lignin and thus remove lignin from the material. Recent articles reporting on enzymes belonging to the protein family DUF152 have shown that they possess a laccase-like activity. Therefor, members of the DUF152 family are interesting targets for possible future applications that involve pre-treatment of cellulolytic materials. In this report structural and preliminary biochemical studies are presented for several novel DUF152 members referred to as B05, B10 and Dv152.</p><p>The proteins were produced in Escherichia coli cells and purified by immobilised metal ion affinity chromatography and size exclusion chromatography. Crystallisation experiments were successful and yielded high-resolution diffraction data for all three enzymes. The crystal structures were determined at 2.10, 2.30 and 1.60 Å resolution for B05 was 2.1 Å, B10 2.3 Å and for Dv152 1.6 Å respectively. The amino acids in the active site were shown to be conserved and included two catalytic 2 histidine residues and one cysteine residue. To further evaluate similarities and differences of structural features, homology models were also generated for DUF152 members that have been biochemically characterised but where no crystal structure is available.</p>
----------------------------------------------------------------------
In diva2:812041 abstract is: <p>This diploma work has been carried out on behalf of Terco. TercosPST 2220 Transmission Line and Distribution Module works as a physicalmodel of a real transmission and distribution grid where five different typesof networks based on length, voltage and apparent effect are available. Thereis today a need of a model where the user self can adjust these parameters sothat the model more precisely can reflect the characteristics that the specificgrid has. Here it’s investigated how the length and its impact on a line canbe varied in a model.A presentation of how the transmission and distribution grid works andare described theoretically provides the basics for the different models thatcan describe a whole network and its properties.Since the R, L and C components needs to be able to be varied to be ableto physically realize this theoretical model, the different methods that thiscan be realized through are investigated. Two approaches are investigated,the cascaded pi-model and variable active-passive reactance (VAPAR).A number of aspects like space, cost and variability makes the variableactive-passive reactance the most suited solution. Its function as a variablevoltage source, made out of an four switches, operated with control techno-logy and pulse width modulation, makes it possible to imitate R, L and Csproperties and effect on a transmission line. The result is that the necessaryR,L,C components are made adjustable in order to be incorporated in aadjustable transmission lin model.The result and the goal are verified with simulations where variableactive-passive reactance is proved able for further development and practicaltests to model transmission and distribution lines with different length.</p><p><strong>Keywords. </strong>Variability, Inverter, Impedance, DC-AC, Pulse width modula-tion, Harmonics, H-bridge, Transmission line, Voltage drop, Reactive effect.</p>

w='Csproperties' val={'c': 'C properties', 's': 'diva2:812041', 'n': 'error in original'}

corrected abstract:
<p>This diploma work has been carried out on behalf of Terco. Tercos PST 2220 Transmission Line and Distribution Module works as a physical model of a real transmission and distribution grid where five different types of networks based on length, voltage and apparent effect are available. There is today a need of a model where the user self can adjust these parameters so that the model more precisely can reflect the characteristics that the specific grid has. Here it’s investigated how the length and its impact on a line can be varied in a model.</p><p>A presentation of how the transmission and distribution grid works and are described theoretically provides the basics for the different models that can describe a whole network and its properties.</p><p>Since the R, L and C components needs to be able to be varied to be able to physically realize this theoretical model, the different methods that this can be realized through are investigated. Two approaches are investigated, the cascaded pi-model and variable active-passive reactance (VAPAR).</p><p>A number of aspects like space, cost and variability makes the variable active-passive reactance the most suited solution. Its function as a variable voltage source, made out of an four switches, operated with control technology and pulse width modulation, makes it possible to imitate R, L and Cs properties and effect on a transmission line. The result is that the necessary R,L,C components are made adjustable in order to be incorporated in a adjustable transmission lin model.</p><p>The result and the goal are verified with simulations where variable active-passive reactance is proved able for further development and practical tests to model transmission and distribution lines with different length.</p>
----------------------------------------------------------------------
In diva2:1217588 abstract is: <p>In vivo, esterases together with lipases catalyse the cleavage of esters into acids and alcohols through hydrolysis.However in anhydrous environments, their activity is reversed to instead catalyze acyl transfer reactions. This reactiontype is responsible for many vital physiological functions, and have the potential to be used for a wide variety ofapplications. MsAcT from M. smegmatis is one of few esterases with the ability to perform acyl transfer reactionsin aqueous solutions. In vitro, MsAcT forms octamers resulting in the formation of hydrophobic channels within thecomplexes. The active site is buried within such a channel which consitute the putative explanation for the uniquecatalytic properties of the enzyme.Two variants of the enzyme; L12A and T93A/F154A were investigated in more detail using acyl donors of differentlengths (dimethyl succinate, -adipate, -suberate, and -sebacate), and varying types of acyl acceptors (1-octanol,butanediol vinyl ether, trimethylolpropane oxetane, and 1,4-butanediol). Both variants showed an extended substratescope compared to the wilde type (wt), with the T93A/F154A accepting acyl donors up to 10 carbons in length(sebacate) and had the highest overall activity for all alcohols tested. However, the L12A variant yields mixedvinyl adipate esters with higher purity than the wt and T93A/F154A variants. With 1,4-butanediol, the wt andT93A/F154A are able to form oligomers, which has never been demonstrated before. A better understanding of theseresults were acquired through molecular dynamics simulation of the enzyme-substrate interactions.The properties of these MsAcT variants could be very interesting from an industrial point of view. Being ableto customize the specificity of the MsAcT would be valuable for the biosynthesis of various substances and materialssuch as hybrid pharmaceuticals and different polymers.Key words: esterase, acyl transfer, MsAcT, mixed dicarboxylic esters, acyl donor, acyl acceptor, specificity,molecular dynamics simulation, immobilization.</p>

w='consitute' val={'c': 'constitute', 's': 'diva2:1217588', 'n': 'no full text'}
w='wilde' val={'c': 'wild', 's': 'diva2:1217588', 'n': 'no full text'}

corrected abstract:
<p>In vivo, esterases together with lipases catalyse the cleavage of esters into acids and alcohols through hydrolysis. However in anhydrous environments, their activity is reversed to instead catalyze acyl transfer reactions. This reaction type is responsible for many vital physiological functions, and have the potential to be used for a wide variety of applications. MsAcT from <em>M. smegmatis</em> is one of few esterases with the ability to perform acyl transfer reactions in aqueous solutions. In vitro, MsAcT forms octamers resulting in the formation of hydrophobic channels within the complexes. The active site is buried within such a channel which constitute the putative explanation for the unique catalytic properties of the enzyme. Two variants of the enzyme; L12A and T93A/F154A were investigated in more detail using acyl donors of different lengths (dimethyl succinate, -adipate, -suberate, and -sebacate), and varying types of acyl acceptors (1-octanol,butanediol vinyl ether, trimethylolpropane oxetane, and 1,4-butanediol). Both variants showed an extended substrate scope compared to the wild type (wt), with the T93A/F154A accepting acyl donors up to 10 carbons in length (sebacate) and had the highest overall activity for all alcohols tested. However, the L12A variant yields mixed vinyl adipate esters with higher purity than the wt and T93A/F154A variants. With 1,4-butanediol, the wt and T93A/F154A are able to form oligomers, which has never been demonstrated before. A better understanding of these results were acquired through molecular dynamics simulation of the enzyme-substrate interactions. The properties of these MsAcT variants could be very interesting from an industrial point of view. Being able to customize the specificity of the MsAcT would be valuable for the biosynthesis of various substances and materials such as hybrid pharmaceuticals and different polymers.</p>
----------------------------------------------------------------------
In diva2:1115384 abstract is: <p>MRI is one of the biggest and most growing imaging techniques. Even though itis one of the most harmless technologies a big portion of the patients experienceanxiety during the exam. By improving the patient experience unnecessary psychologicalstress for the patient can be prevented, the patient movement wouldthen decrease and therefore the imaging can be improved without changing thetechnique. Participant observations at four dierent MRI departments werecompleted with six interviews with radiographers and technical MRI personnelin order to get insight in the work around an MRI exam and the problemsthat patients experience. The data collection resulted in three improvementareas: the atmosphere of the waiting room, the atmosphere of the MRI roomand the headset used by the patient during the MRI exam. These improvementareas were paired up with solution suggestions which were then controlled andcommented by one MRI specialist, one MRI developer and one radiographer tovalidate the suggestions. The conclusion was that there is already much doneto improve the environment in the MRI room, even though more can be done.The waiting room, on the other hand ,has not been an object for studies orfor improvements before. Therefore more calculation about how big of a protit could be, to improving the atmosphere in the waiting room, should be doneso one knows how much resources one can be put into that improvement area.Lastly there are potential solutions for how to create a much better headset butbecause the generated solutions in this area are so technically challenging moreresearch has to be done before it can be realised.</p>

w='protit' val={'c': 'profit', 's': 'diva2:1115384'}

Note that the "hand ,has" is an error in the original.
corrected abstract:
<p>MRI is one of the biggest and most growing imaging techniques. Even though it is one of the most harmless technologies a big portion of the patients experience anxiety during the exam. By improving the patient experience unnecessary psychological stress for the patient can be prevented, the patient movement would then decrease and therefore the imaging can be improved without changing the technique. Participant observations at four different MRI departments were completed with six interviews with radiographers and technical MRI personnel in order to get insight in the work around an MRI exam and the problems that patients experience. The data collection resulted in three improvement areas: the atmosphere of the waiting room, the atmosphere of the MRI room and the headset used by the patient during the MRI exam. These improvement areas were paired up with solution suggestions which were then controlled and commented by one MRI specialist, one MRI developer and one radiographer to validate the suggestions. The conclusion was that there is already much done to improve the environment in the MRI room, even though more can be done. The waiting room, on the other hand ,has not been an object for studies or for improvements before. Therefore more calculation about how big of a profit it could be, to improving the atmosphere in the waiting room, should be done so one knows how much resources one can be put into that improvement area. Lastly there are potential solutions for how to create a much better headset but because the generated solutions in this area are so technically challenging more research has to be done before it can be realised.</p>
----------------------------------------------------------------------
In diva2:1773505 abstract is: <p>Home-based exercise is a popular physical activity of maintaining fitness, health andwellness in general. However, without proper supervision and basic knowledge of theexercises in the workout plan, there is an increased risk of injury. Considering that noteveryone is willing to attend crowded gyms or schedule professional personal trainingsessions, in this study, a novel feedback system is proposed, in the form of a mobileapplication. Accelerometer and gyroscope data were collected from 10 volunteersperforming 3 exercises, squats, lunges and bridges, with inertial sensors attachedto their back lumbar region, on both shanks and on both thighs. Each participantperformed 5 repetitions of the correct technique and 5 repetitions of 4 mistakes foreach exercise. The accuracies of 3 classifiers, a SVM, a RF and DT were comparedwith the SVM performing the best across all 3 exercises. The best location and numberof sensors was determined by examining the accuracy of a SVM model for 15 uniquemulti-sensor configurations. The best performing setup, being the configuration with 2sensors, one at the lumbar area and one at the shank, was used in exploring the efficacyof different data processing techniques. Time-domain statistical features, sensor angletimeseries and the filtered signal timeseries were evaluated as input to a NN. The timedomainfeatures performed the best achieving the highest accuracy in all 3 exercises,with an accuracy of 67% for the squats, 87% for the lunges and 75% for the hip bridges.Overall, the final model demonstrated promising capabilities of classifying exercisetechnique of basic lower-body exercises, with a real-time feedback implementationbeing a feasible solution for self-efficient fitness.</p>

corrected abstract:
<p>Home-based exercise is a popular physical activity of maintaining fitness, health and wellness in general. However, without proper supervision and basic knowledge of the exercises in the workout plan, there is an increased risk of injury. Considering that not everyone is willing to attend crowded gyms or schedule professional personal training sessions, in this study, a novel feedback system is proposed, in the form of a mobile application. Accelerometer and gyroscope data were collected from 10 volunteers performing 3 exercises, squats, lunges and bridges, with inertial sensors attached to their back lumbar region, on both shanks and on both thighs. Each participant performed 5 repetitions of the correct technique and 5 repetitions of 4 mistakes for each exercise. The accuracies of 3 classifiers, a SVM, a RF and DT were compared with the SVM performing the best across all 3 exercises. The best location and number of sensors was determined by examining the accuracy of a SVM model for 15 unique multi-sensor configurations. The best performing setup, being the configuration with 2 sensors, one at the lumbar area and one at the shank, was used in exploring the efficacy of different data processing techniques. Time-domain statistical features, sensor angle timeseries and the filtered signal timeseries were evaluated as input to a NN. The time-domain features performed the best achieving the highest accuracy in all 3 exercises, with an accuracy of 67% for the squats, 87% for the lunges and 75% for the hip bridges. Overall, the final model demonstrated promising capabilities of classifying exercise technique of basic lower-body exercises, with a real-time feedback implementation being a feasible solution for self-efficient fitness.</p>
----------------------------------------------------------------------
In diva2:935006 abstract is: <p>Wireless devices search for access points when they want to connect to a network. A devicechooses an access point based on the received signal strength between the device and theaccess point. That method is good for staying connected in a local area network but it doesnot always offer the best performance, which can result in a slower connection. This is thestandard method of connection for wireless clients, which will be referred to as the standardprotocol. Larger networks commonly have a lot of access points in an area, which increasesthe coverage area and makes loss of signal a rare occurrence. Overlapping coverage zonesare also common, offering multiple choices for a client. The company Inteno wanted an alternativeconnection method for their gateways. The new method that was developed wouldforce the client to connect to an access point depending on the bitrate to the master, as wellas the received signal strength. These factors are affected by many different parameters.These parameters were noise, signal strength, link-rate, bandwidth usage and connectiontype. A new metric had to be introduced to make the decision process easier by unifying theavailable parameters. The new metric that was introduced is called score. A score system wascreated based on these metrics. The best suited access point would be the one with the highestscore. The developed protocol chose the gateway with the highest bitrate available, while thestandard protocol would invariably pick the closest gateway regardless. The developed protocolcould have been integrated to the standard protocol to gain the benefits of both. Thiscould not be accomplished since the information was not easily accessible on Inteno’s gatewaysand had to be neglected in this thesis.</p>

partal corrected: diva2:935006: <p>Wireless devices search for access points when they want to connect to a network. A device chooses an access point based on the received signal strength between the device and the access point. That method is good for staying connected in a local area network but it does not always offer the best performance, which can result in a slower connection. This is the standard method of connection for wireless clients, which will be referred to as the standard protocol. Larger networks commonly have a lot of access points in an area, which increases the coverage area and makes loss of signal a rare occurrence. Overlapping coverage zones are also common, offering multiple choices for a client. The company Inteno wanted an alternative connection method for their gateways. The new method that was developed would force the client to connect to an access point depending on the bitrate to the master, as well as the received signal strength. These factors are affected by many different parameters. These parameters were noise, signal strength, link-rate, bandwidth usage and connection type. A new metric had to be introduced to make the decision process easier by unifying the available parameters. The new metric that was introduced is called score. A score system was created based on these metrics. The best suited access point would be the one with the highest score. The developed protocol chose the gateway with the highest bitrate available, while the standard protocol would invariably pick the closest gateway regardless. The developed protocol could have been integrated to the standard protocol to gain the benefits of both. This could not be accomplished since the information was not easily accessible on Inteno’s gateways and had to be neglected in this thesis .</p>

corrected abstract:
<p>Wireless devices search for access points when they want to connect to a network. A device chooses an access point based on the received signal strength between the device and the access point. That method is good for staying connected in a local area network but it does not always offer the best performance, which can result in a slower connection. This is the standard method of connection for wireless clients, which will be referred to as the standard protocol. Larger networks commonly have a lot of access points in an area, which increases the coverage area and makes loss of signal a rare occurrence. Overlapping coverage zones are also common, offering multiple choices for a client. The company Inteno wanted an alternative connection method for their gateways. The new method that was developed would force the client to connect to an access point depending on the bitrate to the master, as well as the received signal strength. These factors are affected by many different parameters. These parameters were noise, signal strength, link-rate, bandwidth usage and connection type. A new metric had to be introduced to make the decision process easier by unifying the available parameters. The new metric that was introduced is called score. A score system was created based on these metrics. The best suited access point would be the one with the highest score. The developed protocol chose the gateway with the highest bitrate available, while the standard protocol would invariably pick the closest gateway regardless. The developed protocol could have been integrated to the standard protocol to gain the benefits of both. This could not be accomplished since the information was not easily accessible on Inteno’s gateways and had to be neglected in this thesis.</p>
----------------------------------------------------------------------
In diva2:1219371 abstract is: <p>The transplantations of biological or non-biological materials are the heart of many medical innovations and lifesaving therapies. Many studies have revealed that a balanced immune modulation after transplantation iscrucial for the materials ability to integrate with the host tissue. Mucins, agroup of heavily glycosylated proteins present in the mucosa, have shown to have an immune modulating capacity in different species, and could therefore be used to modulate the immune response to implanted devices. To explore this possibility, we aim to develop biomaterials, rich in mucins,which have the capacity to coat the transplants to act as an interfacebetween the body and the transplanted material. Monolayer mucincoating can be obtained through the simple adsorption of mucins onvariety of surfaces. However, mucin monolayers can have limitations suchas imperfect coverage and instability over time. We thus aim to developmucin-rich thin films using the layer-by-layer assembly technique.To achieve this goal, we synthesized ClickMucins by functionalizingbovine submaxillary mucins (BSM) with tetrazine and norbornene via twografting strategies. We either targeted the carboxyl groups of the mucinsor the cis-glycol groups of the mucin-associated glycans. The click reaction between tetrazine and norbornene allows the functionalized BSM to formpure mucin hydrogels via covalent bonding. The layer-by-layer assembly ofalternating layers of tetrazine and norbornene-functionalized mucin haddifferent characteristics depending on the functionalization method used.We found significant differences in the initial adsorption of the mucins tothe surface and in the buildup of the second layer. Functionalization bytargeting the carboxyl group gave better adsorption to the substratesurface, while functionalization by targeting the cis-glycol gave a more significant second-layer buildup. The buildup of the multilayer film waslimited to two layers, which we hypothesized could be due to disbalanced grafting density of the two functionalities on the mucins.</p>

w='SMITH' val={'c': 'SMITh', 's': 'diva2:1219371', 'n': 'Université Claude Bernard Lyon 1 - see https://www.icbms.fr/en/team/10-smith-html'}

corrected abstract:
<p>The transplantations of biological or non-biological materials are the heart of many medical innovations and lifesaving therapies. Many studies have revealed that a balanced immune modulation after transplantation is crucial for the materials ability to integrate with the host tissue. Mucins, a group of heavily glycosylated proteins present in the mucosa, have shown to have an immune modulating capacity in different species, and could therefore be used to modulate the immune response to implanted devices. To explore this possibility, we aim to develop biomaterials, rich in mucins, which have the capacity to coat the transplants to act as an interface between the body and the transplanted material. Monolayer mucin coating can be obtained through the simple adsorption of mucins on variety of surfaces. However, mucin monolayers can have limitations such as imperfect coverage and instability over time. We thus aim to develop mucin-rich thin films using the layer-by-layer assembly technique. To achieve this goal, we synthesized ClickMucins by functionalizing bovine submaxillary mucins (BSM) with tetrazine and norbornene via two grafting strategies. We either targeted the carboxyl groups of the mucins or the cis-glycol groups of the mucin-associated glycans. The click reaction between tetrazine and norbornene allows the functionalized BSM to form pure mucin hydrogels via covalent bonding. The layer-by-layer assembly of alternating layers of tetrazine and norbornene-functionalized mucin had different characteristics depending on the functionalization method used. We found significant differences in the initial adsorption of the mucins to the surface and in the buildup of the second layer. Functionalization by targeting the carboxyl group gave better adsorption to the substrate surface, while functionalization by targeting the cis-glycol gave a more significant second-layer buildup. The buildup of the multilayer film was limited to two layers, which we hypothesized could be due to disbalanced grafting density of the two functionalities on the mucins.</p>
----------------------------------------------------------------------
In diva2:1604124 abstract is: <p>Radiotherapy has become an ever more successful treatment option for cancer.Advances in imaging protocols combined with precise therapy devices suchas linear accelerators contribute towards millimeter precision of treatmentdelivery with far fewer side effects. The ultimate goal of radiotherapy is tomaximize tumor control while minimizing adverse effects to healthy tissues,more importantly organs at risk surrounding the tumor. External beamradiotherapy is currently on the brink of breaking a new frontier: MagneticResonance Imaging (MRI) guided tumor tracking. Here, a combined linearaccelerator and MRI system can be used to treat and follow the tumor duringirradiation, called Real-time Adaptive Radiotherapy (ART). Tailoring of thebeam shape, by means of the Multi-leaf Collimator (MLC) on the fly has thepotential to complete a fully automated radiotherapy process.</p><p>Recent advances in Reinforcement Learning (RL), a sub field of artificialintelligence has pushed the frontiers further in sequential decision making processesfurther in various fields. In a MLC tracking scenario, we hypothesizethat an RL agent trained on real-time tumor delineations and dose informationcould fulfill a specified dosimetric criteria on the fly over the moving target.To investigate the feasibility of RL for MLC tracking further: we designeda simulator, devised an appropriate RL framework and interfaced them to aDeep Q-Network (DQN) algorithm.</p><p>Our results demonstrate the feasibility of employing RL for MLC trackingalong with numerous design choices that need to be considered while developingsuch a system. We believe to have taken the first step to bridge MLCtracking and RL by proposing a closed loop solution using dose information.</p>


corrected abstract:
<p>Radiotherapy has become an ever more successful treatment option for cancer. Advances in imaging protocols combined with precise therapy devices such as linear accelerators contribute towards millimeter precision of treatment delivery with far fewer side effects. The ultimate goal of radiotherapy is to maximize tumor control while minimizing adverse effects to healthy tissues, more importantly organs at risk surrounding the tumor. External beam radiotherapy is currently on the brink of breaking a new frontier: Magnetic Resonance Imaging (MRI) guided tumor tracking. Here, a combined linear accelerator and MRI system can be used to treat and follow the tumor during irradiation, called Real-time Adaptive Radiotherapy (ART). Tailoring of the beam shape, by means of the Multi-leaf Collimator (MLC) on the fly has the potential to complete a fully automated radiotherapy process.</p><p>Recent advances in Reinforcement Learning (RL), a sub field of artificial intelligence has pushed the frontiers further in sequential decision making processes further in various fields. In a MLC tracking scenario, we hypothesize that an RL agent trained on real-time tumor delineations and dose information could fulfill a specified dosimetric criteria on the fly over the moving target. To investigate the feasibility of RL for MLC tracking further: we designed a simulator, devised an appropriate RL framework and interfaced them to a Deep Q-Network (DQN) algorithm.</p><p>Our results demonstrate the feasibility of employing RL for MLC tracking along with numerous design choices that need to be considered while developing such a system. We believe to have taken the first step to bridge MLC tracking and RL by proposing a closed loop solution using dose information.</p>
----------------------------------------------------------------------
In diva2:1774643 abstract is: <p>The work of this master thesis, at the Royal Institute of Technology, is primarily a study of rootcauses of fatal injuries at work, related to building and construction industries includinginstallation. As a term, root cause has not only obvious synonyms, since the meaning can vary fromword to word. The author of the thesis is discussing how a root cause should be defined with somealternatives. Here, a common nominator can be that the level of impact caused should not be builtinto the term. Moreover, the definition must not hinder the practical use of root causes through unintentional misunderstanding.</p><p>Fundamentally, this study is to determine whether a root cause is appropriate for use related toaccident prevention at work. Consequently, the author found 19 of them organised under theseprincipal headings: responsibility, risk analysis, communication, and protective measure.</p><p>This study is accomplished through a qualitative research methodology only, based on aliterature study. A substantial part of it was to analyse court judgments related to fatal, and workrelatedaccidents, and if possible, find root causes. There were two interviews by focus groups withone employer organisation, Byggföretagen, and one trade union, Byggnads, represented.</p><p>Since one of the purposes was to make a suitability assessment of the root causes and seriouslydiscuss these in the light of the introduced trails, a major part of this thesis is about the underlyingreasons for the fatal accidents of the industries. Hence, the result section of the report is worthwhileto read since it is based on wide and deep knowledge by the interviewed representatives in workenvironment related issues of the building and construction industries.</p><p>Another part of the thesis is highlighting how useful statistics of accidents, from the SwedishWork Environment Authority (SWEA) and related sources, are. From the authors side, there arepoints of view about how precise the data of the registers are, since there is a growing labour importconsisting of posted workers and a workforce within the black-grey market.</p><p>Some conclusions that are drawn: Societal actors would benefit from more suitable data androot causes, possible to use as tools, in both proactive and reactive purposes.</p>

corrected abstract:
<p>The work of this master thesis, at the Royal Institute of Technology, is primarily a study of root causes of fatal injuries at work, related to building and construction industries including installation. As a term, root cause has not only obvious synonyms, since the meaning can vary from word to word. The author of the thesis is discussing how a root cause should be defined with some alternatives. Here, a common nominator can be that the level of impact caused should not be built into the term. Moreover, the definition must not hinder the practical use of root causes through unintentional misunderstanding.</p><p>Fundamentally, this study is to determine whether a root cause is appropriate for use related to accident prevention at work. Consequently, the author found 19 of them organised under these principal headings: responsibility, risk analysis, communication, and protective measure.</p><p>This study is accomplished through a qualitative research methodology only, based on a literature study. A substantial part of it was to analyse court judgments related to fatal, and work-related accidents, and if possible, find root causes. There were two interviews by focus groups with one employer organisation, Byggföretagen, and one trade union, Byggnads, represented.</p><p>Since one of the purposes was to make a suitability assessment of the root causes and seriously discuss these in the light of the introduced trails, a major part of this thesis is about the underlying reasons for the fatal accidents of the industries. Hence, the result section of the report is worthwhile to read since it is based on wide and deep knowledge by the interviewed representatives in work environment related issues of the building and construction industries.</p><p>Another part of the thesis is highlighting how useful statistics of accidents, from the Swedish Work Environment Authority (SWEA) and related sources, are. From the authors side, there are points of view about how precise the data of the registers are, since there is a growing labour import consisting of posted workers and a workforce within the black-grey market.</p><p>Some conclusions that are drawn: Societal actors would benefit from more suitable data and root causes, possible to use as tools, in both proactive and reactive purposes.</p>
----------------------------------------------------------------------
In diva2:1455037 abstract is: <p>State-of-the-art fluorescent imaging research is strictly limited to eight fluorophore labels duringthe study of intercellular interactions among organelles. The number of excited fluorophore colorsis restricted due to overlap in the narrow spectra of visual wavelength. However, this requires aconsiderable effort of analysis to be able to tell the overlapping signals apart. Significant overlapalready occurs with the use of more than four fluorophores and is leaving researchers limited to asmall number of labels and the hard decision to prioritize between cellular labels to use.</p><p>Except for the physical limitations of fluorescent labeling, the labeling itself causes behavioralabnormalities due to sample perturbation. In addition to this, the labeling dye or dye-adjacentantibodies are potentially causing phototoxicity and photobleaching thus limiting the timescale oflive cell imaging. Nontoxic imaging modalities such as transmitted-light microscopes, such asbright-field and phase contrast methods, are available but not nearly achieving images of thespecificity as when using fluorophore labeling.</p><p>An approach that could increase the number of organelles simultaneously studied withfluorophore labels, while being cost-effective and nontoxic as transmitted-light microscopes wouldbe an invaluable tool in the quest to enhance knowledge of cellular studies of organelles. Here wepresent a deep learning solution, using convolutional neural networks built to predict thefluorophore labeling effect on the nucleus, from a transmitted-light input. This solution renders afluorescent channel available for another marker and would eliminate the process of labeling thenucleus with dye or dye-conjugated antibodies by instead using deep convolutional neuralnetworks.</p>

corrected abstract:
<p>State-of-the-art fluorescent imaging research is strictly limited to eight fluorophore labels during the study of intercellular interactions among organelles. The number of excited fluorophore colors is restricted due to overlap in the narrow spectra of visual wavelength. However, this requires a considerable effort of analysis to be able to tell the overlapping signals apart. Significant overlap already occurs with the use of more than four fluorophores and is leaving researchers limited to a small number of labels and the hard decision to prioritize between cellular labels to use.</p><p>Except for the physical limitations of fluorescent labeling, the labeling itself causes behavioral abnormalities due to sample perturbation. In addition to this, the labeling dye or dye-adjacent antibodies are potentially causing phototoxicity and photobleaching thus limiting the timescale of live cell imaging. Nontoxic imaging modalities such as transmitted-light microscopes, such as bright-field and phase contrast methods, are available but not nearly achieving images of the specificity as when using fluorophore labeling.</p><p>An approach that could increase the number of organelles simultaneously studied with fluorophore labels, while being cost-effective and nontoxic as transmitted-light microscopes would be an invaluable tool in the quest to enhance knowledge of cellular studies of organelles. Here we present a deep learning solution, using convolutional neural networks built to predict the fluorophore labeling effect on the nucleus, from a transmitted-light input. This solution renders a fluorescent channel available for another marker and would eliminate the process of labeling the nucleus with dye or dye-conjugated antibodies by instead using deep convolutional neural networks.</p>
----------------------------------------------------------------------
In diva2:1673165 abstract is: <p>Non-invasive methods to evaluate skeletal muscle oxidative capacity have beenemerging as a viable substitute for invasive methods in recent years. One ofthose methods utilises near-infrared spectroscopy (NIRS) to calculate V O2mrecovery off-kinetics following an exercise. The data analysis of the measuredsignals from the NIRS is still done manually in a time-consuming and dauntingprocess. The present thesis aimed to develop software, associated with theNIRS method, capable of analysing the recovery from a repeated arterialocclusion protocol following an exercise to assess muscle oxidative capacity.Additionally, to analyse the recovery from ischemic preconditioning as a singletest to assess muscle oxidative capacity. A method that has never been utilisedbefore.11 active, healthy subjects were analysed to calculate their recovery rate.Subjects underwent ischemic preconditioning before exercising for 6 minutesat 80% of gas exchange threshold. A repeated arterial occlusion protocol wascarried out after the exercise. A software was developed in R that utilised linearregression as well as exponential fitting to calculate the recovery rate of eachsubject during both the ischemic preconditioning and the occlusion protocol.The calculated results were compared to predetermined recovery rate results ofeach subject. The calculated results of the repeated arterial occlusion protocolgave similar results to the predetermined ones and even more data on eachsubject’s recovery from an exercise. The calculated results of the ischemicpreconditioning were promising and implied that ischemic preconditioning asa single test can be utilised as a method to assess muscle oxidative capacity.However, further research is required to confirm it. </p>

corrected abstract:
<p>Non-invasive methods to evaluate skeletal muscle oxidative capacity have been emerging as a viable substitute for invasive methods in recent years. One of those methods utilises near-infrared spectroscopy (NIRS) to calculate <em>V O<sub>2</sub>m</em> recovery off-kinetics following an exercise. The data analysis of the measured signals from the NIRS is still done manually in a time-consuming and daunting process. The present thesis aimed to develop software, associated with the NIRS method, capable of analysing the recovery from a repeated arterial occlusion protocol following an exercise to assess muscle oxidative capacity. Additionally, to analyse the recovery from ischemic preconditioning as a single test to assess muscle oxidative capacity. A method that has never been utilised before.</p><p>11 active, healthy subjects were analysed to calculate their recovery rate. Subjects underwent ischemic preconditioning before exercising for 6 minutes at 80% of gas exchange threshold. A repeated arterial occlusion protocol was carried out after the exercise. A software was developed in R that utilised linear regression as well as exponential fitting to calculate the recovery rate of each subject during both the ischemic preconditioning and the occlusion protocol. The calculated results were compared to predetermined recovery rate results of each subject. The calculated results of the repeated arterial occlusion protocol gave similar results to the predetermined ones and even more data on each subject’s recovery from an exercise. The calculated results of the ischemic preconditioning were promising and implied that ischemic preconditioning as a single test can be utilised as a method to assess muscle oxidative capacity. However, further research is required to confirm it.</p>
----------------------------------------------------------------------
In diva2:1454447 abstract is: <p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish andcultivated fish, causing severe losses in the fish industry every year. Aquacultured fishesexists worldwide and constitutes an important food source for humans ever since overfishingstarted. Previously used methods for controlling pathogenic infections e.g. malachite green,has been proven non safe. It is today crucial to find an environmentally friendly method forcontrolling pathogenic infection such as saprolegniosis. One of six identified cellulosesynthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls andtherefore suggested as a potential target as drug control for saprolegniosis. To enablecharacterization of the structure and function of SpCesA3.1, it needs to be purified in itscatalytically active full-lengths form. The aim of this project was to test a method forheterologous expression, solubilization and purification of SpCesA3.1. The expected projectoutcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in itscatalytically active form. Solubilization and purification of SpCesA3.1 was assessedexperimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agentand IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experimentsresulted in successful heterologous expressions in yeast strain LoGSA and FGY217,transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negativepurification of SpCesA3.1. Results were compared to previous research and a protocol onhow to proceed with the optimization of solubilization and purification of SpCesA3.1 ispresented here. It is proposed that the presented protocol is applied in further research on thissubject.</p>

partal corrected: diva2:1454447: <p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish and cultivated fish, causing severe losses in the fish industry every year. Aquacultured fishes exists worldwide and constitutes an important food source for humans ever since overfishing started. Previously used methods for controlling pathogenic infections e.g. malachite green, has been proven non safe. It is today crucial to find an environmentally friendly method for controlling pathogenic infection such as saprolegniosis. One of six identified cellulose synthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls and therefore suggested as a potential target as drug control for saprolegniosis. To enable characterization of the structure and function of SpCesA3.1, it needs to be purified in its catalytically active full-lengths form. The aim of this project was to test a method for heterologous expression, solubilization and purification of SpCesA3.1. The expected project outcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in its catalytically active form. Solubilization and purification of SpCesA3.1 was assessed experimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agent and IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experiments resulted in successful heterologous expressions in yeast strain LoGSA and FGY217, transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negative purification of SpCesA3.1. Results were compared to previous research and a protocol on how to proceed with the optimization of solubilization and purification of SpCesA3.1 is presented here. It is proposed that the presented protocol is applied in further research on this subject.</p>

corrected abstract:
<p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish and cultivated fish, causing severe losses in the fish industry every year. Aquacultured fishes exists worldwide and constitutes an important food source for humans ever since overfishing started. Previously used methods for controlling pathogenic infections e.g. malachite green, has been proven non safe. It is today crucial to find an environmentally friendly method for controlling pathogenic infection such as saprolegniosis. One of six identified cellulose synthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls and therefore suggested as a potential target as drug control for saprolegniosis. To enable characterization of the structure and function of SpCesA3.1, it needs to be purified in its catalytically active full-lengths form. The aim of this project was to test a method for heterologous expression, solubilization and purification of SpCesA3.1. The expected project outcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in its catalytically active form. Solubilization and purification of SpCesA3.1 was assessed experimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agent and IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experiments resulted in successful heterologous expressions in yeast strain LoGSA and FGY217, transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negative purification of SpCesA3.1. Results were compared to previous research and a protocol on how to proceed with the optimization of solubilization and purification of SpCesA3.1 is presented here. It is proposed that the presented protocol is applied in further research on this subject.</p>
----------------------------------------------------------------------
In diva2:1454447 abstract is: <p>The oomycete Saprolegnia parasitica is a pathogen that infects mainly wild fish andcultivated fish, causing severe losses in the fish industry every year. Aquacultured fishesexists worldwide and constitutes an important food source for humans ever since overfishingstarted. Previously used methods for controlling pathogenic infections e.g. malachite green,has been proven non safe. It is today crucial to find an environmentally friendly method forcontrolling pathogenic infection such as saprolegniosis. One of six identified cellulosesynthases in S. parasitica (SpCesA3.1) is highly expressed in the hyphal cell walls andtherefore suggested as a potential target as drug control for saprolegniosis. To enablecharacterization of the structure and function of SpCesA3.1, it needs to be purified in itscatalytically active full-lengths form. The aim of this project was to test a method forheterologous expression, solubilization and purification of SpCesA3.1. The expected projectoutcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in itscatalytically active form. Solubilization and purification of SpCesA3.1 was assessedexperimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agentand IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experimentsresulted in successful heterologous expressions in yeast strain LoGSA and FGY217,transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negativepurification of SpCesA3.1. Results were compared to previous research and a protocol onhow to proceed with the optimization of solubilization and purification of SpCesA3.1 ispresented here. It is proposed that the presented protocol is applied in further research on thissubject.</p>

Note: I am unsure why the "Sp" in italizied in just to places: "The aim of this project was to test a method for
heterologous expression, solubilization and purification of SpCesA3.1. The expected project
outcome was an optimized protocol for the purification of membrane protein SpCesA3.1 in its
catalytically active form." However, this is the way it is in https://kth.diva-portal.org/smash/get/diva2:1454447/SUMMARY01.pdf
corrected abstract:
<p>The oomycete <em>Saprolegnia parasitica</em> is a pathogen that infects mainly wild fish and cultivated fish, causing severe losses in the fish industry every year. Aquacultured fishes exists worldwide and constitutes an important food source for humans ever since overfishing started. Previously used methods for controlling pathogenic infections e.g. malachite green, has been proven non safe. It is today crucial to find an environmentally friendly method for controlling pathogenic infection such as saprolegniosis. One of six identified cellulose synthases in <em>S. parasitica</em> (SpCesA3.1) is highly expressed in the hyphal cell walls and therefore suggested as a potential target as drug control for saprolegniosis. To enable characterization of the structure and function of SpCesA3.1, it needs to be purified in its catalytically active full-lengths form. The aim of this project was to test a method for heterologous expression, solubilization and purification of <em>Sp</em>CesA3.1. The expected project outcome was an optimized protocol for the purification of membrane protein <em>Sp</em>CesA3.1 in its catalytically active form. Solubilization and purification of SpCesA3.1 was assessed experimentally, using detergent n-Dodecyl-B-D-Maltoside (DDM) as a solubilization agent and IMAC with preequilibrated Ni-NTA His•Bind® Resin protein purification. Experiments resulted in successful heterologous expressions in yeast strain LoGSA and FGY217, transformed with pDD-8HIS-GFP-SpCesA3 plasmid. However, there was a negative purification of SpCesA3.1. Results were compared to previous research and a protocol on how to proceed with the optimization of solubilization and purification of SpCesA3.1 is presented here. It is proposed that the presented protocol is applied in further research on this subject.</p>
----------------------------------------------------------------------
In diva2:1454857 abstract is: <p>Breast cancer is the most common cancer and the largest cause of cancer-related deathsworldwide for women. Risk prediction of breast cancer allows individualised andpreventative treatment. Many risk factors are known, such as age and breast density, buttoday there are no validated blood biomarkers for the risk prediction of breast cancer.Using blood samples for the prediction of breast cancer risk would increase thepossibilities for a more individualised, preventive treatment before diseasemanifestation.</p><p>To work towards such a goal, protein profiles had been generated with antibodies fromthe Human Protein Atlas project in human plasma samples. A total of 711 proteins werethen analysed for their potential relationship with breast density and breast cancer. Theproteomics data was generated using multiplexed immunoassays developed atKTH/Science for Life Laboratory (SciLifeLab), Stockholm, Sweden, on plasma samplesfrom 585 participants of the KARMA project at Karolinska Institutet in Stockholm,Sweden.</p><p>In this thesis project, the main focus was to apply advanced data analysis tools, beginningwith filtering and normalisation of the data. The participants were then stratified into fiveclusters based on differences in their plasma proteomics profiles using archetypalanalysis. Differences between the archetype clusters were found for clinical parameters,such as case control status, tumour characteristics and therapy.</p><p>Using the data from hundreds of plasma proteomes, substantially different molecularprofiles were found between the clusters obtained from archetypal analysis. A hypothesiswas that these differences might be likely driven by clinical traits that remain detectablein the circulating proteome over many years. In the future, more samples and clinical aswell as other omics data will be investigated to verify the observation.</p>

corrected abstract:
<p>Breast cancer is the most common cancer and the largest cause of cancer-related deaths worldwide for women. Risk prediction of breast cancer allows individualised and preventative treatment. Many risk factors are known, such as age and breast density, but today there are no validated blood biomarkers for the risk prediction of breast cancer. Using blood samples for the prediction of breast cancer risk would increase the possibilities for a more individualised, preventive treatment before disease manifestation.</p><p>To work towards such a goal, protein profiles had been generated with antibodies from the Human Protein Atlas project in human plasma samples. A total of 711 proteins were then analysed for their potential relationship with breast density and breast cancer. The proteomics data was generated using multiplexed immunoassays developed at KTH/Science for Life Laboratory (SciLifeLab), Stockholm, Sweden, on plasma samples from 585 participants of the KARMA project at Karolinska Institutet in Stockholm, Sweden.</p><p>In this thesis project, the main focus was to apply advanced data analysis tools, beginning with filtering and normalisation of the data. The participants were then stratified into five clusters based on differences in their plasma proteomics profiles using archetypal analysis. Differences between the archetype clusters were found for clinical parameters, such as case control status, tumour characteristics and therapy.</p><p>Using the data from hundreds of plasma proteomes, substantially different molecular profiles were found between the clusters obtained from archetypal analysis. A hypothesis was that these differences might be likely driven by clinical traits that remain detectable in the circulating proteome over many years. In the future, more samples and clinical as well as other omics data will be investigated to verify the observation.</p>
----------------------------------------------------------------------
In diva2:1077206 abstract is: <p>Patients treated at intensive care units (ICUs) are failing in one or several organs and requireappropriate monitoring and treatment in order to maintain a meaningful life. Today clinicians inintensive care units (ICUs) manage a large amount of data generated from monitoring devices.The monitoring parameters can either be noted down manually on a monitoring sheet or, for some parameters, transferred automatically to storage. In both cases the information is stored withthe aim to support clinicians throughout the intensive care and be easily accessible. Patient datamanagement systems (PDMSs) facilitate ICUs to retrieve and integrate data. Before managinga new configuration of patient data system, it is required that the ICU makes careful analysis ofwhat data desired to be registered. This pilot study provides knowledge of how the monitoringis performed in an Intensive Care Unit in an emergency hospital in Stockholm.The aim of this thesis project was to collect data about what the clinicians require and whatequipment they use today for monitoring. Requirement elicitation is a technique to collectrequirements. Methods used to collect data were active observations and qualitative interviews.Patterns have been found about what the assistant nurses, nurses and physicians’ require of systems supporting the clinician’s with monitoring parameters. Assistant nurses would like tobe released from tasks of taking notes manually. They also question the need for atomized datacollection since they are present observing the patient bed-side. Nurses describe a demanding burden of care and no more activities increasing that burden of care is required. Physicians require support in order to see how an intervention leads to a certain result for individual patients.The results also show that there is information about decision support but no easy way to applythem, better than the ones used today. Clinicians state that there is a need to be able to evaluatethe clinical work with the help of monitoring parameters. The results provide knowledge about which areas the clinicians needs are not supported enough by the exciting tools.To conclude results show that depending on what profession and experience the clinicians have the demands on monitoring support di↵ers. Monitoring at the ICU is performed while observing individual patients, parameters from medical devices, results from medical tests and physical examinations. Information from all these sources is considered by the clinicians and is desired to be supported accordingly before clinicians commit to action resulting in certain treatment,diagnosis and/or care.</p>

corrected abstract:
<p>Patients treated at intensive care units (ICUs) are failing in one or several organs and require appropriate monitoring and treatment in order to maintain a meaningful life. Today clinicians in intensive care units (ICUs) manage a large amount of data generated from monitoring devices. The monitoring parameters can either be noted down manually on a monitoring sheet or, for some parameters, transferred automatically to storage. In both cases the information is stored with the aim to support clinicians throughout the intensive care and be easily accessible. Patient data management systems (PDMSs) facilitate ICUs to retrieve and integrate data. Before managing a new configuration of patient data system, it is required that the ICU makes careful analysis of what data desired to be registered. This pilot study provides knowledge of how the monitoring is performed in an Intensive Care Unit in an emergency hospital in Stockholm.</p><p>The aim of this thesis project was to collect data about what the clinicians require and what equipment they use today for monitoring. Requirement elicitation is a technique to collect requirements. Methods used to collect data were active observations and qualitative interviews.</p><p>Patterns have been found about what the assistant nurses, nurses and physicians’ require of systems supporting the clinician’s with monitoring parameters. Assistant nurses would like to be released from tasks of taking notes manually. They also question the need for atomized data collection since they are present observing the patient bed-side. Nurses describe a demanding burden of care and no more activities increasing that burden of care is required. Physicians require support in order to see how an intervention leads to a certain result for individual patients. The results also show that there is information about decision support but no easy way to apply them, better than the ones used today. Clinicians state that there is a need to be able to evaluate the clinical work with the help of monitoring parameters. The results provide knowledge about which areas the clinicians needs are not supported enough by the exciting tools.</p><p>To conclude results show that depending on what profession and experience the clinicians have the demands on monitoring support differs. Monitoring at the ICU is performed while observing individual patients, parameters from medical devices, results from medical tests and physical examinations. Information from all these sources is considered by the clinicians and is desired to be supported accordingly before clinicians commit to action resulting in certain treatment, diagnosis and/or care.</p>
----------------------------------------------------------------------
In diva2:1454837 abstract is: <p>Recombinant silk proteins show promising use in applications such as tissue engineering andas materials for medical purposes. One of these proteins, 4RepCT, has earlier beenfunctionalized with various peptide domains and proteins to introduce desired functions.Genetic fusion has mainly been used, but also Sortase A coupling, allowing production indifferent expression hosts of the proteins to be coupled. Previous studies using the enzymeSortase A suggest that five glycines at the N-terminal of the silk protein improve theefficiency of the coupling. The aim of the project was to produce such a silk protein and toevaluate the Sortase A coupling with the IgG-binding domain Z and two single-chain variablefragments, in solution and to silk coatings. First, production of a silk protein with a solubilityand purification tag at the C-terminal was investigated but provided insufficient amounts dueto difficulties in protein expression. Instead a new construct, similar to a standard silk proteinwith the tag on the N-terminal side of the protein, was produced to obtain G5-FN-4RepCT,which was able to self-assemble into silk fibers and nanowires. The new silk protein wasfunctionalized by Sortase A coupling in solution and the efficiency was analyzed using SDSPAGE.G5-FN-4RepCT was proven to achieve an increased product amount and formationrate, and lower Sortase A concentrations could also be used compared to the standard silk, GFN-4RepCT. The sortase coupling to silk coatings, followed by functional binding analysis,were evaluated using a biosensor. The results indicated coupling to the coatings, however,also unspecific binding of Sortase A and further analysis is required. In conclusion, thesortase coupling in solution was improved using the new G5-silk protein, andfunctionalization to other silk materials, e.g. nanowires, show potential for various medicalapplications, such as cancer immunotherapy.</p>

w='SDSPAGE' val={'c': 'SDS-PAGE', 's': 'diva2:1454837', 'n': 'correct in original'}

corrected abstract:
<p>Recombinant silk proteins show promising use in applications such as tissue engineering and as materials for medical purposes. One of these proteins, 4RepCT, has earlier been functionalized with various peptide domains and proteins to introduce desired functions. Genetic fusion has mainly been used, but also Sortase A coupling, allowing production in different expression hosts of the proteins to be coupled. Previous studies using the enzyme Sortase A suggest that five glycines at the N-terminal of the silk protein improve the efficiency of the coupling. The aim of the project was to produce such a silk protein and to evaluate the Sortase A coupling with the IgG-binding domain Z and two single-chain variable fragments, in solution and to silk coatings. First, production of a silk protein with a solubility and purification tag at the C-terminal was investigated but provided insufficient amounts due to difficulties in protein expression. Instead a new construct, similar to a standard silk protein with the tag on the N-terminal side of the protein, was produced to obtain G<sub>5</sub>-FN-4RepCT, which was able to self-assemble into silk fibers and nanowires. The new silk protein was functionalized by Sortase A coupling in solution and the efficiency was analyzed using SDS-PAGE. G<sub>5</sub>-FN-4RepCT was proven to achieve an increased product amount and formation rate, and lower Sortase A concentrations could also be used compared to the standard silk, GFN-4RepCT. The sortase coupling to silk coatings, followed by functional binding analysis, were evaluated using a biosensor. The results indicated coupling to the coatings, however, also unspecific binding of Sortase A and further analysis is required. In conclusion, the sortase coupling in solution was improved using the new G<sub>5</sub>-silk protein, and functionalization to other silk materials, e.g. nanowires, show potential for various medical applications, such as cancer immunotherapy.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-24
======================================================================
title: "Production and BiochemicalCharacterisation of Glycoside Hydrolases from Chitinophaga pinensis"
==> "Production and Biochemical Characterisation of Glycoside Hydrolases from <em>Chitinophaga pinensis</em>"

In diva2:1454416 abstract is: <p>The increased use of chemical pesticides worldwide poses a problem where they can leak out of thesoil and cause damage to plants, animals and even humans. Because of this, interest in biologicalpesticides as an alternative to synthetic chemicals is increasing. One potential candidate for bacterialbiocontrol of fungal phytopathogens is the soil bacterium Chitinophaga pinensis which has beenshown to grow on fungal fruiting bodies and produce anti-fungal peptides that disrupt themembrane of the fungi. It is also predicted to produce a large number of enzymes that can attack thefungal cell wall. From genome sequencing, two proteins of interest have been found: a GH16glycoside hydrolase and one consisting of a GH64 glycoside hydrolase domain and a CBM6carbohydrate binding domain. This study shows that all proteins and protein domains can beeffectively and reliably expressed and purified from E. coli. The GH16 protein shows potential activityon β-1,3-galactan, as predicted. The two-domain protein shows activity on both β-1,3-glucan and β-1,6-glucan, compared to the single GH64 domain alone, which only shows the expected activity on β-1,3-glucan. This study has shown that the CBM􀏲 binds to β-1,6-glucan, something not observedpreviously and essential for GH64 to have any activity β-1,6-glucan. I have also shown that CBM6 isincapable of binding to β-1,6-glucan by itself and only binds when part of the full length protein.These data give us a better understanding of how C. pinensis is able to degrade polysaccharides fromnatural biomass, gives insight into the biology of this soil bacterium, and helps with the developmentof a biological pesticide.</p>


corrected abstract:
<p>The increased use of chemical pesticides worldwide poses a problem where they can leak out of the soil and cause damage to plants, animals and even humans. Because of this, interest in biological pesticides as an alternative to synthetic chemicals is increasing. One potential candidate for bacterial biocontrol of fungal phytopathogens is the soil bacterium <em>Chitinophaga pinensis</em> which has been shown to grow on fungal fruiting bodies and produce anti-fungal peptides that disrupt the membrane of the fungi. It is also predicted to produce a large number of enzymes that can attack the fungal cell wall. From genome sequencing, two proteins of interest have been found: a GH16 glycoside hydrolase and one consisting of a GH64 glycoside hydrolase domain and a CBM6 carbohydrate binding domain. This study shows that all proteins and protein domains can be effectively and reliably expressed and purified from <em>E. coli</em>. The GH16 protein shows potential activity on β-1,3-galactan, as predicted. The two-domain protein shows activity on both β-1,3-glucan and β-1,6-glucan, compared to the single GH64 domain alone, which only shows the expected activity on β-1,3-glucan. This study has shown that the CBM6 binds to β-1,6-glucan, something not observed previously and essential for GH64 to have any activity β-1,6-glucan. I have also shown that CBM6 is incapable of binding to β-1,6-glucan by itself and only binds when part of the full length protein. These data give us a better understanding of how <em>C. pinensis</em> is able to degrade polysaccharides from natural biomass, gives insight into the biology of this soil bacterium, and helps with the development of a biological pesticide.</p>
----------------------------------------------------------------------
In diva2:1680873 abstract is: <p>Perioperative hypotension (PH), commonly a side effect of anesthesia,is one of the main mortality causes during the 30 posterior days of asurgical procedure. Novel research lines propose combining machinelearning algorithms with the Arterial Blood Pressure (ABP) waveform tonotify healthcare professionals about the onset of a hypotensive event withtime advance and prevent its occurrence. Nevertheless, ABP waveformsare heterogeneous among patients, consequently, a general model maypresent different predictive capabilities per individual. This project aimsat improving the performance of an artificial neural network (ANN) topredict hypotension events with time advance by applying personalizedmachine learning techniques, like data grouping and domain adaptation. Wehypothesize its implementation will allow us to cluster patients with similardemographic and ABP discriminative characteristics and tailor the modelto each specific group, resulting in a worst overall but better individualperformance. Results present a slight but not clinical significant improvementwhen comparing AUROC values between the group-specific and the generalmodel. This suggests even though personalization could be a good approach todealing with patient heterogeneity, the clustering algorithm presented in thisthesis is not sufficient to make the ANN clinically feasible.</p>


corrected abstract:
<p>Perioperative hypotension (PH), commonly a side effect of anesthesia, is one of the main mortality causes during the 30 posterior days of a surgical procedure. Novel research lines propose combining machine learning algorithms with the Arterial Blood Pressure (ABP) waveform to notify healthcare professionals about the onset of a hypotensive event with time advance and prevent its occurrence. Nevertheless, ABP waveforms are heterogeneous among patients, consequently, a general model may present different predictive capabilities per individual. This project aims at improving the performance of an artificial neural network (ANN) to predict hypotension events with time advance by applying personalized machine learning techniques, like data grouping and domain adaptation. We hypothesize its implementation will allow us to cluster patients with similar demographic and ABP discriminative characteristics and tailor the model to each specific group, resulting in a worst overall but better individual performance. Results present a slight but not clinical significant improvement when comparing AUROC values between the group-specific and the general model. This suggests even though personalization could be a good approach to dealing with patient heterogeneity, the clustering algorithm presented in this thesis is not sufficient to make the ANN clinically feasible.</p>
----------------------------------------------------------------------
In diva2:1038973 abstract is: <p>Genetically encoded, site-specific incorporation of unnatural amino acids (UAA)into proteins through selective recoding of an amber stop codon provides apowerful route for expressing synthetic proteins in living cells. Recoding of theamber stop codon is achieved by introducing an amber suppressortRNA/synthetase pair orthogonal to the endogenous tRNA complement intocells. Methanosarcina is a methane producing archaea with the unusualcapability of suppressing the stop codon (specifically the amber codon). Bysuppressing the amber codon Methanosarcina facilitate the incorporation of thenon-canonical amino acid pyrrolysine (pyl). The suppressing mechanismoriginates from a evolutionary unique Pyrrolysyl-tRNA synthetase (PylRS) and itsmatching tRNApyl. The PylRS has been further evolved and modified to allowincorporation of a wide range of UAAs. Amber suppression is today used tocontrol and study protein function in living cells. By making a series of wellcontrolledexperiments with HEK293T cells we aimed to develop this techniqueinto a robust and general tool for mammalian cell biology. Specifically we weretesting the incorporation of the unnatural amino acid bicyclononyne (BCN) by aset of known PylRS mutants. Our results suggest the mutant aaRS PylRS “AF” isthe most robust and efficient synthetase for BCN. We have improved ambersuppression by determining which factors leads to a more efficient method andsimultaneously decreasing the cost of the method.</p>

w='tRNApyl' val={'c': 'tRNA<sup>pyl</sub>', 's': 'diva2:1038973'}

corrected abstract:
<p>Genetically encoded, site-specific incorporation of unnatural amino acids (UAA) into proteins through selective recoding of an amber stop codon provides a powerful route for expressing synthetic proteins in living cells. Recoding of the amber stop codon is achieved by introducing an amber suppressor tRNA/synthetase pair orthogonal to the endogenous tRNA complement into cells. <em>Methanosarcina</em> is a methane producing archaea with the unusual capability of suppressing the stop codon (specifically the amber codon). By suppressing the amber codon <em>Methanosarcina</em> facilitate the incorporation of the non-canonical amino acid pyrrolysine (pyl). The suppressing mechanism originates from a evolutionary unique Pyrrolysyl-tRNA synthetase (PylRS) and its matching tRNA<sup>pyl</sub>. The PylRS has been further evolved and modified to allow incorporation of a wide range of UAAs. Amber suppression is today used to control and study protein function in living cells. By making a series of well-controlled experiments with HEK293T cells we aimed to develop this technique into a robust and general tool for mammalian cell biology. Specifically we were testing the incorporation of the unnatural amino acid bicyclononyne (BCN) by a set of known PylRS mutants. Our results suggest the mutant aaRS PylRS “AF” is the most robust and efficient synthetase for BCN. We have improved amber suppression by determining which factors leads to a more efficient method and simultaneously decreasing the cost of the method.</p>
----------------------------------------------------------------------
In diva2:1665663 abstract is: <p>Convolutional neural networks (CNN) have come a long way and can be trained toclassify many of the objects around us. Despite this, researchers do not fullyunderstand how CNN models learn features (edges, shapes, contours, etc.) fromdata. For this reason, it is reasonable to investigate if a CNN model can learn toclassify objects under extreme conditions. An example of such an extreme conditioncould be a car that drives towards the camera at night, and therefore does not haveany distinct features because the light from the headlights covers large parts of thecar.The aim of this thesis is to investigate how the performance of a CNN model isaffected, when trained on objects under extreme conditions. A YOLOv4 model willbe trained on three different extreme cases: light polluted vehicles, nighttimeobjects and snow-covered vehicles. A validation will then be conducted on a testdataset to see if the performance decreases or improves, compared to when themodel trained is on normal conditions. Generally, the training was stable for allextreme cases and the results show an improved or similar performance incomparison to the normal cases. This indicates that models can be trained with allextreme cases. Snow-covered vehicles with mosaic data augmentation and the IOUthreshold 0,25 had the best overall performance compared to the normal cases, witha difference of +14,95% in AP for cars, −0,73% in AP for persons, +8,08% in AP fortrucks, 0 in precision and +9% in recall. </p>


corrected abstract:
<p>Convolutional neural networks (CNN) have come a long way and can be trained to classify many of the objects around us. Despite this, researchers do not fully understand how CNN models learn features (edges, shapes, contours, etc.) from data. For this reason, it is reasonable to investigate if a CNN model can learn to classify objects under extreme conditions. An example of such an extreme condition could be a car that drives towards the camera at night, and therefore does not have any distinct features because the light from the headlights covers large parts of the car.</p><p>The aim of this thesis is to investigate how the performance of a CNN model is affected, when trained on objects under extreme conditions. A YOLOv4 model will be trained on three different extreme cases: light polluted vehicles, nighttime objects and snow-covered vehicles. A validation will then be conducted on a test dataset to see if the performance decreases or improves, compared to when the model trained is on normal conditions. Generally, the training was stable for all extreme cases and the results show an improved or similar performance in comparison to the normal cases. This indicates that models can be trained with all extreme cases. Snow-covered vehicles with mosaic data augmentation and the IOU threshold 0,25 had the best overall performance compared to the normal cases, with a difference of +14,95% in AP for cars, −0,73% in AP for persons, +8,08% in AP for trucks, 0 in precision and +9% in recall.</p>
----------------------------------------------------------------------
title: "Improving Type 1 DiabetesPatients’ Quality of LifeThrough Data Collection"
==> "Improving Type 1 Diabetes Patients’ Quality of Life Through Data Collection"


In diva2:1529839 abstract is: <p>Type 1 diabetes (T1D) is a complex chronic disease without treatment. When anindividual is diagnosed with T1D they are taught how to monitor blood glucoselevel as well as external insulin administration. While this management strategyhelps prolong the individual’s life, there are other lifestyle factors not consideredthat negatively impact the patients’ life.</p><p>This thesis aims to investigate the types of data that can be gathered to benefit T1D patients and healthcare specialists by improving life quality.</p><p>To do so, the work employs a literature review and its qualitative analysis, aninterviewing process and its qualitative analysis as well as overall findings analysiswhere data is interpreted in order to identify areas of interest, common topics andtrends. 43 literature publications, 3 healthcare professionals and 3 T1D patientsparticipated in this study.</p><p>Results show initial education is limited leaving patients to initiate their ownresearch which could be a cause for stress. Technological integration does not seemchallenging provided the right training of more complex solutions. Education asa means to reduce stress seems effective both for patients but also for their socialnetworks. Finally, there are currently useful data markers not being used that couldprovide a wider range of information to healthcare specialists aiding in better patientcare and improved T1D patients’ Quality of Life (QOL).</p><p>To conclude, T1D is a complex chronic disease that requires both clinical andnon-clinical interventions. It is not sufficient to only address its clinical implicationsbut is important to investigate factors that impact the lifestyle and quality of life. Byextracting proper data markers, collecting and analyzing them, it is believed thattechnology can assist healthcare and ultimately improve T1D patient’s quality oflife.</p>

corrected abstract:
<p>Type 1 diabetes (T1D) is a complex chronic disease without treatment. When an individual is diagnosed with T1D they are taught how to monitor blood glucose level as well as external insulin administration. While this management strategy helps prolong the individual’s life, there are other lifestyle factors not considered that negatively impact the patients’ life.</p><p>This thesis aims to investigate the types of data that can be gathered to benefit T1D patients and healthcare specialists by improving life quality.</p><p>To do so, the work employs a literature review and its qualitative analysis, an interviewing process and its qualitative analysis as well as overall findings analysis where data is interpreted in order to identify areas of interest, common topics and trends. 43 literature publications, 3 healthcare professionals and 3 T1D patients participated in this study.</p><p>Results show initial education is limited leaving patients to initiate their own research which could be a cause for stress. Technological integration does not seem challenging provided the right training of more complex solutions. Education as a means to reduce stress seems effective both for patients but also for their social networks. Finally, there are currently useful data markers not being used that could provide a wider range of information to healthcare specialists aiding in better patient care and improved T1D patients’ Quality of Life (QOL).</p><p>To conclude, T1D is a complex chronic disease that requires both clinical and non-clinical interventions. It is not sufficient to only address its clinical implications but is important to investigate factors that impact the lifestyle and quality of life. By extracting proper data markers, collecting and analyzing them, it is believed that technology can assist healthcare and ultimately improve T1D patient’s quality of life.</p>
----------------------------------------------------------------------
title: "Sweat Lactate Sensor Integrated with Microfluidicand Iontophoresis System for Analysing Sweat without Physical Activity"
==> "Sweat Lactate Sensor Integrated with Microfluidic and Iontophoresis System for Analysing Sweat without Physical Activity"


In diva2:1692807 abstract is: <p>Background: Understanding lactate levels can provide important information aboutour body’s condition. For athletes, this can improve their training and prevent earlyfatigue. In healthcare, monitoring lactate can provide valuable information and potentially prevent life-threatening episodes. Lactate can be measured non-invasively byanalyzing sweat. This is advantageous over the typical blood sampling since it is saferand pain-free. Sweat can be stimulated by using a method called iontophoresis. Itapplies a small current between two electrodes placed on the skin’s surface, deliveringsubstances to the inner layer of the skin.</p><p>Objectives: The aim of this study was to design a device that implements iontophoresis to activate sweat production and uses a microfluidic system to collect thesweat and deliver it to a lactate sensor and provide a signal.</p><p>Methodology: A device was designed in AutoCAD and 3D printed. It was improvedby trial and error. A sweat collecting test was performed to validate the iontophoresissystem. The efficiency of the microfluidic system was tested by recording the time ittakes to collect enough sweat to get a lactate signal. Finally, calibration tests wereperformed to validate the lactate signal in the form of batch-mode and flow-mode.</p><p>Results: The sweat collection test produced 27 µL of sweat in 15 minutes and 49 µLin 30 minutes. The microfluidic system delivered sweat to the sensor and activated itin less than 3 minutes. The linearity of the batch-mode calibration, R2-value, was0.9994, and for the flow-mode it was 0.8908.Conclusions: The iontophoresis system stimulated sweat production, which themicrofluidic system delivered to the lactate sensor successfully. The lactate sensorwas implemented into the device, and a signal was detected. However, it could not becalibrated efficiently enough to display the electric signal as a lactate concentration.</p>

corrected abstract:
<p><strong>Background</strong>: Understanding lactate levels can provide important information about our body’s condition. For athletes, this can improve their training and prevent early fatigue. In healthcare, monitoring lactate can provide valuable information and potentially prevent life-threatening episodes. Lactate can be measured non-invasively by analyzing sweat. This is advantageous over the typical blood sampling since it is safer and pain-free. Sweat can be stimulated by using a method called iontophoresis. It applies a small current between two electrodes placed on the skin’s surface, delivering substances to the inner layer of the skin.</p><p><strong>Objectives</strong>: The aim of this study was to design a device that implements iontophoresis to activate sweat production and uses a microfluidic system to collect the sweat and deliver it to a lactate sensor and provide a signal.</p><p><strong>Methodology</strong>: A device was designed in AutoCAD and 3D printed. It was improved by trial and error. A sweat collecting test was performed to validate the iontophoresis system. The efficiency of the microfluidic system was tested by recording the time it takes to collect enough sweat to get a lactate signal. Finally, calibration tests were performed to validate the lactate signal in the form of batch-mode and flow-mode.</p><p><strong>Results</strong>: The sweat collection test produced 27 µL of sweat in 15 minutes and 49 µL in 30 minutes. The microfluidic system delivered sweat to the sensor and activated it in less than 3 minutes. The linearity of the batch-mode calibration, R<sup>2</sup>-value, was 0.9994, and for the flow-mode it was 0.8908.</p><p><strong>Conclusions</strong>: The iontophoresis system stimulated sweat production, which the microfluidic system delivered to the lactate sensor successfully. The lactate sensor was implemented into the device, and a signal was detected. However, it could not be calibrated efficiently enough to display the electric signal as a lactate concentration.</p>
----------------------------------------------------------------------
In diva2:734297 abstract is: <p>The Arterial Spin Labelling (ASL) method is a Magnetic Resonance technique used toquantify the cerebral perfusion. It has the big advantage to be non-invasive so doesn’tneed the injection of any contrast agent. But due to a relatively low Signal-to-NoiseRatio (SNR) of the signal acquired (only approximately 1% of the image intensity), ithas been hampered to be widely used in a clinical setting so far.The primary objective of this project is to make the method more robust by improvingthe quality of the images, the SNR, and by reducing the acquisition time. DifferentASL protocols with different sets of parameters have been investigated. The modificationsperformed on the protocol have been investigated by analyzing images acquired onhealthy volunteers. An optimized protocol leading to a good trade-off between the differentaspects of the method, has been suggested. It is characterized by a 3:43:44:0mm3with a two-segment acquisition.A more advanced ASL method implies the acquisition of images at different inversiontimes (TI), which is called the mutli-TI method. The influence of the range of TI used inthe method has been explored. An optimized TI range (from 410ms to 3860ms, sampledevery 150ms) has been suggested to make the ASL method as performant as possible.A numerical model and a fitting algorithm have been used to extract the informationon the perfusion from the images acquired. Different models have been investigated aswell as their influence on the reliability of the results.Finally, a criterion has been implemented to evaluate the reliability of the results sothat the clinician or the user of the method can figure out how much he can count onthe results provided by the method.</p>


w='mutli-TI' val={'c': 'multi-TI', 's': 'diva2:734297', 'n': 'error in original'}

corrected abstract:
<p>The Arterial Spin Labelling (ASL) method is a Magnetic Resonance technique used to quantify the cerebral perfusion. It has the big advantage to be non-invasive so doesn’t need the injection of any contrast agent. But due to a relatively low Signal-to-Noise Ratio (SNR) of the signal acquired (only approximately 1% of the image intensity), it has been hampered to be widely used in a clinical setting so far.</p><p>The primary objective of this project is to make the method more robust by improving the quality of the images, the SNR, and by reducing the acquisition time. Different ASL protocols with different sets of parameters have been investigated. The modifications performed on the protocol have been investigated by analyzing images acquired on healthy volunteers. An optimized protocol leading to a good trade-off between the different aspects of the method, has been suggested. It is characterized by a 3.4×3.4×4.0mm<sup>3</sup> with a two-segment acquisition.</p><p>A more advanced ASL method implies the acquisition of images at different inversion times (TI), which is called the mutli-TI method. The influence of the range of TI used in the method has been explored. An optimized TI range (from 410ms to 3860ms, sampled every 150ms) has been suggested to make the ASL method as performant as possible.</p><p>A numerical model and a fitting algorithm have been used to extract the information on the perfusion from the images acquired. Different models have been investigated as well as their influence on the reliability of the results.</p><p>Finally, a criterion has been implemented to evaluate the reliability of the results so that the clinician or the user of the method can figure out how much he can count on the results provided by the method.</p>
----------------------------------------------------------------------
In diva2:1884448 abstract is: <p>This thesis addresses the pressing issue of frailty management in an aging population,aiming to develop a simulation model to evaluate the effectiveness of preventioninterventions among elderly individuals. Through a systematic approach integratingliterature reviews, interviews, and computational modeling, five separate simulationmodels are created to analyze intervention effects on frailty components. Theresults highlight significant differences in intervention impacts, with nutritionalinterventions notably delaying frailty onset in slowness and activity components byup to 4.5 and 26 years, respectively. Physical interventions also play a crucial rolein delaying frailty, particularly in slowness and activity. However, combinationinterventions do not simply sum benefits, suggesting potential interactions amonginterventions. Additionally, cognitive interventions exhibit notable impacts onexhaustion and weakness components, emphasizing their multifaceted nature inpromoting overall well-being in older adults. The simulation models underscorethe importance of considering intervention compatibility and synergistic effects.This research contributes evidence-based insights into effective frailty preventionstrategies, bridging theoretical understanding with practical application for healthcareprofessionals, policymakers, and stakeholders involved in promoting healthyaging.</p>


corrected abstract:
<p>This thesis addresses the pressing issue of frailty management in an aging population, aiming to develop a simulation model to evaluate the effectiveness of prevention interventions among elderly individuals. Through a systematic approach integrating literature reviews, interviews, and computational modeling, five separate simulation models are created to analyze intervention effects on frailty components. The results highlight significant differences in intervention impacts, with nutritional interventions notably delaying frailty onset in slowness and activity components by up to 4.5 and 26 years, respectively. Physical interventions also play a crucial role in delaying frailty, particularly in slowness and activity. However, combination interventions do not simply sum benefits, suggesting potential interactions among interventions. Additionally, cognitive interventions exhibit notable impacts on exhaustion and weakness components, emphasizing their multifaceted nature in promoting overall well-being in older adults. The simulation models underscore the importance of considering intervention compatibility and synergistic effects. This research contributes evidence-based insights into effective frailty prevention strategies, bridging theoretical understanding with practical application for healthcare professionals, policymakers, and stakeholders involved in promoting healthy aging.</p>
----------------------------------------------------------------------
In diva2:1763199 abstract is: <p>A cost-effective small robot car that is remote-controlled via Wi-Fi hasbeen designed and tested for inspecting and identifying potentialhazards at accident sites. The robot car is intended for reconnaissanceand inspection purposes and can contribute to formulating anadequate action plan. Wheel suspension and wheels were constructedusing a 3D printer. The finished robot car is equipped with theRaspberry Pi microcontroller, which has severalfeatures that make ituseful in various scenarios. There is a camera that allows remoteinspection of the car's surroundings. In addition to the camera, thereare three sensors connected to the Raspberry Pi unit: a gas sensor todetect dangerous gases, an ultrasonic sensor to measure the distanceto the nearest object, and a temperature sensor to measure theambient temperature. The robot car uses a motor control module tocontrol its movement and two servo motors to enable the rotation ofthe camera in the vertical and horizontal directions. The robot car ispowered by a battery, and two voltage converters are used to regulatethe voltage to the motor control module and the Raspberry Pi unit. Byintegrating these components into a single unit and programming theRaspberry Pi unit to control them, the robot car can effectively assistin investigating and managing potential hazards.</p>


Note: "severalfeatures" is set as one word in the thesis.
corrected abstract:
<p>A cost-effective small robot car that is remote-controlled via Wi-Fi has been designed and tested for inspecting and identifying potential hazards at accident sites. The robot car is intended for reconnaissance and inspection purposes and can contribute to formulating an adequate action plan. Wheel suspension and wheels were constructed using a 3D printer. The finished robot car is equipped with the Raspberry Pi microcontroller, which has severalfeatures that make it useful in various scenarios. There is a camera that allows remote inspection of the car's surroundings. In addition to the camera, there are three sensors connected to the Raspberry Pi unit: a gas sensor to detect dangerous gases, an ultrasonic sensor to measure the distance to the nearest object, and a temperature sensor to measure the ambient temperature. The robot car uses a motor control module to control its movement and two servo motors to enable the rotation of the camera in the vertical and horizontal directions. The robot car is powered by a battery, and two voltage converters are used to regulate the voltage to the motor control module and the Raspberry Pi unit. By integrating these components into a single unit and programming the Raspberry Pi unit to control them, the robot car can effectively assist in investigating and managing potential hazards.</p>
----------------------------------------------------------------------
In diva2:1441893 abstract is: <p>Three-dimensional (3D) printing has an important role for fabrication of degradable scaffoldsfor soft tissue regeneration. Among the 3D printing techniques, photopolymerization-based 3Dprinting is one of fastest growing, offering environmental benefits and high precision of 3Dobjects. In this approach, photocurable macromonomers/monomers are cross-linked layer bylayer in the presence of photoinitiators under visible or UV light to fabricate 3D designedobjects. However, a limited biomedical material selection has prevented it from spreading overclinical application. Furthermore, poly(ε-caprolactone), a common degradable polymer usedfor 3D printing, shows not satisfactory physical properties for soft tissue regeneration. Thedearth of materials with proper properties raises the need for novel degradable materials,which should be not only compatible for photopolymerization-based 3D printing but alsosuitable for soft and gel-like scaffold fabrication.</p><p>Here, the aim was to design photocurable macromonomers consisting of oligo(ε-caprolactoneran-p-dioxanone), oCLDX, with acrylate chain-end groups. A metal-free synthetic strategy wasdeveloped for the bulk ring-opening of ε-caprolactone (CL) and p-dioxanone (DX) at roomtemperature using diphenyl phosphate (DPP) as organocatalyst and multifunctional initiators.</p><p>The oligomers had low dispersity (&lt;1.2) and targeted molecular weight around 2000 g mol-1.The random sequence and the control over chain growth of oCLDXs were confirmed byreactivity ratios using 1D and 2D NMR analysis. Kinetics study of co-oligomerizationdemonstrated that within DPP-catalysed reaction, DX possessed higher reactivity than CL andthe ring-opening co-oligomerization followed an activated monomer mechanism (AMM). Thetopology of the co-oligomers could also be varied by using different alcohol initiators.</p><p>The co-oligomers possessed lower degree of crystallinity than homopolymers of DX or CL and,depending on the composition, they were liquid at room temperature. The lower melting pointand gel-like appearance make them good candidates for photopolymerization-based 3Dprinting. The suitability toward photopolymerization was proven for the ethylene glycol-initiatedco-oligomer containing 30 mol% of DX. The cross-linked gels were soft but brittle and showedgood water uptake capacity.</p>

w='glycol-initiatedco-oligomer' val={'c': 'glycol-initiated co-oligomer', 's': 'diva2:1441893', 'n': 'correct in original'}

corrected abstract:
<p>Three-dimensional (3D) printing has an important role for fabrication of degradable scaffolds for soft tissue regeneration. Among the 3D printing techniques, photopolymerization-based 3D printing is one of fastest growing, offering environmental benefits and high precision of 3D objects. In this approach, photocurable macromonomers/monomers are cross-linked layer by layer in the presence of photoinitiators under visible or UV light to fabricate 3D designed objects. However, a limited biomedical material selection has prevented it from spreading over clinical application. Furthermore, poly(ε-caprolactone), a common degradable polymer used for 3D printing, shows not satisfactory physical properties for soft tissue regeneration. The dearth of materials with proper properties raises the need for novel degradable materials, which should be not only compatible for photopolymerization-based 3D printing but also suitable for soft and gel-like scaffold fabrication.</p><p>Here, the aim was to design photocurable macromonomers consisting of oligo(ε-caprolactone-ran-p-dioxanone), oCLDX, with acrylate chain-end groups. A metal-free synthetic strategy was developed for the bulk ring-opening of ε-caprolactone (CL) and p-dioxanone (DX) at room temperature using diphenyl phosphate (DPP) as organocatalyst and multifunctional initiators.</p><p>The oligomers had low dispersity (&lt;1.2) and targeted molecular weight around 2000 g mol<sup>-1</sup>. The random sequence and the control over chain growth of oCLDXs were confirmed by reactivity ratios using 1D and 2D NMR analysis. Kinetics study of co-oligomerization demonstrated that within DPP-catalysed reaction, DX possessed higher reactivity than CL and the ring-opening co-oligomerization followed an activated monomer mechanism (AMM). The topology of the co-oligomers could also be varied by using different alcohol initiators.</p><p>The co-oligomers possessed lower degree of crystallinity than homopolymers of DX or CL and, depending on the composition, they were liquid at room temperature. The lower melting point and gel-like appearance make them good candidates for photopolymerization-based 3D printing. The suitability toward photopolymerization was proven for the ethylene glycol-initiated co-oligomer containing 30 mol% of DX. The cross-linked gels were soft but brittle and showed good water uptake capacity.</p>
----------------------------------------------------------------------
In diva2:1038989 abstract is: <p>Interstitial Fluid is a complex sample, highly abundant in the human body that can give information regardingtissue secretion, intracellular signaling and tissue health status. The composition of the interstitial fluid can giveinformation regarding the processes occurring in muscles and alterations due to pathological changes occurringduring disease progression. Currently this sample has not yet been characterized within rare diseases like musculardystrophies. Facioscapulohumeral Muscular Dytrophy is an inherited progressive myopathy, characterized by thedegeneration and progressive muscular fiber necrosis of muscles from the face, upper arms and lower limbs. It canbe diagnosed; but in an advanced stage where weakness in the muscles have already occur. Meanwhile there is nocurrent understanding of the mechanisms happening in the muscle. In this project an immunoassay protocol wasdeveloped using suspension bead array technology to create an optimal method to analyze the protein content ofthese samples. The technological platform allows antibody-based capturing and detection of protein targets frombiotinylated biological samples. By modifying an existing protocol for analysis of serum and plasma samplesabundance of 63 protein targets was measured in muscle interstitial fluid from healthy individuals and patientsaffected by facioscapulohumeral dystrophy (FSHD), The optimized steps were the sample pre-treatment, the assaybuffer dilution ratio and the incubation time for capturing the protein targets. The findings of this project indicatethat using 1 μl of muscle interstitial fluid sample with minimized dilution factor and 60-fold molar excess biotinrelative to sample protein concentration enables detection of Interstitial fluid protein components. The proteinsdetected are ret finger protein-like 4B (RFPL4B) and albumin in from affected muscle and histone cluster(HIST1H3A) and albumin in non affected muscle.</p>

w='Dytrophy' val={'c': 'Dystrophy', 's': 'diva2:1038989', 'n': 'error in original'}

corrected abstract:
<p>Interstitial Fluid is a complex sample, highly abundant in the human body that can give information regarding tissue secretion, intracellular signaling and tissue health status. The composition of the interstitial fluid can give information regarding the processes occurring in muscles and alterations due to pathological changes occurring during disease progression. Currently this sample has not yet been characterized within rare diseases like muscular dystrophies. Facioscapulohumeral Muscular Dytrophy is an inherited progressive myopathy, characterized by the degeneration and progressive muscular fiber necrosis of muscles from the face, upper arms and lower limbs. It can be diagnosed; but in an advanced stage where weakness in the muscles have already occur. Meanwhile there is no current understanding of the mechanisms happening in the muscle. In this project an immunoassay protocol was developed using suspension bead array technology to create an optimal method to analyze the protein content of these samples. The technological platform allows antibody-based capturing and detection of protein targets from biotinylated biological samples. By modifying an existing protocol for analysis of serum and plasma samples abundance of 63 protein targets was measured in muscle interstitial fluid from healthy individuals and patients affected by facioscapulohumeral dystrophy (FSHD), The optimized steps were the sample pre-treatment, the assay buffer dilution ratio and the incubation time for capturing the protein targets. The findings of this project indicate that using 1 μl of muscle interstitial fluid sample with minimized dilution factor and 60-fold molar excess biotin relative to sample protein concentration enables detection of Interstitial fluid protein components. The proteins detected are ret finger protein-like 4B (RFPL4B) and albumin in from affected muscle and histone cluster (HIST1H3A) and albumin in non affected muscle.</p>
----------------------------------------------------------------------
In diva2:1217907 abstract is: <p>Modern medication allows patients to be treated more efficiently if taken properly.When patients themselves are responsible for their medication, it can be wronglydosed or forgotten, which changes the conditions for its effects. Forgetfulness combinedwith impaired senses make it difficult for the patient to take care of his medication.The solution to this problem today is increased home care, where healthcare professionalshelp patients to take the right medication at the right time. A technical solutioncan be an embedded system that reminds the patient, saves missed dosages, and canprovide more information to doctors who can make more informed decisions for furthercare.The results show that a pill dispenser can be used to remind patients to take their medicationand send missed dosages with near field communication. As a microcontroller,a STM32 Nucleo was used, with a state machine as software solution. In further studies,a real-time operating system can be used. The system was programmed with Arduino'sdevelopment environment, but an alternative is IAR. The prototype of the projectcould achieve the specifications battery life with most modules, except to thesound module used. To achieve this result, two theoretical extremes were created inwhich the prototype was tested.The pill dispenser can lead to reduced waste of pills and a lower cost, as fewer homevisits are required. In addition, this results in increased independence for the patient.These factors cause the pill dispenser to contribute to increased sustainable development.</p>

corrected abstract:
<p>Modern medication allows patients to be treated more efficiently if taken properly. When patients themselves are responsible for their medication, it can be wrongly dosed or forgotten, which changes the conditions for its effects. Forgetfulness combined with impaired senses make it difficult for the patient to take care of his medication.</p><p>The solution to this problem today is increased home care, where healthcare professionals help patients to take the right medication at the right time. A technical solution can be an embedded system that reminds the patient, saves missed dosages, and can provide more information to doctors who can make more informed decisions for further care.</p><p>The results show that a pill dispenser can be used to remind patients to take their medication and send missed dosages with near field communication. As a microcontroller, a STM32 Nucleo was used, with a state machine as software solution. In further studies, a real-time operating system can be used. The system was programmed with Arduino's development environment, but an alternative is IAR. The prototype of the project could achieve the specifications battery life with most modules, except to the sound module used. To achieve this result, two theoretical extremes were created in which the prototype was tested.</p><p>The pill dispenser can lead to reduced waste of pills and a lower cost, as fewer home visits are required. In addition, this results in increased independence for the patient. These factors cause the pill dispenser to contribute to increased sustainable development.</p>
----------------------------------------------------------------------
In diva2:1451771 abstract is: <p>AbstractThere are a lot of old software in the world that has not been supported or kept up todate and would need to be updated to seal security vulnerabilities, as well as to updatefunctions in the program. In those cases where the source code has been lost ordeliberately deleted, would it be possible to use reverse engineering to retrieve thesource code?This study aims to show what java bytecode is and how it is used, as well as how oneis able to go from java bytecode back to source code in a process called Reverse Engineering.Furthermore, the study will show previous work in reverse engineering,in obfuscation and to explain further details about what Java Virtual machine,bytecode and obfuscation is and how they work. Three programs of various complexityare made into bytecode and then obfuscated. The difference between the originalcode and the obfuscated code are then analyzed.The results show that it is possible to reverse engineer obfuscated code but someparts. Obfuscation does protect the code, as all the variable names are changed andevery unused method are removed, as well as some methods changed to non-conventionalways to program.KeywordsReverse engineering, Java, JVM, bytecode, obfuscation, safety.</p>


corrected abstract:
<p>There are a lot of old software in the world that has not been supported or kept up to date and would need to be updated to seal security vulnerabilities, as well as to update functions in the program. In those cases where the source code has been lost or deliberately deleted, would it be possible to use reverse engineering to retrieve the source code?</p><p>This study aims to show what java bytecode is and how it is used, as well as how one is able to go from java bytecode back to source code in a process called Reverse Engineering. Furthermore, the study will show previous work in reverse engineering, in obfuscation and to explain further details about what Java Virtual machine, bytecode and obfuscation is and how they work. Three programs of various complexity are made into bytecode and then obfuscated. The difference between the original code and the obfuscated code are then analyzed.</p><p>The results show that it is possible to reverse engineer obfuscated code but some parts. Obfuscation does protect the code, as all the variable names are changed and every unused method are removed, as well as some methods changed to non-conventional ways to program.</p>
----------------------------------------------------------------------
In diva2:1454824 abstract is: <p>Transcription factors (TFs) are key regulatory proteins that regulate transcriptionthrough precise, but highly variable binding events to cis-regulatory elements.The complexity of their regulatory patterns makes it difficult to determinethe roles of different TFs, a task which the field is still struggling with.Experimental procedures for this purpose, such as knock out experiments, arehowever costly and time consuming, and with the ever-increasing availabilityof sequencing data, computational methods for inferring the activity of TFsfrom such data have become of great interest. Current methods are howeverlacking in several regards, which necessitates further exploration of alternatives.</p><p>A novel tool for estimating the activity of individual TFs over time fromlongitudinal mRNA expression data was in this project therefore put togetherand tested on data from Mus musculus liver and brain. The tool is based onprincipal component analysis, which is applied to data subsets containing theexpression data of genes likely regulated by a specific TF to acquire an estimationof its activity. Though initial tests on 17 selected TFs showed issues withunspecific trends in the estimations, further testing is required for a statementon the potential of the estimator.</p>

corrected abstract:
<p><em>Transcription factors</em> (TFs) are key regulatory proteins that regulate transcription through precise, but highly variable binding events to cis-regulatory elements. The complexity of their regulatory patterns makes it difficult to determine the roles of different TFs, a task which the field is still struggling with. Experimental procedures for this purpose, such as knock out experiments, are however costly and time consuming, and with the ever-increasing availability of sequencing data, computational methods for inferring the activity of TFs from such data have become of great interest. Current methods are however lacking in several regards, which necessitates further exploration of alternatives.</p><p>A novel tool for estimating the activity of individual TFs over time from longitudinal mRNA expression data was in this project therefore put together and tested on data from <em>Mus musculus</em> liver and brain. The tool is based on principal component analysis, which is applied to data subsets containing the expression data of genes likely regulated by a specific TF to acquire an estimation of its activity. Though initial tests on 17 selected TFs showed issues with unspecific trends in the estimations, further testing is required for a statement on the potential of the estimator.</p>
----------------------------------------------------------------------
In diva2:1454438 abstract is: <p>Hemicelluloses are synthesized by proteins encoded by genes from the cellulose synthasegene superfamily. One subgroup of this gene family is the cellulose synthase-like B, which islargely uncharacterized and unexplored. The common model organism Nicotianabenthamiana has one such gene in its genome, NbCslB, encoding a membrane protein. Theexpression of this gene has previously been studied in vivo, but in order to study the protein invitro a viable solubilization and purification protocol is required. This study evaluated the useof the detergent n-Dodecyl β-D-maltoside (DDM) for solubilization, followed by purificationusing immobilized metal ion affinity chromatography (IMAC), and thereafter reconstitutionof the protein into proteoliposomes. SDS-PAGE as well as Western blot analyses showed thatthe purification was successful and provided a pure sample of protein. Throughout theanalyses performed, an anti-FLAG antibody was discovered to bind well to the protein, andthereby be especially useful for analysis. An activity assay was performed on the purifiedprotein, to characterize its function and evaluate whether the protein had maintained itsactivity and conformation after the steps of purification and reconstitution. No activity couldbe detected in the enzymatic assay, which indicated that the purification protocol may havebeen too rough on the protein, that the reconstitution was not successful, or that the assayconditions were not optimal. These results can be used as a base for future research, where theprotocols for solubilization, purification, and reconstitution should be further refined in orderto obtain an end result where the purified protein is active. When an active and pure proteinsample is achieved, it will be possible to perform further attempts at characterizing thefunction of the protein using enzymatic activity assays. Additionally, the results showed thatthe choice of antibody can be crucial for proper analysis of this protein.</p>


corrected abstract:
<p>Hemicelluloses are synthesized by proteins encoded by genes from the cellulose synthase gene superfamily. One subgroup of this gene family is the cellulose synthase-like B, which is largely uncharacterized and unexplored. The common model organism <em>Nicotiana benthamiana</em> has one such gene in its genome, NbCslB, encoding a membrane protein. The expression of this gene has previously been studied <em>in vivo</em>, but in order to study the protein <em>in vitro</em> a viable solubilization and purification protocol is required. This study evaluated the use of the detergent n-Dodecyl β-D-maltoside (DDM) for solubilization, followed by purification using immobilized metal ion affinity chromatography (IMAC), and thereafter reconstitution of the protein into proteoliposomes. SDS-PAGE as well as Western blot analyses showed that the purification was successful and provided a pure sample of protein. Throughout the analyses performed, an anti-FLAG antibody was discovered to bind well to the protein, and thereby be especially useful for analysis. An activity assay was performed on the purified protein, to characterize its function and evaluate whether the protein had maintained its activity and conformation after the steps of purification and reconstitution. No activity could be detected in the enzymatic assay, which indicated that the purification protocol may have been too rough on the protein, that the reconstitution was not successful, or that the assay conditions were not optimal. These results can be used as a base for future research, where the protocols for solubilization, purification, and reconstitution should be further refined in order to obtain an end result where the purified protein is active. When an active and pure protein sample is achieved, it will be possible to perform further attempts at characterizing the function of the protein using enzymatic activity assays. Additionally, the results showed that the choice of antibody can be crucial for proper analysis of this protein.</p>
----------------------------------------------------------------------
In diva2:1732100 abstract is: <p>It is well-known that many people like to listen to music when they are running,something that is not always available for people with hearing aids [1].The experience of music is not often taken into account when people have lostparts of their hearing, but music is in many ways important for the well-being[2]. In this project has an application been developed, to make certain audioparameters accessible for adjustments according to the users individual hearing.The parameters chosen were latency, compression, balance and loudness. Theseparameters have been chosen with the argument that they have big impact onthe experience of the music. As an example there might be latency betweenthe hearing aids between the left and the right hearing. Very often people withhearing aids have different hearing at the left and the right ear, therefore thebalance was chosen as a parameter. Compression were chosen with respect tothe fact that hearing loss often leads to less capability to hear frequencies indifferent frequencies areas. Loudness was chosen due to that it is a subjectiveparameter, that will have different impact on different individuals. The parametershave been chosen with a hypothesis that they may affect the hearingexperience for different individuals.</p><p></p><p>The result show that the participants changed audio settings, each of theaudio parameters to different values and appreciated the function to changethese parameters. The participants said that the application developed in thisproject, improved their music experience and they said that they would appreciateto use an application like this, to get more value of listening to music. Theapplication was well designed and easy to use during a sport session.</p>


corrected abstract:
<p>It is well-known that many people like to listen to music when they are running, something that is not always available for people with hearing aids [1]. The experience of music is not often taken into account when people have lost parts of their hearing, but music is in many ways important for the well-being [2]. In this project has an application been developed, to make certain audio parameters accessible for adjustments according to the users individual hearing. The parameters chosen were latency, compression, balance and loudness. These parameters have been chosen with the argument that they have big impact on the experience of the music. As an example there might be latency between the hearing aids between the left and the right hearing. Very often people with hearing aids have different hearing at the left and the right ear, therefore the balance was chosen as a parameter. Compression were chosen with respect to the fact that hearing loss often leads to less capability to hear frequencies in different frequencies areas. Loudness was chosen due to that it is a subjective parameter, that will have different impact on different individuals. The parameters have been chosen with a hypothesis that they may affect the hearing experience for different individuals.</p><p>The result show that the participants changed audio settings, each of the audio parameters to different values and appreciated the function to change these parameters. The participants said that the application developed in this project, improved their music experience and they said that they would appreciate to use an application like this, to get more value of listening to music. The application was well designed and easy to use during a sport session.</p>
----------------------------------------------------------------------
In diva2:1308051 abstract is: <p>Genome organization is increasingly believed to influence and participate in gene regulation. Thestudy of the interactions that constitute this organization is increasingly performed using chromosomeconformation capture (3C) technologies. While good for the study of pairwise interactions betweenelements, higher order structures are inaccessible due to the use of proximal ligation to link elements.This thesis thus endeavored to develop a method to study these higher order structures in an effort tomove away from proximal ligation. Instead of pairwise linking interacting fragments through ligation,strategies for barcoding of chromatin complexes were tested. These complexes were extracted fromcrosslinked cells and captured using chromatin immunoprecipitation (ChIP) procedure. The fragmentswithin the complex were then fitted with adaptors and barcodes according to two approaches. The first(called Approach A) being adaptor ligation with subsequent ligation onto barcoded beads, the second(called Approach B) tagmentation with subsequent barcoding through emulsion PCR (emPCR).Several steps of optimization were performed and a total six libraries created and sequenced, twousing approach A and four using approach B. Analysis of these libraries demonstrated progress inseveral key areas such as barcode clusters containing multiple fragments and phasing. In the B datasetswas also found an enrichment form short range interactions, in accordance with 3C observations. TheB datasets outperformed A in most regards and was thus deemed the preferred path for future studies.The greatest challenge yet to overcome is to lower duplication rates which currently are at a minimumof 79%. To decrease rates several parameters for optimization have been identified for futuredevelopment.</p>


corrected abstract:
<p>Genome organization is increasingly believed to influence and participate in gene regulation. The study of the interactions that constitute this organization is increasingly performed using chromosome conformation capture (3C) technologies. While good for the study of pairwise interactions between elements, higher order structures are inaccessible due to the use of proximal ligation to link elements. This thesis thus endeavored to develop a method to study these higher order structures in an effort to move away from proximal ligation. Instead of pairwise linking interacting fragments through ligation, strategies for barcoding of chromatin complexes were tested. These complexes were extracted from crosslinked cells and captured using chromatin immunoprecipitation (ChIP) procedure. The fragments within the complex were then fitted with adaptors and barcodes according to two approaches. The first (called Approach A) being adaptor ligation with subsequent ligation onto barcoded beads, the second (called Approach B) tagmentation with subsequent barcoding through emulsion PCR (emPCR). Several steps of optimization were performed and a total six libraries created and sequenced, two using approach A and four using approach B. Analysis of these libraries demonstrated progress in several key areas such as barcode clusters containing multiple fragments and phasing. In the B datasets was also found an enrichment form short range interactions, in accordance with 3C observations. The B datasets outperformed A in most regards and was thus deemed the preferred path for future studies. The greatest challenge yet to overcome is to lower duplication rates which currently are at a minimum of 79%. To decrease rates several parameters for optimization have been identified for future development.</p>
----------------------------------------------------------------------
title: "How Covid-19 has affected thepsychosocial work environmentfor gig-workers in the fooddelivery sector: A qualitative interview study"
==> "How Covid-19 has affected the psychosocial work environmentfor gig-workers in the food delivery sector: A qualitative interview study"

In diva2:1593641 abstract is: <p>The topic of gig-work has been widely debated in the western world for its laborrights and work environment issues. Whereas companies operating within thegig-economy boast the freedom and flexibility of gig-work as perks for workers,unions highlight the precarious nature and poor working conditions of gig-workas exploitative of workers and push for unionizing gig-workers and getting themcovered by collective bargaining agreements. However, due to the novelty of thephenomenon, not much scholarly work has been done on the subject and its longterm implications for the labor markets of developed capitalist countries.In Sweden, conditions caused by the Covid-19 pandemic led to an influx ofworkers in the food-delivery sector due to increased demands for services andhigh unemployment as gig-work was regarded as a means to sustain one’slivelihood during a time of economic uncertainty. As these food-delivery couriershad to work outside the home, where the chance of contracting the virus was high,while also having jobs where they were not regarded as employees, the aim ofthis thesis project was to investigate how the Covid-19 pandemic has affectedfood-delivery couriers’ perceived exposure to psychosocial stressors and risksthrough a qualitative interview study. The findings were generated throughinductive reasoning from analyzing conducted six interviews and discussedthrough the lens of the Human, Technology and Organization (HTO)-model andthe Job-Demands-Control-Support (JDCS)-model.</p>

corrected abstract:
<p>The topic of gig-work has been widely debated in the western world for its labor rights and work environment issues. Whereas companies operating within the gig-economy boast the freedom and flexibility of gig-work as perks for workers, unions highlight the precarious nature and poor working conditions of gig-work as exploitative of workers and push for unionizing gig-workers and getting them covered by collective bargaining agreements. However, due to the novelty of the phenomenon, not much scholarly work has been done on the subject and its longterm implications for the labor markets of developed capitalist countries.</p><p>In Sweden, conditions caused by the Covid-19 pandemic led to an influx of workers in the food-delivery sector due to increased demands for services and high unemployment as gig-work was regarded as a means to sustain one’s livelihood during a time of economic uncertainty. As these food-delivery couriers had to work outside the home, where the chance of contracting the virus was high, while also having jobs where they were not regarded as employees, the aim of this thesis project was to investigate how the Covid-19 pandemic has affected food-delivery couriers’ perceived exposure to psychosocial stressors and risks through a qualitative interview study. The findings were generated through inductive reasoning from analyzing conducted six interviews and discussed through the lens of the Human, Technology and Organization (HTO)-model and the Job-Demands-Control-Support (JCDS)-model.</p>
----------------------------------------------------------------------
In diva2:1454829 abstract is: <p>Myelodysplastic syndrome (MDS) constitutes a group of hematological disorders that impairshematopoiesis and frequently predisposes to acute myeloid leukemia (AML). Mutations intwo homologous and evolutionary conserved tumor suppressors called SAMD9 and SAMD9Lhave recently been associated with increased susceptibility to MDS/AML, but also with awider spectrum of diseases, including life-threatening autoinflammatory syndromes as well asataxia. Representing a novel class of tumor suppressors, they have been implicated in thecontrol of viral infection and cell cycle regulation. However, the structure-functionrelationships and cellular activity of these proteins remain undefined. In order to elucidate themolecular mechanisms by which SAMD9 and SAMD9L restrict cell proliferation and howdisease-causing mutations can potentiate their antiproliferative activity, stable cell lines withinducible expression of SAMD9L wild-type, patient-derived or predicted damaging variantswere established and evaluated by Western blot as well as a quantitative cell cycle assay.From generated results, a genotype-phenotype relationship was established for two noveldisease-associated variants. Furthermore, a region important for controlling theantiproliferative activity of SAMD9L was defined. These results can help to explain clinicalcases with novel variants and provide deepened understanding of the cellular mechanismswhereby SAMD9L regulates cell cycle. With poor prognosis and limited treatments available,such molecular insight is of outmost importance since it can aid diagnostics and may lead tonovel therapies that can benefit patients.</p>


corrected abstract:
<p>Myelodysplastic syndrome (MDS) constitutes a group of hematological disorders that impairs hematopoiesis and frequently predisposes to acute myeloid leukemia (AML). Mutations in two homologous and evolutionary conserved tumor suppressors called SAMD9 and SAMD9L have recently been associated with increased susceptibility to MDS/AML, but also with a wider spectrum of diseases, including life-threatening autoinflammatory syndromes as well as ataxia. Representing a novel class of tumor suppressors, they have been implicated in the control of viral infection and cell cycle regulation. However, the structure-function relationships and cellular activity of these proteins remain undefined. In order to elucidate the molecular mechanisms by which SAMD9 and SAMD9L restrict cell proliferation and how disease-causing mutations can potentiate their antiproliferative activity, stable cell lines with inducible expression of SAMD9L wild-type, patient-derived or predicted damaging variants were established and evaluated by Western blot as well as a quantitative cell cycle assay. From generated results, a genotype-phenotype relationship was established for two novel disease-associated variants. Furthermore, a region important for controlling the antiproliferative activity of SAMD9L was defined. These results can help to explain clinical cases with novel variants and provide deepened understanding of the cellular mechanisms whereby SAMD9L regulates cell cycle. With poor prognosis and limited treatments available, such molecular insight is of outmost importance since it can aid diagnostics and may lead to novel therapies that can benefit patients.</p>
----------------------------------------------------------------------
In diva2:1697647 abstract is: <p>Interactive projected augmented reality is a subfield within projected augmented reality, where the interactivity is about projecting virtual data onto an entity that canpotentially be in movement. This projection can be accomplished by using a calibrated projector-depth camera system that detects entities using computer vision.This interactive system has a varying number of application areas; however, a criticalproblem emerges, which is the accuracy of these systems. The accuracy in this caseis how correctly the projection takes place on the specific entity, the accuracy istherefore an important aspect to validate if certain applications are possible to implement correctly. The solution for this problem is by implementing interactive projectedaugmented reality and perform prototype tests with the implemented system andthen further analyzing the accuracy with test data. The prototype is calibrated withexisting tools from previous studies, detects using pose detection, which can thenproject points on specific body parts that also follows the person during movement.The result that got developed was a prototype that was tested for accuracy. The testsare done using computer vision to extract measurement data such as the projectedpoint and the expected point.</p><p>The result from the analysis of the test data showedthat the accuracy of the projection is suitable for applications that do not requireexceptional accuracy, such as entertainment, art, games and so on. Further research is required for applications that require exceptional accuracy such ashealthcare and surgical applications.</p>

corrected abstract:
<p>Interactive projected augmented reality is a subfield within projected augmented reality, where the interactivity is about projecting virtual data onto an entity that can potentially be in movement. This projection can be accomplished by using a calibrated projector-depth camera system that detects entities using computer vision. This interactive system has a varying number of application areas; however, a critical problem emerges, which is the accuracy of these systems. The accuracy in this case is how correctly the projection takes place on the specific entity, the accuracy is therefore an important aspect to validate if certain applications are possible to implement correctly. The solution for this problem is by implementing interactive projected augmented reality and perform prototype tests with the implemented system and then further analyzing the accuracy with test data. The prototype is calibrated with existing tools from previous studies, detects using pose detection, which can then project points on specific body parts that also follows the person during movement.</p><p>The result that got developed was a prototype that was tested for accuracy. The tests are done using computer vision to extract measurement data such as the projected point and the expected point. The result from the analysis of the test data showed that the accuracy of the projection is suitable for applications that do not require exceptional accuracy, such as entertainment, art, games and so on. Further research is required for applications that require exceptional accuracy such as healthcare and surgical applications.</p>
----------------------------------------------------------------------
In diva2:1180456 abstract is: <p>To ensure that production is equivalent to consumption, hydro power units are used becauseof their high reliability as a regulation power source.Regulation is used due to the unbalance in the power grid between the production and con-sumption, for which reserved hydro power units for Primary and Secondary regulation areused. Primary regulation is done to balance the frequency in the grid by automatically ad-justing the water flow that runs through such units.Secondary regulation units are used for compensation of lost power production or for takingaway the surplus of power production and has become more intermittent today, which re-sults in more starts and stops of units. The increased intermittency of the units causes strainson the various components especially the generators. Therefore it is of great interest to knowthe total start-stop cost of the secondary regulated units. The theoretically calculated costsmay not coincide with the actual costs, as there are many parameters and complex calcula-tions that are included in the start-stop costs.A revised version of a calculation model was done for the secondary regulated units, basedon an earlier work, and showed higher start-stop costs compared to the earlier study, mostlybecause of these parameters:• Start and stop frequency for each unit has increased because of a higher flexibility in theelectrical market prices.• Age of the generators and turbines. Many of these components are near their time forrehabilitation which results in a higher maintenance cost.• The average failure frequency for each hydropower unit was thoroughly investigated andshowed to be higher than earlier estimated. A more individually calculated failure fre-quency for each unit showed to be higher than the earlier used standard average values.One important result is that the units with the lowest start and stop costs are preferred to beused, with the exception that the units with high start and stop cost, because of considerableleakage cost, should be used as much as possible.</p>


corrected abstract:
<p>To ensure that production is equivalent to consumption, hydro power units are used because of their high reliability as a regulation power source.</p><p>Regulation is used due to the unbalance in the power grid between the production and consumption, for which reserved hydro power units for Primary and Secondary regulation are used. Primary regulation is done to balance the frequency in the grid by automatically adjusting the water flow that runs through such units.</p><p>Secondary regulation units are used for compensation of lost power production or for taking away the surplus of power production and has become more intermittent today, which results in more starts and stops of units. The increased intermittency of the units causes strains on the various components especially the generators. Therefore it is of great interest to know the total start-stop cost of the secondary regulated units. The theoretically calculated costs may not coincide with the actual costs, as there are many parameters and complex calculations that are included in the start-stop costs.</p><p>A revised version of a calculation model was done for the secondary regulated units, based on an earlier work, and showed higher start-stop costs compared to the earlier study, mostly because of these parameters:<ul><li>Start and stop frequency for each unit has increased because of a higher flexibility in the electrical market prices.</li><li>Age of the generators and turbines. Many of these components are near their time for rehabilitation which results in a higher maintenance cost.</li><li>The average failure frequency for each hydropower unit was thoroughly investigated and showed to be higher than earlier estimated. A more individually calculated failure frequency for each unit showed to be higher than the earlier used standard average values.</li></ul></p><p>One important result is that the units with the lowest start and stop costs are preferred to be used, with the exception that the units with high start and stop cost, because of considerable leakage cost, should be used as much as possible.</p>
----------------------------------------------------------------------
In diva2:1454816 abstract is: <p>The grey wolf was the first animal to be domesticated, and its descendant, the dog, is unique in its broad phenotypicdiversity, short time span of evolution and extraordinary relationship with humans. By studying differences in regulatorypatterns, such as regulatory DNA, it is possible to unveil the genetic changes that generate phenotypical differencesbetween the wolf and the dog. This could provide novel information about the unique evolution and domestication of thedog. The main objective of this thesis was to provide an epigenetic map for two types of regulatory DNA-regions: enhancersand promoters, in wolf liver tissue. The regions were identified by locating the histone markers H3K4me3, H3K4me1,H3K27Ac and the transcription factor CTCF, using the recently developed method CUT&amp;Run. CUT&amp;Run uses antibodiesto target the histone markers of interest, and protein A/G-MNase to cut and release the DNA interacting with the histonemarker. The extracted DNA was sequenced and heatmaps were generated based on each antibody’s enrichment arounddog regulatory regions. Following this, a gene ontology list enrichment analysis on wolf was made, and it was comparedto previous studies on dog. The method was successful for all samples, although at varying degrees. The list enrichmentanalysis results firstly show that many regulatory regions in wolf liver tissue are associated with metabolic processes, andsecondly suggest that wolves and dogs share the recently evolved regulatory regions associated with viral process, proteintargeting and mRNA catabolic process in their liver tissue. Furthermore, the results indicate that there are several recentlyevolved dog enhancers that were evolved after the lineage separation between the wolf and the dog. However, theconclusions that can be made solely on this thesis are limited, primarily due to sample restrictions, and further studies arerequired.</p>

partal corrected: diva2:1454816: <p>The grey wolf was the first animal to be domesticated, and its descendant, the dog, is unique in its broad phenotypic diversity, short time span of evolution and extraordin ary relationship with humans. By studying differences in regulatory patterns, such as regulatory DNA, it is possible to unveil the genetic changes that generate phenotypical differences between the wolf and the dog. This could provide novel information about the unique evolution and domestication of the dog. The main objective of this thesis was to provide an epigenetic map for two types of regulatory DNA-regions: enhancers and promoters, in wolf liver tissue. The regions were identified by locating the his tone markers H3K4me3, H3K4me1, H3K27Ac and the transcription factor CTCF, using the recently developed method CUT&amp;Run. CUT&amp;Run uses antibodies to target the his tone markers of interest, and protein A/G-MNase to cut and release the DNA interacting with the his tonemarker. The extracted DNA was sequenced and heatmaps were generated based on each antibody’s enrichment around dog regulatory regions. Following this, a gene ontology list enrichment analysis on wolf was made, and it was compared to previous studies on dog. The method was successful for all samples, although at varying degrees. The list enrichment analysis results firstly show that many regulatory regions in wolf liver tissue are associated with metabolic processes, and secondly suggest that wolves and dogs share the recently evolved regulatory regions associated with viral process, protein targeting and mRNA catabolic process in their liver tissue. Furthermore, the results indicate that there are several recently evolved dog enhancers that were evolved after the lineage separation between the wolf and the dog. However, the conclusions that can be made solely on this thesis are limited, primarily due to sample restrictions, and further studies are required.</p>

corrected abstract:
<p>The grey wolf was the first animal to be domesticated, and its descendant, the dog, is unique in its broad phenotypic diversity, short time span of evolution and extraordinary relationship with humans. By studying differences in regulatory patterns, such as regulatory DNA, it is possible to unveil the genetic changes that generate phenotypical differences between the wolf and the dog. This could provide novel information about the unique evolution and domestication of the dog. The main objective of this thesis was to provide an epigenetic map for two types of regulatory DNA-regions: enhancers and promoters, in wolf liver tissue. The regions were identified by locating the histone markers H3K4me3, H3K4me1, H3K27Ac and the transcription factor CTCF, using the recently developed method CUT&amp;Run. CUT&amp;Run uses antibodies to target the histone markers of interest, and protein A/G-MNase to cut and release the DNA interacting with the histone marker. The extracted DNA was sequenced and heatmaps were generated based on each antibody’s enrichment around dog regulatory regions. Following this, a gene ontology list enrichment analysis on wolf was made, and it was compared to previous studies on dog. The method was successful for all samples, although at varying degrees. The list enrichment analysis results firstly show that many regulatory regions in wolf liver tissue are associated with metabolic processes, and secondly suggest that wolves and dogs share the recently evolved regulatory regions associated with viral process, protein targeting and mRNA catabolic process in their liver tissue. Furthermore, the results indicate that there are several recently evolved dog enhancers that were evolved after the lineage separation between the wolf and the dog. However, the conclusions that can be made solely on this thesis are limited, primarily due to sample restrictions, and further studies are required.</p>
----------------------------------------------------------------------
In diva2:751697 abstract is: <p>A common problem in the workplace is sharing digital documents with coworkers. Forsome companies the problem extends to wanting the documentskept internally backedup and controlling which people in the company has rights to read and revise certaindocuments.This paper shows different systems and models for access control, version control,and distribution of the documents that can be used to create asystem that solves theseproblems.One requirement for this system was a user interface where users can upload, down-load and manage access to their documents. Another requirement was a service thathandles version control for the documents, and a way to quickly connect and distributethe documents. The system also needed to be able to handle access control of the ver-sioned documents on document level, referred to as "fine grained access control" in thispaper.These models and systems were evaluated based on aspects of the access control mod-els, version control systems, and distribution systems andprotocols. After evaluating,appropriate selections were made to create a prototype to test the system as a whole.The prototype ended up meeting the goals that Nordicstationset for the project butonly with basic functionality. Functionality for retrieving any version from a docu-ments history, controlling access for the documents at document level, and a simpleweb based user interface for managing the documents.</p>

w='ver-sioned' val={'c': 'versioned', 's': 'diva2:751697'}

corrected abstract:
<p>A common problem in the workplace is sharing digital documents with coworkers. For some companies the problem extends to wanting the documents kept internally backed up and controlling which people in the company has rights to read and revise certain documents.</p><p>This paper shows different systems and models for access control, version control, and distribution of the documents that can be used to create a system that solves these problems.</p><p>One requirement for this system was a user interface where users can upload, download and manage access to their documents. Another requirement was a service that handles version control for the documents, and a way to quickly connect and distribute the documents. The system also needed to be able to handle access control of the versioned documents on document level, referred to as "fine grained access control" in this paper.</p><p>These models and systems were evaluated based on aspects of the access control models, version control systems, and distribution systems and protocols. After evaluating, appropriate selections were made to create a prototype to test the system as a whole.</p><p>The prototype ended up meeting the goals that Nordicstation set for the project but only with basic functionality. Functionality for retrieving any version from a documents history, controlling access for the documents at document level, and a simple web based user interface for managing the documents.</p>
----------------------------------------------------------------------
In diva2:1454429 abstract is: <p>Inflammatory Bowel Disease (IBD) is a chronic disorder that affects millions of peopleworldwide. Although the etiology behind the disease is yet unknown, current theoriespropose a complex interplay between genetic susceptibility, exposure to environmentalfactors and exacerbated immune responses. While important efforts have been made to linkgenetics and environmental factors to IBD pathogenesis, a major challenge remains to assignthem a causative role. Particularly since most of the IBD-risk genetic polymorphisms arefound in non-coding regions (NCRs) with unknown regulatory activity, and for the lack ofknowledge about how environmental factors can modulate the function of these elements invivo . A main problem to address this challenge in IBD research is the lack of an appropriatemodel system in vivo that allows for high-throughput experiments with combinations ofdifferent IBD-risk factors, while keeping the in vivo context. In this work, we sought toovercome this issue by using a zebrafish reporter for a specific human IBD-risk NCR, inorder to investigate the modulation of this element by two groups of common environmentalfactors: pollutants, such as PolyFluoroAlkyl Substances (PFASs); and diet, by activation ofdietary sensors.</p><p>We found that the activity of the WT-NCR in zebrafish larvae was increased in the presenceof PFAS, while the activation of the dietary sensor PPAR δ decreased the activity. These datalead us to suggest that the function of PFAS can be counteracted by PPARδ activation.Therefore, we propose zebrafish as a suitable in vivo model in which we can screen forpotentially harmful or beneficial effects of environmental factors in the activity of humannon-coding regions.</p>

corrected abstract:
<p>Inflammatory Bowel Disease (IBD) is a chronic disorder that affects millions of people worldwide. Although the etiology behind the disease is yet unknown, current theories propose a complex interplay between genetic susceptibility, exposure to environmental factors and exacerbated immune responses. While important efforts have been made to link genetics and environmental factors to IBD pathogenesis, a major challenge remains to assign them a causative role. Particularly since most of the IBD-risk genetic polymorphisms are found in non-coding regions (NCRs) with unknown regulatory activity, and for the lack of knowledge about how environmental factors can modulate the function of these elements <em>in vivo</em>. A main problem to address this challenge in IBD research is the lack of an appropriate model system <em>in vivo</em> that allows for high-throughput experiments with combinations of different IBD-risk factors, while keeping the <em>in vivo</em> context. In this work, we sought to overcome this issue by using a zebrafish reporter for a specific human IBD-risk NCR, in order to investigate the modulation of this element by two groups of common environmental factors: pollutants, such as PolyFluoroAlkyl Substances (PFASs); and diet, by activation of dietary sensors.</p><p>We found that the activity of the WT-NCR in zebrafish larvae was increased in the presence of PFAS, while the activation of the dietary sensor PPARδ decreased the activity. These data lead us to suggest that the function of PFAS can be counteracted by PPARδ activation. Therefore, we propose zebrafish as a suitable <em>in vivo</em> model in which we can screen for potentially harmful or beneficial effects of environmental factors in the activity of human non-coding regions.</p>
----------------------------------------------------------------------
In diva2:1147592 abstract is: <p>Virulent strains of S. pneumoniae are known to evade the alternative pathway ofcomplement immunity by means of a surface bound protein called PspC that recruits thehuman complement regulator Factor H. Factor H is a self surface marker that inhibits theactivity of the alternative pathway of complement immunity and is comprised of 20Complement Control Protein (CCP) domains in a “bead on a string” fashion. It has beenconcluded that PspC can use two different mechanisms of binding Factor H. The PspCallele of the TIGR4 strain of S. pneumoniae has been shown to have affinity for the 9thCCP of Factor H through a “lock and key” mechanism mediated by a critical Tyr90residue. However, PspC from the D39 strain of S. pneumoniae does not posess Tyr90 andit hasn’t been conclusively shown which CCP it binds to. In an effort to elucidate thismechanism, individual CCP domains were expressed as fusion proteins with MaltoseBinding Protein in an E. coli based expression system. The fusion proteins were used inexperiments with recombinant PspC cloned from the BHN_418 strain of S. pneumoniaewhich is homologous to that of D39 PspC. Affinity interactions were investigated with apulldown assay, copurification, microscale thermophoresis and ligand tracer assays. Theresults are inconclusive. The recombinant PspC is shown to bind full length Factor H butnot any of the individual CCP-MBP fusion proteins, most likely due to the CCPs failingto achieve proper tertiary structure during expression.</p>

w='linterplanetary' val={'c': 'interplanetary', 's': 'diva2:1147592', 'n': 'The abstract does not match the thesis - it is completely different'}
w='potentia' val={'c': 'potential', 's': 'diva2:1147592', 'n': 'The abstract does not match the thesis - it is completely different'}
w='posess' val={'c': 'possess', 's': 'diva2:1147592', 'n': 'error in original'}
w='Tyr90' val={'c': 'Tyr<sup>90</sup>', 's': 'diva2:1147592', 'n': 'appears without superscript in abstract'}

Note also that E.coli should have been set in italics, but is not in the actual abstract.


corrected abstract:
<p>Virulent strains of <em>S. pneumoniae</em> are known to evade the alternative pathway of complement immunity by means of a surface bound protein called PspC that recruits the human complement regulator Factor H. Factor H is a self surface marker that inhibits the activity of the alternative pathway of complement immunity and is comprised of 20 Complement Control Protein (CCP) domains in a “bead on a string” fashion. It has been concluded that PspC can use two different mechanisms of binding Factor H. The PspC allele of the TIGR4 strain of <em>S. pneumoniae</em> has been shown to have affinity for the 9<sup>th</sup> CCP of Factor H through a “lock and key” mechanism mediated by a critical Tyr90 residue. However, PspC from the D39 strain of <em>S. pneumoniae</em> does not posess Tyr90 and it hasn’t been conclusively shown which CCP it binds to. In an effort to elucidate this mechanism, individual CCP domains were expressed as fusion proteins with Maltose Binding Protein in an E. coli based expression system. The fusion proteins were used in experiments with recombinant PspC cloned from the BHN_418 strain of <em>S. pneumoniae</em> which is homologous to that of D39 PspC. Affinity interactions were investigated with a pulldown assay, copurification, microscale thermophoresis and ligand tracer assays. The results are inconclusive. The recombinant PspC is shown to bind full length Factor H but not any of the individual CCP-MBP fusion proteins, most likely due to the CCPs failing to achieve proper tertiary structure during expression.</p>
----------------------------------------------------------------------
In diva2:1332164 abstract is: <p>Prostheses available today are either passive or active. Active prostheses canapply external energy to the joints, while passive cannot. A well functionalprosthesis that is adapted to the wearer can improve the locomotion and alsodecrease the metabolic cost. Existing active prostheses are mainly controlledby information from the prosthesis itself in order to detect the phases in thegait. Due to lack of information from the contralateral leg, asymmetry in thegait can be present.Therefor, this study aims to investigate if it is feasible to use kinematic informationfrom a individual’s sound leg in order to prescribe the motion ofan aid. Three healthy test subjects were used to collect kinematic and EMGdata in a motion capture lab. Two different movements were performed: "Startand stop" and "Start, stop and squat". The data obtained was processed and togetherwith some created criteria a script could calculate knee and ankle anglesfor both reference and mirrored leg. The results conclude that it is feasible tomirror position information from one leg to the other. Further research is recommendedto gain more knowledge about from different trials together withreal aids.</p>

corrected abstract:
<p>Prostheses available today are either passive or active. Active prostheses can apply external energy to the joints, while passive cannot. A well functional prosthesis that is adapted to the wearer can improve the locomotion and also decrease the metabolic cost. Existing active prostheses are mainly controlled by information from the prosthesis itself in order to detect the phases in the gait. Due to lack of information from the contralateral leg, asymmetry in the gait can be present.</p><p>Therefor, this study aims to investigate if it is feasible to use kinematic information from a individual’s sound leg in order to prescribe the motion of an aid. Three healthy test subjects were used to collect kinematic and EMG data in a motion capture lab. Two different movements were performed: "Start and stop" and "Start, stop and squat". The data obtained was processed and together with some created criteria a script could calculate knee and ankle angles for both reference and mirrored leg. The results conclude that it is feasible to mirror position information from one leg to the other. Further research is recommended to gain more knowledge about from different trials together with real aids.</p>
----------------------------------------------------------------------
In diva2:860831 abstract is: <p>Henriksdal wastewater treatment plant is one of the largest plants in Europe located underthe ground. The wastewater is treated mechanically, chemically and biologically.</p><p>The basic concept for the biological treatment to work properly is that the amount ofmicroorganisms is kept at a high level. This is done by recirculating some of the sludge backto the biological pools. Periodically, when the flow into the plant is very high, the capacity ofthe biological step is not sufficient. At these times, there are currently two possible options:- Pretreated water is directed past the biological treatment to the sand filters.- Pretreated water is directed past the biological treatment and the sand filtersstraight out to the recipient.</p><p>To solve these problems, Henriksdal is planning a reconstruction of the biological treatment.During the reconstruction, the capacity of the biological treatment is going to be even lessthan it is today.</p><p>Hence there is an interest, at high flows, to increase the purification in the pre-precipitationstep with new types of precipitation chemicals.</p><p>The aim of this project is to examine two different precipitation chemicals in lab- and in largescale. This was done by flocculation tests in the lab with a flocculator to determine thedosage for each chemical to be tested in large scale. The two chemicals that were testedwere iron chloride (PIX-111) and polyaluminiumchloride (PAX-21).</p><p>The results from the labtests showed that an optimal dose for PIX was 0,12 ml PIX/l water.While the optimal dose for PAX was 0,10 ml PAX/l water.</p><p>The results from the large scale test with PAX showed a high reduction in both turbidity andsuspended particles. The reduction of both was between 50-75 %.</p><p>The results from the large scale test with PIX did not show as high reduction as that of PAX.The reduction of turbidity was around 20 %, with a maximum value of 36,6 %. The reductionof phosphate was between 70-80 %.</p><p>During the project there were only a few days of high water flows and only PAX could betested during these conditions. Hence, the two chemicals cannot be compared accuratelyuntil also PIX is tested during high water flows</p>

w='PIX' val={'c': 'PIX-111', 's': 'diva2:860831', 'n': 'PIX is used in the abstract aas shorthand for PIX-111'}

corrected abstract:
<p>Henriksdal wastewater treatment plant is one of the largest plants in Europe located under the ground. The wastewater is treated mechanically, chemically and biologically.</p><p>The basic concept for the biological treatment to work properly is that the amount of microorganisms is kept at a high level. This is done by recirculating some of the sludge back to the biological pools. Periodically, when the flow into the plant is very high, the capacity of the biological step is not sufficient. At these times, there are currently two possible options:<ul><li>Pretreated water is directed past the biological treatment to the sand filters.</li><li>Pretreated water is directed past the biological treatment and the sand filters straight out to the recipient.</li></ul></p><p>To solve these problems, Henriksdal is planning a reconstruction of the biological treatment. During the reconstruction, the capacity of the biological treatment is going to be even less than it is today.</p><p>Hence there is an interest, at high flows, to increase the purification in the pre-precipitation step with new types of precipitation chemicals.</p><p>The aim of this project is to examine two different precipitation chemicals in lab- and in large scale. This was done by flocculation tests in the lab with a flocculator to determine the dosage for each chemical to be tested in large scale. The two chemicals that were tested were iron chloride (PIX-111) and polyaluminiumchloride (PAX-21).</p><p>The results from the labtests showed that an optimal dose for PIX was 0,12 ml PIX/l water. While the optimal dose for PAX was 0,10 ml PAX/l water.</p><p>The results from the large scale test with PAX showed a high reduction in both turbidity and suspended particles. The reduction of both was between 50-75 %.</p><p>The results from the large scale test with PIX did not show as high reduction as that of PAX. The reduction of turbidity was around 20 %, with a maximum value of 36,6 %. The reduction of phosphate was between 70-80 %.</p><p>During the project there were only a few days of high water flows and only PAX could be tested during these conditions. Hence, the two chemicals cannot be compared accurately until also PIX is tested during high water flows</p>
----------------------------------------------------------------------
In diva2:1261107 abstract is: <p>The wet-end of the centre ply of the paperboard machine n˚ 2 (“Kartongmaskin 2” or KM2) inIggesund was simulated to obtain both qualitative and quantitative results on its start-up dynamics andits variation damping capacity. The accuracy of the model was controlled by comparing the simulationresults with data from the real production process. Furthermore, alternative strategies with theobjective of reducing the time needed for start-ups and grade changes were evaluated.The modeling was done using the Paperfront simulation software. Variables such as layout for themixing system, retention level and amplitude and period of inlet concentration were used asparameters. As alternative operation strategies for start-up and grade change, increased flowrate in theearly phase of a start-up and the temporary overdosing during the beginning of a stock change wereevaluated.The results indicate a strong control of the mixing system over the short circulation both in dynamicsat start-up and in general operation with a time constant for the machine close to 35 minutes. Inaddition, variations in consistency are more easily controlled by a double-chest mixing system and, ingeneral, variations are more easily damped the higher the active volume in the mixing system. Finally,times needed for start-up and composition changes could be reduced by as much as 50% by using amore proactive approach.</p>

corrected abstract:
<p>The wet-end of the centre ply of the paperboard machine n˚ 2 (“Kartongmaskin 2” or KM2) in Iggesund was simulated to obtain both qualitative and quantitative results on its start-up dynamics and its variation damping capacity. The accuracy of the model was controlled by comparing the simulation results with data from the real production process. Furthermore, alternative strategies with the objective of reducing the time needed for start-ups and grade changes were evaluated.</p><p>The modeling was done using the Paperfront simulation software. Variables such as layout for the mixing system, retention level and amplitude and period of inlet concentration were used as parameters. As alternative operation strategies for start-up and grade change, increased flowrate in the early phase of a start-up and the temporary overdosing during the beginning of a stock change were evaluated.</p><p>The results indicate a strong control of the mixing system over the short circulation both in dynamics at start-up and in general operation with a time constant for the machine close to 35 minutes. In addition, variations in consistency are more easily controlled by a double-chest mixing system and, in general, variations are more easily damped the higher the active volume in the mixing system. Finally, times needed for start-up and composition changes could be reduced by as much as 50% by using a more proactive approach.</p>
----------------------------------------------------------------------
In diva2:826739 abstract is: <p>Many European growers of organic fruit have large losses in yield due to reduced quality oftheir products because of pest insect damage. These pest insect damages have made it difficultfor farmers all over Europe to grow organic fruit without risking significant economic losses.In organic soft fruit production there are no effective control measures for many of these pestinsects. Strawberry production is one part of the ecological fruit-production that is severleyaffected by these pest insects. This has become a big problem as there are no effective controlmeasures for pest insects. For herbivore insects species host plant volatiles are of majorimportance in location of host plants for feeding and oviposition. Therefore, there is potentialfor using these insect-host plant interactions to develop new strategies and effective controlmeasures for pest insects. For doing that it is needed to know which volatile organiccompounds the strawberry plants are releasing and which of these compounds that are used byinsects to detect the plants.</p><p>In this project, volatiles from buds, leaves and flowers from plants of wild strawberry speciesand cultivated strawberry species has been collected. Dynamic and static methods forsampling of volatiles have been used. Quantitative and qualitative analyses of volatiles havebeen carried out by gas chromatography and mass spectrometry techniques. The study showsthat the wild species in total releases six chemical compounds; Benzaldehyde , Limonene ,Benzylalcohol, Methyl salicylate, α - Muurolene and E,E-α - Farnesene . The study alsoshows that the cultivated species in total releases six chemical compounds; Limonene,Benzylalcohol , Methyl salicylate , p-Anisaldehyde, and (Z) -3- hexene- 1 -ol acetate. Withquantification it was found that p- Anisaldehyde is the chemical compound with the largestquantity emitted from the cultivated flowering plants .</p>

w='severley' val={'c': 'severely', 's': 'diva2:826739', 'n': 'no full text'}

corrected abstract:
<p>Many European growers of organic fruit have large losses in yield due to reduced quality of their products because of pest insect damage. These pest insect damages have made it difficult for farmers all over Europe to grow organic fruit without risking significant economic losses. In organic soft fruit production there are no effective control measures for many of these pestinsects. Strawberry production is one part of the ecological fruit-production that is severely affected by these pest insects. This has become a big problem as there are no effective controlmeasures for pest insects. For herbivore insects species host plant volatiles are of major importance in location of host plants for feeding and oviposition. Therefore, there is potential for using these insect-host plant interactions to develop new strategies and effective controlmeasures for pest insects. For doing that it is needed to know which volatile organic compounds the strawberry plants are releasing and which of these compounds that are used by insects to detect the plants.</p><p>In this project, volatiles from buds, leaves and flowers from plants of wild strawberry species and cultivated strawberry species has been collected. Dynamic and static methods for sampling of volatiles have been used. Quantitative and qualitative analyses of volatiles have been carried out by gas chromatography and mass spectrometry techniques. The study shows that the wild species in total releases six chemical compounds; Benzaldehyde, Limonene, Benzylalcohol, Methyl salicylate, α -Muurolene and E,E-α-Farnesene. The study also shows that the cultivated species in total releases six chemical compounds; Limonene, Benzylalcohol, Methyl salicylate, p-Anis aldehyde, and (Z)-3- hexene-1-ol acetate. With quantification it was found that p- Anis aldehyde is the chemical compound with the largest quantity emitted from the cultivated flowering plants.</p>
----------------------------------------------------------------------
In diva2:1421348 abstract is: <p>Within the type 1 diabetic research several applications have been evaluated to improve lifequality among type 1 diabetic patients. The aim of this project was to validate in vitro thefeasibility of using mucin hydrogel for islet transplantation . The project started with macroencapsulation of Min6-M9 cancer cell line extracted from 13 weeks old mice. The projectcontinued with encapsulation of Min6-M9 cells in microscopic hydrogel using a microfluidicdevice to evaluate the possibility to automate the encapsulation process, but also to facilitatethe future transplantation process upon surgery due to practical reasons since microscopicmucin hydrogel has smaller diameter than macro-encapsulated mucin hydrogel. Finally,primary islets isolated from 4 months old C57B1/6J laboratory mice was obtained and macroencapsulated in mucin hydrogel. In the experiments the samples were cultivated long termand continuously tested through quantitative data assay e.g. insulin secretion assay andAlamar Blue assay followed with qualitative data analysis through e.g. bright field imagesand measurements of the spherical diameters from single cells to clusters. This was followedwith LIVE/DEAD staining. From the obtained data from both Min6-M9 inmacro/microscopic hydrogel and primary islets it was possibility to show the long term isletssurvival of encapsulated Min6-M9 cells and primary islets. In addition, all samples secretedinsulin wherein primary islets had a clear insulin response upon glucose stimulation. Fromthe experiments in vitro the conclusion can be made that mucin hydrogel do supportencapsulated islets long term function and survival. From this, the feasibility usingencapsulated islets in mucin hydrogel needs to be investigated in vivo.</p>

partal corrected: diva2:1421348: <p>Within the type 1 diabetic research several applications have been evaluated to improve life quality among type 1 diabetic patients. The aim of this project was to validate in vitro the feasibility of using mucin hydrogel for islet transplantation . The project started with macroencapsulation of Min6-M9 cancer cell line extracted from 13 weeks old mice. The project continued with encapsulation of Min6-M9 cells in microscopic hydrogel using a microfluidic device to evaluate the possibility to automate the encapsulation process, but also to facilitate the future transplantation process upon surgery due to practical reasons since microscopic mucin hydrogel has smaller diameter than macro-encapsulated mucin hydrogel. Fin ally, primary islets isolated from 4 months old C57B1/6J laboratory mice was obtained and macroencapsulated in mucin hydrogel. In the experiments the samples were cultivated long term and continuously tested through quantitative data assay e.g. insulin secretion assay and Alamar Blue assay followed with qualitative data analysis through e.g. bright field images and measurements of the spherical diameters from single cells to clusters. This was followed with LIVE/DEAD staining. From the obtained data from both Min6-M9 in macro/microscopic hydrogel and primary islets it was possibility to show the long term islets survival of encapsulated Min6-M9 cells and primary islets. In addition, all samples secreted insulin wherein primary islets had a clear insulin response upon glucose stimulation. Fromthe experiments in vitro the conclusion can be made that mucin hydrogel do support encapsulated islets long term function and survival. From this, the feasibility using encapsulated islets in mucin hydrogel needs to be investigated in vivo.</p>

Markup is assumed to be italics for "in vitro" and "in vivo"

corrected abstract:
<p>Within the type 1 diabetic research several applications have been evaluated to improve life quality among type 1 diabetic patients. The aim of this project was to validate <em>in vitro</em> the feasibility of using mucin hydrogel for islet transplantation. The project started with macroencapsulation of Min6-M9 cancer cell line extracted from 13 weeks old mice. The project continued with encapsulation of Min6-M9 cells in microscopic hydrogel using a microfluidic device to evaluate the possibility to automate the encapsulation process, but also to facilitate the future transplantation process upon surgery due to practical reasons since microscopic mucin hydrogel has smaller diameter than macro-encapsulated mucin hydrogel. Finally, primary islets isolated from 4 months old C57B1/6J laboratory mice was obtained and macroencapsulated in mucin hydrogel. In the experiments the samples were cultivated long term and continuously tested through quantitative data assay e.g. insulin secretion assay and Alamar Blue assay followed with qualitative data analysis through e.g. bright field images and measurements of the spherical diameters from single cells to clusters. This was followed with LIVE/DEAD staining. From the obtained data from both Min6-M9 in macro/microscopic hydrogel and primary islets it was possibility to show the long term islets survival of encapsulated Min6-M9 cells and primary islets. In addition, all samples secreted insulin wherein primary islets had a clear insulin response upon glucose stimulation. From the experiments <em>in vitro</em> the conclusion can be made that mucin hydrogel do support encapsulated islets long term function and survival. From this, the feasibility using encapsulated islets in mucin hydrogel needs to be investigated <em>in vivo</em>.</p>
----------------------------------------------------------------------
In diva2:1366646 abstract is: <p>Remote control and management functions are widely utilized in multiple industries.The remote control and management functions has allowed for peopleto connect and interact to solve technical problems more efficiently. However,the healthcare organizations have not utilized the remote controlling and managementfunctions to a degree similar to other industries. Telephoning ande-mailing are still two mainstream ways of work when it comes to solvingtechnical support issues in-house. In order to understand what the technicalpersonnel and the clinical users at a hospital desires in new solutions, thismaster thesis project aimed at finding the existing needs in terms of remotecontrolling and management functions. To find these needs, Q-methodologywas applied for collection of subjective data from healthcare personnel abouta software device that aims at providing remote controlling and managementfunctions. In addition to finding and defining the needs, this thesis also aimedat examining how well such systems can address these needs.</p><p>Performing this methodology three factors where found representing three differentattitudes regarding the needs for remote functions. The three factorsare "Technical Communication is Significant", "Functionality Appreciativeand Experienced" and "Do if fast!". These factors and their interpretationhelps to be aware of and to evaluate remote support solutions in a systematicway.</p>

partal corrected: diva2:1366646: <p>Remote control and management functions are widely utilized in multiple industries. The remote control and management functions has allowed for people to connect and interact to solve technical problems more efficiently. However, the healthcare organizations have not utilized the remote controlling and management functions to a degree similar to other industries. Telephoning and e-mailing are still two mainstream ways of work when it comes to solving technical support issues in-house. In order to understand what the technical personnel and the clinical users at a hospital desires in new solutions, this master thesis project aimed at finding the existing needs in terms of remote controlling and management functions. To find these needs, Q-methodologywas applied for collection of subjective data from healthcare personnel about a software device that aims at providing remote controlling and management functions. In addition to finding and defining the needs, this thesis also aimed at examining how well such systems can address these needs.</p><p>Performing this methodology three factors where found representing three different attitudes regarding the needs for remote functions. The three factors are "Technical Communication is Significant", "Functionality Appreciative and Experienced" and "Do if fast!". These factors and their interpretation helps to be aware of and to evaluate remote support solutions in a systematic way.</p>

The error in lack of left double quote is in the actual abstract (as is the use of the right double quote).

corrected abstract:
<p>Remote control and management functions are widely utilized in multiple industries. The remote control and management functions has allowed for people to connect and interact to solve technical problems more efficiently. However, the healthcare organizations have not utilized the remote controlling and management functions to a degree similar to other industries. Telephoning and e-mailing are still two mainstream ways of work when it comes to solving technical support issues in-house. In order to understand what the technical personnel and the clinical users at a hospital desires in new solutions, this master thesis project aimed at finding the existing needs in terms of remote controlling and management functions. To find these needs, Q-methodology was applied for collection of subjective data from healthcare personnel about a software device that aims at providing remote controlling and management functions. In addition to finding and defining the needs, this thesis also aimed at examining how well such systems can address these needs.</p><p>Performing this methodology three factors where found representing three different attitudes regarding the needs for remote functions. The three factors are ”Technical Communication is Significant”, ”Functionality Appreciative and Experienced” and ”Do if fast!”. These factors and their interpretation helps to be aware of and to evaluate remote support solutions in a systematic way.</p>
----------------------------------------------------------------------
title: "Digitalization of Healthcare for Heart Failure Patients: An Analysis ofthe Opportunities and Restrictions in the Implementation of Self-Monitoring Systems for Heart Failure Patients in Sweden"
==> "Digitalization of Healthcare for Heart Failure Patients: An Analysis of the Opportunities and Restrictions in the Implementation of Self-Monitoring Systems for Heart Failure Patients in Sweden"

In diva2:1679918 abstract is: <p>The Swedish government and the Swedish Association of Local Authorities and Regions, cametogether on a vision that Sweden will be the most advanced country in offering opportunities fordigitalization and e-health by the year of 2025. Self-monitoring is one aspect that can digitizeworkflow in healthcare.</p><p>This master thesis sought to investigate the opportunities and restrictions when implementingself-monitoring for heart failure patients in Swedish healthcare. Starting from the holisticperspective as an analysis of the clinician’s interest and the need for self-monitoring systemsby conducting a mixed method approach. Surveys were sent out to different county councilsin Sweden working with heart failure patients, and interviews were conducted withrepresentatives from primary care, specialist care, and advanced home health care. Secondly,interviews were conducted with stakeholders from the field of implementation in order togather qualitative results on the organizational, legal, and technical influencing factors,challenges, and requirements in Swedish healthcare. The qualitative data were processedthrough thematic analysis and the quantitative data was processed through descriptive analysis.</p><p>The results gathered from clinicians consisted of three areas regarding functionality demandsin self-monitoring systems. The results gathered in the aspect of implementation consisted ofthree main organizational influencing aspects, three solution models from a legal perspective,as well as three aspects related in the field of technology and IT.</p>

corrected abstract:
<p>The Swedish government and the Swedish Association of Local Authorities and Regions, came together on a vision that Sweden will be the most advanced country in offering opportunities for digitalization and e-health by the year of 2025. Self-monitoring is one aspect that can digitize workflow in healthcare.</p><p>This master thesis sought to investigate the opportunities and restrictions when implementing self-monitoring for heart failure patients in Swedish healthcare. Starting from the holistic perspective as an analysis of the clinician’s interest and the need for self-monitoring systems by conducting a mixed method approach. Surveys were sent out to different county councils in Sweden working with heart failure patients, and interviews were conducted with representatives from primary care, specialist care, and advanced home health care. Secondly, interviews were conducted with stakeholders from the field of implementation in order to gather qualitative results on the organizational, legal, and technical influencing factors, challenges, and requirements in Swedish healthcare. The qualitative data were processed through thematic analysis and the quantitative data was processed through descriptive analysis.</p><p>The results gathered from clinicians consisted of three areas regarding functionality demands in self-monitoring systems. The results gathered in the aspect of implementation consisted of three main organizational influencing aspects, three solution models from a legal perspective, as well as three aspects related in the field of technology and IT.</p>
----------------------------------------------------------------------
In diva2:1678223 abstract is: <p>The ability of an individual to withstand elevated head-to-toe gravitoinertial (+Gz) forces is determined by the capacity of their body to maintain sufficient head-level arterial pressure. Recent studies have shown a relationship between resting blood-vessel stiffness and an individual’s +Gz-tolerance, although the mechanisms behind this relationship are unclear. The aim of this project is to determine whether or not +Gz-tolerance is affected by a change inresting vasomotor tone. To evaluate this relationship, seven participants were asked to complete a +Gz-tolerance protocol using a human-use centrifugeon two different occasions. On both visits, gradual onset rate (0.1 G.s−1)and rapid onset rate (3.5 G.s−1) tests were done to evaluate the participants+Gz-tolerance. On one of the two visits, prior to the +Gz-tolerance testing,participants performed a 20-min cycle intervention to induce postexercisehypotension, with the aim of temporarily reducing participants’ resting bloodpressure and vasomotor tone. The cycling intervention was successful atinducing postexercise hypotension, as mean arterial pressure was significantlylower on the cycling visit (P&lt;0.05). +Gz-tolerance was significantly lower(P&lt;0.05) on the cycling visit compared with the non-cycling visit for both theGOR and ROR tests (absolute difference of 0.5 G and 0.25 G, respectively).The effect of the type of test on +Gz-tolerance was not influenced by the effectof the cycling intervention (P&gt;0.05). Being the most documented mechanismlinked to postexercise hypotension, sustained vasodilation was assumed tohave occurred. This would have increased distensibility of the affected vessels,explaining the decrease in +Gz-tolerance. The decrease in +Gz-tolerance wassimilar for both tests, indicating that the baroreflex was not affected by thecycling intervention. Assuming that vasodilation occurred, this study showedthat a decrease in resting vasomotor tone decreased +Gz-tolerance, indicatingthe importance of this variable in the relationship between resting blood-vesselstiffness and an individual’s +Gz-tolerance.</p>


corrected abstract:
<p>The ability of an individual to withstand elevated head-to-toe gravitoinertial (+Gz) forces is determined by the capacity of their body to maintain sufficient head-level arterial pressure. Recent studies have shown a relationship between resting blood-vessel stiffness and an individual’s +Gz-tolerance, although the mechanisms behind this relationship are unclear. The aim of this project is to determine whether or not +Gz-tolerance is affected by a change in resting vasomotor tone. To evaluate this relationship, seven participants were asked to complete a +Gz-tolerance protocol using a human-use centrifuge on two different occasions. On both visits, gradual onset rate (0.1 G.s<sup>−1</sup>) and rapid onset rate (3.5 G.s<sup>−1</sup>) tests were done to evaluate the participants +Gz-tolerance. On one of the two visits, prior to the +Gz-tolerance testing, participants performed a 20-min cycle intervention to induce postexercise hypotension, with the aim of temporarily reducing participants’ resting blood pressure and vasomotor tone. The cycling intervention was successful at inducing postexercise hypotension, as mean arterial pressure was significantly lower on the cycling visit (<em>P</em>&lt;0.05). +Gz-tolerance was significantly lower (<em>P</em>&lt;0.05) on the cycling visit compared with the non-cycling visit for both the GOR and ROR tests (absolute difference of 0.5 G and 0.25 G, respectively). The effect of the type of test on +Gz-tolerance was not influenced by the effect of the cycling intervention (P&gt;0.05). Being the most documented mechanism linked to postexercise hypotension, sustained vasodilation was assumed to have occurred. This would have increased distensibility of the affected vessels, explaining the decrease in +Gz-tolerance. The decrease in +Gz-tolerance was similar for both tests, indicating that the baroreflex was not affected by the cycling intervention. Assuming that vasodilation occurred, this study showed that a decrease in resting vasomotor tone decreased +Gz-tolerance, indicating the importance of this variable in the relationship between resting blood-vessel stiffness and an individual’s +Gz-tolerance.</p>
----------------------------------------------------------------------
In diva2:773317 abstract is: <p>The Mandometer® Method helps people with eating disorders by teaching them how to eat ina ”correct” way by training with a product called Mandometer®. The users learn the correcteating behavior by receiving feedback from the display on the Mandometer® handheldcomputer on whether they eat too fast or too slow during the meal. Mando Group AB, thecompany which has developed the Mandometer® Method, has also created a computerprogram that works as a simulation of a meal with the Mandometer®. The purpose of thisstudy was to examine if training with the simulation could give the same effect as trainingwith Mandometer®. By letting a group of healthy testpersons practice with the simulationduring three weeks a result was given which clearly showed that the eating behavior of thetest persons had changed in the desired direction. The result indicates that there is a possibilityto add the simulation to the Mandometer® Method in the future. However, more testing anddeveloping of the program is first required.To improve Mandometer® further, a new version of the product has been developed wherethe handheld computer has been replaced by a cellphone and its functions by anapplication. The functionality and usability of the application was tested with the methodcognitive walkthrough, which showed that most of the pages of the application maintaineda high standard. With some adjustment of the pages where the usability was low, theapplication should be able to replace the old version of Mandometer® in the future.</p>

The actual abstract uses the double right quotation mark for both start and end of a quote.
The first "testpersons" does not have a space, while the second use does.

corrected abstract:
<p>The Mandometer® Method helps people with eating disorders by teaching them how to eat in a ”correct” way by training with a product called Mandometer®. The users learn the correct eating behavior by receiving feedback from the display on the Mandometer® handheld computer on whether they eat too fast or too slow during the meal. Mando Group AB, the company which has developed the Mandometer® Method, has also created a computer program that works as a simulation of a meal with the Mandometer®. The purpose of this study was to examine if training with the simulation could give the same effect as training with Mandometer®. By letting a group of healthy testpersons practice with the simulation during three weeks a result was given which clearly showed that the eating behavior of the test persons had changed in the desired direction. The result indicates that there is a possibility to add the simulation to the Mandometer® Method in the future. However, more testing and developing of the program is first required.</p><p>To improve Mandometer® further, a new version of the product has been developed where the handheld computer has been replaced by a cellphone and its functions by an application. The functionality and usability of the application was tested with the method cognitive walkthrough, which showed that most of the pages of the application maintained a high standard. With some adjustment of the pages where the usability was low, the application should be able to replace the old version of Mandometer® in the future.</p>
----------------------------------------------------------------------
In diva2:731857 abstract is: <p>This report presents the design and manufacturing process of a bionic signal messagebroker (BSMB), intended to allow communication between implanted electrodes andprosthetic legs designed by Ossur. The BSMB processes and analyses the data intorelevant information to control the bionic device. The intention is to carry out eventdetection in the BSMB, where events in the muscle signal are matched to the events ofthe gait cycle (toe-o, stance, swing).The whole system is designed to detect muscle contraction via sensors implantedin residual muscles and transmit the signals wireless to a control unit that activatesassociated functions of a prosthetic leg. Two users, one transtibial and one transfemoral,underwent surgery in order to get electrodes implantable into their residual leg muscles.They are among the rst users in the world to get this kind of implanted sensors.A prototype of the BSMB was manufactured. The process took more time thanexpected, mainly due to the fact that it was decided to use a ball grid array (BGA)microprocessor in order to save space. That meant more complicated routing and higherstandards for the manufacturing of the board. The results of the event detection indicatethat the data from the implanted electrodes can be used in order to get sucient controlover prosthetic legs. These are positive ndings for users of prosthetic legs and shouldincrease their security and quality of life.It is important to keep in mind when the results of this report are evaluated that allthe testing carried out were only done on one user each.</p>

w='ndings' val={'c': 'findings', 's': 'diva2:731857', 'n': 'missing ligature'}
w='sucient' val={'c': 'sufficient', 's': 'diva2:731857', 'n': 'missing ligature'}

corrected abstract:
<p>This report presents the design and manufacturing process of a bionic signal message broker (BSMB), intended to allow communication between implanted electrodes and prosthetic legs designed by Össur. The BSMB processes and analyses the data into relevant information to control the bionic device. The intention is to carry out event detection in the BSMB, where events in the muscle signal are matched to the events of the gait cycle (toe-off, stance, swing).</p><p>The whole system is designed to detect muscle contraction via sensors implanted in residual muscles and transmit the signals wireless to a control unit that activates associated functions of a prosthetic leg. Two users, one transtibial and one transfemoral, underwent surgery in order to get electrodes implantable into their residual leg muscles. They are among the first users in the world to get this kind of implanted sensors.</p><p>A prototype of the BSMB was manufactured. The process took more time than expected, mainly due to the fact that it was decided to use a ball grid array (BGA) microprocessor in order to save space. That meant more complicated routing and higher standards for the manufacturing of the board. The results of the event detection indicate that the data from the implanted electrodes can be used in order to get sufficient control over prosthetic legs. These are positive findings for users of prosthetic legs and should increase their security and quality of life.</p><p>It is important to keep in mind when the results of this report are evaluated that all the testing carried out were only done on one user each.</p>
----------------------------------------------------------------------
In diva2:1315699 abstract is: <p>This report investigates the feasibility of several types of biomass to be used as feedstock forproduction of biochar by slow pyrolysis. A literature review and case studies for all investigatedfeedstocks resulted in two models: one for the characterization of physical and chemical propertiesof biochar at different high treatment temperatures, and the other for determining to what degreethe system will be thermally self-sustaining, if at all. This by determining the energy required by thereactor in comparison to the energy available in the pyrolysis gas. The primary investigatedfeedstocks were: fibre sludge, lignin pellets, olive wastes, sunflower seeds and exhausted coffeeresidue. Additionally, cashew nut shells, coconut shells, rice husks and almond shells were alsoinvestigated to determine their suitability for future use by Stockholm Exergi. The literature reviewshowed that there are various process parameters or parameters within the composition of thefeedstock that effects both the quality of the produced biochar, product distributions, and benefitswithin the system. To quantify the effect of all the parameters proved difficult due to the lack ofdata. However, enough data regarding the effects of the treatment temperature was collected andcould be used for modelling. Model 1 showed that biochar produced from nutshells generallyproduced biochar of higher quality than biochar made from kernels, different types of straw andfeedstocks with high content of water and ash. Most nutshells would, according to the conductedcase study, be more suited for processes where the primary objective is production of bio-oil. Model2 showed that almond shells and olive kernels should generate a thermally self-sustaining process attemperatures above 400 °C.</p>

corrected abstract:
<p>This report investigates the feasibility of several types of biomass to be used as feedstock for production of biochar by slow pyrolysis. A literature review and case studies for all investigated feedstocks resulted in two models: one for the characterization of physical and chemical properties of biochar at different high treatment temperatures, and the other for determining to what degree the system will be thermally self-sustaining, if at all. This by determining the energy required by the reactor in comparison to the energy available in the pyrolysis gas. The primary investigated feedstocks were: fibre sludge, lignin pellets, olive wastes, sunflower seeds and exhausted coffee residue. Additionally, cashew nut shells, coconut shells, rice husks and almond shells were also investigated to determine their suitability for future use by Stockholm Exergi. The literature review showed that there are various process parameters or parameters within the composition of the feedstock that effects both the quality of the produced biochar, product distributions, and benefits within the system. To quantify the effect of all the parameters proved difficult due to the lack of data. However, enough data regarding the effects of the treatment temperature was collected and could be used for modelling. Model 1 showed that biochar produced from nutshells generally produced biochar of higher quality than biochar made from kernels, different types of straw and feedstocks with high content of water and ash. Most nutshells would, according to the conducted case study, be more suited for processes where the primary objective is production of bio-oil. Model 2 showed that almond shells and olive kernels should generate a thermally self-sustaining process at temperatures above 400 °C.</p>
----------------------------------------------------------------------
title: "Production and evaluation of spider silk proteins forsortase A-mediated functionalization"
==>    "Production and evaluation of spider silk proteins for sortase A-mediated functionalization"


In diva2:1455140 abstract is: <p>The recombinant spider silk protein 4RepCT has been functionalized in many ways throughthe years. Sortase A coupling is an important method used when proteins or binders cannot beexpressed as fusion proteins with 4RepCT, because of size, complexity or if the protein is inneed of post-translational modifications. Previous studies have shown that an increasingnumber of glycine residues increase the yield and rate of the enzymatic coupling. The projectaim was to investigate two new silk proteins, 4RepCT-Srt and G5-4RepCT, to be able toevaluate if the sortase A coupling could be improved. The IgG binding domain Z(containingthree N-terminal glycine residues) was coupled both in solution and on coatings of 4RepCTSrt,both on coatings of 4RepCT-Srt and in solution using SDS-PAGE. IgG-fluorophore wasused to be able to detect and get an estimation of coupled product (4RepCT-Z), and SDSPAGEwas used to estimate the amount and rate of product formation, which was comparedto the wild type silk, G-4RepCT. The G5-4RepCT had to be re-cloned, due to difficulties inexpression and purification of the previous construct. The results show that three or moreglycines improve the rate of coupling, and 4RepCT-Srt works at least as efficient as the4RepCT. Improved sortase coupling could be beneficial for future cancer immunotherapyusing 4RepCT silk, where sortase can be used to covalently attach antibody fragments to thesilk.</p>

w='inexpression' val={'c': 'in expression', 's': 'diva2:1455140', 'n': 'correct in original'}

corrected abstract:
<p>The recombinant spider silk protein 4RepCT has been functionalized in many ways through the years. Sortase A coupling is an important method used when proteins or binders cannot be expressed as fusion proteins with 4RepCT, because of size, complexity or if the protein is in need of post-translational modifications. Previous studies have shown that an increasing number of glycine residues increase the yield and rate of the enzymatic coupling. The project aim was to investigate two new silk proteins, 4RepCT-Srt and G<sub>5</sub>-4RepCT, to be able to evaluate if the sortase A coupling could be improved. The IgG binding domain Z(containing three N-terminal glycine residues) was coupled both in solution and on coatings of 4RepCTSrt, both on coatings of 4RepCT-Srt and in solution using SDS-PAGE. IgG-fluorophore was used to be able to detect and get an estimation of coupled product (4RepCT-Z), and SDS-PAGE was used to estimate the amount and rate of product formation, which was compared to the wild type silk, G-4RepCT. The G<sub>5</sub>-4RepCT had to be re-cloned, due to difficulties in expression and purification of the previous construct. The results show that three or more glycines improve the rate of coupling, and 4RepCT-Srt works at least as efficient as the 4RepCT. Improved sortase coupling could be beneficial for future cancer immunotherapy using 4RepCT silk, where sortase can be used to covalently attach antibody fragments to the silk.</p>
----------------------------------------------------------------------
title: "Production and evaluation ofspider silk protein for sortase Amediatedfunctionalization"
==>    "Production and evaluation of spider silk protein for sortase A mediated functionalization"

This version without full text seems to be the same thesis as diva2:1455140
----------------------------------------------------------------------
In diva2:706730 abstract is: <p>The aim of this thesis was to analyze and evaluate possible matches betweeninvoices and assets from two different systems. This degree project was done incooperation with DGC, a network operator with an network stretching beyondthe borders of Sweden. The network solutions are provided through rentingthe copper and fiber connections (access services) from network owners andusing these to connect customers to their own network backbone. As a firststep towards a clearer overview of which customer pays for what, there was aneed to connect incomes generated from a paying customer with the outcomegenerated for renting an asset.  The approach taken to solve the problem involved analyzing the theory offuzzy logic and developing system support for matching using fuzzy logic ina .NET Windows Presentation Foundation application using the Model ViewViewModel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly createpermanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map theinvoices to the assets tremendously. This will make future steps in automatedcost analysis possible, giving the company a better overview of their financesand helping them to more easily find strange occurrences where an asset is beingrented without a customer paying for it, or vice versa.</p>

partal corrected: diva2:706730: <p>The aim of this thesis was to analyze and evaluate possible matches between invoices and assets from two different systems. This degree project was done in cooperation with DGC, a network operator with an network stretching beyond the borders of Sweden. The network solutions are provided through renting the copper and fiber connections (access services) from network owners and using these to connect customers to their own network backbone. As a first step towards a clearer overview of which customer pays for what, there was a need to connect incomes generated from a paying customer with the outcome generated for renting an asset.  The approach taken to solve the problem involved analyzing the theory of fuzzy logic and developing system support for matching using fuzzy logic in a .NET Windows Presentation Foundation application using the Model ViewViewModel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly create permanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map the invoices to the assets tremendously. This will make future steps in automated cost analysis possible, giving the company a better overview of their fin ancesand helping them to more easily find strange occurrences where an asset is be ingrented without a customer paying for it, or vice versa.</p>
w='ViewViewModel' val={'c': 'Model–view–viewmodel', 's': 'diva2:706730', 'n': 'no full text'}

corrected abstract:
<p>The aim of this thesis was to analyze and evaluate possible matches between invoices and assets from two different systems. This degree project was done in cooperation with DGC, a network operator with an network stretching beyond the borders of Sweden. The network solutions are provided through renting the copper and fiber connections (access services) from network owners and using these to connect customers to their own network backbone. As a first step towards a clearer overview of which customer pays for what, there was a need to connect incomes generated from a paying customer with the outcome generated for renting an asset.  The approach taken to solve the problem involved analyzing the theory of fuzzy logic and developing system support for matching using fuzzy logic in a .NET Windows Presentation Foundation application using the Model Model–view–viewmodel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly create permanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map the invoices to the assets tremendously. This will make future steps in automated cost analysis possible, giving the company a better overview of their fin ancesand helping them to more easily find strange occurrences where an asset is be ingrented without a customer paying for it, or vice versa.</p>
In diva2:706730 abstract is: <p>The aim of this thesis was to analyze and evaluate possible matches betweeninvoices and assets from two different systems. This degree project was done incooperation with DGC, a network operator with an network stretching beyondthe borders of Sweden. The network solutions are provided through rentingthe copper and fiber connections (access services) from network owners andusing these to connect customers to their own network backbone. As a firststep towards a clearer overview of which customer pays for what, there was aneed to connect incomes generated from a paying customer with the outcomegenerated for renting an asset.  The approach taken to solve the problem involved analyzing the theory offuzzy logic and developing system support for matching using fuzzy logic ina .NET Windows Presentation Foundation application using the Model ViewViewModel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly createpermanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map theinvoices to the assets tremendously. This will make future steps in automatedcost analysis possible, giving the company a better overview of their financesand helping them to more easily find strange occurrences where an asset is beingrented without a customer paying for it, or vice versa.</p>

w='ViewViewModel' val={'c': 'Model–view–viewmodel', 's': 'diva2:706730', 'n': 'no full text'}

corrected abstract:
<p>The aim of this thesis was to analyze and evaluate possible matches between invoices and assets from two different systems. This degree project was done in cooperation with DGC, a network operator with an network stretching beyond the borders of Sweden. The network solutions are provided through renting the copper and fiber connections (access services) from network owners and using these to connect customers to their own network backbone. As a first step towards a clearer overview of which customer pays for what, there was a need to connect incomes generated from a paying customer with the outcome generated for renting an asset.  The approach taken to solve the problem involved analyzing the theory of fuzzy logic and developing system support for matching using fuzzy logic in a .NET Windows Presentation Foundation application using the Model Model–view–viewmodel design pattern.  The result became a fully functional implementation, integrated in an alreadyexisting tool called Oasis. This new tool allows users to quickly create permanent mappings between invoices and assets.  This solution will decrease the time it would have taken to manually map the invoices to the assets tremendously. This will make future steps in automated cost analysis possible, giving the company a better overview of their finances and helping them to more easily find strange occurrences where an asset is being rented without a customer paying for it, or vice versa.</p>
----------------------------------------------------------------------
In diva2:1142793 abstract is: <p>Gas sensing in medical applications requiresmall, precise and sensitive sensors. This projecthas developed a laboratory setup for characterisationof a waveguide-based gas sensor for carbon dioxide andmethane working in the mid-IR range of 2 - 10 μm. Thissetup utilizes an IR-camera to image the waveguideswhen a mid-IR laser is coupled into them. Along thelaboratory work, a program for optimisation of waveguidelength has been made and a study of on-marketmedical carbon dioxide sensors has been done. Thelaboratory setup shows potential for good measurementof waveguide losses, but several problems was identifiedwith the measurement methods currently used. Fromthe sensor study, the standard performance for currentsensors is presented as well as areas where gas sensorscould be improved. Size, speed and accuracy were someof the characteristics a waveguide-based sensor couldimprove on and open up for new sensor application in,for example, hand-held medical devices.</p>


The PDF abstract uses Unicode Character “µ” (U+00B5) - while the version below uses Unicode Character “μ” (U+03BC).

corrected abstract:
<p>Gas sensing in medical applications require small, precise and sensitive sensors. This project has developed a laboratory setup for characterisation of a waveguide-based gas sensor for carbon dioxide and methane working in the mid-IR range of 2 - 10 μm. This setup utilizes an IR-camera to image the waveguides when a mid-IR laser is coupled into them. Along the laboratory work, a program for optimisation of waveguide length has been made and a study of on-market medical carbon dioxide sensors has been done. The laboratory setup shows potential for good measurement of waveguide losses, but several problems was identified with the measurement methods currently used. From the sensor study, the standard performance for current sensors is presented as well as areas where gas sensors could be improved. Size, speed and accuracy were some of the characteristics a waveguide-based sensor could improve on and open up for new sensor application in, for example, hand-held medical devices.</p>
----------------------------------------------------------------------
In diva2:1454472 abstract is: <p>Growing concerns of the negative effects on the environment and dependency of fossil fuelsare major driving forces for finding novel sustainable production pathways for plastic.Metabolic engineering has emerged as a powerful tool to enable microorganisms to producenon-native metabolites. The aim of this project was recombinant production of 4-hydroxybutyrate (4-HB) by expressing two enzymes in the model organism Escherichia coli.α-ketoglutarate decarboxylase (SucA) from Mycobacterium smegmatis followed by 4-hydroxybutyrate dehydrogenase (4-HBd) from Clostridium kluyveri was expressed inEscherichia coli. Results showed that the genes were successfully transformed and expressedin E. coli and after protein purification a concentration of 0.9 g/L SucA and 9.8 g/L 4-HBdwas achieved. Furthermore, some protein activity was detected by a coupled reaction withSucA and 4-HBd. When the enzymes got coupled together a change in NADH concentrationcould be detected spectrophotometrically. The enzymes were also tested for substratespecificity by using substrates with various carbon chain lengths and a decrease in NADHconcentration was seen. However, a decrease in the negative control for the experiments wasalso seen indicating a breakdown of NADH over time rather than consumption. Therefore, noconclusion could be drawn about the promiscuity of the enzymes. Lastly a single plasmidssystem was tested where both the genes were ligated on the same plasmid (pCDF duet) andexpressed successfully in E. coli Bl21DE3.</p>

corrected abstract:
<p>Growing concerns of the negative effects on the environment and dependency of fossil fuels are major driving forces for finding novel sustainable production pathways for plastic. Metabolic engineering has emerged as a powerful tool to enable microorganisms to produce non-native metabolites. The aim of this project was recombinant production of 4-hydroxybutyrate (4-HB) by expressing two enzymes in the model organism <em>Escherichia coli</em>. α-ketoglutarate decarboxylase (SucA) from <em>Mycobacterium smegmatis</em> followed by 4-hydroxybutyrate dehydrogenase (4-HBd) from Clostridium kluyveri was expressed in <em>Escherichia coli</em>. Results showed that the genes were successfully transformed and expressed in <em>E. coli</em> and after protein purification a concentration of 0.9 g/L SucA and 9.8 g/L 4-HBd was achieved. Furthermore, some protein activity was detected by a coupled reaction with SucA and 4-HBd. When the enzymes got coupled together a change in NADH concentration could be detected spectrophotometrically. The enzymes were also tested for substrate specificity by using substrates with various carbon chain lengths and a decrease in NADH concentration was seen. However, a decrease in the negative control for the experiments was also seen indicating a breakdown of NADH over time rather than consumption. Therefore, no conclusion could be drawn about the promiscuity of the enzymes. Lastly a single plasmids system was tested where both the genes were ligated on the same plasmid (pCDF duet) and expressed successfully in <em>E. coli</em> Bl21DE3.</p>
----------------------------------------------------------------------
In diva2:1553336 abstract is: <p>Muscle morphological parameters such as fascicle length (FL), pennationangle (PA) and physiologic cross-sectional area (PCSA) can provide an insightinto the reasons of the deteriorated muscle functions caused by pathologies.This study investigates the 3D structure of the lower leg muscles using 3Dfreehand ultrasound (3DfUS). This imaging modality uses a motion capturesystem to track the position of the US probe during acquisition and thusreconstruct the structure of the tissues in 3D. In this study, two subjects werescanned on the medial gastrocnemius (MG) and tibialis anterior (TA) musclesin the lower leg using 3DfUS system. The FL and PA of the muscles werecalculated and compared with the values previously measured using diffusiontensor imaging (DTI). The results using 3DfUS were averagely 19.2% largerin FL and 2.9%larger in PA. In conclusion, 3DfUS can successfully determinemuscle morphological parameters within a physiologically acceptable range.But the differences in FL observed between the two imaging modalities werequite big, which probably was due to the differences in sample size and area.The values can also differ greatly within the 3DfUS measurements as a resultof different manipulations during data processing, and the 3DfUS protocolneeds to be further improved in future studies.</p>


w='pennationangle' val={'c': 'pennation angle', 's': 'diva2:1553336', 'n': 'there was a new line between the words'}

corrected abstract:
<p>Muscle morphological parameters such as fascicle length (FL), pennation angle (PA) and physiologic cross-sectional area (PCSA) can provide an insight into the reasons of the deteriorated muscle functions caused by pathologies. This study investigates the 3D structure of the lower leg muscles using 3D freehand ultrasound (3DfUS). This imaging modality uses a motion capture system to track the position of the US probe during acquisition and thus reconstruct the structure of the tissues in 3D. In this study, two subjects were scanned on the medial gastrocnemius (MG) and tibialis anterior (TA) muscles in the lower leg using 3DfUS system. The FL and PA of the muscles were calculated and compared with the values previously measured using diffusion tensor imaging (DTI). The results using 3DfUS were averagely 19.2% larger in FL and 2.9% larger in PA. In conclusion, 3DfUS can successfully determine muscle morphological parameters within a physiologically acceptable range. But the differences in FL observed between the two imaging modalities were quite big, which probably was due to the differences in sample size and area. The values can also differ greatly within the 3DfUS measurements as a result of different manipulations during data processing, and the 3DfUS protocol needs to be further improved in future studies.</p>
----------------------------------------------------------------------
In diva2:1451620 abstract is: <p>Abstract</p><p>In recent years, interest has increased for so-called chatbot agents which uses patternrecognition to provide solutions in the customer service area that offer businessbenefits in the form of reduced staff costs combined with high customer accessibility.In order to optimize customer benefit, frequent maintenance is required. The accuracyof the system is increased by adapting chatbot response based on previous conversations.In order to investigate further improvement measures in pattern recognitionfor chatbot systems, a comparative study is conducted between two differenttechniques. The aim of the study is to determine which of the two techniques thatinterpret the intent of a question being asked most accurately.Several prototypes have been developed during the study, to evaluate the techniqueslemmatization and stemming for the Swedish and English language.The result show that lemmatization and stemming techniques for the Swedish languagecan be used to increase the accuracy of a pattern recognition system in textclassification and that lemmatization do so to a larger extent than stemming. Prototypestested with the English language show evidence which suggests that lemmatizationincrease the level of accuracy in comparison with the use of stemming, whichitself did not increase the accuracy.</p><p>Keywords</p><p>Conversational AI, NLP, NLU, Rasa, Chatbot-optimization, Chatbot, Text classification,lemmatization, stemming</p>



corrected abstract:
<p>In recent years, interest has increased for so-called chatbot agents which uses pattern recognition to provide solutions in the customer service area that offer business benefits in the form of reduced staff costs combined with high customer accessibility.</p><p>In order to optimize customer benefit, frequent maintenance is required. The accuracy of the system is increased by adapting chatbot response based on previous conversations. In order to investigate further improvement measures in pattern recognition for chatbot systems, a comparative study is conducted between two different techniques. The aim of the study is to determine which of the two techniques that interpret the intent of a question being asked most accurately.</p><p>Several prototypes have been developed during the study, to evaluate the techniques lemmatization and stemming for the Swedish and English language.</p><p>The result show that lemmatization and stemming techniques for the Swedish language can be used to increase the accuracy of a pattern recognition system in text classification and that lemmatization do so to a larger extent than stemming. Prototypes tested with the English language show evidence which suggests that lemmatization increase the level of accuracy in comparison with the use of stemming, which itself did not increase the accuracy.</p>
----------------------------------------------------------------------
In diva2:1688735 abstract is: <p>The spread of the Covid-19 has led to a worldwide pandemic which altered many aspects ofhuman life including higher education. As a result of a worldwide pandemic the educationsystem shifted from university campuses to virtual setups. This shift had a major impact onthe faculty members and professors teaching across the globe. While there the phenomenonwas being studied from students‟ perspective, this study highlights the impact of digitalteaching on the university professors at two universities in Sweden and Pakistan.The aim of this thesis project was to conduct a comparative study which explores howuniversity professors adapted their work environment in light of COVID-19 and e-learning.The universities primarily being studied are KTH Royal Institute of Technology situated inStockholm, Sweden and National University of Science and Technology situated inIslamabad, Pakistan. The ambition was also to discern measures to cater positive health andwork environment, and a diverse knowledge pool of best practices through a qualitativeinterview based study. These findings were generated through inductive reasoning byanalyzing eleven interviews conducted in both countries. The discussion was steered by theHuman, Technology, and Organization (HTO) - Model and concepts of ResilienceEngineering.</p>

corrected abstract:
<p>The spread of the Covid-19 has led to a worldwide pandemic which altered many aspects of human life including higher education. As a result of a worldwide pandemic the education system shifted from university campuses to virtual setups. This shift had a major impact on the faculty members and professors teaching across the globe. While there the phenomenon was being studied from students‟ perspective, this study highlights the impact of digital teaching on the university professors at two universities in Sweden and Pakistan. The aim of this thesis project was to conduct a comparative study which explores how university professors adapted their work environment in light of COVID-19 and e-learning. The universities primarily being studied are KTH Royal Institute of Technology situated in Stockholm, Sweden and National University of Science and Technology situated in Islamabad, Pakistan. The ambition was also to discern measures to cater positive health and work environment, and a diverse knowledge pool of best practices through a qualitative interview based study. These findings were generated through inductive reasoning by analyzing eleven interviews conducted in both countries. The discussion was steered by the Human, Technology, and Organization (HTO) - Model and concepts of Resilience Engineering.</p>
----------------------------------------------------------------------
In diva2:949728 abstract is: <p>Patients with cerebral palsy account for great upper extremities deviationswhile walking. However, the number of studies assessing their upper bodygait kinematics are rare and no studies have been conducted interested inthe whole body kinematics during walking. In this study, we created awhole body index, the Body Profile Score made of modified existing kinematicindexes assessing the gait pattern of children with cerebral palsy. TheBody Profile Score (BPS) is an average of combination of the Gait ProfileScore (GPS), a modified Trunk Profile Score (TPS), a modified Arm PostureScore (APS) and a also new index called Head Profile Score (HPS), basedon a similar calculation. Dierent versions of the BPS were tested on threegroups: a control group, a CP group before botulinum toxin A treatmentand a CP group after botulinum toxin A treatment. The results showed apoor level of linear correlations between the dierent BPS versions and theGait Profile Score, indicating that lower body indexes such as the GPS orGait Deviation Index (GDI) and full body index such as the BPS do not renderthe same information. The BPS is the first index proposing a full bodykinematic analysis and aims at showing that such an analysis is needed ingait assessment of spastic children in order to have a realistic overview ofthe pathological walking condition.</p>


w='Dierent' val={'c': 'Different', 's': ['diva2:1344757', 'diva2:949728'], 'n': 'missing ligature'}

corrected abstract:
<p>Patients with cerebral palsy account for great upper extremities deviations while walking. However, the number of studies assessing their upper body gait kinematics are rare and no studies have been conducted interested in the whole body kinematics during walking. In this study, we created a whole body index, the Body Profile Score made of modified existing kinematic indexes assessing the gait pattern of children with cerebral palsy. The Body Profile Score (BPS) is an average of combination of the Gait Profile Score (GPS), a modified Trunk Profile Score (TPS), a modified Arm Posture Score (APS) and a also new index called Head Profile Score (HPS), based on a similar calculation. Different versions of the BPS were tested on three groups: a control group, a CP group before botulinum toxin A treatment and a CP group after botulinum toxin A treatment. The results showed a poor level of linear correlations between the different BPS versions and the Gait Profile Score, indicating that lower body indexes such as the GPS or Gait Deviation Index (GDI) and full body index such as the BPS do not render the same information. The BPS is the first index proposing a full body kinematic analysis and aims at showing that such an analysis is needed in gait assessment of spastic children in order to have a realistic overview of the pathological walking condition.</p>
----------------------------------------------------------------------
In diva2:1451610 abstract is: <p>Abstract</p><p>Fingerprinting a website is the process of identifying what technologies a websiteuses, such as their used web applications and JavaScript frameworks. Currentfingerprinting methods use manually created fingerprints for each technology itlooks for. These fingerprints consist of multiple text strings that are matchedagainst an HTTP response from a website. Creating these fingerprints for eachtechnology can be time-consuming, which limits what technologies fingerprints canbe built for. This thesis presents a potential solution by utilizing unsupervisedmachine learning techniques to cluster websites by their used web application andJavaScript frameworks, without requiring manually created fingerprints. Oursolution uses multiple bag-of-words models combined with the dimensionalityreduction technique t-SNE and clustering algorithm OPTICS. Results show thatsome technologies, for example, Drupal, achieve a precision of 0.731 and recall of0.485 without any training data. These results lead to the conclusion that theproposed solution could plausibly be used to cluster websites by their webapplication and JavaScript frameworks in use. However, further work is needed toincrease the precision and recall of the results.</p><p>Keywords</p><p>Clustering, fingerprinting, OPTICS, t-SNE, headless browser, bag-of-words,unsupervised machine learning</p>


corrected abstract:
<p>Fingerprinting a website is the process of identifying what technologies a website uses, such as their used web applications and JavaScript frameworks. Current fingerprinting methods use manually created fingerprints for each technology it looks for. These fingerprints consist of multiple text strings that are matched against an HTTP response from a website. Creating these fingerprints for each technology can be time-consuming, which limits what technologies fingerprints can be built for. This thesis presents a potential solution by utilizing unsupervised machine learning techniques to cluster websites by their used web application and JavaScript frameworks, without requiring manually created fingerprints. Our solution uses multiple bag-of-words models combined with the dimensionality reduction technique t-SNE and clustering algorithm OPTICS. Results show that some technologies, for example, Drupal, achieve a precision of 0.731 and recall of 0.485 without any training data. These results lead to the conclusion that the proposed solution could plausibly be used to cluster websites by their web application and JavaScript frameworks in use. However, further work is needed to increase the precision and recall of the results.</p>
----------------------------------------------------------------------
In diva2:1887799 abstract is: <p>Exoskeletons have gone through an unprecedented evolution resulting in vast evaluation approaches inresearch. Within several work sectors and industries the exoskeletons show a potential in having an impact onwork-related musculoskeletal disorders by the ability to reduce musculoskeletal load.</p><p>In the context of the construction sector, field assessments of exoskeletons are lacking. This study will try toexplore the current research gap by assessing a passive upper limb supporting exoskeleton in a constructionwork setting from conducting a surface electromyographic measurement and administering a usability questionnaire.</p><p>Preliminary results suggest that the exoskeleton evaluated tends to reduce muscle activation but may notprovide sufficient rest time for the users to not be in the risk zone of developing MSDs. The results also showthat preliminary usability ratings are ‘Excellent’ but decreases to ‘Good’ or ‘OK’ when more time is spentwearing and using the exoskeleton. This gives an indication of a higher acceptability of the system in shortertime of usage. When considering individual differences, the exoskeleton shows a variety in the impact onmuscle activation as well as perceived usability, which aligns with previous literature. The results also hints thatfactors connected to usability such as, comfort or adaptability which might facilitate the use of exoskeletoncould be important to consider when implementing an exoskeleton, but also in early design or developmentstages.</p><p>To the knowledge of the author, there is no literature in the same spirit as this one. The fact that this is a pilotstudy provides a unique exploratory research perspective into a field assessment conducted in a large-scaleconstruction project.</p>


corrected abstract:
<p>Exoskeletons have gone through an unprecedented evolution resulting in vast evaluation approaches in research. Within several work sectors and industries the exoskeletons show a potential in having an impact on work-related musculoskeletal disorders by the ability to reduce musculoskeletal load.</p><p>In the context of the construction sector, field assessments of exoskeletons are lacking. This study will try to explore the current research gap by assessing a passive upper limb supporting exoskeleton in a construction work setting from conducting a surface electromyographic measurement and administering a usability questionnaire.</p><p>Preliminary results suggest that the exoskeleton evaluated tends to reduce muscle activation but may not provide sufficient rest time for the users to not be in the risk zone of developing MSDs. The results also show that preliminary usability ratings are ‘Excellent’ but decreases to ‘Good’ or ‘OK’ when more time is spent wearing and using the exoskeleton. This gives an indication of a higher acceptability of the system in shorter time of usage. When considering individual differences, the exoskeleton shows a variety in the impact on muscle activation as well as perceived usability, which aligns with previous literature. The results also hints that factors connected to usability such as, comfort or adaptability which might facilitate the use of exoskeleton could be important to consider when implementing an exoskeleton, but also in early design or development stages.</p><p>To the knowledge of the author, there is no literature in the same spirit as this one. The fact that this is a pilot study provides a unique exploratory research perspective into a field assessment conducted in a large-scale construction project.</p>
----------------------------------------------------------------------
In diva2:1298660 abstract is: <p>While the rodent model has long been used in brain research, there exists no standardisedprocessing routine that can be employed for analysis and investigation of disease models. Thepresent thesis attempts to investigate a diseased brain model by implementing a collection ofscripts, combined with algorithms from existing neuroimaging software, and adapting themto the rodent brain, in an attempt to examine when and how monaural canal atresia affectsthe functional connectivity of the brain. We show that it is possible to use software tailoredto the human brain to pre-process the rodent model. Following conventional pipelines andresting state functional MRI (rs-fMRI)-specific strategies, the developed processing routineimplements the most basic steps suggested in literature. On the single-subject level, skullstripping was done using Mialite software, motion correction and distortion correction werebased on FMRIB software library (FSL) algorithms and motion artefacts were removed usingICA-based Automatic Removal Of Motion Artifacts (ICA-AROMA). Following denoising,normalisation to standard space, smoothing and temporal filtering, group level analysis wasperformed. A univariate, hypothesis-driven method and a multivariate, data-driven methodwere used for group comparison and statistical inference. While seed-based correlationanalysis (SCA) did not return any significant results, independent component analysis (ICA) identified two components that show activation in areas of interest.</p>


corrected abstract:
<p>While the rodent model has long been used in brain research, there exists no standardised processing routine that can be employed for analysis and investigation of disease models. The present thesis attempts to investigate a diseased brain model by implementing a collection of scripts, combined with algorithms from existing neuroimaging software, and adapting them to the rodent brain, in an attempt to examine when and how monaural canal atresia affects the functional connectivity of the brain. We show that it is possible to use software tailored to the human brain to pre-process the rodent model. Following conventional pipelines and resting state functional MRI (rs-fMRI)-specific strategies, the developed processing routine implements the most basic steps suggested in literature. On the single-subject level, skull stripping was done using Mialite software, motion correction and distortion correction were based on FMRIB software library (FSL) algorithms and motion artefacts were removed using ICA-based Automatic Removal Of Motion Artifacts (ICA-AROMA). Following denoising, normalisation to standard space, smoothing and temporal filtering, group level analysis was performed. A univariate, hypothesis-driven method and a multivariate, data-driven method were used for group comparison and statistical inference. While seed-based correlation analysis (SCA) did not return any significant results, independent component analysis (ICA) identified two components that show activation in areas of interest.</p>
----------------------------------------------------------------------
title: "Client controlled, secure endpointto-endpoint storage in the cloud"
==>    "Client controlled, secure endpoint-to-endpoint storage in the cloud"

In diva2:1451799 abstract is: <p>Abstract</p><p>Softronic's customers do not want them to store sensitive data in a cloud environment as theydistrust the cloud providers with keeping sensitive data secret and are afraid of violating GDPR.Softronic wants to prove that data can be kept protected using encryption, even though it is storedin a cloud, and the goal of this thesis is to find a cryptographic solution with good security and performance.</p><p>The chosen solution was to implement object-level encryption with both encryption and decryptiondone on-site at Softronic with the cloud provider kept outside of the encryption process. Encrypteddata can then safely be stored in the cloud and decrypted on demand on-site again.</p><p>The cryptography used in the solution was determined after multiple evaluations comparingencryption algorithms and the effects of key lengths, block sizes, and modes of operation. Theevaluations showed big performance differences between encryption algorithms as well as fordifferent encryption modes, where the biggest difference was between those with and withoutintegrity checks built-in. The key length used did not affect object-level encryption performance andthe biggest key size can, therefore, be used for maximum security. The different block sizes did notaffect performance either, but a 128-bit one, as opposed to a 64-bit one, requires less maintenance,as key rotations are not required as frequently.</p><p>The secure transport protocol, TLS, performed in-transit encryption of the object-level encrypteddata as it was sent to the cloud for storage which adversely affects performance. TLS encryptionsuites were, therefore, evaluated to find the one with the smallest performance impact. Theevaluations found that the key size affected performance when doing in-transit encryption, asopposed to object-level encryption, and that the encryption suite, TLS_AES_128_GCM_SHA256,with the smallest key performed the best.</p><p>Keywords</p><p>Encryption, data protection, cloud databases, symmetric encryption, TLS, GDPR, AEAD, Crypto</p>


corrected abstract:
<p>Softronic's customers do not want them to store sensitive data in a cloud environment as they distrust the cloud providers with keeping sensitive data secret and are afraid of violating GDPR. Softronic wants to prove that data can be kept protected using encryption, even though it is stored in a cloud, and the goal of this thesis is to find a cryptographic solution with good security and performance.</p><p>The chosen solution was to implement object-level encryption with both encryption and decryption done on-site at Softronic with the cloud provider kept outside of the encryption process. Encrypted data can then safely be stored in the cloud and decrypted on demand on-site again.</p><p>The cryptography used in the solution was determined after multiple evaluations comparing encryption algorithms and the effects of key lengths, block sizes, and modes of operation. The evaluations showed big performance differences between encryption algorithms as well as for different encryption modes, where the biggest difference was between those with and without integrity checks built-in. The key length used did not affect object-level encryption performance and the biggest key size can, therefore, be used for maximum security. The different block sizes did not affect performance either, but a 128-bit one, as opposed to a 64-bit one, requires less maintenance, as key rotations are not required as frequently.</p><p>The secure transport protocol, TLS, performed in-transit encryption of the object-level encrypted data as it was sent to the cloud for storage which adversely affects performance. TLS encryption suites were, therefore, evaluated to find the one with the smallest performance impact. The evaluations found that the key size affected performance when doing in-transit encryption, as opposed to object-level encryption, and that the encryption suite, TLS_AES_128_GCM_SHA256, with the smallest key performed the best.</p>
----------------------------------------------------------------------
In diva2:1218172 abstract is: <p>In case of build out and rebuilding electricity grids, the electrical characteristicschange. This means that protection features may need to be adjusted to suit theelectrical grids’ new characteristics. Adjustments may lead to loss of selectivity betweenprotective relays. Selectivity means that only the protection device closest tothe fault break the current, so only the smallest possible part of the electrical gridgets disconnected. To assure selectivity, selective tripping schedule is created andprotective relays are adjusted if needed. Rotebro substation in Sollentuna is ownedand operated by Sollentuna Energi &amp; Miljö (SEOM). The substation is facing rebuildingand its selective tripping schedule needs to be reviewed.Since the problem is that selectivity between the protective relays needs to bechecked the goal is to create a selective tripping schedule for Rotebro substation.This is based on the substation protective settings and the connected electricalgrids characteristics. The thesis resulted in a model of Rotebro substation createdin the computer program PSS Sincal. From this a selective tripping schedule weregenerated that shows that small adjustments of the protective relays can be made.A general analysis was also conducted which shows that a selective tripping scheduleis of great importance to the society.</p>


corrected abstract:
<p>In case of build out and rebuilding electricity grids, the electrical characteristics change. This means that protection features may need to be adjusted to suit the electrical grids’ new characteristics. Adjustments may lead to loss of selectivity between protective relays. Selectivity means that only the protection device closest to the fault break the current, so only the smallest possible part of the electrical grid gets disconnected. To assure selectivity, selective tripping schedule is created and protective relays are adjusted if needed. Rotebro substation in Sollentuna is owned and operated by Sollentuna Energi &amp; Miljö (SEOM). The substation is facing rebuilding and its selective tripping schedule needs to be reviewed.</p><p>Since the problem is that selectivity between the protective relays needs to be checked the goal is to create a selective tripping schedule for Rotebro substation. This is based on the substation protective settings and the connected electrical grids characteristics. The thesis resulted in a model of Rotebro substation created in the computer program PSS Sincal. From this a selective tripping schedule were generated that shows that small adjustments of the protective relays can be made. A general analysis was also conducted which shows that a selective tripping schedule is of great importance to the society.</p>
----------------------------------------------------------------------
title: "Implementation of a MobileHealthcare Solution at an InpatientWard"
==>    "Implementation of a Mobile Healthcare Solution at an Inpatient Ward"

In diva2:1438504 abstract is: <p>Abstract</p><p>Healthcare is a complex system under great pressure for meeting the patients' needs.Implementing technology at inpatient wards might possibly support healthcare professionalsand improve quality of care. However, these technologies might come withissues and the system might not be used as intended.</p><p>This master thesis project investigates how healthcare professionals communicateat an inpatient ward and how this might be affected by implementing a MobileHealthcare Solution (MHS). Further, it sought to question why healthcare professionsmight, or might not, use the MHS as a support of their daily work and whatsome reasons for this might be. Research methods were of qualitative approach.Field studies were performed at an inpatient ward and further, two healthcare professionalswere interviewed. Grounded Theory (GT) was chosen as a method toprocess the data and obtain understanding for communication at the inpatient ward.</p><p>The results showed that healthcare professionals communicate verbally, written andby reading, using different tools. The most prominent ways of communication wereverbally, where it was common to report or discuss about a patient. The means forcommunication did not get drastically affected by implementing the MHS and reasonsfor this were of social, technical and organizational types. Some reasons for notusing the MHS were habits and due to healthcare professionals perceiving the MHSas more time consuming than manual handling. However, a specic investigation of whether this might affect the usage of the MHS is yet needed.</p><p>Keywords: Mobile Healthcare Solutions, Electronic Health Records, HealthcareInnovation System, User and Implementation, Ethnographyii</p>

w='specic' val={'c': 'specific', 's': ['diva2:1438504', 'diva2:949949'], 'n': 'missing ligature'}
w='Ethnographyii' val={'c': 'Ethnographyy', 's': 'diva2:1438504', 'n': 'correct in original - note this is a keyword and not in the abstract'}

corrected abstract:
<p>Healthcare is a complex system under great pressure for meeting the patients’ needs. Implementing technology at inpatient wards might possibly support healthcare professionals and improve quality of care. However, these technologies might come with issues and the system might not be used as intended.</p><p>This master thesis project investigates how healthcare professionals communicate at an inpatient ward and how this might be affected by implementing a Mobile Healthcare Solution (MHS). Further, it sought to question why healthcare professions might, or might not, use the MHS as a support of their daily work and what some reasons for this might be. Research methods were of qualitative approach. Field studies were performed at an inpatient ward and further, two healthcare professionals were interviewed. Grounded Theory (GT) was chosen as a method to process the data and obtain understanding for communication at the inpatient ward.</p><p>The results showed that healthcare professionals communicate verbally, written and by reading, using different tools. The most prominent ways of communication were verbally, where it was common to report or discuss about a patient. The means for communication did not get drastically affected by implementing the MHS and reasons for this were of social, technical and organizational types. Some reasons for not using the MHS were habits and due to healthcare professionals perceiving the MHS as more time consuming than manual handling. However, a specific investigation of whether this might affect the usage of the MHS is yet needed.</p>
----------------------------------------------------------------------
In diva2:1454864 abstract is: <p>Poor prognosis for high-risk neuroblastoma patients makes it necessary to find novel treatmentstrategies. This work aims to understand the cell cycle behavior of various high-riskneuroblastoma cell lines following chemotherapy treatment. Here, we mapped the expressionof cell cycle dependent proteins, p21 and p27, in seven high-risk neuroblastoma celllines. All cell lines showed an overall impaired growth following doxorubicin treatment.However, regrowth was observed in all cell lines between day 6 to 15 by forming colonies.The expression of p21 and p27 was measured in all cell lines showing an upregulationof p21 in 3 out of 5 p53 mutated cell lines while it was downregulated in the 2 cell lineswith a p53 wild type. Furthermore, inhibition assays using inhibitors of CHK1/2, p21 ,andSKP2 were performed. The results were promising as the CHK1/2 inhibitor reduced cellviability in all tested cell lines, while the p21 inhibitor had an effect in 3 out of 6 testedcell lines and the SKP2 inhibitor in 4 out of 6 tested cell lines. Confluency measurementover 15 days showed impaired growth following treatment with the CHK1/2 inhibitor for3 out of 6 tested cell lines and p21 inhibitor in 1 out of 6 tested cell lines. The obtainedresults were encouraging and might aid in finding a novel treatment strategy preventingresistance and relapse in neuroblastoma. However, further studies are needed in order tovalidate the efficacy and safety of these promising drugs in neuroblastoma patients.</p>


Note the "p21 ,and" is in the actual abstract in the thesis.

corrected abstract:
<p>Poor prognosis for high-risk neuroblastoma patients makes it necessary to find novel treatment strategies. This work aims to understand the cell cycle behavior of various high-risk neuroblastoma cell lines following chemotherapy treatment. Here, we mapped the expression of cell cycle dependent proteins, p21 and p27, in seven high-risk neuroblastoma cell lines. All cell lines showed an overall impaired growth following doxorubicin treatment. However, regrowth was observed in all cell lines between day 6 to 15 by forming colonies. The expression of p21 and p27 was measured in all cell lines showing an upregulation of p21 in 3 out of 5 p53 mutated cell lines while it was downregulated in the 2 cell lines with a p53 wild type. Furthermore, inhibition assays using inhibitors of CHK1/2, p21 ,and SKP2 were performed. The results were promising as the CHK1/2 inhibitor reduced cell viability in all tested cell lines, while the p21 inhibitor had an effect in 3 out of 6 tested cell lines and the SKP2 inhibitor in 4 out of 6 tested cell lines. Confluency measurement over 15 days showed impaired growth following treatment with the CHK1/2 inhibitor for 3 out of 6 tested cell lines and p21 inhibitor in 1 out of 6 tested cell lines. The obtained results were encouraging and might aid in finding a novel treatment strategy preventing resistance and relapse in neuroblastoma. However, further studies are needed in order to validate the efficacy and safety of these promising drugs in neuroblastoma patients.</p>
----------------------------------------------------------------------
In diva2:1472559 abstract is: <p>AbstractDevelopment and deployment of software has transitioned from being aprocess that could take years to being as short as a few hours. The reason forthis is the DevOps methodology gaining momentum in the industry and theuse of automation tools such as Jenkins. DevOps is a collaborative effortbetween development and operation of software where automation has animportant role. Security has not had a high priority due to short developmentcycles and if security testing is done within organizations, it’s usually donemanually a few months apart. Could security tests be automated and give ahigh enough bar for application security within DevOps? To answer thisquestion different types of application security testing has been done toward abenchmarking tool to test for efficiency. A software prototype has beendeveloped to measure other factors such as automation and developmenttools. The conclusion is that there are good possibilities for automated securitytests but the answer to what is best depends on the type of application to testand what organization is in question.KeywordsApplication Security Testing, SAST, DAST, IAST, SCA, DevSecOps, CI/CD, OWASP</p>

corrected abstract:
<p>Development and deployment of software has transitioned from being a process that could take years to being as short as a few hours. The reason for this is the DevOps methodology gaining momentum in the industry and the use of automation tools such as Jenkins. DevOps is a collaborative effort between development and operation of software where automation has an important role. Security has not had a high priority due to short development cycles and if security testing is done within organizations, it’s usually done manually a few months apart. Could security tests be automated and give a high enough bar for application security within DevOps? To answer this question different types of application security testing has been done toward a benchmarking tool to test for efficiency. A software prototype has been developed to measure other factors such as automation and development tools. The conclusion is that there are good possibilities for automated security tests but the answer to what is best depends on the type of application to test and what organization is in question.</p>
----------------------------------------------------------------------
title: "Design, production and evaluation of rigid helix fusion between affibody molecules and framework proteins aimed for Affibody:antigen structure determination using single-particlecryo-EM"
==>    "Design, production and evaluation of rigid helix fusion between affibody molecules and framework proteins aimed for Affibody:antigen structure determination using single-particle cryo-EM"

In diva2:1454440 abstract is: <p>Single particle cryo-electron microscopy (cryo-EM) is an emerging and growing technique forstructural analysis of proteins. However, limitations within the technique leaves the majority ofproteins too small for structural analysis by cryo-EM with high resolution. A method tocircumvent this limitation has been to rigidly link the protein of interest to a larger symmetricprotein scaffold followed by analysis of the larger complex. Preferably, the linkage should beperformed genetically via a shared rigid alpha helix junction, improving the resolution due torestricted flexibility. However, creating rigid linkages between proteins with shared fusionhelices is not trivial and requires that both proteins have a terminal helix available. To workaround this, alpha-helix containing affinity proteins such as affibodies or DARPins have beenshown amenable to fusion via shared rigid helices to different scaffolds, providing bio-affinitybasedplatforms for non-covalent capture of target proteins for structural studies by cryo-EM.In this project new designs of rigidly alpha-helix-linked affibody-scaffold fusion proteins forcryo-EM applications were designed and initially evaluated with the long-term goal to create ageneral affibody-based cryo-EM scaffold platform. Analyses of in silico designed andrecombinantly produced fusion proteins using size exclusion chromatography, massspectrometry and surface plasmon resonance show at least one promising candidate for futurestructural analysis using cryo-EM.</p>

corrected abstract:
<p>Single particle cryo-electron microscopy (cryo-EM) is an emerging and growing technique for structural analysis of proteins. However, limitations within the technique leaves the majority of proteins too small for structural analysis by cryo-EM with high resolution. A method to circumvent this limitation has been to rigidly link the protein of interest to a larger symmetric protein scaffold followed by analysis of the larger complex. Preferably, the linkage should be performed genetically via a shared rigid alpha helix junction, improving the resolution due to restricted flexibility. However, creating rigid linkages between proteins with shared fusion helices is not trivial and requires that both proteins have a terminal helix available. To work around this, alpha-helix containing affinity proteins such as affibodies or DARPins have been shown amenable to fusion via shared rigid helices to different scaffolds, providing bio-affinity-based platforms for non-covalent capture of target proteins for structural studies by cryo-EM. In this project new designs of rigidly alpha-helix-linked affibody-scaffold fusion proteins for cryo-EM applications were designed and initially evaluated with the long-term goal to create a general affibody-based cryo-EM scaffold platform. Analyses of in silico designed and recombinantly produced fusion proteins using size exclusion chromatography, mass spectrometry and surface plasmon resonance show at least one promising candidate for future structural analysis using cryo-EM.</p>
----------------------------------------------------------------------
In diva2:1275060 abstract is: <p>This report examines preoperative body wash performed by the patients themselves at home and itspotential to prevent surgical site infections. This was accomplished by studying current recommendedprotocols and active substances in Sweden in relation to a potential replacement product. Products identified and included in the report are Descutan, manufactured by Fresenius Kabi AB and Prontoderm by B. Braun Medical AB, with the active substances of chlorhexidine and polyhexanide.After comparisons, an experiment and evaluations, a new proposed protocol has been developed.The proposed protocol involves decolonization of bacteria, including multidrug resistant organisms.In that way, an alternative to handle the increased antibiotic resistance in the world is described. Inaddition, costs of the products have been compared and by avoiding surgical site infections, potentialsavings have been calculated.From the review, ambiguous results were obtained regarding the effect of preoperative body wash onsurgical site infections.In summary, there are many benefits and a lot of money to save, with the new active substance andby avoiding surgical site infections. The conclusion is that Prontoderm is more expensive, but thatthe total cost of the product kit could be reduced by adjusting the size of the products after protocoland dosage. Further, it was decided that three washes are sufficient for a full preoperative body wash.Keywords: surgical site infection, chlorhexidine, polyhexanide, decolonization, preoperative bodywash, Prontoderm, Descutan, patient protocol, multidrug resistant organisms.</p>

corrected abstract:
<p>This report examines preoperative body wash performed by the patients themselves at home and its potential to prevent surgical site infections. This was accomplished by studying current recommended protocols and active substances in Sweden in relation to a potential replacement product. Products identified and included in the report are Descutan, manufactured by Fresenius Kabi AB and Prontoderm by B. Braun Medical AB, with the active substances of chlorhexidine and polyhexanide.</p><p>After comparisons, an experiment and evaluations, a new proposed protocol has been developed. The proposed protocol involves decolonization of bacteria, including multidrug resistant organisms. In that way, an alternative to handle the increased antibiotic resistance in the world is described. In addition, costs of the products have been compared and by avoiding surgical site infections, potential savings have been calculated.</p><p>From the review, ambiguous results were obtained regarding the effect of preoperative body wash on surgical site infections.</p><p>In summary, there are many benefits and a lot of money to save, with the new active substance and by avoiding surgical site infections. The conclusion is that Prontoderm is more expensive, but that the total cost of the product kit could be reduced by adjusting the size of the products after protocol and dosage. Further, it was decided that three washes are sufficient for a full preoperative body wash.</p>
----------------------------------------------------------------------
In diva2:1764375 abstract is: <p>Limited processor and memory capacity is a major challenge for logging sensorsignals in engine control units. In order to be able to store larger amounts of data,compression can be used. To successfully implement compression algorithms inmotor control units, it is essential that the algorithms can effectively handle thelimitations associated with processor capacity while achieving an acceptable level ofcompression.This thesis compares compression algorithms on sensor data from motor controlunits in order to investigate which algorithm(s) are best suited to implement forthis application. The work aims to improve the possibilities of logging sensor dataand thus make the troubleshooting of the engine control units more efficient. Thiswas done by developing a system that performs compression on sampled sensorsignals and calculates the compression time and ratio.The results indicated that delta-of-delta compression performed better than xorcompression for the tested data sets. Delta-of-delta had a significantly bettercompression ratio while the differences between the algorithms regardingcompression time were minor. Delta-of-delta compression was judged to have goodpotential for implementation in engine control unit logging systems. The algorithmis deemed to be well suited for logging smaller time series during important events.For continuous logging of larger time series, further research is suggested in orderto investigate the possibility of improving the compression ratio further. </p>

corrected abstract:
<p>Limited processor and memory capacity is a major challenge for logging sensor signals in engine control units. In order to be able to store larger amounts of data, compression can be used. To successfully implement compression algorithms in motor control units, it is essential that the algorithms can effectively handle the limitations associated with processor capacity while achieving an acceptable level of compression.</p><p>This thesis compares compression algorithms on sensor data from motor control units in order to investigate which algorithm(s) are best suited to implement for this application. The work aims to improve the possibilities of logging sensor data and thus make the troubleshooting of the engine control units more efficient. This was done by developing a system that performs compression on sampled sensor signals and calculates the compression time and ratio.</p><p>The results indicated that delta-of-delta compression performed better than xor compression for the tested data sets. Delta-of-delta had a significantly better compression ratio while the differences between the algorithms regarding compression time were minor. Delta-of-delta compression was judged to have good potential for implementation in engine control unit logging systems. The algorithm is deemed to be well suited for logging smaller time series during important events. For continuous logging of larger time series, further research is suggested in order to investigate the possibility of improving the compression ratio further.</p>
----------------------------------------------------------------------
title: "MULTI-CENTER QUANTITATIVE MEASUREMENT OF T1 ANDT2 RELAXATION TIMES IN THE RAT BRAIN"
==>    "MULTI-CENTER QUANTITATIVE MEASUREMENT OF T1 AND T2 RELAXATION TIMES IN THE RAT BRAIN"

In diva2:1231794 abstract is: <p>This project revolves around the measurement of T1 and T2 relaxation times in the ratbrain, in a multi-center way. That is to say, elaborate an efficient protocol to calculate highresolution 3D map of the brain. This protocol should be applied in different centers andreturn similar results. Finally, procedures should be defined to ease the collaborationbetween the different centers. The first step consisted in in vitro experiments, in whichdifferent sequences were tested. It resulted that the MDEFT sequence with inversionpreparation (MPRAGE) gives the best results in the shortest time for T1. For T2, theMSME sequence was chosen. The next step moved on in vivo experiments on three rats inorder to get used to manipulating living animals and make new adjustments. As thephysiology is not the same on in vitro and in vivo experiments, some parameters had to beslightly adapted. Once the final 2h-protocol was established, it was tested on a populationof ten rats. Experiments were made at the GIN and CRMBM. Different fitting pipelineswere tried (GIN, CRMBM, MIRCEN). The brain was segmented into different regions. Itresulted that the GIN and CRMBM pipelines return the same T1 values using the differentdatasets. The MIRCEN pipeline under-estimates by 200 ms. The three pipelines return similar T2 values. The GIN and CRMBM datasets provide comparable T1 values, but theGIN center presents slightly higher T2 values. Regarding the multi-center collaboration,the different pipelines were ported to the VIP platform so that the scientific community caneasily reuse them.</p>


corrected abstract:
<p>This project revolves around the measurement of T1 and T2 relaxation times in the rat brain, in a multi-center way. That is to say, elaborate an efficient protocol to calculate high resolution 3D map of the brain. This protocol should be applied in different centers and return similar results. Finally, procedures should be defined to ease the collaboration between the different centers. The first step consisted in <em>in vitro</em> experiments, in which different sequences were tested. It resulted that the MDEFT sequence with inversion preparation (MPRAGE) gives the best results in the shortest time for T1. For T2, the MSME sequence was chosen. The next step moved on <em>in vivo</em> experiments on three rats in order to get used to manipulating living animals and make new adjustments. As the physiology is not the same on <em>in vitro</em> and <em>in vivo</em> experiments, some parameters had to be slightly adapted. Once the final 2h-protocol was established, it was tested on a population of ten rats. Experiments were made at the GIN and CRMBM. Different fitting pipelines were tried (GIN, CRMBM, MIRCEN). The brain was segmented into different regions. It resulted that the GIN and CRMBM pipelines return the same T1 values using the different datasets. The MIRCEN pipeline under-estimates by 200 ms. The three pipelines return similar T2 values. The GIN and CRMBM datasets provide comparable T1 values, but the GIN center presents slightly higher T2 values. Regarding the multi-center collaboration, the different pipelines were ported to the VIP platform so that the scientific community can easily reuse them.</p>
----------------------------------------------------------------------
title: "Reduction of selected pharmaceutical substances byfungi isolated from wastewater"
==>    "Reduction of selected pharmaceutical substances by fungi isolated from wastewater"

In diva2:1218610 abstract is: <p>In recent years, adverse effects have been shown in marine living organisms due to pharmaceuticalsubstances released into nature. The emission of pharmaceutical substances into the environmentis mainly due to excretion by humans, which conventional wastewater treatment technologies arenot always capable of removing. Previous studies have observed fungi as promising treatmentagents for removal of pharmaceutical substances. However, the optimum removal efficiency forthe fungi has been observed at pH ranging from 3.5 to 5.5, while pH of the wastewater rangesbetween 6.5 – 10.This study has investigated the ability of removing pharmaceutical substances by using fungiisolated from a sewage wastewater sample. Initially ten fungal isolates were cultivated in presenceof carbamazepine, diclofenac, ibuprofen and sulfamethoxazol and all ten isolates tolerated a totalpharmaceutical concentration of 5 mg/l. Further, the ability of the isolated fungi to reducecarbamazepine and diclofenac was investigated and Trametes versicolor was used as control fungus.Isolate 6, identified to Aspergillus luchuensis, showed a complete reduction of diclofenac after 10 days,cultivated in a medium with an initial pH of 6.3. Furthermore, A. luchuensis was also observed toreduce diclofenac faster compared to, the previous studied fungi, T. versicolor. From what is known,these results have not been observed in any previous studies. In conclusion, this study has foundA. luchuensis as a promising treatment agent for removal of pharmaceuticals, but also that there areother fungi present in wastewater that can be further studied for this purpose.</p>

w='sulfamethoxazol' val={'c': 'sulfamethoxazole', 's': 'diva2:1218610', 'n': 'no full text'}

Assuming that the orginal had properly italicized species.
corrected abstract:
<p>In recent years, adverse effects have been shown in marine living organisms due to pharmaceutical substances released into nature. The emission of pharmaceutical substances into the environment is mainly due to excretion by humans, which conventional wastewater treatment technologies are not always capable of removing. Previous studies have observed fungi as promising treatment agents for removal of pharmaceutical substances. However, the optimum removal efficiency for the fungi has been observed at pH ranging from 3.5 to 5.5, while pH of the wastewater ranges between 6.5 – 10. This study has investigated the ability of removing pharmaceutical substances by using fungi isolated from a sewage wastewater sample. Initially ten fungal isolates were cultivated in presence of carbamazepine, diclofenac, ibuprofen and sulfamethoxazole and all ten isolates tolerated a total pharmaceutical concentration of 5 mg/l. Further, the ability of the isolated fungi to reduce carbamazepine and diclofenac was investigated and <em>Trametes versicolor</em> was used as control fungus. Isolate 6, identified to <em>Aspergillus luchuensis</em>, showed a complete reduction of diclofenac after 10 days, cultivated in a medium with an initial pH of 6.3. Furthermore, <em>A. luchuensis</em> was also observed to reduce diclofenac faster compared to, the previous studied fungi, <em>T. versicolor</em>. From what is known, these results have not been observed in any previous studies. In conclusion, this study has found <em>A. luchuensis</em> as a promising treatment agent for removal of pharmaceuticals, but also that there are other fungi present in wastewater that can be further studied for this purpose.</p>
----------------------------------------------------------------------
In diva2:1225421 abstract is: <p>The primary motivation of this bachelor thesis was to address the acute lack of affordablemedical technologies in low-resource settings in order to reduce child mortality in theneonatal period. It is essential to develop affordable medical devices for empowering healthworkers and village doctors and train them to use smart devices and appropriate ICT tools,and connect them to the medical experts to manage the serious health problems. The workincludes: a) developing programs for safe use of devices and manage consultation with themedical experts and b) developing appropriate e-Learning content on health education fordisease prevention.</p><p>A handheld, safe and user friendly heart rate monitor has been developed using smart andlow cost sensors. The device consists of two major parts. First, low cost sensors (piezo andoptical) interfaced with analog front-end circuits including capacitors, resistors andamplifiers. Second, the filtered and amplified signal is digitally processed and converted toprovide statistically significant information about the patient’s heart rate and presentirregularity report in the heart activity if any. The piezoelectric sensor head is placed onthe point where the radial artery crosses the bones of the wrist, whereas the photoelectricsensor head is placed on a fingertip in order to detect the arterial pulse rate.</p><p>At present, the interpretation of sensor readings is focused towards determining the heartrate and dynamics of heart functional characteristics. This low cost handheld device isbeing further enhanced with more sensors to provide additional relevant vital parametersfor quality treatment guidelines.</p>

corrected abstract:
<p>The primary motivation of this bachelor thesis was to address the acute lack of affordable medical technologies in low-resource settings in order to reduce child mortality in the neonatal period. It is essential to develop affordable medical devices for empowering health workers and village doctors and train them to use smart devices and appropriate ICT tools, and connect them to the medical experts to manage the serious health problems. The work includes: a) developing programs for safe use of devices and manage consultation with the medical experts and b) developing appropriate e-Learning content on health education for disease prevention.</p><p>A handheld, safe and user friendly heart rate monitor has been developed using smart and low cost sensors. The device consists of two major parts. First, low cost sensors (piezo and optical) interfaced with analog front-end circuits including capacitors, resistors and amplifiers. Second, the filtered and amplified signal is digitally processed and converted to provide statistically significant information about the patient’s heart rate and present irregularity report in the heart activity if any. The piezoelectric sensor head is placed on the point where the radial artery crosses the bones of the wrist, whereas the photoelectric sensor head is placed on a fingertip in order to detect the arterial pulse rate.</p><p>At present, the interpretation of sensor readings is focused towards determining the heart rate and dynamics of heart functional characteristics. This low cost handheld device is being further enhanced with more sensors to provide additional relevant vital parameters for quality treatment guidelines.</p>
----------------------------------------------------------------------
In diva2:1188957 abstract is: <p>The establishment of the European Union Water Framework Directive (2000/60/EG) has created a common platform for the EU Member States regarding action in the field of waterpolicy. In 2004 the directive was implemented in the Swedish legislation and it constitutes the foundation of the water management in Sweden. The responsibility of the water management is assigned to a water authority that consists of five County Administrative Boards. The objects of their work are to attain good quality of water environment and prevent further deterioration. In order to determine the quality status of a so-called surface water body, concentration of various chemicals substances in the water are measured. Some of these substances are toxic, persistent and bioaccumulative organic pollutants. Potential sources of such substances are partly waste disposal facilities. It is therefore important for waste management operators to possess knowledge of how their activities affect the surrounding environment, which they can achieve by performing measurements and studies.</p><p>The waste disposal facility Högbytorp in Upplands-Bro is operated by Ragn-Sells Avfallsbehandling AB and handles reception, recycling and disposal of different types of waste. This project was aimed at supplementing parts of Ragn-Sell’s earlier characterization of landfill leachate at Högbytorp and giving them an improved supervision of their emissions regarding a number of determined organic pollutants. Examples of substances included were brominated flame retardants, polycyclic aromatic hydrocarbons and perfluoroalkylated substances, which are classified as priority substances within the water policy. The project would also provide information on the extent of reduction of these substances in various leachate treatment steps at Högbytorp. To achieve these objectives, sampling and analyzeshave been performed on Högbytorp’s leachate and on their soil plant system, but also on the surface water in the nearby recipient and on treated process water sent to the wastewater treatment plant Käppala. Measured concentrations in these sampling points have been evaluated by comparing them to relevant values like environmental quality standards. Massbalances of PFOS, perfluorooctane sulfonate, was performed to understand how the substanceis affected in different treatment steps and to estimate their purification efficiency. The project's delimitations consisted of the selected organic pollutants that have been analyzed and the particular sampling points.</p><p>The results of the evaluation of organic pollutants showed that depending on the sampling point some substances exceeded their comparative values while others measured below. However, levels of PFOS were particularly distinctive as they exceeded at least one comparative value in every sampling point. In general, higher concentrations were found in untreated leachate compared to treated leachate, indicating that Högbytorp's leachate treatment steps are able to reduce concentrations of organic pollutants in leachate. By comparing the concentration levels before and after the nitrification/denitrification treatment plant, the reduction of analyzed substances was classified as good to very good in this treatment step. Byusing mass balances regarding PFOS the purification efficiency of both the nitrification/denitrification treatment plant and the treatment ponds were estimated to be very good. A possible explanation for the reduction of PFOS was thought to be the ability of the substance to adsorb to sludge. Support to this hypothesis was given by the PFOS content measured in the sludge of the nitrification/denitrification plant.</p><p>Measurements in the soil plant system showed lower levels of naphthalene, anthracene and fluoranthene in the area where salix grows, which indicate that the plant has a reducing effecton polycyclic aromatic hydrocarbons in the soil. However, it was not possible to estimate the purification efficiency of the soil plant system using a mass balance due to uncertain assumptions and insufficient data. When comparing concentration levels in the recipient upstream and downstream the wastefacility, increased concentrations were detected downstream, which could be a result of the waste facility's operations. Evaluation of the concentration levels of substances downstream showed that the majority was measured below the environmental quality standards. Regardingthe evaluation of the treated process water led to the wastewater treatment plant Käppala, all substances evaluated fell below the risk criteria of REVAQ meaning that the impact on these wage sludge in the wastewater treatment plant should be considered tolerable with the assumption that the process water represents the total leachate flow to the wastewater treatment plant. With only regard to tolerable influence on sewage sludge, the connection of these waters can be maintained.</p><p>Due to the fact that most samples were taken only at one point, the results may be doubtful andtherefore misleading. In order to obtain more representative results, the sampling of this project should be supplemented with several samples taken over a longer period of time.</p><p>Based on the results of the project, several conclusions could be drawn and proposals forfurther studies could be given. For the reason that PFOS seems to adsorb to sludge, it is relevant to look over the management of sludge removal from the nitrification/denitrificationplant and the treatment ponds. Because of the high reduction of organic pollutants in thenitrification/denitrification plant it is important to avoid flow of leachate past the plant, whichmeans that the plant’s capacity should be investigated. Since concentrations of analyzed pollutants in the recipient were higher downstream the waste facility, it is likely that emissionsoccur from the facility. It is therefore a good idea to examine potential sources of emissions within the facility area, in order to prevent emissions to the recipient.</p>


corrected abstract:
<p>The establishment of the European Union Water Framework Directive (2000/60/EG) has created a common platform for the EU Member States regarding action in the field of water policy. In 2004 the directive was implemented in the Swedish legislation and it constitutes the foundation of the water management in Sweden. The responsibility of the water management is assigned to a water authority that consists of five County Administrative Boards. The objects of their work are to attain good quality of water environment and prevent further deterioration. In order to determine the quality status of a so-called surface water body, concentration of various chemicals substances in the water are measured. Some of these substances are toxic, persistent and bioaccumulative organic pollutants. Potential sources of such substances are partly waste disposal facilities. It is therefore important for waste management operators to possess knowledge of how their activities affect the surrounding environment, which they can achieve by performing measurements and studies.</p><p>The waste disposal facility Högbytorp in Upplands-Bro is operated by Ragn-Sells Avfallsbehandling AB and handles reception, recycling and disposal of different types of waste. This project was aimed at supplementing parts of Ragn-Sell’s earlier characterization of landfill leachate at Högbytorp and giving them an improved supervision of their emissions regarding a number of determined organic pollutants. Examples of substances included were brominated flame retardants, polycyclic aromatic hydrocarbons and perfluoroalkylated substances, which are classified as priority substances within the water policy. The project would also provide information on the extent of reduction of these substances in various leachate treatment steps at Högbytorp. To achieve these objectives, sampling and analyzes have been performed on Högbytorp’s leachate and on their soil plant system, but also on the surface water in the nearby recipient and on treated process water sent to the wastewater treatment plant Käppala. Measured concentrations in these sampling points have been evaluated by comparing them to relevant values like environmental quality standards. Mass balances of PFOS, perfluorooctane sulfonate, was performed to understand how the substance is affected in different treatment steps and to estimate their purification efficiency. The project's delimitations consisted of the selected organic pollutants that have been analyzed and the particular sampling points.</p><p>The results of the evaluation of organic pollutants showed that depending on the sampling point some substances exceeded their comparative values while others measured below. However, levels of PFOS were particularly distinctive as they exceeded at least one comparative value in every sampling point. In general, higher concentrations were found in untreated leachate compared to treated leachate, indicating that Högbytorp's leachate treatment steps are able to reduce concentrations of organic pollutants in leachate. By comparing the concentration levels before and after the nitrification/denitrification treatment plant, the reduction of analyzed substances was classified as good to very good in this treatment step. By using mass balances regarding PFOS the purification efficiency of both the nitrification/denitrification treatment plant and the treatment ponds were estimated to be very good. A possible explanation for the reduction of PFOS was thought to be the ability of the substance to adsorb to sludge. Support to this hypothesis was given by the PFOS content measured in the sludge of the nitrification/denitrification plant.</p><p>Measurements in the soil plant system showed lower levels of naphthalene, anthracene and fluoranthene in the area where salix grows, which indicate that the plant has a reducing effect on polycyclic aromatic hydrocarbons in the soil. However, it was not possible to estimate the purification efficiency of the soil plant system using a mass balance due to uncertain assumptions and insufficient data.</p><p>When comparing concentration levels in the recipient upstream and downstream the waste facility, increased concentrations were detected downstream, which could be a result of the waste facility's operations. Evaluation of the concentration levels of substances downstream showed that the majority was measured below the environmental quality standards. Regarding the evaluation of the treated process water led to the wastewater treatment plant Käppala, all substances evaluated fell below the risk criteria of REVAQ meaning that the impact on the sewage sludge in the wastewater treatment plant should be considered tolerable with the assumption that the process water represents the total leachate flow to the wastewater treatment plant. With only regard to tolerable influence on sewage sludge, the connection of these waters can be maintained.</p><p>Due to the fact that most samples were taken only at one point, the results may be doubtful and therefore misleading. In order to obtain more representative results, the sampling of this project should be supplemented with several samples taken over a longer period of time.</p><p>Based on the results of the project, several conclusions could be drawn and proposals for further studies could be given. For the reason that PFOS seems to adsorb to sludge, it is relevant to look over the management of sludge removal from the nitrification/denitrification plant and the treatment ponds. Because of the high reduction of organic pollutants in the nitrification/denitrification plant it is important to avoid flow of leachate past the plant, which means that the plant’s capacity should be investigated. Since concentrations of analyzed pollutants in the recipient were higher downstream the waste facility, it is likely that emissions occur from the facility. It is therefore a good idea to examine potential sources of emissions within the facility area, in order to prevent emissions to the recipient.</p>
----------------------------------------------------------------------
title: "Swedish biomedical text-miningand classification"
==>    "Swedish biomedical text-mining and classification"

In diva2:1451783 abstract is: <p>AbstractManual classification of text is both time consuming and expensive. However, it is anecessity within the field of biomedicine, for example to be able to quantify biomedical data.In this study, two different approaches were researched regarding the possibility of usingsmall amounts of training data, in order to create text classification models that are able tounderstand and classify biomedical texts. The study researched whether a specialized modelshould be considered a requirement for this purpose, or if a generic model might suffice. Thetwo models were based on publicly available versions, one specialized to understand Englishbiomedical texts, and the other to understand ordinary Swedish texts. The Swedish modelwas introduced to a new field of texts while the English model had to work on translatedSwedish texts.The results were quite low, but did however indicate that the method with the Swedish modelwas more reliable, performing almost twice as well as the English correspondence. The studyconcluded that there was potential in using general models as a base, and then tuning theminto more specialized fields, even with small amounts of data.KeywordsNLP, text-mining, biomedical texts, classification, labelling, models, BERT, machinelearning, FIC, ICF.</p>


corrected abstract:
<p>Manual classification of text is both time consuming and expensive. However, it is a necessity within the field of biomedicine, for example to be able to quantify biomedical data. In this study, two different approaches were researched regarding the possibility of using small amounts of training data, in order to create text classification models that are able to understand and classify biomedical texts. The study researched whether a specialized model should be considered a requirement for this purpose, or if a generic model might suffice. The two models were based on publicly available versions, one specialized to understand English biomedical texts, and the other to understand ordinary Swedish texts. The Swedish model was introduced to a new field of texts while the English model had to work on translated Swedish texts.</p><p>The results were quite low, but did however indicate that the method with the Swedish model was more reliable, performing almost twice as well as the English correspondence. The study concluded that there was potential in using general models as a base, and then tuning them into more specialized fields, even with small amounts of data.</p>
----------------------------------------------------------------------
In diva2:1260679 abstract is: <p>Pretreatment of biomass plays an important role in IGCC processes. Optimized operating conditions improve overall process eciency, product diversity and producergas composition. This project focuses on the pretreatment of spruce woodbiomass using high pressure superheated steam in a packed bed reactor with heated walls. The objective is to optimize the operation for ecient energy consumption and uniform thermal treatment of wood particles along the reactor. For this purpose,two models have been developed: Single-phase superheated steam in porous media where local thermal nonequilibrium condition prevails, Multi-phase ow in porous media.The rst model investigates the essential parameters to be taken into accountto ensure a uniform thermal treatment as the pyrolytic agent i.e. steam exchanges heat with solid particles. It is found that solid matrix and steam reach thermalequilibrium at early stages of the process however, sharp decrease in uid temperatureis observed. This trend is believed to be aected mostly by uid inlet velocity and solid initial temperature. The model suggests that the initial solid temperaturedoes not play an important role compared to the uid inlet velocity.The focus of the second model is optimization of reactor heat source to avoidthe complications attributed by a two-phase ow. The model can predict the phasesaturation of uid in the porous media along with the pressure and velocity prolesfor a given inlet ow condition and reactor heating power. A reactor with heatsource localized at the inlet is recommended to satisfy the thermal requirements ofthe system.Steam treatment of biomass at elevated pressures is a novel technology whichis believed to bring about many advantages for IGCC process. However, detailed optimization is necessary to benet from these advantages.</p>

w='eciency' val={'c': 'efficiency', 's': 'diva2:1260679', 'n': 'missing ligrature'}
w='ecient' val={'c': 'efficient', 's': 'diva2:1260679', 'n': 'missing ligrature'}
w='prolesfor' val={'c': 'profiles for', 's': 'diva2:1260679', 'n': 'missing ligature'}
w='benet' val={'c': 'benefit', 's': 'diva2:1260679', 'n': 'missing ligature'}
There is a error in the original as "process however" should probably be "process; however"
There were ligatures of "fl" missing in fluid and flow.

corrected abstract:
<p>Pretreatment of biomass plays an important role in IGCC processes. Optimized operating conditions improve overall process efficiency, product diversity and producer gas composition. This project focuses on the pretreatment of spruce wood biomass using high pressure superheated steam in a packed bed reactor with heated walls. The objective is to optimize the operation for efficient energy consumption and uniform thermal treatment of wood particles along the reactor. For this purpose, two models have been developed:<ul><li>Single-phase superheated steam in porous media where local thermal non-equilibrium condition prevails,</li><li>Multi-phase flow in porous media.</li></ul></p><p>The first model investigates the essential parameters to be taken into account to ensure a uniform thermal treatment as the pyrolytic agent i.e. steam exchanges heat with solid particles. It is found that solid matrix and steam reach thermal equilibrium at early stages of the process however, sharp decrease in fluid temperature is observed. This trend is believed to be affected mostly by fluid inlet velocity and solid initial temperature. The model suggests that the initial solid temperature does not play an important role compared to the fluid inlet velocity.</p><p>The focus of the second model is optimization of reactor heat source to avoid the complications attributed by a two-phase flow. The model can predict the phase saturation of fluid in the porous media along with the pressure and velocity profiles for a given inlet flow condition and reactor heating power. A reactor with heat source localized at the inlet is recommended to satisfy the thermal requirements of the system.</p><p>Steam treatment of biomass at elevated pressures is a novel technology which is believed to bring about many advantages for IGCC process. However, detailed optimization is necessary to benefit from these advantages.</p>
----------------------------------------------------------------------
In diva2:1889618 abstract is: <p>Background: In this thesis, it is investigated how different bathroom designs impact the useand effectiveness of assistive equipment by standardized patients and assistant nurses during homecare activities.</p><p>Aim: In this study, the aim is to quantitatively assess the impact of various ergonomicbathroom designs on the effectiveness of assistive equipment usage in home care settings.</p><p>Method: Through the analysis of video data from 42 experimental sessions, includingstandardized patients using walkers and wheelchairs across three different bathroom layouts, theeffectiveness of ergonomic designs in facilitating or hindering interactions with assistivetechnologies is examined.</p><p>Results: The experimental results indicate significantly higher frequency and duration ofhelpful interactions with assistive devices in the 'Equipped' bathroom layout during specific tasks,particularly for a standardized patient using walker. Although the results were not uniformlysignificant across all tasks and mobility levels, the data indicate a trend toward improved functionalindependence and safety in well-designed ergonomic environments.</p><p>Conclusion: Based on the findings, it is concluded that integrating ergonomic principles intohome care settings is critically important for enhancing the quality of life and care delivery. Thisstudy provides results emphasizing the necessity for thoughtful, user-centered designs in home careenvironments, particularly for aging populations. In this study, it is suggested that such designssignificantly impact the frequency and duration of assistive equipment usage, thereby promotingfunctional independence and safety for elderly users, and improving both caregiver efficiency andpatient autonomy.</p>

corrected abstract:
<p><strong>Background:</strong> In this thesis, it is investigated how different bathroom designs impact the use and effectiveness of assistive equipment by standardized patients and assistant nurses during home care activities.</p><p><strong>Aim:</strong> In this study, the aim is to quantitatively assess the impact of various ergonomic bathroom designs on the effectiveness of assistive equipment usage in home care settings.</p><p><strong>Method:</strong> Through the analysis of video data from 42 experimental sessions, including standardized patients using walkers and wheelchairs across three different bathroom layouts, the effectiveness of ergonomic designs in facilitating or hindering interactions with assistive technologies is examined.</p><p><strong>Results:</strong> The experimental results indicate significantly higher frequency and duration of helpful interactions with assistive devices in the 'Equipped' bathroom layout during specific tasks, particularly for a standardized patient using walker. Although the results were not uniformly significant across all tasks and mobility levels, the data indicate a trend toward improved functional independence and safety in well-designed ergonomic environments.</p><p><strong>Conclusion:</strong> Based on the findings, it is concluded that integrating ergonomic principles into home care settings is critically important for enhancing the quality of life and care delivery. This study provides results emphasizing the necessity for thoughtful, user-centered designs in home care environments, particularly for aging populations. In this study, it is suggested that such designs significantly impact the frequency and duration of assistive equipment usage, thereby promoting functional independence and safety for elderly users, and improving both caregiver efficiency and patient autonomy.</p>
----------------------------------------------------------------------
In diva2:1454420 abstract is: <p>Ouabain and other cardiotonic steroids are known to inhibit Na + ,K + -ATPase (NKA), theion pump responsible for the ionic gradient across the plasma membrane. These steroidsdisplay a selective toxicity towards several tumour cells in comparison to primary humancells, however, the mechanism behind this is not yet understood. Here, we examined theouabain toxicity in renal epithelial cells, proximal tubular cells (PTCs) of different origin. Weinvestigated the relative cytotoxicity in cancer cells (A-498) and papilloma virus-transformedPTCs (HK-2) as well as to primary human PTCs (hPTC) to validate key components in theeffect.</p><p>In exposure to ouabain, we examined the cell viability and density by MTT and CrystalViolet assays, and cell migration by a scratch assay. The cytotoxic effect was also studied invarious pH, glucose and potassium ion concentrations. In addition, apoptosis was examinedby the TUNEL assay, and if ouabain kills cancer cells through activation of thevolume-regulated anion channel VRAC channel via NKA.</p><p>We found that there is a decrease in viable cells when cells are exposed to ouabain ≥ 10nM, however, the effect was not seen to be selective towards cancer cells, nor due toapoptosis and the activation of VRAC. The cytotoxic effect was greater in more acidicextracellular pH ~6.8, but independent of glucose concentration in the medium. Interestingly,the effect was also reversed at an increased extracellular concentration of potassium, andouabain did selectively inhibit the cancer cells to migrate. Thus, there could be potential forouabain to act as an anti-cancer agent for renal cancer and to inhibit tumour metastasization.</p>


w='forouabain' val={'c': 'for ouabain', 's': 'diva2:1454420', 'n': 'correct in original'}
w='Na+ ,K + -ATPase' val={'c': 'Na<sup>+</sup>,K<sup>+</sup>-ATPase', 's': 'diva2:1454420', 'n': 'correct in original'}

corrected abstract:
<p>Ouabain and other cardiotonic steroids are known to inhibit Na<sup>+</sup>,K<sup>+</sup>-ATPase (NKA), the ion pump responsible for the ionic gradient across the plasma membrane. These steroids display a selective toxicity towards several tumour cells in comparison to primary human cells, however, the mechanism behind this is not yet understood. Here, we examined the ouabain toxicity in renal epithelial cells, proximal tubular cells (PTCs) of different origin. We investigated the relative cytotoxicity in cancer cells (A-498) and papilloma virus-transformed PTCs (HK-2) as well as to primary human PTCs (hPTC) to validate key components in the effect.</p><p>In exposure to ouabain, we examined the cell viability and density by MTT and Crystal Violet assays, and cell migration by a scratch assay. The cytotoxic effect was also studied in various pH, glucose and potassium ion concentrations. In addition, apoptosis was examined by the TUNEL assay, and if ouabain kills cancer cells through activation of the volume-regulated anion channel VRAC channel via NKA.</p><p>We found that there is a decrease in viable cells when cells are exposed to ouabain ≥ 10 nM, however, the effect was not seen to be selective towards cancer cells, nor due to apoptosis and the activation of VRAC. The cytotoxic effect was greater in more acidic extracellular pH ~6.8, but independent of glucose concentration in the medium. Interestingly, the effect was also reversed at an increased extracellular concentration of potassium, and ouabain did selectively inhibit the cancer cells to migrate. Thus, there could be potential for ouabain to act as an anti-cancer agent for renal cancer and to inhibit tumour metastasization.</p>
----------------------------------------------------------------------
In diva2:1689508 abstract is: <p>TRIM21 is a cytosolic ubiquitin ligase and an antibody receptor that providesa last line of defense against invading pathogens. By utilizing the diversity ofantibody repertoire to identify pathogens, TRIM21 serves as a link betweenintrinsic cellular defense and adaptive immunity. A variety of diseases havebeen linked to mutations of the TRIM family, including cancer, inflammatorydiseases, and autoimmune diseases. In this project, TRIM21 was producedand purified from Escherichia coli, (E.coli). Protein characterization wasperformed with SDS-PAGE, size exclusion chromatography and cryo-electronmicroscopy (cryo-EM). Previously TRIM21 has been shown to form a dimerwhen produced in SF9. Results from size exclusion chromatography show thatTRIM21 form a larger complex when expressed in E.coli. Cryo-EM resultsshow that the complex structure is more globular than previously thought.Purified TRIM21 was bound to the antibody IC100. SDS-PAGE and sizeexclusion chromatography results show much lower affinity to antibodies thanexpected.</p>


corrected abstract:
<p>TRIM21 is a cytosolic ubiquitin ligase and an antibody receptor that provides a last line of defense against invading pathogens. By utilizing the diversity of antibody repertoire to identify pathogens, TRIM21 serves as a link between intrinsic cellular defense and adaptive immunity. A variety of diseases have been linked to mutations of the TRIM family, including cancer, inflammatory diseases, and autoimmune diseases. In this project, TRIM21 was produced and purified from <em>Escherichia coli</em>, (<em>E.coli</em>). Protein characterization was performed with SDS-PAGE, size exclusion chromatography and cryo-electron microscopy (cryo-EM). Previously TRIM21 has been shown to form a dimer when produced in <em>SF9</em>. Results from size exclusion chromatography show that TRIM21 form a larger complex when expressed in <em>E.coli</em>. Cryo-EM results show that the complex structure is more globular than previously thought. Purified TRIM21 was bound to the antibody IC100. SDS-PAGE and size exclusion chromatography results show much lower affinity to antibodies than expected.</p>
----------------------------------------------------------------------
In diva2:1766059 abstract is: <p>Brain tumor is a disease characterized by uncontrolled growth of abnormal cells inthe brain. The brain is responsible for regulating the functions of all other organs,hence, any atypical growth of cells in the brain can have severe implications for itsfunctions. The number of global mortality in 2020 led by cancerous brains was estimatedat 251,329. However, early detection of brain cancer is critical for prompttreatment and improving patient’s quality of life as well as survival rates. Manualmedical image classification in diagnosing diseases has been shown to be extremelytime-consuming and labor-intensive. Convolutional Neural Networks (CNNs) hasproven to be a leading algorithm in image classification outperforming humans. Thispaper compares five CNN architectures namely: VGG-16, VGG-19, AlexNet, EffecientNetB7,and ResNet-50 in terms of performance and accuracy using transferlearning. In addition, the authors discussed in this paper the economic impact ofCNN, as an AI approach, on the healthcare sector. The models’ performance isdemonstrated using functions for loss and accuracy rates as well as using the confusionmatrix. The conducted experiment resulted in VGG-19 achieving best performancewith 97% accuracy, while EffecientNetB7 achieved worst performance with93% accuracy.</p>


corrected abstract:
<p>Brain tumor is a disease characterized by uncontrolled growth of abnormal cells in the brain. The brain is responsible for regulating the functions of all other organs, hence, any atypical growth of cells in the brain can have severe implications for its functions. The number of global mortality in 2020 led by cancerous brains was estimated at 251,329. However, early detection of brain cancer is critical for prompt treatment and improving patient’s quality of life as well as survival rates. Manual medical image classification in diagnosing diseases has been shown to be extremely time-consuming and labor-intensive. Convolutional Neural Networks (CNNs) has proven to be a leading algorithm in image classification outperforming humans. This paper compares five CNN architectures namely: VGG-16, VGG-19, AlexNet, EffecientNetB7, and ResNet-50 in terms of performance and accuracy using transfer learning. In addition, the authors discussed in this paper the economic impact of CNN, as an AI approach, on the healthcare sector. The models’ performance is demonstrated using functions for loss and accuracy rates as well as using the confusion matrix. The conducted experiment resulted in VGG-19 achieving best performance with 97% accuracy, while EffecientNetB7 achieved worst performance with 93% accuracy.</p>
----------------------------------------------------------------------
In diva2:1240203 abstract is: <p>In today’s paper and board production, quality control is made on a single cross direction (CD)sample from each tambour. As several different properties are analysed, only a limited number of measurement results are obtained for one property. Therefore, the measurement results might not be representative for the properties of the entire width of the tambour. The first objective of the project was to investigate variations of thickness, surface roughness and mechanical properties with a much higher resolution and number of measurements. The results of the measurement were compared with the routine quality control of the mill. The second objective of the project was to evaluate the influence of the wire shake unit in the centreply on the properties of the produced board. The measurements were performed on Iggesundpaperboard samples.The high-resolution measurements were performed using the STFI structural thicknessmeasurement device, an OptiTopo topography measurement device and a modified Autolinedevice at RISE Bioeconomy. The statistical evaluation of the results was performed in Matlab.Standard deviation, local variance and a frequency analysis were calculated for the thicknessmeasurements. Only standard deviation was considered for the topography data. For the mechanical properties, the distribution was evaluated using the Weibull distribution, since theresults had a single-sided distribution. In addition, the properties were analysed as a function of their location, for example to identify deterministic deviations in cross direction.The results of the first part of the project showed that the everyday control conducted in Iggesund is sufficient for most of the properties. Greatest difference was found at the edges ofthe samples, where Iggesund standard quality control does not detect a major variation inproperties, as no measurements are performed that close to the edge of the web. For example,at one edge, the high frequent measurements showed a significant drop in thickness which were not detected with the everyday quality control.In the second part of the project, the effect of a shake unit on the paper properties was evaluated. Here it was seen that the thickness variation were reduced, which also can be interpreted as an improvement of formation in the centre ply of the paperboard. As for thesurface roughness a slight improvement was found. Also for the mechanical properties, the shake unit appeared to improve the uniformity of the product</p>


corrected abstract:
<p>In today’s paper and board production, quality control is made on a single cross direction (CD) sample from each tambour. As several different properties are analysed, only a limited number of measurement results are obtained for one property. Therefore, the measurement results might not be representative for the properties of the entire width of the tambour. The first objective of the project was to investigate variations of thickness, surface roughness and mechanical properties with a much higher resolution and number of measurements. The results of the measurement were compared with the routine quality control of the mill. The second objective of the project was to evaluate the influence of the wire shake unit in the centre ply on the properties of the produced board. The measurements were performed on Iggesund paperboard samples.</p><p>The high-resolution measurements were performed using the STFI structural thickness measurement device, an OptiTopo topography measurement device and a modified Autoline device at RISE Bioeconomy. The statistical evaluation of the results was performed in Matlab. Standard deviation, local variance and a frequency analysis were calculated for the thickness measurements. Only standard deviation was considered for the topography data. For the mechanical properties, the distribution was evaluated using the Weibull distribution, since the results had a single-sided distribution. In addition, the properties were analysed as a function of their location, for example to identify deterministic deviations in cross direction.</p><p>The results of the first part of the project showed that the everyday control conducted in Iggesund is sufficient for most of the properties. Greatest difference was found at the edges of the samples, where Iggesund standard quality control does not detect a major variation in properties, as no measurements are performed that close to the edge of the web. For example, at one edge, the high frequent measurements showed a significant drop in thickness which were not detected with the everyday quality control.</p><p>In the second part of the project, the effect of a shake unit on the paper properties was evaluated. Here it was seen that the thickness variation were reduced, which also can be interpreted as an improvement of formation in the centre ply of the paperboard. As for the surface roughness a slight improvement was found. Also for the mechanical properties, the shake unit appeared to improve the uniformity of the product</p>
----------------------------------------------------------------------
In diva2:1859260 abstract is: <p>The purpose of this thesis was to validate a method for quantitatively determining PFAS in soil,sludge, and sediment. The validation involved a number of different tests to evaluate themethod's reporting limit, precision, accuracy, measurement uncertainty, and specificity. Excelwas used to calculate and summarize the results from the different tests. Blank and spikedsamples were analyzed to investigate the reporting limit. Control samples were analyzed toinvestigate precision and specificity. A variety of tests were performed to investigate accuracy;ISTD matching for some PFAS analytes that lacked a direct ISTD match, comparisons againstcertified reference material, analysis of previously performed tests that had already beenanalyzed at ALS Prague, and spiked sample matrices. The measurement uncertainty wasestimated and evaluated in consultation with the quality manager.</p><p>The results of the method validation confirm that the method has been shown to be wellvalidated. All tests performed during the validation process have consistently met and exceededthe established acceptance criteria for the validation. Criteria included, among others, relativestandard deviation (RSD), recovery, bias, and concentration for various test parameters. Thecomprehensive method validation provided strong evidence that the method is reliable and canbe used to generate reliable results in PFAS analysis.</p>


corrected abstract:
<p>The purpose of this thesis was to validate a method for quantitatively determining PFAS in soil, sludge, and sediment. The validation involved a number of different tests to evaluate the method's reporting limit, precision, accuracy, measurement uncertainty, and specificity. Excel was used to calculate and summarize the results from the different tests. Blank and spiked samples were analyzed to investigate the reporting limit. Control samples were analyzed to investigate precision and specificity. A variety of tests were performed to investigate accuracy; ISTD matching for some PFAS analytes that lacked a direct ISTD match, comparisons against certified reference material, analysis of previously performed tests that had already been analyzed at ALS Prague, and spiked sample matrices. The measurement uncertainty was estimated and evaluated in consultation with the quality manager.</p><p>The results of the method validation confirm that the method has been shown to be well validated. All tests performed during the validation process have consistently met and exceeded the established acceptance criteria for the validation. Criteria included, among others, relative standard deviation (RSD), recovery, bias, and concentration for various test parameters. The comprehensive method validation provided strong evidence that the method is reliable and can be used to generate reliable results in PFAS analysis.</p>
----------------------------------------------------------------------
In diva2:1455134 abstract is: <p>The numerous SNPs discovered in genome-wide association studies (GWAS) are predominantlylocated in non-coding regions. This project aimed to investigate and compare the effect of four rarevariants and four common variants on protein production. The variants were located in enhancers,regulatory and non-coding regions of the genome that affect the transcription by interacting withproteins bound to promoters. Foremost, eight primer pairs were designed to retrieve the SNPs from aselection of available DNA templates, by PCR. Thereafter, a series of transformations of E. coli wereperformed to obtain the inserts within the final luciferase vector, subsequently used for a luciferaseassay, where bioluminescence is utilized for assessment of the effect size of each correspondingvariant. The primary analytic methods used during the project for validation of the results weresequencing and gel electrophoresis. The result indicated a presence of both rare and common variants,with a heterogeneous contribution on gene expression. In conclusion, an effect from both rare andcommon variants have been confirmed in several earlier studies. However, it is desirable toinvestigate the contribution to effect size further and possibly obtain a quantitative measurement ofthe effect.</p>


corrected abstract:
<p>The numerous SNPs discovered in genome-wide association studies (GWAS) are predominantly located in non-coding regions. This project aimed to investigate and compare the effect of four rare variants and four common variants on protein production. The variants were located in enhancers, regulatory and non-coding regions of the genome that affect the transcription by interacting with proteins bound to promoters. Foremost, eight primer pairs were designed to retrieve the SNPs from a selection of available DNA templates, by PCR. Thereafter, a series of transformations of <em>E. coli</em> were performed to obtain the inserts within the final luciferase vector, subsequently used for a luciferase assay, where bioluminescence is utilized for assessment of the effect size of each corresponding variant. The primary analytic methods used during the project for validation of the results were sequencing and gel electrophoresis. The result indicated a presence of both rare and common variants, with a heterogeneous contribution on gene expression. In conclusion, an effect from both rare and common variants have been confirmed in several earlier studies. However, it is desirable to investigate the contribution to effect size further and possibly obtain a quantitative measurement of the effect.</p>
----------------------------------------------------------------------
title: "Are legal requirements enough forpreventing occupational accidents?"
==>    "Are legal requirements enough for preventing occupational accidents?"

In diva2:1775516 abstract is: <p>The increasing number of occupational health and safety issues is a problem. Legislationsthat are anchored in European law, such as “machinery directive 2006/42/EC”, the “Use ofwork equipment 2009/104/EC” and the Swedish AFS 2001:1 (Systematic Work EnvironmentManagement) are defined but still lack the power to stop accidents/ incidents fromhappening. When risks are being made conscious they are not stopped by the legalrequirements in place.</p><p>Scientific approaches such as the Swiss cheese model, safety management systems (SMS),and HTO (Human- Technology- Organization) explain how increased complexity inside asocio-technical system needs more attention. As the cases of accidents/ incidents in anoccupational setting still increase a need for solving this appears, with the help of sciencebasedtools.</p><p>In cooperation with the company AFRY, I conducted four interviews (n=4) and analyzed twoABRA (activity-based risk assessments) already conducted by the company. Using thecommon themes identified from the interviews to analyze the ABRA helped to identify twokey problems: unclear communication and insufficient knowledge.</p><p>With that in mind, I’m advocating for an increased emphasis on risk communication andresilience engineering. With the awareness that communication must be clearer and thatknowledge has to be increased, it is possible to work proactively on decreasing occupationalaccidents by mitigating the risks.</p>

corrected abstract:
<p>The increasing number of occupational health and safety issues is a problem. Legislations that are anchored in European law, such as “machinery directive 2006/42/EC”, the “Use of work equipment 2009/104/EC” and the Swedish AFS 2001:1 (Systematic Work Environment Management) are defined but still lack the power to stop accidents/ incidents from happening. When risks are being made conscious they are not stopped by the legal requirements in place.</p><p>Scientific approaches such as the Swiss cheese model, safety management systems (SMS), and HTO (Human- Technology- Organization) explain how increased complexity inside a socio-technical system needs more attention. As the cases of accidents/ incidents in an occupational setting still increase a need for solving this appears, with the help of science-based tools.</p><p>In cooperation with the company AFRY, I conducted four interviews (n=4) and analyzed two ABRA (activity-based risk assessments) already conducted by the company. Using the common themes identified from the interviews to analyze the ABRA helped to identify two key problems: unclear communication and insufficient knowledge.</p><p>With that in mind, I’m advocating for an increased emphasis on risk communication and resilience engineering. With the awareness that communication must be clearer and that knowledge has to be increased, it is possible to work proactively on decreasing occupational accidents by mitigating the risks.</p>
----------------------------------------------------------------------
In diva2:1217939 abstract is: <p>Banks internet activity have opened new possibilities for third party actors within the fintechindustry who can now create innovative economy and payment solutions by directlyinteracting with bank accounts. A problem for the fintech companies is that there is noavailable information about what technologies are available for them to use to get access tothe banks services and which technology is the most advantageous for their purpose.In this work, the four biggest swedish banks Nordea, Handelsbanken, SEB and Swedbankhave been examined to find out what options third party actors have to integrate with thebanks services. The examination showed that fintech companies will be able to or are able touse one of the three techniques SFTP, REST or web automation. A test environment for thethree technologies was developed to simulate real use cases for fintech companies. CPU-,RAM- and network usage as well as total operating time was measured for the threetechnologies. For sending information REST was the more effective technology when it cameto lower data volumes while SFTP was the most efficient with bigger data volumes. RESTwas the most efficient for retrieving information, no matter the data volume. Web automationwas the least efficient compared to both the other technologies.</p>


corrected abstract:
<p>Banks internet activity have opened new possibilities for third party actors within the fintech industry who can now create innovative economy and payment solutions by directly interacting with bank accounts. A problem for the fintech companies is that there is no available information about what technologies are available for them to use to get access to the banks services and which technology is the most advantageous for their purpose.</p><p>In this work, the four biggest swedish banks Nordea, Handelsbanken, SEB and Swedbank have been examined to find out what options third party actors have to integrate with the banks services. The examination showed that fintech companies will be able to or are able to use one of the three techniques SFTP, REST or web automation. A test environment for the three technologies was developed to simulate real use cases for fintech companies. CPU-, RAM- and network usage as well as total operating time was measured for the three technologies. For sending information REST was the more effective technology when it came to lower data volumes while SFTP was the most efficient with bigger data volumes. REST was the most efficient for retrieving information, no matter the data volume. Web automation was the least efficient compared to both the other technologies.</p>
----------------------------------------------------------------------
title: "Evaluating the Next Generation of Building Automation – IoT SmartBuildings"
==>    "Evaluating the Next Generation of Building Automation – IoT Smart Buildings"

In diva2:1451575 abstract is: <p>Abstract</p><p>Building automation systems typically use proprietary hardware and included softwarefor their automation which can make the systems vendor-locked. Establishedprogramming standards limits the freedom of companies to improve their automationsystems with the growth of the IT era. The purpose of the thesis is to investigateand evaluate IoT solutions and implement one of these methods as proof ofconcept and to elicit new aspects for analysis and discussion.</p><p>With the literature study three different methods was discovered followed bya comparative study. These methods include: Porting the existing software andmoving the automation process. Replacing the hardware with smaller computers.Adding a server as translator between the building and the cloud.</p><p>The methods have different use cases with the objective of integrating a cloudservice to create smarter building automation system to reduce energy consumptionin buildings. One of the methods was proven to be most suitable for implementationbased on requirements set by experts in the field. The method chosenwas porting a smaller portion of an existing BAS to a new programming language.</p><p>The final prototype was completed with a ported program, from IEC 61131-3 standard to Java and the automation was moved from a programmable logiccontroller to an edge unit. The discussion focuses on different ways of optimizingthe system, one of the optimization is to move the automation process to cloudcomputing. Energy managements are considered by collecting data and metadatain the cloud to create energy profiles for reduced energy consumption.</p><p>Keywords: internet of things, building management system, buildingautomation system, programmable logic controller, porting, legacy, energymanagement, cloud services</p>


corrected abstract:
<p>Building automation systems typically use proprietary hardware and included software for their automation which can make the systems vendor-locked. Established programming standards limits the freedom of companies to improve their automation systems with the growth of the IT era. The purpose of the thesis is to investigate and evaluate IoT solutions and implement one of these methods as proof of concept and to elicit new aspects for analysis and discussion.</p><p>With the literature study three different methods was discovered followed by a comparative study. These methods include: <em>Porting the existing software and moving the automation process.</em> <em>Replacing the hardware with smaller computers.</em> <em>Adding a server as translator between the building and the cloud.</em></p><p>The methods have different use cases with the objective of integrating a cloud service to create smarter building automation system to reduce energy consumption in buildings. One of the methods was proven to be most suitable for implementation based on requirements set by experts in the field. The method chosen was porting a smaller portion of an existing BAS to a new programming language.</p><p>The final prototype was completed with a ported program, from IEC 61131-3 standard to Java and the automation was moved from a programmable logic controller to an edge unit. The discussion focuses on different ways of optimizing the system, one of the optimization is to move the automation process to cloud computing. Energy managements are considered by collecting data and metadata in the cloud to create energy profiles for reduced energy consumption.</p>
----------------------------------------------------------------------
In diva2:1454998 abstract is: <p>Mesenchymal stromal cells (MSCs) possess immunomodulatory properties which make themattractive for the treatment of various inflammatory diseases. Upon intravascular administration,MSCs get trapped in the lungs due to their size. A few hours after their administration, the vast majorityof MSCs can neither be found in the lungs nor anywhere else in the body. To explain the long-termimmunomodulatory effect following MSC administration in concert with the rapid disappearance ofMSCs, it was hypothesized that monocytes phagocytose MSCs and on behalf of them induce observedimmunomodulatory effects. While the phagocytosis of MSCs derived from bone marrow and umbilicalcord by monocytes was already observed in vitro, this report aims to investigate whether this is alsotrue for MSCs derived from the human placental decidua basalis (DBMSCs).</p><p>Here, we report that CD14+ monocytes alone or monocytes within peripheral blood mononuclear cells(PBMCs) are capable of effectively phagocytosing DBMSCs. Kinetic studies showed that phagocytosisstarted after 2 hours of co-culture and reached a significant number of engulfed DBMSCs within24 hours of co-culture in vitro. Interestingly, a positive effect on monocyte survival could be observedfor monocytes co-cultured with DBMSCs. Further, the phagocytosis of DBMSCs by purifiedCD14+ monocytes resulted in an upregulation of cell surface markers CD163 and CD206 indicating thepolarization of monocytes towards an anti-inflammatory M2 phenotype. The generation ofCD16+CD163+CD206+ monocytes was significantly different if co-culture experiments were performedwith purified CD14+ monocytes, but not for monocytes mixed with other PBMCs, suggesting thatpolarization of monocytes by DBMSCs can be regulated by surrounding cells.</p>


corrected abstract:
<p>Mesenchymal stromal cells (MSCs) possess immunomodulatory properties which make them attractive for the treatment of various inflammatory diseases. Upon intravascular administration, MSCs get trapped in the lungs due to their size. A few hours after their administration, the vast majority of MSCs can neither be found in the lungs nor anywhere else in the body. To explain the long-term immunomodulatory effect following MSC administration in concert with the rapid dis appearance of MSCs, it was hypothesized that monocytes phagocytose MSCs and on behalf of them induce observed immunomodulatory effects. While the phagocytosis of MSCs derived from bone marrow and umbilical cord by monocytes was already observed <em>in vitro</em>, this report aims to investigate whether this is also true for MSCs derived from the human placental decidua basalis (DBMSCs).</p><p>Here, we report that CD14+ monocytes alone or monocytes within peripheral blood mononuclear cells (PBMCs) are capable of effectively phagocytosing DBMSCs. Kinetic studies showed that phagocytosis started after 2 hours of co-culture and reached a significant number of engulfed DBMSCs within 24 hours of co-culture <em>in vitro<em>. Interestingly, a positive effect on monocyte survival could be observed for monocytes co-cultured with DBMSCs. Further, the phagocytosis of DBMSCs by purifiedCD14+ monocytes resulted in an upregulation of cell surface markers CD163 and CD206 indicating the polarization of monocytes towards an anti-inflammatory M2 phenotype. The generation of CD16+CD163+CD206+ monocytes was significantly different if co-culture experiments were performed with purified CD14+ monocytes, but not for monocytes mixed with other PBMCs, suggesting that polarization of monocytes by DBMSCs can be regulated by surrounding cells.</p>
----------------------------------------------------------------------
In diva2:1455009 abstract is: <p>As the aging population is increasing worldwide, so is the prevalence of neurodegenerativediseases such as Alzheimer’s disease (AD), Parkinson’s disease (PD), frontotemporal dementia(FTD) and amyotrophic lateral sclerosis (ALS). Reliable biomarkers able to aid the diagnosis anddifferentiation of these diseases are needed in order to start the right treatment as early as possible.Due to its representative state of the central nervous system, cerebrospinal fluid (CSF) is afavorable sample material for biomarker discovery within neurodegenerative diseases. Alteredprotein levels of this body fluid might serve as a biomarker, but further validation of earlierfindings is needed. The aim of this project was to validate earlier studies suggesting potentialprotein biomarkers in CSF. From a list of 80 potential biomarkers in the CSF of patient samples,eight were chosen to be included in this validation effort. By utilizing a suspension bead array ina sandwich assay setup, 21 antibodies were tested in an initial screening. Antibody pairs that couldmeasure the protein levels in a dilution dependent manner was further optimized before individualpatient samples were analyzed. Sandwich assays targeting the three proteins Amphiphysin(AMPH), Chitotriosidase-1 (CHIT1) and Beta-synuclein (SNCB) were successfully developed andcorrelated to earlier generated data using a suspension bead array with a single binder setup.Therefore, the earlier findings of elevated levels of AMPH and SNCB in AD patients and CHIT1in ALS patients were successfully validated.</p>


corrected abstract:
<p>As the aging population is increasing worldwide, so is the prevalence of neurodegenerative diseases such as Alzheimer’s disease (AD), Parkinson’s disease (PD), frontotemporal dementia (FTD) and amyotrophic lateral sclerosis (ALS). Reliable biomarkers able to aid the diagnosis and differentiation of these diseases are needed in order to start the right treatment as early as possible. Due to its representative state of the central nervous system, cerebrospinal fluid (CSF) is a favorable sample material for biomarker discovery within neurodegenerative diseases. Altered protein levels of this body fluid might serve as a biomarker, but further validation of earlier findings is needed. The aim of this project was to validate earlier studies suggesting potential protein biomarkers in CSF. From a list of 80 potential biomarkers in the CSF of patient samples, eight were chosen to be included in this validation effort. By utilizing a suspension bead array in a sandwich assay setup, 21 antibodies were tested in an initial screening. Antibody pairs that could measure the protein levels in a dilution dependent manner was further optimized before individual patient samples were analyzed. Sandwich assays targeting the three proteins Amphiphysin (AMPH), Chitotriosidase-1 (CHIT1) and Beta-synuclein (SNCB) were successfully developed and correlated to earlier generated data using a suspension bead array with a single binder setup. Therefore, the earlier findings of elevated levels of AMPH and SNCB in AD patients and CHIT1 in ALS patients were successfully validated.</p>
----------------------------------------------------------------------
title: "Evaluation of tools for automatedacceptance testing of webapplications"
==>    "Evaluation of tools for automated acceptance testing of web applications"

In diva2:935212 abstract is: <p>Auddly provides a music management tool that gathers all information about a musical piece in oneplace. The acceptance testing on their web application is done manually, which has become bothtime and money consuming. To solve this problem, an evaluation on automated acceptance testingwas done to find a testing tool suitable for their web application. The evaluation was performed byfinding the current existing testing strategies to later compare the tools implementing these strategies.When analyzing the results it was found that two testing strategies were best suited for automatedacceptance testing. The Visual Recognition strategy that identifies components using screenshotsand the Record and Replay strategy that identifies them by their underlying ID. The choice betweenthem depends on which of these properties are modified more often.It was also found that automating acceptance testing is best applied for regression testing, otherwiseit should be performed with a manual approach.It was made clear that the Selenium tool, which uses the Record and Replay strategy, was best suitedfor Auddly’s acceptance testing. Selenium is able to test AJAX-calls with a manual modificationand is a free and open source tool with a large community.</p>


corrected abstract:
<p>Auddly provides a music management tool that gathers all information about a musical piece in one place. The acceptance testing on their web application is done manually, which has become both time and money consuming. To solve this problem, an evaluation on automated acceptance testing was done to find a testing tool suitable for their web application. The evaluation was performed by finding the current existing testing strategies to later compare the tools implementing these strategies.</p><p>When analyzing the results it was found that two testing strategies were best suited for automated acceptance testing. The Visual Recognition strategy that identifies components using screenshots and the Record and Replay strategy that identifies them by their underlying ID. The choice between them depends on which of these properties are modified more often. It was also found that automating acceptance testing is best applied for regression testing, otherwise it should be performed with a manual approach.</p><p>It was made clear that the Selenium tool, which uses the Record and Replay strategy, was best suited for Auddly’s acceptance testing. Selenium is able to test AJAX-calls with a manual modification and is a free and open source tool with a large community.</p>
----------------------------------------------------------------------
In diva2:1342590 abstract is: <p>Ionizing radiation is often used within medicine for diagnosis and treatments. Because ionizingradiation can be harmful to the body, it is important to know how it affects the tissue. Dosimetryis the study of how ionizing radiation deposits energy in a material. To measure how much ionizingradiation is deposited in the body, gas-filled detectors are often used. An ionization chamber isa type of gas-filled detector and exists in different shapes and sizes, depending on what kind ofmeasurements it is made for. Because ionization chambers are relatively expensive, it is often notpossible to buy one for each type of measurement that is to be done. This results in ionizationchambers being used for measurements they are not optimized for. This report evaluates thepossibility of 3D printing ionization chambers to make it easier to optimize them for specificmeasurements. The process included creating models of ionization chambers using CAD-software,slicing them and then 3D printing them. The 3D printed models were then brought to the SwedishRadiation Safety Authority for measurements. The ionization chambers were connected to highvoltage, and exposed to ionizing radiation in the form of high-intensity gamma-ray fields. Theoutput current of the ionization chamber was measured, which is proportional to the field intensity.The results were similar to those of a commercial ionization chamber. The conclusion is that it ispossible to 3D print ionization chambers. However, to get more accurate results, the design has tobe further optimized and more measurements need to be done.</p>


corrected abstract:
<p>Ionizing radiation is often used within medicine for diagnosis and treatments. Because ionizing radiation can be harmful to the body, it is important to know how it affects the tissue. Dosimetry is the study of how ionizing radiation deposits energy in a material. To measure how much ionizing radiation is deposited in the body, gas-filled detectors are often used. An ionization chamber is a type of gas-filled detector and exists in different shapes and sizes, depending on what kind of measurements it is made for. Because ionization chambers are relatively expensive, it is often not possible to buy one for each type of measurement that is to be done. This results in ionization chambers being used for measurements they are not optimized for. This report evaluates the possibility of 3D printing ionization chambers to make it easier to optimize them for specific measurements. The process included creating models of ionization chambers using CAD-software, slicing them and then 3D printing them. The 3D printed models were then brought to the Swedish Radiation Safety Authority for measurements. The ionization chambers were connected to high voltage, and exposed to ionizing radiation in the form of high-intensity gamma-ray fields. The output current of the ionization chamber was measured, which is proportional to the field intensity. The results were similar to those of a commercial ionization chamber. The conclusion is that it is possible to 3D print ionization chambers. However, to get more accurate results, the design has to be further optimized and more measurements need to be done.</p>
----------------------------------------------------------------------
In diva2:1456255 abstract is: <p>When a material is adhered onto a specific surface it is relevant to know how to make thematerial stay on the surface. By investigating different primers to use with a triazine-basedadhesive, further improvements to using the adhesive on metals can be achieved. This studyfound that an adhesive of (2,4,6-trioxo-1,3,5-triazinane-1,3,5-triyl)tris(ethane-2,1-diyl)tris(3-mercaptopropanoate) (TEMPIC) and 1,3,5-triallyl-1,3,5-triazinane-2,4,6-trione(TATATO) adhered to titanium and stainless steel, two clinically used metal surfaces. Itfurther found that between a phosphonic acid primer, a biomimetic catechol primer and acommercially available silane primer the phosphonic acid primer gave the best adhesion.These results could be because of a higher amount of crosslinking for the phosphonic acidprimer. For further testing increased pH and increased amount as well as increasedhydrolysation time for the catechol and silane primers respectively is suggested. Shearstrength testing was used to determine the adhesion strength. The shear strength testswere done with conditioning in phosphate buffer saline (PBS) solution for 24h beforehand</p>


corrected abstract:
<p>When a material is adhered onto a specific surface it is relevant to know how to make the material stay on the surface. By investigating different primers to use with a triazine-based adhesive, further improvements to using the adhesive on metals can be achieved. This study found that an adhesive of (2,4,6-trioxo-1,3,5-triazinane-1,3,5-triyl)tris(ethane-2,1-diyl) tris(3-mercaptopropanoate) (TEMPIC) and 1,3,5-triallyl-1,3,5-triazinane-2,4,6-trione (TATATO) adhered to titanium and stainless steel, two clinically used metal surfaces. It further found that between a phosphonic acid primer, a biomimetic catechol primer and a commercially available silane primer the phosphonic acid primer gave the best adhesion. These results could be because of a higher amount of crosslinking for the phosphonic acid primer. For further testing increased pH and increased amount as well as increased hydrolysation time for the catechol and silane primers respectively is suggested. Shear strength testing was used to determine the adhesion strength. The shear strength tests were done with conditioning in phosphate buffer saline (PBS) solution for 24h beforehand.</p>
----------------------------------------------------------------------
In diva2:1216523 abstract is: <p>The use of solar cells is continuously increasing in Sweden and the powergenerated by the solar cells is usually stored in lead acid batteries. These batterieshave a bad impact on the environment as much energy and environmentallyhazardous materials like lead and sulfuric acid are required to manufacture thesebatteries. Östersjökompaniet AB and many of its customers realize the importanceof sustainable thinking and were interested in knowing if it was possible tomaximize the lifetime of these batteries. During the course of the work, differentmethods of battery charging and discharging were analyzed that could affect thebatteries lifetime and how to take care of them to optimize them. A chargecontroller was used to optimize the charge of the battery. To calculate theremaining state of charge in the battery, the Extended voltmeter method was used.A prototype that was able to charge the batteries optimally, warn when the batterycapacity became too low, and a user-friendly application for battery monitoring wasdesigned. The calculated lifetime of a battery is not an exact science. According tostudies the lifetime of a battery can be doubled if it is c</p>

w='powergenerated' val={'c': 'power generated', 's': 'diva2:1216523', 'n': 'there was a newline between the words'}

Note that the abstract in DiVA was trunced, the version below also includes the missing text.
corrected abstract:
<p>The use of solar cells is continuously increasing in Sweden and the power generated by the solar cells is usually stored in lead acid batteries. These batteries have a bad impact on the environment as much energy and environmentally hazardous materials like lead and sulfuric acid are required to manufacture these batteries. Östersjökompaniet AB and many of its customers realize the importance of sustainable thinking and were interested in knowing if it was possible to maximize the lifetime of these batteries. During the course of the work, different methods of battery charging and discharging were analyzed that could affect the batteries lifetime and how to take care of them to optimize them. A charge controller was used to optimize the charge of the battery. To calculate the remaining state of charge in the battery, the Extended voltmeter method was used. A prototype that was able to charge the batteries optimally, warn when the battery capacity became too low, and a user-friendly application for battery monitoring was designed. The calculated lifetime of a battery is not an exact science. According to studies the lifetime of a battery can be doubled if it is charged and discharged in an optimal way compared to when it is fully discharged.</p>
----------------------------------------------------------------------
In diva2:1447001 abstract is: <p>In today’s society, with several environmental challenges such as global warming, the demand for cleanand renewable fuels is ever increasing. Since the aviation industry in Sweden is responsible for the sameamount of greenhouse gas emissions as the motor traffic, a change to a non-polluting energy source forflying vehicles would be considerable progress. Therefore, this project has designed a hybrid system of abattery and a fuel cell and investigated how different combinations of battery and fuel cell sizes perform ina drive cycle, through computer modelling. As batteries possess a high specific power but are heavy, thefuel cells with high specific energy complement them with a sustained and lightweight power supply,which makes the hybrid perfect for aviation. The bachelor thesis is a part of Project Green Raven, aninterdisciplinary collaboration with the institutions of Applied Electrochemistry, Mechatronics andEngineering Mechanics at KTH Royal Institute of Techology. The drive cycle simulations were done inSimulink, and several assumptions regarding the power profile, fuel cell measurements and power weremade. Three different energy management strategies were set up, determining the fuel cell powerdepending on hydrogen availability and state of charge of the battery. The strategies were called 35/65,20/80 and 0/100, and the difference between them was at which state of charge intervals the fuel cellchanged its power output. The best strategy proved to be 0/100, since it was the only option which causedno degradation of the fuel cell whatsoever.</p>

w='Techology' val={'c': 'Technology', 's': 'diva2:1447001', 'n': 'error in original'}

corrected abstract:
<p>In today’s society, with several environmental challenges such as global warming, the demand for clean and renewable fuels is ever increasing. Since the aviation industry in Sweden is responsible for the same amount of greenhouse gas emissions as the motor traffic, a change to a non-polluting energy source for flying vehicles would be considerable progress. Therefore, this project has designed a hybrid system of a battery and a fuel cell and investigated how different combinations of battery and fuel cell sizes perform in a drive cycle, through computer modelling. As batteries possess a high specific power but are heavy, the fuel cells with high specific energy complement them with a sustained and lightweight power supply, which makes the hybrid perfect for aviation. The bachelor thesis is a part of Project Green Raven, an interdisciplinary collaboration with the institutions of Applied Electrochemistry, Mechatronics and Engineering Mechanics at KTH Royal Institute of Techology. The drive cycle simulations were done in Simulink, and several assumptions regarding the power profile, fuel cell measurements and power were made. Three different energy management strategies were set up, determining the fuel cell power depending on hydrogen availability and state of charge of the battery. The strategies were called 35/65, 20/80 and 0/100, and the difference between them was at which state of charge intervals the fuel cell changed its power output. The best strategy proved to be 0/100, since it was the only option which caused no degradation of the fuel cell whatsoever.</p>
----------------------------------------------------------------------
title: "Compartmentmentalized immuno-sequencing (cI-Seq): identification of immune complex interactions"
==>    "Compartmentalized Immuno-Sequencing (cI-Seq): Identification of immune complex interactions"

In diva2:1049472 abstract is: <p>Today, a lot of proteomic research is aimed at discovering disease specific proteins. This requires theavailability of high-throughput, ultra-sensitive protein detection methods. Compartmentalized immunosequencing(cI-Seq) is a proximity-independent immuno-polymerase chain reaction (IPCR) based proteindetection method. Antigen recognition in cI-Seq is mediated by antibody pairs in which one of theantibodies is conjugated to a DNA-probe. The affinity recognition events occur in emulsion droplets inwhich the DNA-probes will be amplified through emulsion PCR (emPCR) and thereafter analyzed usingMassively Parallel Sequencing (MPS). The amplifiable nature of the DNA-probes improves the sensitivityof the detection, while the use of emulsion droplets and MPS increases the multiplex capacity andthroughput. Ultimately, cI-Seq enables analysis and detection even of lowly abundant proteins therebyincreasing the probability of discovering novel disease specific proteins.</p><p>In this project, conjugation of DNA probes to antibodies was performed through two different approaches;Covalent Conjugation and Conjugation using Biotin and NeutrAvidin. Both of these approaches showedadvantageous and disadvantageous features. However, neither of them succeeded in producing stableconjugates in an efficient and reproducible manner. After conjugation, the DNA-conjugated antibodieswere used in immune complex formation. However, the immune complexes either failed to form or wereformed in an inefficient manner.</p>


corrected abstract:
<p>Today, a lot of proteomic research is aimed at discovering disease specific proteins. This requires the availability of high-throughput, ultra-sensitive protein detection methods. Compartmentalized immunosequencing (cI-Seq) is a proximity-independent immuno-polymerase chain reaction (IPCR) based protein detection method. Antigen recognition in cI-Seq is mediated by antibody pairs in which one of the antibodies is conjugated to a DNA-probe. The affinity recognition events occur in emulsion droplets in which the DNA-probes will be amplified through emulsion PCR (emPCR) and thereafter analyzed using Massively Parallel Sequencing (MPS). The amplifiable nature of the DNA-probes improves the sensitivity of the detection, while the use of emulsion droplets and MPS increases the multiplex capacity and throughput. Ultimately, cI-Seq enables analysis and detection even of lowly abundant proteins thereby increasing the probability of discovering novel disease specific proteins.</p><p>In this project, conjugation of DNA probes to antibodies was performed through two different approaches; <em>Covalent Conjugation</em> and <em>Conjugation using Biotin and NeutrAvidin</em>. Both of these approaches showed advantageous and disadvantageous features. However, neither of them succeeded in producing stable conjugates in an efficient and reproducible manner. After conjugation, the DNA-conjugated antibodies were used in immune complex formation. However, the immune complexes either failed to form or were formed in an inefficient manner.</p>
----------------------------------------------------------------------
In diva2:1109024 abstract is: <p>This study has been analyzing if machine learning could be useful to work-relatedscheduling. The analysis was based on predictions generated by prototypes usingbusiness calendars. The business calendars were collected from two service and installationcompanies in the Stockholm region. An analysis was conducted regardingif the application could be practically applied to devices such as a smartphone. Theanalysis was based on tests regarding the prototypes required time to perform theirtasks.Three prototypes were developed with algorithms that made them predictive. Density-based Spatial Clustering of Applications with Noise (DBSCAN), Logistic Regressionand Weighted K-Nearest Neighbors (wKNN) were the implemented algorithms.DBSCAN was the best-performing algorithm according to the tests. However, a conclusioncould not be found concerning whether machine learning could be useful.The number of successful predictions did not exceed the number of available timeson concerned days, which was assumed as unsatisfying results. In addition, the prototypesneeded a significant amount of resources which could be a problem in practicaluse.</p>


corrected abstract:
<p>This study has been analyzing if machine learning could be useful to work-related scheduling. The analysis was based on predictions generated by prototypes using business calendars. The business calendars were collected from two service and installation companies in the Stockholm region. An analysis was conducted regarding if the application could be practically applied to devices such as a smartphone. The analysis was based on tests regarding the prototypes required time to perform their tasks.</p><p>Three prototypes were developed with algorithms that made them predictive. Density-based Spatial Clustering of Applications with Noise (DBSCAN), Logistic Regression and Weighted K-Nearest Neighbors (wKNN) were the implemented algorithms. DBSCAN was the best-performing algorithm according to the tests. However, a conclusion could not be found concerning whether machine learning could be useful. The number of successful predictions did not exceed the number of available times on concerned days, which was assumed as unsatisfying results. In addition, the prototypes needed a significant amount of resources which could be a problem in practical use.</p>
----------------------------------------------------------------------
title: "Ethical Hacking of an IoT-device:Threat Assessment andPenetration Testing"
==>    "Ethical Hacking of an IoT-device: Threat Assessment and Penetration Testing"

In diva2:1472577 abstract is: <p>Abstract</p><p>Internet of things (IoT) devices are becoming more prevalent.Due to a rapidly growing market of these appliances, impropersecurity measures lead to an expanding range of attacks. There isa devoir of testing and securing these devices to contribute to amore sustainable society. This thesis has evaluated the securityof an IoT-refrigerator by using ethical hacking, where a threatmodel was produced to identify vulnerabilities. Penetration testswere performed based on the threat model. The results from thepenetration tests did not find any exploitable vulnerabilities. Theconclusion from evaluating the security of this Samsungrefrigerator can say the product is secure and contributes to aconnected, secure, and sustainable society.</p><p>Keywords</p><p>Internet of things (IoT), device, security, penetration testing,threat assessment, vulnerabilities</p>

corrected abstract:
<p>Internet of things (IoT) devices are becoming more prevalent. Due to a rapidly growing market of these appliances, improper security measures lead to an expanding range of attacks. There is a devoir of testing and securing these devices to contribute to a more sustainable society. This thesis has evaluated the security of an IoT-refrigerator by using ethical hacking, where a threat model was produced to identify vulnerabilities. Penetration tests were performed based on the threat model. The results from the penetration tests did not find any exploitable vulnerabilities. The conclusion from evaluating the security of this <em>Samsung</em> refrigerator can say the product is secure and contributes to a connected, secure, and sustainable society.</p>
---------------------------------------------------------------------
In diva2:1528033 abstract is: <p>Security in applications, to protect users and information, is becoming increasingly importantas society is becoming more digitized. During the duration of this report two major informationleaks, of sensitive Swedish classified information, occurred. The aim of this report is to findwhat security risks exist for webapplications, general measures that can be implemented to address these risks and how The General Data Protection Regulation (GDPR) is related to securityin applications. In order to achieve these general measures a security evaluation method was alsoused to be able to evaluate how the various measures protect webapplications, and function asrequired. The results of this report provide a general list of actions that should be implementedin application backends, but to provide a similar for the frontend more wokrs is required, wherethe frontend action list in this report is minimal. The safety evaluation method also proved to bea part of the action lists in order to be able to test the security even on operational applications.The results of the GDPR survey showed that no specific requirements are set from GDPR,instead the law has the task of raising the priority by making the consequences that can arisefrom incorrect handling of users’ personal data more serious / costly to the organization</p>


w='wokrs' val={'c': 'works', 's': 'diva2:1528033', 'n': 'error in original'}

corrected abstract:
<p>Security in applications, to protect users and information, is becoming increasingly important as society is becoming more digitized. During the duration of this report two major information leaks, of sensitive Swedish classified information, occurred. The aim of this report is to find what security risks exist for webapplications, general measures that can be implemented to address these risks and how The General Data Protection Regulation (GDPR) is related to security in applications. In order to achieve these general measures a security evaluation method was also used to be able to evaluate how the various measures protect webapplications, and function as required. The results of this report provide a general list of actions that should be implemented in application backends, but to provide a similar for the <em>frontend</em> more wokrs is required, where the frontend action list in this report is minimal. The safety evaluation method also proved to be a part of the action lists in order to be able to test the security even on operational applications. The results of the GDPR survey showed that no specific requirements are set from GDPR, instead the law has the task of raising the priority by making the consequences that can arise from incorrect handling of users’ personal data more serious / costly to the organization.</p>
----------------------------------------------------------------------
title: "Promotion of Physical Activities of NightShift Nurses with Gamification: A Study of Investigating of Physical Activity among Night Shift Nurses and PromotingGamification"
==>    "Promotion of Physical Activities of Night Shift Nurses with Gamification: A Study of Investigating of Physical Activity among Night Shift Nurses and Promoting Gamification"

In diva2:1529843 abstract is: <p>This study shows the investigation of physical activeness among the nurses whowork in night shifts in hospitals and motivates them to do physical activities in theform of gamification in their working place. A qualitative approach is applied forgathering the data in the form of interviewing nurses to inquiry the problems andtried to find out the real opinions and scenarios related with physical inactivenessbased on the nurse’s interpretation. Working-time, working-loads, leisure-time,behaviors towards physical activity, laziness, less knowledge about voluntarymovements are coming out from the findings of this study. This paper suggests someactions such as walking, doing physical exercises, playing games etc. to encouragenurses to do more physical activities in a fun way in the workplace. A fun game,called ‘Healthy steps’ is designed based on the suggested actions in the form ofgamification is presented in this paper to promote physical activity at workplace andto encourage the nurses to participate lo lead a healthy lifestyle.</p>

w='lo' val={'c': 'to', 's': 'diva2:1529843', 'n': 'error in original - the text does not make sense - "lo lead a healthy lifestyle" - I think should be "to participate in living a healthy lifestyle." to match the Swedish'}

corrected abstract:
<p>This study shows the investigation of physical activeness among the nurses who work in night shifts in hospitals and motivates them to do physical activities in the form of gamification in their working place. A qualitative approach is applied for gathering the data in the form of interviewing nurses to inquiry the problems and tried to find out the real opinions and scenarios related with physical inactiveness based on the nurse’s interpretation. Working-time, working-loads, leisure-time, behaviors towards physical activity, laziness, less knowledge about voluntary movements are coming out from the findings of this study. This paper suggests some actions such as walking, doing physical exercises, playing games etc. to encourage nurses to do more physical activities in a fun way in the workplace. A fun game, called ‘Healthy steps’ is designed based on the suggested actions in the form of gamification is presented in this paper to promote physical activity at workplace and to encourage the nurses to participate lo lead a healthy lifestyle.</p>
----------------------------------------------------------------------
In diva2:1142521 abstract is: <p>The goal of this project was to investigate how elution buffer properties affectthe protein elution in the polishing purification step with a cation exchange chromatographyconsisting of POROS XS resin. Hence be able to better predict thebehavior of the resin, two model proteins were used for this investigation (K100and Y102). This project included lab experiments (small scale), and design ofexperiments using MODDE software to evaluate the optimal conditions. Thefactors pH, conductivity, sample load and buffer concentration were investigatedon antibody Y102 while only pH and conductivity were investigated as factorsfor antibody K100. The evaluated responses were the elution volume, aggregateamount and the amount of monomeric antibody (target protein). The results indicatedthat the elution behaviour for POROS XS resin was antibody-dependentand not general for the resin. The responses obtained different models for thetwo antibodies investigated. However, since the aggregate and monomeric proteinfor K100 had curvature indications more experiments should be performedto establish the different behaviour of aggregate and monomer for the respectiveproteins.</p>

corrected abstract:
<p>The goal of this project was to investigate how elution buffer properties affect the protein elution in the polishing purification step with a cation exchange chromatography consisting of POROS XS resin. Hence be able to better predict the behavior of the resin, two model proteins were used for this investigation (K100 and Y102). This project included lab experiments (small scale), and design of experiments using MODDE software to evaluate the optimal conditions. The factors pH, conductivity, sample load and buffer concentration were investigated on antibody Y102 while only pH and conductivity were investigated as factors for antibody K100. The evaluated responses were the elution volume, aggregate amount and the amount of monomeric antibody (target protein). The results indicated that the elution behaviour for POROS XS resin was antibody-dependent and not general for the resin. The responses obtained different models for the two antibodies investigated. However, since the aggregate and monomeric protein for K100 had curvature indications more experiments should be performed to establish the different behaviour of aggregate and monomer for the respective proteins.</p>
----------------------------------------------------------------------
In diva2:648355 abstract is: <p>Full migration to IPv6 brings the need to adjust datacommunication services for the new generationof IP protocols with maintained or expanded functionality. This thesis’ goals is to submitone or more solutions that meets requirements and the technical conditions that enables thecompany DGC:s to expand the service IP-VPN for IPv6. This includes address assignmenttechniques like prefix delegation and automatic address configuration in existing network infrastructure.Solutions are presented in six scenarios that have been investigated considering tests, analysis andexperienced problems. The investigation formed the criteria scalability, configuration complexity,compatibility, support by RFC:s and requirements stated by DGC that adds to the evaluationof the most suitable solution.The evaluation has resulted in a recommended scenario that is implementable according to givengoals.Techniques that may influence the choice of most suitable solution, but that is not yet available,are discussed and presented to point out what may needed to be considered in the future.</p>

w='DGC:s' val={'c': 'DGC', 's': 'diva2:648355', 'n': 'error in original'}
w='RFC:s' val={'c': 'RFCs', 's': 'diva2:648355', 'n': 'error in original'}

corrected abstract:
<p>Full migration to IPv6 brings the need to adjust datacommunication services for the new generation of IP protocols with maintained or expanded functionality. This thesis’ goals is to submit one or more solutions that meets requirements and the technical conditions that enables the company DGC:s to expand the service IP-VPN for IPv6. This includes address assignment techniques like prefix delegation and automatic address configuration in existing network infrastructure.</p><p>Solutions are presented in six scenarios that have been investigated considering tests, analysis and experienced problems. The investigation formed the criteria scalability, configuration complexity, compatibility, support by RFC:s and requirements stated by DGC that adds to the evaluation of the most suitable solution.</p><p>The evaluation has resulted in a recommended scenario that is implementable according to given goals.</p><p>Techniques that may influence the choice of most suitable solution, but that is not yet available, are discussed and presented to point out what may needed to be considered in the future.</p>
----------------------------------------------------------------------
In diva2:1244910 abstract is: <p>Motor intent and control rely on complex high-level and spinal networks. Untilnow, little is known about this system’s organization and mechanisms. Whilecognitive abilities play an essential role in planning movements, learning andmemorizing, their involvement during stereotyped tasks execution, aslocomotion, is still controversial. Recently, the relationship between cognitivefunctions and gait has received increasing attention.Here, a machine learning approach is used to investigate the engagement ofdi↵erent cortical areas during motor activity. In particular, data coming fromthree subjects with implanted electrodes have been analyzed in the frequencydomain to predict their tasks’ state. The choice of intracortical data hasallowed to elude motion artifacts’ presence and exploitation concern. Goodand satisfactory results have been achieved in the case of not highlystereotyped activity. During ambulation, an evidence of an engagement of thebrain has been shown even if with lower classification performances. Moreover,the cortical areas that have emerged in this analysis seem to be in line withthe relative functionality hypothesized in literature.</p>

corrected abstract:
<p>Motor intent and control rely on complex high-level and spinal networks. Until now, little is known about this system’s organization and mechanisms. While cognitive abilities play an essential role in planning movements, learning and memorizing, their involvement during stereotyped tasks execution, as locomotion, is still controversial. Recently, the relationship between cognitive functions and gait has received increasing attention. Here, a machine learning approach is used to investigate the engagement of different cortical areas during motor activity. In particular, data coming from three subjects with implanted electrodes have been analyzed in the frequency domain to predict their tasks’ state. The choice of intracortical data has allowed to elude motion artifacts’ presence and exploitation concern. Good and satisfactory results have been achieved in the case of not highly stereotyped activity. During ambulation, an evidence of an engagement of the brain has been shown even if with lower classification performances. Moreover, the cortical areas that have emerged in this analysis seem to be in line with the relative functionality hypothesized in literature.</p>
----------------------------------------------------------------------
In diva2:1455143 abstract is: <p>Saprolegnia parasitica, a eukaryotic oomycete, is a pathogen that infects freshwater aquaticorganisms by attaching itself to an injured part of the organism and forming mycelium patches onits skin. The mycelium patches severely weaken the epithelial tissue, leading to cellular necrosisand death by haemodilution. Currently, the lack of an effective drug against the infection of thepathogen has resulted in heavy losses of the global annual fish production.</p><p>In the current study, an in silico and in vitro analysis was performed. In silico, the predictedproteomes of S. parasitica, Homo sapiens, and 31 distinct fish species were compared, and thenon-homologous essential genes were extracted. The genes were then matched against drugbank databases to acquire FDA-approved drugs that could inhibit the non-homologous essentialproteins. Out of these, three genes were selected and expressed into four different type ofEscherichia coli (E. coli) competent cells: BL21, C41, C43, and Rosetta 2. Proteins which weresuccessfully expressed were purified and specific assays were designed to check their activities.</p><p>All three proteins were expressed but in low amounts. The identity of all expressed proteins wasalso confirmed by mass spectrometry (MS). Biochemical assay was performed for two of theselected proteins, though, further optimization and inhibitor testing needs to be performed forfurther characterization. Biochemical assay could not be performed for the third selected proteindue to the unavailability of the required substrate.</p>


corrected abstract:
<p><em>Saprolegnia parasitica</em>, a eukaryotic oomycete, is a pathogen that infects freshwater aquatic organisms by attaching itself to an injured part of the organism and forming mycelium patches on its skin. The mycelium patches severely weaken the epithelial tissue, leading to cellular necrosis and death by haemodilution. Currently, the lack of an effective drug against the infection of the pathogen has resulted in heavy losses of the global annual fish production.</p><p>In the current study, an <em>in silico</em> and <em>in vitro</em> analysis was performed. In silico, the predicted proteomes of <em>S. parasitica</em>, <em>Homo sapiens</em>, and 31 distinct fish species were compared, and the non-homologous essential genes were extracted. The genes were then matched against drug bank databases to acquire FDA-approved drugs that could inhibit the non-homologous essential proteins. Out of these, three genes were selected and expressed into four different type of <em>Escherichia coli</em> (<em>E. coli</em>) competent cells: BL21, C41, C43, and Rosetta 2. Proteins which were successfully expressed were purified and specific assays were designed to check their activities.</p><p>All three proteins were expressed but in low amounts. The identity of all expressed proteins was also confirmed by mass spectrometry (MS). Biochemical assay was performed for two of the selected proteins, though, further optimization and inhibitor testing needs to be performed for further characterization. Biochemical assay could not be performed for the third selected protein due to the unavailability of the required substrate.</p>
----------------------------------------------------------------------
In diva2:1664916 abstract is: <p>Collecting and deploying online games made by inexperienced developers can behard. This is something KTH (Royal Institute of Technology) has a problem withpertaining to a course involving SDL and SDL_Net programming. A good solutionto this problem is to host these games on a website. An easy-to-use way of compilingand deploying multiplayer games and game-servers written in C as web applicationsand web servers was needed. Specifically for games written in C using SDL andSDL_net libraries. The compiler toolchain Emscripten was used to compile gameand server code from C to WebAssembly, that could then be used through the generated JavaScript functions. Communication between the client and the server washandled by WebSockets. As much of the Emscripten specific functions were to behidden behind C libraries, emulating the format of SDL_Net. The finished solutionsthat emulated the format of SDL_Net, consisted of two new libraries, one for theserver and the other for the client. The libraries successfully emulated the TCP partsof SDL_Net library. The browsers event scheduler necessitates applications to beable to return control back to it. This meant that the game codes endlessly loopingfunctions had to be rewritten to enable rendering in the browser. </p>

corrected abstract:
<p>Collecting and deploying online games made by inexperienced developers can be hard. This is something KTH (Royal Institute of Technology) has a problem with pertaining to a course involving SDL and SDL_Net programming. A good solution to this problem is to host these games on a website. An easy-to-use way of compiling and deploying multiplayer games and game-servers written in C as web applications and web servers was needed. Specifically for games written in C using SDL and SDL_net libraries. The compiler toolchain Emscripten was used to compile game and server code from C to WebAssembly, that could then be used through the generated JavaScript functions. Communication between the client and the server was handled by WebSockets. As much of the Emscripten specific functions were to be hidden behind C libraries, emulating the format of SDL_Net. The finished solutions that emulated the format of SDL_Net, consisted of two new libraries, one for the server and the other for the client. The libraries successfully emulated the TCP parts of SDL_Net library. The browsers event scheduler necessitates applications to be able to return control back to it. This meant that the game codes endlessly looping functions had to be rewritten to enable rendering in the browser.</p>
----------------------------------------------------------------------
In diva2:1562805 abstract is: <p>Railway interlocking is used for safe control of train traffic. A computer based interlockingsystem ensures that trains can be driven on a railway line by locking train paths and preventingunauthorized combinations of train paths. The connection between the different parts of a yard(objects) and the interlocking system is performed with the help of Object Controllers (OC).The Swedish name for OC is “utdelar”. Track circuits are railway objects used for thedetection of trains on a specific railway section.</p><p>EBILOCK 950 will replace EBILOCK 850 at Östra Station. Through a literature study and asystem analysis, this work will investigate the effects this has on delays in the 75 Hz AC trackcircuit (SASIB) and the audio frequency track circuit DigiCode, together with the delays fromthe Object Controller Systems, JZU840 and OCS950. This was done with the aim of findingout how the cycle time in EBILOCK 950 is affected by the delays from the track circuits andOC. The cycle time is the time it takes for the interlocking system to receive a status updatefrom OC, until the interlocking system sends new commands to be executed by the objects.</p><p>The reason for the additional delays for the track circuits is a consequence of contact bounces.The contact bounces occur when the track circuit relay change position and means that thearmature in the relay does not stop immediately but bounces and the stabilizes. If the relay inthe track circuits can guarantee an absolute change of position more quickly, it is possible toreduce the cycle time and increase the capacity at Östra Station.</p>


corrected abstract:
<p>Railway interlocking is used for safe control of train traffic. A computer based interlocking system ensures that trains can be driven on a railway line by locking train paths and preventing unauthorized combinations of train paths. The connection between the different parts of a yard (objects) and the interlocking system is performed with the help of Object Controllers (OC). The Swedish name for OC is “utdelar”. Track circuits are railway objects used for the detection of trains on a specific railway section.</p><p>EBILOCK 950 will replace EBILOCK 850 at Östra Station. Through a literature study and a system analysis, this work will investigate the effects this has on delays in the 75 Hz AC track circuit (SASIB) and the audio frequency track circuit DigiCode, together with the delays from the Object Controller Systems, JZU840 and OCS950. This was done with the aim of finding out how the cycle time in EBILOCK 950 is affected by the delays from the track circuits and OC. The cycle time is the time it takes for the interlocking system to receive a status update from OC, until the interlocking system sends new commands to be executed by the objects.</p><p>The reason for the additional delays for the track circuits is a consequence of contact bounces. The contact bounces occur when the track circuit relay change position and means that the armature in the relay does not stop immediately but bounces and the stabilizes. If the relay in the track circuits can guarantee an absolute change of position more quickly, it is possible to reduce the cycle time and increase the capacity at Östra Station.</p>
----------------------------------------------------------------------

In diva2:801739 abstract is: <p>The existing seasonal influenza vaccine does not provide broad long-term protection against seasonal influenza and must be remanufactured yearly due to frequens mutations and reassortment of theinfluenza genes. A universal influenza vaccine with the ability to raise long lasting immunity is the focus of several studies, including the Edufluvac project. Edufluvac is based on virus-likeparticles, a modern recombinant platform wellsuited for vaccinatin applications. Redbiotec's rePAX® technology allows the generation of multivalent recombinant baculovirus which generatesvirus-like particles presenting multiple proteins on the surface in insect cell culture. Any effects oninsect cell culture protein expression brought on by the regulatory elements controlling each gene in the baculovirus, or by the genome position of the baculovirus genes, could affect the composition of the virus-like  particles. The ai of this thesis was to elicit a better understanding of the protein expression by analysing multi-protein influenza virus-like particles and virus-like particles encoding a reporter gene regulated by different promoter and terminator combinations. Different bivalent and tetravalent influenza gene bacmids were cloned as well as seven bacmids encoding a YFP gene regulated by different promoter and terminator combinations. Spodopera frugiperda cells weretransfected with the bacmids and harvested recombinant baculovirus was used to perform testexpressions in High-Five™ cells. The resulting protein expression levels from the bivalent andtetravalent recombinant baculovirus were analyzed and compared by Western blots and ELISA assays. The expression of YFP in infected Spodoptera and High-Five™ cells was monitored byfluorescence microscopy and measured with FACS to quantify protein expression differencesbetween the seven promoter and terminator combinations. Analysis of the bivalent constructs indicated that the order of the genes in a recombinant baculovirus does not affect the protein expression in High-Five™ cells. The analysis of the tetravalent constructs revelaed positionalvariations in expressin of the H1 and M1 genes, but the number of test expressions and recombinant baculovirus construct clones included in the analysis were not hogh enough to allow a definitive conclusion. Of the different promoter and terminator constructs highest mean fluorescence intensity was reached with the reference combination. The early promoter yielded mean fluorescent intensitites that were close to the values of the negative control in both cell lines. </p>

w='frequens' val={'c': 'frequent', 's': 'diva2:801739', 'n': 'correct in original'}
w='hogh' val={'c': 'high', 's': 'diva2:801739', 'n': 'correct in the original'}
w='revelaed' val={'c': 'revealed', 's': 'diva2:801739', 'n': 'correct in original'}
w='vaccinatin' val={'c': 'vaccination', 's': 'diva2:801739', 'n': 'correct in original'}
w='intensitites' val={'c': 'intensities', 's': 'diva2:801739'}
w='Spodopera' val={'c': 'Spodoptera', 's': 'diva2:801739', 'n': 'correct in original'}

corrected abstract:
<p>The existing seasonal influenza vaccine does not provide broad long-term protection against seasonal influenza and must be remanufactured yearly due to frequent mutations and reassortment of the influenza genes [1,2]. A universal influenza vaccine with the ability to raise long lasting immunity is the focus of several studies, including the Edufluvac project [3]. Edufluvac is based on virus-like particles, a modern recombinant platform well suited for vaccination applications. Redbiotec’s rePAX® technology allows the generation of multivalent recombinant baculovirus which generates virus-like particles presenting multiple proteins on the surface in insect cell culture. Any effects on insect cell culture protein expression brought on by the regulatory elements controlling each gene in the baculovirus, or by the genome position of the baculovirus genes, could affect the composition of the virus-like particles. The aim of this thesis was to elicit a better understanding of the protein expression by analyzing multi-protein influenza virus-like particles and virus-like particles encoding a reporter gene regulated by different promoter and terminator combinations. Different bivalent and tetravalent influenza gene bacmids were cloned as well as seven bacmids encoding a YFP gene regulated by different promoter and terminator combinations. <em>Spodoptera frugiperda</em> cells were transfected with the bacmids and harvested recombinant baculovirus was used to perform test expressions in High-Five™ cells. The resulting protein expression levels from the bivalent and tetravalent recombinant baculovirus were analyzed and compared by Western blots and ELISA assays. The expression of YFP in infected <em>Spodoptera</em> and High-Five™ cells was monitored by fluorescence microscopy and measured with FACS to quantify protein expression differences between the seven promoter and terminator combinations. Analysis of the bivalent constructs indicated that the order of the genes in a recombinant baculovirus does not affect the protein expression in High-Five™ cells. The analysis of the tetravalent constructs revealed positional variations in expression of the H1 and M1 genes, but the number of test expressions and recombinant baculovirus construct clones included in the analysis were not high enough to allow a definitive conclusion. Of the different promoter and terminator constructs highest mean fluorescence intensity was reached with the reference combination. The early promoter yielded mean fluorescent intensities that were close to the values of the negative control in both cell lines.</p>
----------------------------------------------------------------------
The above were all sent on or before 2024-09-25
======================================================================
In diva2:1451767 abstract is: <p>AbstractThe starting point for this thesis is to determine how well a chatbot can be adapted for emailconversations. If chatbots can be customized for email conversations, the need for humanwork in monotonous and predictable email conversations could be reduced. An existingchatbot framework has been implemented and a dialogue decision system for responseconstruction based on the classified intents has been developed. The whole system is called aconversational unit. The goal of the conversation unit is to achieve a precision value of 0.90,which was achieved with an implementation of Watson assistant. The precision value is usedto measure how well the conversation unit can be adapted for mail conversations. Thedialogue decision system should be able to give different answers depending on what intentsare classified and answers that depend on the existence of other intents should be independentof the order the intents were classified. The developed dialogue decision system achieves theset goals.KeywordsChatbot, NLU, Mailbot, Dialog decision system4</p>


corrected abstract:
<p>The starting point for this thesis is to determine how well a chatbot can be adapted for email conversations. If chatbots can be customized for email conversations, the need for human work in monotonous and predictable email conversations could be reduced. An existing chatbot framework has been implemented and a dialogue decision system for response construction based on the classified intents has been developed. The whole system is called a conversational unit. The goal of the conversation unit is to achieve a precision value of 0.90, which was achieved with an implementation of Watson assistant. The precision value is used to measure how well the conversation unit can be adapted for mail conversations. The dialogue decision system should be able to give different answers depending on what intents are classified and answers that depend on the existence of other intents should be independent of the order the intents were classified. The developed dialogue decision system achieves the set goals.</p>
----------------------------------------------------------------------
In diva2:1454819 abstract is: <p>In this project, we investigate the functional role of rare variants in common diseasecontext. The field of common disease genomics has been stalled mainly due to theslow pace of translating genetic variant information into clinical care. In particular,the role of rare variants in modulating common disease risk is not well explored.</p><p>Using Capture Hi-C, Sahlén Lab has previously mapped promoter-enhancerinteractions of aortic endothelial cells taken from six individuals. This enabled theisolation of private variants that overlapped with private interactions for eachindividual. Here, we have cloned eight enhancer sequences containing such rarevariants into pGL3 reporter vectors and transfected them into a human cell line. Wethen performed a luciferase assay to measure the effect size of the chosen variants.</p><p>Our results display the numerous steps necessary to prepare genomic DNA regionsfor a luciferase assay. The assay showed that 2 alternative alleles, rs185055141 andrs147010901, altered luciferase expression and therefore may have an effect on generegulation.</p><p>This study will hopefully serve as a guide for future research efforts in evaluating theeffect of rare variants in enhancer sequences. It is very relevant in today’s society toincrease our understanding of the genetic landscape of complex diseases.</p>


corrected abstract:
<p>In this project, we investigate the functional role of rare variants in common disease context. The field of common disease genomics has been stalled mainly due to the slow pace of translating genetic variant information into clinical care. In particular, the role of rare variants in modulating common disease risk is not well explored.</p><p>Using Capture Hi-C, Sahlén Lab has previously mapped promoter-enhancer interactions of aortic endothelial cells taken from six individuals. This enabled the isolation of private variants that overlapped with private interactions for each individual. Here, we have cloned eight enhancer sequences containing such rare variants into pGL3 reporter vectors and transfected them into a human cell line. We then performed a luciferase assay to measure the effect size of the chosen variants.</p><p>Our results display the numerous steps necessary to prepare genomic DNA regions for a luciferase assay. The assay showed that 2 alternative alleles, rs185055141 and rs147010901, altered luciferase expression and therefore may have an effect on gene regulation.</p><p>This study will hopefully serve as a guide for future research efforts in evaluating the effect of rare variants in enhancer sequences. It is very relevant in today’s society to increase our understanding of the genetic landscape of complex diseases.</p>
----------------------------------------------------------------------
In diva2:1889754 abstract is: <p>At SHL Medical in Nacka, Sweden, autoinjectors are developed and during their developmentstage, the autoinjectors are tested on a soft tissue model made of gel. The gel however haschallenges with robustness and customization. To eliminate these challenges, a soft tissuemodel of silicone was created. The aim of this project was to determine whether the twomaterials could generate the same results and, if therefore, the gel model was replaceable.</p><p>Four different autoinjector models were chosen to be tested on four different silicone modelswith different geometry and stiffness. This was done to obtain a broad observation of how thesilicone performed during testing. All autoinjectors were tested on all different siliconemodels and the gel model to compare their results.</p><p>We could conclude that the silicone model generates similar results to the gel model and thatthe gel model can be replaced by the silicone model. Additionally, it was concluded whichstiffness and geometry of the silicone model generates the most similar results to the gelmodel. Apart from this, the project gave guidance of how the silicone model can further bedeveloped to better match the gel model. </p>


corrected abstract:
<p>At SHL Medical in Nacka, Sweden, autoinjectors are developed and during their development stage, the autoinjectors are tested on a soft tissue model made of gel. The gel however has challenges with robustness and customization. To eliminate these challenges, a soft tissue model of silicone was created. The aim of this project was to determine whether the two materials could generate the same results and, if therefore, the gel model was replaceable.</p><p>Four different autoinjector models were chosen to be tested on four different silicone models with different geometry and stiffness. This was done to obtain a broad observation of how the silicone performed during testing. All autoinjectors were tested on all different silicone models and the gel model to compare their results.</p><p>We could conclude that the silicone model generates similar results to the gel model and that the gel model can be replaced by the silicone model. Additionally, it was concluded which stiffness and geometry of the silicone model generates the most similar results to the gel model. Apart from this, the project gave guidance of how the silicone model can further be developed to better match the gel model.</p>
----------------------------------------------------------------------
In diva2:1665259 abstract is: <p>The application of instruments in the practice of explosive sports, to measure an athlete's force production, can be used to determine how well an athlete performs. Present instruments on the marketare most often applied to sprint starts and function to capture an athlete on film, and link the eventto the data retrieved from the instrument. The disadvantages with present day systems are that theyare either too expensive or lack flexibility and portability as they are usually delivered with a lot ofperipherals.The purpose of this thesis was to develop a system of a mobile application and logical code to a microcontroller that register horizontal forces at sprint starts from an analog source. The collected datawas calculated and transmitted using BLE communication to the mobile application which presentsthe force data to the user.Moreover, the purpose was also to have the acquired force data time-synchronized with a mobile device to make it possible to evaluate events against external sources such as IMU and high-speed film.The result for the logic developed throughout this work demonstrate that it is possible to retrievesampled force data from a microcontroller via BLE communication. It was possible to present thecalculated force data visually to an end user with a mobile application and have the events time synchronized using time synchronization algorithms. However, the results can be further improved bydevelopment of the system.</p>


corrected abstract:
<p>The application of instruments in the practice of explosive sports, to measure an athlete's force production, can be used to determine how well an athlete performs. Present instruments on the market are most often applied to sprint starts and function to capture an athlete on film, and link the event to the data retrieved from the instrument. The disadvantages with present day systems are that they are either too expensive or lack flexibility and portability as they are usually delivered with a lot of peripherals.</p><p>The purpose of this thesis was to develop a system of a mobile application and logical code to a microcontroller that register horizontal forces at sprint starts from an analog source. The collected data was calculated and transmitted using BLE communication to the mobile application which presents the force data to the user.</p><p>Moreover, the purpose was also to have the acquired force data time-synchronized with a mobile device to make it possible to evaluate events against external sources such as IMU and high-speed film.</p><p>The result for the logic developed throughout this work demonstrate that it is possible to retrieve sampled force data from a microcontroller via BLE communication. It was possible to present the calculated force data visually to an end user with a mobile application and have the events time synchronized using time synchronization algorithms. However, the results can be further improved by development of the system.</p>
----------------------------------------------------------------------
In diva2:1520078 abstract is: <p>Due to increase use of plastics, bio-based polymers as packaging materials have garneredmuch attention in recent years due to environmental concerns. Several protein materials, e.g.wheat gluten, have been in focus for significant research towards new biobased plastics andresults are promising. Bio-based and environmentally friendly plastics gather much interestand attention today due to the green-house generating effects of conventional petroleum-basedplastics.</p><p>A protein material was investigated here for its plastic material properties. The protein puritywas ca. 65 %. The films were created by first grinding the protein flakes to a fine powder andmixing it with glycerol. The glycerol content was 30 %. The material was subsequently hotpressed.To test the plastic, multiple techniques and methods were used. TGA, DSC, FT-IR, WVTR,OTR and tensile testing. In general, the material was relatively weak. As most protein plasticsit had also poor water barrier properties, however it had relatively good oxygen-barrierproperties.</p><p>In conclusion it is a material that could have a bright future as it is made from biomass insteadof petroleum, which means that it is more environmentally friendly. With modification andimprovement, it can be a good plastic for several applications in future.</p>


corrected abstract:
<p>Due to increase use of plastics, bio-based polymers as packaging materials have garnered much attention in recent years due to environmental concerns. Several protein materials, e.g. wheat gluten, have been in focus for significant research towards new biobased plastics and results are promising. Bio-based and environmentally friendly plastics gather much interest and attention today due to the green-house generating effects of conventional petroleum-based plastics.</p><p>A protein material was investigated here for its plastic material properties. The protein purity was ca. 65 %. The films were created by first grinding the protein flakes to a fine powder and mixing it with glycerol. The glycerol content was 30 %. The material was subsequently hot-pressed.</p><p>To test the plastic, multiple techniques and methods were used. TGA, DSC, FT-IR, WVTR, OTR and tensile testing. In general, the material was relatively weak. As most protein plastics it had also poor water barrier properties, however it had relatively good oxygen-barrier properties.</p><p>In conclusion it is a material that could have a bright future as it is made from biomass instead of petroleum, which means that it is more environmentally friendly. With modification and improvement, it can be a good plastic for several applications in future.</p>
----------------------------------------------------------------------
In diva2:1322258 abstract is: <p>Following circumstances such as mergers and acquisitions, the IT systemsassociated with the participating organisations may need to share access towardsservices and systems with eachother. Access towards systems and services is oftencontrolled using secret information such as passwords or keys. This implies thatsharing access between IT systems is achieved by sharing secret information.</p><p>This thesis proposes new methods for automatic synchronization of secretsbetween different secret management systems that may not be natively compatiblewith one another. After examining how the already existing secret managementsystems function as well as created a data centric threat model, a system design wasproposed. A secret proxy connects to each secret management system which in turnconnects to a central secret distributor that handles and updates the other proxies.</p><p>The results indicate that such a system can be implemented and securely distributesecrets automatically. By synchronizing secrets automatically, the work involvedwith supporting several secret management systems in parallel which all needaccess to some common secrets could be reduced.</p>


In the actual abstract "eachother" is set as one word.

corrected abstract:
<p>Following circumstances such as mergers and acquisitions, the IT systems associated with the participating organisations may need to share access towards services and systems with eachother. Access towards systems and services is often controlled using secret information such as passwords or keys. This implies that sharing access between IT systems is achieved by sharing secret information.</p><p>This thesis proposes new methods for automatic synchronization of secrets between different secret management systems that may not be natively compatible with one another. After examining how the already existing secret management systems function as well as created a data centric threat model, a system design was proposed. A secret proxy connects to each secret management system which in turn connects to a central secret distributor that handles and updates the other proxies.</p><p>The results indicate that such a system can be implemented and securely distribute secrets automatically. By synchronizing secrets automatically, the work involved with supporting several secret management systems in parallel which all need access to some common secrets could be reduced.</p>
----------------------------------------------------------------------
In diva2:1260917 abstract is: <p>CHO (Chinese hamster ovary) cells have become the workhorse for industrial production of biologics.Deciphering the internal metabolism of these cells will be a valuable information to design new medium and achieve advanced process control. In this study, the observations pertaining toamino acid metabolism of CHO cells in response to varied amino acid balance in the medium wasobserved and recorded. Pseudo perfusion method formed the basis in conducting the experimentsto determine the uptake and production rates of all amino acids in different culture medium together with other important cell culture metabolites like glucose, ammonia, lactate, product titer,biomass and viability, etc. Cells showed different metabolic response to different tested balanceof amino acids. The response generated were recorded for using it with mathematical tools infuture. The above objective was achieved by conducting a series of 3 experiments. Experiment 01;to determine how low a commercial medium can be diluted to still have acceptable growth and performance in terms of productivity, viability, etc. Experiment 02; to record response of cells tovaried amino acid balance in the medium. This experiment was later redesigned to amplify themetabolic response seen in the experiment. Conducting a similar experiment at higher cell density was thought to help us see amplified metabolic response and this was done as Experiment 03. Theresults of this experiment in terms of amino acid uptake and production rates is yet to be performedto see if we obtained an amplified response.</p>


corrected abstract:
<p>CHO (Chinese hamster ovary) cells have become the workhorse for industrial production of biologics. Deciphering the internal metabolism of these cells will be a valuable information to design new medium and achieve advanced process control. In this study, the observations pertaining to amino acid metabolism of CHO cells in response to varied amino acid balance in the medium was observed and recorded. Pseudo perfusion method formed the basis in conducting the experiments to determine the uptake and production rates of all amino acids in different culture medium to gether with other important cell culture metabolites like glucose, ammonia, lactate, product titer, biomass and viability, etc. Cells showed different metabolic response to different tested balance of amino acids. The response generated were recorded for using it with mathematical tools in future. The above objective was achieved by conducting a series of 3 experiments. Experiment 01;to determine how low a commercial medium can be diluted to still have acceptable growth and performance in terms of productivity, viability, etc. Experiment 02; to record response of cells to varied amino acid balance in the medium. This experiment was later redesigned to amplify the metabolic response seen in the experiment. Conducting a similar experiment at higher cell density was thought to help us see amplified metabolic response and this was done as Experiment 03. The results of this experiment in terms of amino acid uptake and production rates is yet to be performed to see if we obtained an amplified response.</p>
----------------------------------------------------------------------
In diva2:1775282 abstract is: <p>Around 17% of the population in Stockholm suffers from speech or voice disorders. In case ofsuch disorders, it is common to visit a speech therapist for medical examination. During theexamination, the speech therapist uses VoiceJournal that is developed by NeoviusSignalsystem och Data AB, which includes different programs for voice recording andanalysis. Although the programs have been operating efficiently, there are areas where thatcould be improved. Prior to a new release of VoiceJournal, the company wanted to get rid ofan annoyance in the form of a dialogue box that should not occur. Additionally, the possibilityto play the recording by remote playback would be further examined given that it did notfunctioned properly. Lastly, a new feature would be implemented that enables the speechtherapist to undo various actions made to the file. This was done by modifying and addingcode. Out of these three programming tasks, the first two were finalized while the third onewas completed to some extent since all the various actions on the recording could not beundone one or multiple times.</p>

corrected abstract:
<p>Around 17% of the population in Stockholm suffers from speech or voice disorders. In case of such disorders, it is common to visit a speech therapist for medical examination. During the examination, the speech therapist uses VoiceJournal that is developed by Neovius Signalsystem och Data AB, which includes different programs for voice recording and analysis. Although the programs have been operating efficiently, there are areas where that could be improved. Prior to a new release of VoiceJournal, the company wanted to get rid of an annoyance in the form of a dialogue box that should not occur. Additionally, the possibility to play the recording by remote playback would be further examined given that it did not functioned properly. Lastly, a new feature would be implemented that enables the speech therapist to undo various actions made to the file. This was done by modifying and adding code. Out of these three programming tasks, the first two were finalized while the third one was completed to some extent since all the various actions on the recording could not be undone one or multiple times.</p>
----------------------------------------------------------------------
In diva2:842556 abstract is: <p>Hearing-impaired individuals that use hearing aids often experience problemswhen exposed to daily situations in life. The di↵erent environments include forexample classrooms, offices and public areas. For the hearing impaired, the correctadjustments of the hearing aids are of great importance. For these settings to beproper, a measurement called the Speech Intelligibility Index (SII), is questionedto be implemented in the clinical hearing care. To answer this question of issuewhether feasibility lay in the implementation of SII in the clinical hearing carewith the interface of hearing aids, a literature study ordered by the Hearing- &amp;Balance Clinic (H&amp;B) at Karolinska University Hospital situated at RosenlundHospital, was initiated. The focus of the study was primarily held in concern ofSII as a hearing aid validation measurement and later extended to SII as a roomacoustic measurement disregarding the e↵ect of hearing aids. The result of thestudy showed that the use of SII in the clinical hearing care is troubled and furtherthat the implementation as a room acoustic measurement is probable but has tobe additionally investigated.</p>

Note severl mishandled ligatures.

corrected abstract:
<p>Hearing-impaired individuals that use hearing aids often experience problems when exposed to daily situations in life. The different environments include for example classrooms, offices and public areas. For the hearing impaired, the correct adjustments of the hearing aids are of great importance. For these settings to be proper, a measurement called the Speech Intelligibility Index (SII), is questioned to be implemented in the clinical hearing care. To answer this question of issue whether feasibility lay in the implementation of SII in the clinical hearing care with the interface of hearing aids, a literature study ordered by the Hearing- &amp; Balance Clinic (H&amp;B) at Karolinska University Hospital situated at Rosenlund Hospital, was initiated. The focus of the study was primarily held in concern of SII as a hearing aid validation measurement and later extended to SII as a room acoustic measurement disregarding the effect of hearing aids. The result of the study showed that the use of SII in the clinical hearing care is troubled and further that the implementation as a room acoustic measurement is probable but has to be additionally investigated.</p>
----------------------------------------------------------------------
In diva2:744710 abstract is: <p>Motivation: The advent of clinical exome sequencing will require new tools to handlecoverage data and making it relevant to clinicians. That means genes over targets, smartsoftware over BED-files, and full stack, automated solutions from BAM-files to genetic testreport. Fresh ideas can also provide new insights into the factors that cause certain regionsof the exome to receive poor coverage.Results: A novel coverage analysis tool for analyzing clinical exome sequencing data has beendeveloped. Named Chanjo, it’s capable of converting between different elements such astargets and exons, supports custom annotations, and provides powerful statistics andplotting options. A coverage investigation using Chanjo linked both extreme GC content andlow sequence complexity to poor coverage. High bait density was shown to increasereliability of exome capture but not improve coverage of regions that had already proventricky. To improve coverage of especially very G+C rich regions, developing new ways toamplify rather than enrich DNA will likely make the biggest difference.</p>


corrected abstract:
<p><strong>Motivation</strong>: The advent of clinical exome sequencing will require new tools to handle coverage data and making it relevant to clinicians. That means genes over targets, smart software over BED-files, and full stack, automated solutions from BAM-files to genetic test report. Fresh ideas can also provide new insights into the factors that cause certain regions of the exome to receive poor coverage.</p><p><strong>Results</strong>: A novel coverage analysis tool for analyzing clinical exome sequencing data has been developed. Named Chanjo, it’s capable of converting between different elements such as targets and exons, supports custom annotations, and provides powerful statistics and plotting options. A coverage investigation using Chanjo linked both extreme GC content and low sequence complexity to poor coverage. High bait density was shown to increase reliability of exome capture but not improve coverage of regions that had already proven tricky. To improve coverage of especially very G+C rich regions, developing new ways to amplify rather than enrich DNA will likely make the biggest difference.</p><p><strong>Availability</strong>: The source code for Chanjo is freely available at <a href="https://github.com/robinandeer/chanjo">https://github.com/robinandeer/chanjo</a>.</p>
----------------------------------------------------------------------
In diva2:1353473 abstract is: <p>In this thesis the solar-powered lightning are investigated as lightning options forwalk and cycle paths in countryside. Rarely used walk and bicycle paths in thecountryside are left without lighting by municipalities and roads associations.Therefore, it is desirable to offer a lightning technology that has zero electricitycosts and no cable routing.</p><p>The study’s hypothesis was that solar-powered lightning installations can be recommendedas street lightning for walk and cycle paths in the countryside. resultsshow that solar radiation in Sweden is low which makes it difficult for Sweden’smunicipalities and associations willing to invest. To increase energy savings insolar lighting system, advanced presence-controlled technology is needed.</p><p>The results show that presence-controlled solar lightning can provide an economicalsaving of 58 % compared to a LED network connected lightning. The savingvary depending on the type of solar lightning used. Solar LED street lighting technologyalso reduces carbon dioxide emissions.</p><p></p>


corrected abstract:
<p>In this thesis the solar-powered lightning are investigated as lightning options for walk and cycle paths in countryside. Rarely used walk and bicycle paths in the countryside are left without lighting by municipalities and roads associations. Therefore, it is desirable to offer a lightning technology that has zero electricity costs and no cable routing.</p><p>The study’s hypothesis was that solar-powered lightning installations can be recommended as street lightning for walk and cycle paths in the countryside. results show that solar radiation in Sweden is low which makes it difficult for Sweden’s municipalities and associations willing to invest. To increase energy savings in solar lighting system, advanced presence-controlled technology is needed.</p><p>The results show that presence-controlled solar lightning can provide an economical saving of 58 % compared to a LED network connected lightning. The saving vary depending on the type of solar lightning used. Solar LED street lighting technology also reduces carbon dioxide emissions.</p>
----------------------------------------------------------------------
In diva2:1095512 the thesis has a rather unique "Graphical abstract" - it would be interesting to know how to include this in the DiVA abstract metadata.


In diva2:1095512 abstract is: <p>The task of medicinal chemists in a drug discoveryproject is to synthesize/design analogues to the screening hits, simultaneouslyincreasing target potency and optimizing the pharmacological properties.  This requires a wide selection of moleculesto be synthesized, where both synthetic feasibility and price of startingmaterials are of great importance. In this work, a synthetic pathway from cheapand readily available starting materials to highly modifiable 2,4-disubstitutedpyrrolidines is demonstrated. Previously reported procedures to similarpyrrolidines use expensive catalysts, requires harsh conditions and requiresnon-commercially available starting materials. The suggested pathway herein has demonstrated great possibility forvariation in the 4-position, including fluoro, difluoro, nitrile and alcoholfunctional groups. There are several areas in which the synthesis can beimproved and expanded upon. Improvements can be made by optimizing thedescribed reaction conditions and further expansion of possible modificationsin both 2- and 4-position could be explored.</p>

corrected abstract:
<p>The task of medicinal chemists in a drug discovery project is to synthesize/design analogues to the screening hits, simultaneously increasing target potency and optimizing the pharmacological properties. This requires a wide selection of molecules to be synthesized, where both synthetic feasibility and price of starting materials are of great importance. In this work, a synthetic pathway from cheap and readily available starting materials to highly modifiable 2,4-disubstituted pyrrolidines is demonstrated. Previously reported procedures to similar pyrrolidines use expensive catalysts, requires harsh conditions and requires non-commercially available starting materials. The suggested pathway herein has demonstrated great possibility for variation in the 4-position, including fluoro, difluoro, nitrile and alcohol functional groups. There are several areas in which the synthesis can be improved and expanded upon. Improvements can be made by optimizing the described reaction conditions and further expansion of possible modifications in both 2- and 4-position could be explored.</p>
----------------------------------------------------------------------
In diva2:1454862 abstract is: <p>Nanobodies, or VHHs, are derived from heavy chain-only antibodies (hcAbs) found in camelids.They overcome some of the inherent limitations of monoclonal antibodies (mAbs) and derivativesthereof, due to their smaller molecular size and higher stability, and thus present an alternative tomAbs therapeutically. Two nanobodies, Nb23 and Nb24, have been shown to similarly inhibit theself-aggregation of a very amyloidogenic variant of β2-microglobulin, D76N β2-m, which hasimplications in hereditary systemic amyloidosis. Here, the structure of Nb23 was modeled withthe Chemical Shift (CS)-Rosetta server using chemical shift assignments from nuclear magneticresonance (NMR) spectroscopy experiments, and partially validated using the already determinedstructure of Nb24, NMR chemical shifts, circular dichroism (CD), and nuclear Overhauser effect(NOE) spectroscopy. Further, the binding properties of Nb23 and Nb24 with the antigen wereinvestigated with isothermal titration calorimetry (ITC), showing that Nb23 has slightly loweraffinity to the antigen than Nb24. The structural analysis indicated that Nb23 has a much moredynamic CDR3 loop with respect to Nb24, which could be a contributing factor to its loweraffinity.</p>

corrected abstract:
<p>Nanobodies, or VHHs, are derived from heavy chain-only antibodies (hcAbs) found in camelids. They overcome some of the inherent limitations of monoclonal antibodies (mAbs) and derivatives thereof, due to their smaller molecular size and higher stability, and thus present an alternative to mAbs therapeutically. Two nanobodies, Nb23 and Nb24, have been shown to similarly inhibit the self-aggregation of a very amyloidogenic variant of β2-microglobulin, D76N β2-m, which has implications in hereditary systemic amyloidosis. Here, the structure of Nb23 was modeled with the Chemical Shift (CS)-Rosetta server using chemical shift assignments from nuclear magnetic resonance (NMR) spectroscopy experiments, and partially validated using the already determined structure of Nb24, NMR chemical shifts, circular dichroism (CD), and nuclear Overhauser effect (NOE) spectroscopy. Further, the binding properties of Nb23 and Nb24 with the antigen were investigated with isothermal titration calorimetry (ITC), showing that Nb23 has slightly lower affinity to the antigen than Nb24. The structural analysis indicated that Nb23 has a much more dynamic CDR3 loop with respect to Nb24, which could be a contributing factor to its lower affinity.</p>
----------------------------------------------------------------------
In diva2:1272665 abstract is: <p>Igelbäcken is a 10 kilometer long watercourse in northwestern Stockholm, starting in Säbysjön,Järfälla and passing Sollentuna, Sundbyberg, Stockholm and Solna where it flows into Edsviken,which is the outlet. The water passes industries, buildings and roads. The aim of this work is toanalyze how Igelbäcken is affected by the surroundings. The work contains a field study andlaboratory work.</p><p>The results show that the water is contaminated. pH, temperature, conductivity and flow weremeasured in the field and COD, alkalinity, chlorine, total phosphorus, color, odour, suspension ofcolloidal particles and precipitate were analyzed in the laboratory. The result shows some veryhigh values. The conductivity and chloride are high at all places, indicating that the water isaffected by roads. The COD is above the acceptable value except in two places. Alkalinity variesbetween 2 mM and 4 mM except in two places. The phosphorus content is also high. Based on theresults, more efforts are required to make Igelbäcken cleaner.</p>


corrected abstract:
<p>Igelbäcken is a 10 kilometer long watercourse in northwestern Stockholm, starting in Säbysjön, Järfälla and passing Sollentuna, Sundbyberg, Stockholm and Solna where it flows into Edsviken, which is the outlet. The water passes industries, buildings and roads. The aim of this work is to analyze how Igelbäcken is affected by the surroundings. The work contains a field study and laboratory work.</p><p>The results show that the water is contaminated. pH, temperature, conductivity and flow were measured in the field and COD, alkalinity, chlorine, total phosphorus, color, odour, suspension of colloidal particles and precipitate were analyzed in the laboratory. The result shows some very high values. The conductivity and chloride are high at all places, indicating that the water is affected by roads. The COD is above the acceptable value except in two places. Alkalinity varies between 2 mM and 4 mM except in two places. The phosphorus content is also high. Based on the results, more efforts are required to make Igelbäcken cleaner.</p>
----------------------------------------------------------------------
In diva2:1449212 abstract is: <p>Medical image analysis is both time consuming and requires expertise. In thisreport, a 2.5D version of the U-net convolution network adapted for automatedkidney segmentation is further developed. Convolution neural networks havepreviously shown expert level performance in image segmentation. Training datafor the network was created by manually segmenting MRI images of kidneys.The 2.5D U-Net network was trained with 64 kidney segmentations fromprevious work. Volume analysis on the network’s kidney segmentation proposalsof 38,000 patients showed that the ammount of segmented voxels that are notpart of the kidneys was 0.35%. After the addition of 56 of our segmentations, itdecreased to just 0.11%, indicating a reduction of about 68%. This is a majorimprovement of the network and an important step towards the development ofpractical applications of automated segmentation.</p>

w='ammount' val={'c': 'amount', 's': 'diva2:1449212', 'n': 'error in original'}

corrected abstract:
<p>Medical image analysis is both time consuming and requires expertise. In this report, a 2.5D version of the U-net convolution network adapted for automated kidney segmentation is further developed. Convolution neural networks have previously shown expert level performance in image segmentation. Training data for the network was created by manually segmenting MRI images of kidneys. The 2.5D U-Net network was trained with 64 kidney segmentations from previous work. Volume analysis on the network’s kidney segmentation proposals of 38,000 patients showed that the ammount of segmented voxels that are not part of the kidneys was 0.35%. After the addition of 56 of our segmentations, it decreased to just 0.11%, indicating a reduction of about 68%. This is a major improvement of the network and an important step towards the development of practical applications of automated segmentation.</p>
----------------------------------------------------------------------
In diva2:908846 abstract is: <p>Each year million of babies are born pre-term, some of these pre-term births occur due to the motherhaving a too soft cervix which can not withstand the forces the baby exposes it to. The aim of thisstudy was to implement and evaluate a programmable shear wave elastography ultrasound system forcervical applications and investigate the optimal settings of shear wave elastography push voltage andshear wave elastography push focus depth. Shear wave elastography is an ultrasound based imagingmodality aiming to evaluate the tissue elasticity by using acoustic radiation forces to induce shear waves.The propagation of the shear waves through the tissue is then tracked in order to calculate the shearwave velocity which is related to the tissue elasticity. B-mode imaging, pushing sequence and planewave imaging have been implemented and measurements have been conducted on four cervical polyvinylalcohol phantoms. The acquired data has been post-processed using Loupas 2D-autocorrector to gainthe axial displacement and enabling tracking of the shear waves to allow evaluation and optimizationof the implemented method. The implemented shear wave technique showed to be able to distinguishcervical phantoms of dierent elasticity and a high pushing voltage and shallow focus push depth havebeen found to produce the most reliable results.</p>

corrected abstract:
<p>Each year million of babies are born pre-term, some of these pre-term births occur due to the mother having a too soft cervix which can not withstand the forces the baby exposes it to. The aim of this study was to implement and evaluate a programmable shear wave elastography ultrasound system for cervical applications and investigate the optimal settings of shear wave elastography push voltage and shear wave elastography push focus depth. Shear wave elastography is an ultrasound based imaging modality aiming to evaluate the tissue elasticity by using acoustic radiation forces to induce shear waves. The propagation of the shear waves through the tissue is then tracked in order to calculate the shear wave velocity which is related to the tissue elasticity. B-mode imaging, pushing sequence and plane wave imaging have been implemented and measurements have been conducted on four cervical polyvinyl alcohol phantoms. The acquired data has been post-processed using Loupas 2D-autocorrector to gain the axial displacement and enabling tracking of the shear waves to allow evaluation and optimization of the implemented method. The implemented shear wave technique showed to be able to distinguish cervical phantoms of different elasticity and a high pushing voltage and shallow focus push depth have been found to produce the most reliable results.</p>
----------------------------------------------------------------------
In diva2:1229241 abstract is: <p>Aliphatic polyesters such as poly(lactide) (PLA), poly(glycolide) (PGA), poly(Ɛ-caprolactone)(PCL) and their copolymers have gained an intense interest for their implementation for tissueregeneration and medical applications because of their unique properties such asbiocompatibility, degradability and easy processing. This master thesis was focused on thefabrication of degradable porous scaffolds for soft tissue regeneration. For this purpose, a twostepprocess was optimized using PLA and PCL: melt-extrusion following 3D printing.In the melt-extrusion optimization, filaments with a specific diameter were fabricated by alab extruder and in-house made winder. To achieve this, extrusion parameters wereadjusted according to the thermal behavior of the polymer. Filaments with a regular diameter forfeeding a commercial 3D printer were produced using PLA and PCL. The printability wasassessed using the produced filaments as well as commercially available filaments. Twodifferent scaffold designs were 3D printed to check how the mechanical properties can be varied.Molecular weight, mechanical and thermal properties were respectively characterized bysize exclusion chromatography (SEC), tensile tests, differential scanning calorimetry (DSC)and thermogravimetric analysis (TGA) for the filaments and scaffoldsfabricated.</p>


corrected abstract:
<p>Aliphatic polyesters such as poly(lactide) (PLA), poly(glycolide) (PGA), poly(Ɛ-caprolactone)(PCL) and their copolymers have gained an intense interest for their implementation for tissue regeneration and medical applications because of their unique properties such as biocompatibility, degradability and easy processing. This master thesis was focused on the fabrication of degradable porous scaffolds for soft tissue regeneration. For this purpose, a two step process was optimized using PLA and PCL: melt-extrusion following 3D printing. In the melt-extrusion optimization, filaments with a specific diameter were fabricated by a lab extruder and in-house made winder. To achieve this, extrusion parameters were adjusted according to the thermal behavior of the polymer. Filaments with a regular diameter for feeding a commercial 3D printer were produced using PLA and PCL. The printability was assessed using the produced filaments as well as commercially available filaments. Two different scaffold designs were 3D printed to check how the mechanical properties can be varied. Molecular weight, mechanical and thermal properties were respectively characterized by size exclusion chromatography (SEC), tensile tests, differential scanning calorimetry (DSC) and thermogravimetric analysis (TGA) for the filaments and scaffolds fabricated.</p>
----------------------------------------------------------------------
In diva2:1045581 abstract is: <p>Strawberry plants (Fragaria x ananassa) and Scots pine (Pinus sylvestris) represent species, withinagriculture and forestry respectively, that are traditionally protected by utilization of pesticidesincluding neurotoxic insecticides. More environmentally friendly protection strategies are thereforehighly desirable. Treating plants with specific metabolites naturally occurring in their tissues might alterepigenetic mechanisms, which in turn may strengthen plants self-defense against diseases and weevilattacks. F. x ananassa and P. sylvestris seeds were treated with 2,5 mM nicotinamide and 2,5 mMnicotinic acid in order to investigate possible epigenetical effects by analyzing changes in the level ofthe DNA methylation. The epigenetic changes, for both plants, were analyzed on the global DNA level.Reduction in the DNA methylation level in strawberry leaves as well as the DNA methylation increase inpine needles were observed by means of LUMA-analysis when HpaII restriction enzyme was used in theanalysis. Further investigation is required in order to understand if NIC and NIA may have a significantimpact on pathogen attack in strawberry plants and Scots pine. More research may also unveil ifnicotinamide and nicotinic acid can play a potential role in more sustainable defense strategies ofplants.</p>


corrected abstract:
<p>Strawberry plants (<em>Fragaria x ananassa</em>) and Scots pine (<em>Pinus sylvestris</em>) represent species, within agriculture and forestry respectively, that are traditionally protected by utilization of pesticides including neurotoxic insecticides. More environmentally friendly protection strategies are therefore highly desirable. Treating plants with specific metabolites naturally occurring in their tissues might alter epigenetic mechanisms, which in turn may strengthen plants self-defense against diseases and weevil attacks. <em>F. x ananassa</em> and <em>P. sylvestris</em> seeds were treated with 2,5 mM nicotin amide and 2,5 mM nicotinic acid in order to investigate possible epigenetical effects by analyzing changes in the level of the DNA methylation. The epigenetic changes, for both plants, were analyzed on the global DNA level. Reduction in the DNA methylation level in strawberry leaves as well as the DNA methylation increase in pine needles were observed by means of LUMA-analysis when HpaII restriction enzyme was used in the analysis. Further investigation is required in order to understand if NIC and NIA may have a significant impact on pathogen attack in strawberry plants and Scots pine. More research may also unveil if nicotinamide and nicotinic acid can play a potential role in more sustainable defense strategies of plants.</p>
----------------------------------------------------------------------
In diva2:1331767 abstract is: <p>The aim of the project was to compare the workflow for an Elekta Linac with and without the surfacescanning system Catalyst and describe pros and cons with both workflows. The findings in the reportcan be used as decision support in development of Elekta products and workflow improvements.</p><p>The method for the project was to do interviews, observations and time measurements at Södersjukhuset(not using Catalyst) and Sundsvalls sjukhus (using Catalyst). The workflows were graded in an as-sessment protocol covering time efficiency, comfort, noise, resources, reliability, cost, dosage and sideeffects. Different workflow scenarios were simulated in AnyLogic.</p><p>The result of the project was that, according to our protocol, the workflow with Catalyst was ratedhigher than without it. The simulations in Anylogic showed that minimizing gaps in the treatment sched-ule generated the same number of patients treated per day, if the positioning could not be done faster.The simulations also showed that removing position verification with cone beam computer tomography(CBCT), an imaging system which is used in addition to the Catalyst system, would increase the numberof treated patients with approximately 33%.</p><p>The conclusion was that there were no great differences in time efficiency between the workflows. How-ever, considering the higher reliability and comfort for the patient, optical surface scanning can improvethe positioning for Elekta Linac and is therefore worth implementing. Minimizing treatment gaps wouldnot improve the workflow. Removing the use of CBCT would increase the number of treated patientsper day.</p>

w='as-sessment' val={'c': 'as-sessment', 's': 'diva2:1331767'}
w='How-ever' val={'c': 'However', 's': 'diva2:1331767'}

corrected abstract:
<p>The aim of the project was to compare the workflow for an Elekta Linac with and without the surface scanning system Catalyst and describe pros and cons with both workflows. The findings in the report can be used as decision support in development of Elekta products and workflow improvements.</p><p>The method for the project was to do interviews, observations and time measurements at Södersjukhuset (not using Catalyst) and Sundsvalls sjukhus (using Catalyst). The workflows were graded in an assessment protocol covering time efficiency, comfort, noise, resources, reliability, cost, dosage and side effects. Different workflow scenarios were simulated in AnyLogic.</p><p>The result of the project was that, according to our protocol, the workflow with Catalyst was rated higher than without it. The simulations in Anylogic showed that minimizing gaps in the treatment schedule generated the same number of patients treated per day, if the positioning could not be done faster. The simulations also showed that removing position verification with cone beam computer tomography (CBCT), an imaging system which is used in addition to the Catalyst system, would increase the number of treated patients with approximately 33%.</p><p>The conclusion was that there were no great differences in time efficiency between the workflows. However, considering the higher reliability and comfort for the patient, optical surface scanning can improve the positioning for Elekta Linac and is therefore worth implementing. Minimizing treatment gaps would not improve the workflow. Removing the use of CBCT would increase the number of treated patients per day.</p>
----------------------------------------------------------------------
In diva2:1353232 abstract is: <p>The work done in this report as requested by H&amp;D Wireless is performed by using an already developed real time positioning system called Griffin Enterprise Positioning Service and integratingit with ultrasound sensors for presence detection in order to enable assets to park in a visualized environment being actualized by tagging the asset with radio technology hardware. The testing environment was deployed with radio technology hardware equipped for transmitting and receiving radio signals for position estimation of tagged objects where hardware tags emits radio signals being received by sensing radio technology chips called sensepoints which also serves as communication links for further data processing.The thesis focus is on evaluating how different radio technologies combined with different positioning techniques perform in terms of accuracy and precision in positioning tests to assess eachones positioning performance characteristic and the technologies upsides and downsides.This was firstly evaluated by comparing three different technology positioning techniques based on one for Bluetooth Low Energy and two using Ultra Wideband technology being subject to generic tests including a static, dynamic and a walking positioning test for each technology.These initial tests were utilized as a foreground to evaluate which of the two positioning techniques based on Ultra Wideband technology that would compete in the parking tests alongside Bluetooth Low Energy that would serve as the primary objective to accomplish in the thesis.A final implication on parking tags between the two technologies is that Bluetooth Low Energy had to be implemented with higher requirement restrictions for parking due to insufficient relative accuracy and precision in parking positioning which also limited its ability to be parked in alternative manners explored but with power efficiency as a highly valuable aspect for consideration of this technology. Parking tag using Ultra Wideband technology proved highly successful as it saw large distance margins to be allowed parking in all test cases as well as exhibiting sufficient positioning performance to be considered for alternative parking methods without risk of exposure for failed attempts of parking.</p>

corrected abstract:
<p>The work done in this report as requested by H&amp;D Wireless is performed by using an already developed real time positioning system called Griffin Enterprise Positioning Service and integrating it with ultrasound sensors for presence detection in order to enable assets to park in a visualized environment being actualized by tagging the asset with radio technology hardware. The testing environment was deployed with radio technology hardware equipped for transmitting and receiving radio signals for position estimation of tagged objects where hardware tags emits radio signals being received by sensing radio technology chips called sensepoints which also serves as communication links for further data processing.</p><p>The thesis focus is on evaluating how different radio technologies combined with different positioning techniques perform in terms of accuracy and precision in positioning tests to assess each ones positioning performance characteristic and the technologies upsides and downsides.</p><p>This was firstly evaluated by comparing three different technology positioning techniques based on one for Bluetooth Low Energy and two using Ultra Wideband technology being subject to generic tests including a static, dynamic and a walking positioning test for each technology. These initial tests were utilized as a foreground to evaluate which of the two positioning techniques based on Ultra Wideband technology that would compete in the parking tests alongside Bluetooth Low Energy that would serve as the primary objective to accomplish in the thesis.</p><p>A final implication on parking tags between the two technologies is that Bluetooth Low Energy had to be implemented with higher requirement restrictions for parking due to insufficient relative accuracy and precision in parking positioning which also limited its ability to be parked in alternative manners explored but with power efficiency as a highly valuable aspect for consideration of this technology. Parking tag using Ultra Wideband technology proved highly successful as it saw large distance margins to be allowed parking in all test cases as well as exhibiting sufficient positioning performance to be considered for alternative parking methods without risk of exposure for failed attempts of parking.</p>
----------------------------------------------------------------------
In diva2:1242939 abstract is: <p>Spatial Transcriptomics (ST) is a microarray-based RNA sequencing technology that allows for genome-wide transcriptome profiling of tissue sections with spatialresolution, which was published in Science by Ståhl and Salmén et al in 2016. Polyadenylated transcripts are captured on a microarray surface through hybridizationwith a barcoded DNA oligo that carries the information necessary to infer spatialposition and transcript uniqueness from the output sequencing reads. The ST protocolutilizes Unique Molecular Identifiers (UMIs) to remove PCR duplicates and obtain reliable estimates of gene counts. However, errors gradually accumulate in the UMIpool as a consequence of enzymatic conversion steps and these errors ought to be addressed computationally in data processing steps to produce reliable gene countestimates.In this thesis, we used ERCC reference RNAs and a set of custom-made DNA oligosas spike-ins in the ST protocol to explore sources of technical variation under controlled experimental conditions. An exploratory analysis of the spike-in data gave new insights into the chemistry which could guide future improvements of the protocol. We also developed a new strategy to bin read alignments before gene quantification and showed that this strategy produces a more reliable output. Finally, we developed an in silicosimulation of the ST library preparation to provide a framework that can be used toevaluate the performance of various computational processing strategies. The simulation was then used to benchmark a set of duplicate removal algorithms used toquantify gene expression.This master thesis project was carried out at SciLifeLab at the division of GeneTechnology under supervision of José Fernandez Navarro.</p>

corrected abstract:
<p>Spatial Transcriptomics (ST) is a microarray-based RNA sequencing technology that allows for genome-wide transcriptome profiling of tissue sections with spatial resolution, which was published in Science by Ståhl and Salmén et al in 2016. Polyadenylated transcripts are captured on a microarray surface through hybridization with a barcoded DNA oligo that carries the information necessary to infer spatial position and transcript uniqueness from the output sequencing reads. The ST protocol utilizes Unique Molecular Identifiers (UMIs) to remove PCR duplicates and obtain reliable estimates of gene counts. However, errors gradually accumulate in the UMI pool as a consequence of enzymatic conversion steps and these errors ought to be addressed computationally in data processing steps to produce reliable gene count estimates.</p><p>In this thesis, we used ERCC reference RNAs and a set of custom-made DNA oligos as spike-ins in the ST protocol to explore sources of technical variation under controlled experimental conditions. An exploratory analysis of the spike-in data gave new insights into the chemistry which could guide future improvements of the protocol. We also developed a new strategy to bin read alignments before gene quantification and showed that this strategy produces a more reliable output. Finally, we developed an <em>in silico</em> simulation of the ST library preparation to provide a framework that can be used to evaluate the performance of various computational processing strategies. The simulation was then used to benchmark a set of duplicate removal algorithms used to quantify gene expression.</p><p>This master thesis project was carried out at SciLifeLab at the division of Gene Technology under supervision of José Fernandez Navarro.</p>
----------------------------------------------------------------------
In diva2:1217278 abstract is: <p>This report investigates which components a portable wireless Bluetooth speakerconsists of and the design parameters a prospective designer should take into consideration.The characteristics of six commercially available speaker models andthe design solutions used in these were examined. The design parameters wereanalyzed to investigate if and how these could be improved upon. Simulations ofspeaker prototypes were done and prototypes of speakers and an amplifier werebuilt. Measurements were made to be able to compare and verify the audio performanceof the prototypes. Moreover, measurements of music were made to investigatehow the RMS-value of music impacts the power of an amplifier in general andin active systems in particular. Design recommendations on how to build a Bluetoothspeaker are given in the end of the report together with suggestions on alternatetechnologies to look into for implementation in future Bluetooth speakers.</p>

corrected abstract:
<p>This report investigates which components a portable wireless Bluetooth speaker consists of and the design parameters a prospective designer should take into consideration. The characteristics of six commercially available speaker models and the design solutions used in these were examined. The design parameters were analyzed to investigate if and how these could be improved upon. Simulations of speaker prototypes were done and prototypes of speakers and an amplifier were built. Measurements were made to be able to compare and verify the audio performance of the prototypes. Moreover, measurements of music were made to investigate how the RMS-value of music impacts the power of an amplifier in general and in active systems in particular. Design recommendations on how to build a Bluetooth speaker are given in the end of the report together with suggestions on alternate technologies to look into for implementation in future Bluetooth speakers.</p>
----------------------------------------------------------------------
In diva2:1278530 abstract is: <p>Today, in the automotive industry, many of the interior parts in the car are made of ABS and PC/ABS polymeric blend. These materials are used in the areas for example: instrument panels, tunnel consoles and door panels. The extensive use of these materials means that it is important to gain in-depth knowledge about the materials,their properties; and also their behaviour when in contact with different chemicals andat different conditions.This study aims to address the potential problem of the polymers used in the interiorof the car - ABS and PC/ABS cracking due to environmental factors. This study proposes to introduce a low-cost test method to compare the polymeric materials and choose the best one for future purposes with the environmental circumstances in mind for materials to have a good service life.During the thesis project, ABS and PC/ABS samples were tested for environmental stress cracking to compare the strained materials against PEG 400 and an assemblyfluid chemical. These tests were conducted at three different temperature levels.Differential Scanning Calorimetry (DSC) was used to verify the polymeric materialsthat the samples were made of. Optical microscope and FTIR were employed to analyzethe samples for crazes / cracks and degradation of material, respectively.This thesis helped in establishing a good starting point for ESC testing of different materials for the organization. The test method was used to test the failure of material sin ESC. It was observed that the chemicals used for the testing were aggressive and accelerated the cracking process in the materials rapidly. Another observation of the tests was that high strain also caused the materials to fail quickly. While comparing the materials, PC/ABS polymer blend was more resistant than ABS materials to cracking when exposed to same strain level during the creep rupture test (test in absence ofchemicals acting as a reference test for ESC).</p>

corrected abstract:
<p>Today, in the automotive industry, many of the interior parts in the car are made of ABS and PC/ABS polymeric blend. These materials are used in the areas for example: instrument panels, tunnel consoles and door panels. The extensive use of these materials means that it is important to gain in-depth knowledge about the materials, their properties; and also their behaviour when in contact with different chemicals and at different conditions.</p><p>This study aims to address the potential problem of the polymers used in the interior of the car - ABS and PC/ABS cracking due to environmental factors. This study proposes to introduce a low-cost test method to compare the polymeric materials and choose the best one for future purposes with the environmental circumstances in mind for materials to have a good service life.</p><p>During the thesis project, ABS and PC/ABS samples were tested for environmental stress cracking to compare the strained materials against PEG 400 and an assembly fluid chemical. These tests were conducted at three different temperature levels. Differential Scanning Calorimetry (DSC) was used to verify the polymeric materials that the samples were made of. Optical microscope and FTIR were employed to analyze the samples for crazes / cracks and degradation of material, respectively.</p><p>This thesis helped in establishing a good starting point for ESC testing of different materials for the organization. The test method was used to test the failure of materials in ESC. It was observed that the chemicals used for the testing were aggressive and accelerated the cracking process in the materials rapidly. Another observation of the tests was that high strain also caused the materials to fail quickly. While comparing the materials, PC/ABS polymer blend was more resistant than ABS materials to cracking when exposed to same strain level during the creep rupture test (test in absence of chemicals acting as a reference test for ESC).</p>
----------------------------------------------------------------------
In diva2:1451795 abstract is: <p>Abstract</p><p>With the rapid increase of bandwidth and complexity in the network of communicationsservice providers, there is need for effective troubleshooting to maintain troublefree networks and in turn high customer satisfaction. An important tool in troubleshootingis visualisation of call flows and messages in the network. This thesis setout to compare different charts and user interfaces for the purpose of displaying callflow data in a network for communication services. Two high-fidelity mock-ups weremade for visualising call flows and two for visualising messages. The mock-ups wereevaluated using user-based evaluation and heuristic evaluation. The mock-ups werethen revised based on the information gathered from the evaluations. The result ofthis thesis suggests that an implementation should use a sequence diagram for visualisationof call flows together with a hierarchical view for visualisation of messages.</p><p>Keywords</p><p>Call flow, visualisation, user interface, communication services, heuristics, sequencediagram, hierarchical view, table view, tree diagram, mock-up</p>


corrected abstract:
<p>With the rapid increase of bandwidth and complexity in the network of communications service providers, there is need for effective troubleshooting to maintain trouble free networks and in turn high customer satisfaction. An important tool in troubleshooting is visualisation of call flows and messages in the network. This thesis set out to compare different charts and user interfaces for the purpose of displaying call flow data in a network for communication services. Two high-fidelity mock-ups were made for visualising call flows and two for visualising messages. The mock-ups were evaluated using user-based evaluation and heuristic evaluation. The mock-ups were then revised based on the information gathered from the evaluations. The result of this thesis suggests that an implementation should use a sequence diagram for visualisation of call flows together with a hierarchical view for visualisation of messages.</p>
----------------------------------------------------------------------
In diva2:1763859 abstract is: <p>The work describes the opportunities to perform preventive maintenance with the help of informationfrom COMTRADE disturbance files. A software algorithm was developed which collects disturbancedata and gives indications if equipment are not working within optimal conditions.Using thisinformation preventive maintenance can be performed based on need instead of scheduling to savetime and money.Together with supervisors from involved companies a software was developed to be used as a supportfor preventive maintenance. The software can extract disturbance times and handle multiple scenariosbased on information collected from disturbance files. A user has access to a algoritm that createsautomatical analysis of the COMTRADE file and a manual tool for extensive analysis when the algoritmdoes not give proper results.Trends over time can be analysed with the algortim, this do require a larger amount of data than whatwas available during the work. </p>


corrected abstract:
<p>The work describes the opportunities to perform preventive maintenance with the help of information from COMTRADE disturbance files. A software algorithm was developed which collects disturbance data and gives indications if equipment are not working within optimal conditions. Using this information preventive maintenance can be performed based on need instead of scheduling to save time and money.</p><p>Together with supervisors from involved companies a software was developed to be used as a support for preventive maintenance. The software can extract disturbance times and handle multiple scenarios based on information collected from disturbance files. A user has access to a algoritm that creates automatical analysis of the COMTRADE file and a manual tool for extensive analysis when the algoritm does not give proper results.</p><p>Trends over time can be analysed with the algortim, this do require a larger amount of data than what was available during the work.</p>
----------------------------------------------------------------------
The PDF file has a bizzare character encoding and one cannot easily extract the text.

In diva2:724099 abstract is: <p>After the extension of the boiler 6 at Högdalenverket during 2010, there has been some problems in the form of increased carbon monoxide levels. There may be several explanations for this problem, which means that complete combustion doesent take place. There have also been problems with the ash-handling system and the sand returning systemt, this results in higher operating costs because it requires a higher consumption of inert material.The project was first divided into two phases, the first phase was to develop material and energy balances for the boiler but also introduce the parameters that we wanted to investigate further in order to possibly identify the causes of the problems that the boiler had. In the second phase experiments were going to be designed in consultation with the contact staff at Fortum to explore these parameters.During the project changes have been made in the project description, as it will require more time and more accurate planning to perform  the desired tests on the boiler. Three proposals on parameters that could be the cause of carbon monoxide problem was presented at the end of phase one and it was found that two of the proposals was not possible to carry out during the project because it would have affected economically and the availibility of the boilier.The third parameter, the bed quality effect on combustion experiment was designed in consultation with those responsible at Fortum.This experiment could also not be performed as there was some operational difficulties with the boiler during the project time. Instead, a description of the design for the experiment has been added to this project, which may at a later stage be used to investigate the bed quality impact on the carbon monoxide issues.Recent changes to the project description meant that purpose instead was to further develop the computational program created during the project's first phase and at the end of the project shall be submitted to the operators at Fortum. This calculation program takes into account the significant parameters that the boiler regulator is governed by today. The simulation program is developed using Microsoft Excel and is based on material and energy balances. This program can be used internally by Fortum to get an idea of ​​how the different mass flow rates would vary at different operating conditions and how the energy balance for the boiler would look like during the changes.When compared with materials and energy made by the boiler supplier revealed that the deviations are very small compared to what the program come up with and is caused mainly due to the assumptions made. These assumptions can be eliminated by performing experiments and collecting more data.The results gained by the calculation program has been compared with the results that was presented by the supplier during their boiler mapping, comparisons have been made regarding flue gas flows, fuel flows, combustion air flow, ash production and energy balances. It has apperad that the deviations are relatively small and the presented scheme provides a theoretical overall perspective of the boiler in line with reality.Although discussions have taken into consideration for the problem of elevated levels of carbon monoxide, and whether the measurement program can be developed further in the future.</p>

w='boilier' val={'c': 'boiler', 's': 'diva2:724099'}
w='doesent' val={'c': "doesn't", 's': 'diva2:724099', 'n': 'error in original'}
w='apperad' val={'c': 'appeared', 's': 'diva2:724099'}
w='availibility' val={'c': 'availability', 's': 'diva2:724099', 'n': 'error in original'}

corrected abstract:
<p>After the extension of the boiler 6 at Högdalenverket during 2010, there has been some problems in the form of increased carbon monoxide levels. There may be several explanations for this problem, which means that complete combustion doesent take place. There have also been problems with the ash-handling system and the sand returning systemt, this results in higher operating costs because it requires a higher consumption of inert material.</p><p>The project was first divided into two phases, the first phase was to develop material and energy balances for the boiler but also introduce the parameters that we wanted to investigate further in order to possibly identify the causes of the problems that the boiler had. In the second phase experiments were going to be designed in consultation with the contact staff at Fortum to explore these parameters.<br>During the project changes have been made in the project description, as it will require more time and more accurate planning to perform the desired tests on the boiler. Three proposals on parameters that could be the cause of carbon monoxide problem was presented at the end of phase one and it was found that two of the proposals was not possible to carry out during the project because it would have affected economically and the availibility of the boiler.<br>The third parameter, the bed quality effect on combustion experiment was designed in consultation with those responsible at Fortum. This experiment could also not be performed as there was some operational difficulties with the boiler during the project time. Instead, a description of the design for the experiment has been added to this project, which may at a later stage be used to investigate the bed quality impact on the carbon monoxide issues. Recent changes to the project description meant that purpose instead was to further develop the computational program created during the project's first phase and at the end of the project shall be submitted to the operators at Fortum. This calculation program takes into account the significant parameters that the boiler regulator is governed by today. The simulation program is developed using Microsoft Excel and is based on material and energy balances. This program can be used internally by Fortum to get an idea of ​​how the different mass flow rates would vary at different operating conditions and how the energy balance for the boiler would look like during the changes.</p><p>When compared with materials and energy made by the boiler supplier revealed that the deviations are very small compared to what the program come up with and is caused mainly due to the assumptions made. These assumptions can be eliminated by performing experiments and collecting more data. The results gained by the calculation program has been compared with the results that was presented by the supplier during their boiler mapping, comparisons have been made regarding flue gas flows, fuel flows, combustion air flow, ash production and energy balances. It has appeared that the deviations are relatively small and the presented scheme provides a theoretical overall perspective of the boiler in line with reality.<br>Although discussions have taken into consideration for the problem of elevated levels of carbon monoxide, and whether the measurement program can be developed further in the future.</p>
----------------------------------------------------------------------
In diva2:1178039 abstract is: <p>Overwork of muscle can be a problem for the double poling cross country skier potentially resultingin lower efficiency. An assignment was established – on behalf of Johnny Nilsson at Gymnastik- ochIdrottshögskolan (GIH) in Stockholm – in order to build a prototype able to monitor and record datafrom musculus trapezius (m. trapezius) through the use of electromyography (EMG). The EMG wasmade using the open source hardware Arduino. The prototype was able to record bilateralmeasurements with the use of EMG-shields, where surface-electrodes were attached to m. trapezius.By creating a prototype based on a rotary potentiometer attached to the elbow joint a reference ofmovement was established by measuring the extension and flexion angle of the elbow. Arduino’s ownIDE was used to program the hardware of the prototype and data was post-processed and presented inMatlab. Data was transferred with the use of an SD-card reader installed on the microcontroller. Withthe help of Peter Arfert at KTH, a 3D-printed model was made for the prototype. The final prototypewas attached to an elite level cross-country skier and tested on a professional treadmill at the LIVIlaboratory in Falun, Sweden. Raw-data was successfully recorded during these trials.</p>

corrected abstract:
<p>Overwork of muscle can be a problem for the double poling cross country skier potentially resulting in lower efficiency. An assignment was established – on behalf of Johnny Nilsson at Gymnastik- och Idrottshögskolan (<em>GIH</em>) in Stockholm – in order to build a prototype able to monitor and record data from musculus trapezius (m. trapezius) through the use of electromyography (<em>EMG</em>). The EMG was made using the open source hardware Arduino. The prototype was able to record bilateral measurements with the use of EMG-shields, where surface-electrodes were attached to m. trapezius. By creating a prototype based on a rotary potentiometer attached to the elbow joint a reference of movement was established by measuring the extension and flexion angle of the elbow. Arduino’s own IDE was used to program the hardware of the prototype and data was post-processed and presented in Matlab. Data was transferred with the use of an SD-card reader installed on the microcontroller. With the help of Peter Arfert at KTH, a 3D-printed model was made for the prototype. The final prototype was attached to an elite level cross-country skier and tested on a professional treadmill at the LIVI laboratory in Falun, Sweden. Raw-data was successfully recorded during these trials.</p>
----------------------------------------------------------------------
In diva2:1698780 abstract is: <p>Parkinson’s Disease (PD) is a neurodegenerative disorder, within this categoryof diseases it is among the most prevalent worldwide. The etiology of PD isbased in progressive deterioration of neural tissue in the basal ganglia (neuronalnuclei located at the base of the cerebrum) and their related structures. Current research is focusing on treatment approaches to either enhance or replaceexisting pharmaceutical treatment approaches, such as dopamine replacementtherapy. In this project, the focus was on finding correlates between movementdata and neurological signals to provide insight into potential biomarkers forcomplex motor symptoms of PD. This will in turn provide a starting point forspecifically targeted closed-loop neural stimulation that alleviates these symptoms. Although the data available at the time of this thesis did not providesufficient insight to derive a conclusion on the neural correlates, a pipeline wasdeveloped, which analyzes and synchronizes kinematic and neural data and willenable further exploration as additional data is obtained.</p>

corrected abstract:
<p>Parkinson’s Disease (PD) is a neurodegenerative disorder, within this category of diseases it is among the most prevalent worldwide. The etiology of PD is based in progressive deterioration of neural tissue in the basal ganglia (neuronal nuclei located at the base of the cerebrum) and their related structures. Current research is focusing on treatment approaches to either enhance or replace existing pharmaceutical treatment approaches, such as dopamine replacement therapy. In this project, the focus was on finding correlates between movement data and neurological signals to provide insight into potential biomarkers for complex motor symptoms of PD. This will in turn provide a starting point for specifically targeted closed-loop neural stimulation that alleviates these symptoms. Although the data available at the time of this thesis did not provide sufficient insight to derive a conclusion on the neural correlates, a pipeline was developed, which analyzes and synchronizes kinematic and neural data and will enable further exploration as additional data is obtained.</p>
----------------------------------------------------------------------
In diva2:1242473 abstract is: <p>Biomass is a viable option to supply the need of renewable energy inthe future. However, the high cost of biomass processing makes the process not viable economically. For that reason, two one dimensionalparticle models that describe the mass and heat transfer of biomasswere developed and solved using the method of lines. The two modelsare based on the previous work of Park et al (Combustion and flame,2010). The first model neglects the importance of the convective fluxterm in the heat and mass balances. The second model considers the convective term, but a velocity expression that assumes constant pressurewas used. The results of the simulation show that the convectiveterm is insignificant for the mass loss and temperature profiles of athick particle. Experiments of biomass spheres were done to furthervalidate the results of the conduction and convection model. The experimentaldata is compatible with the results of the conduction and convection model.</p>

Note there is not a period after "al" in the actual abstract.
corrected abstract:
<p>Biomass is a viable option to supply the need of renewable energy in the future. However, the high cost of biomass processing makes the process not viable economically. For that reason, two one dimensional particle models that describe the mass and heat transfer of biomass were developed and solved using the method of lines. The two models are based on the previous work of Park et al (Combustion and flame, 2010). The first model neglects the importance of the convective flux term in the heat and mass balances. The second model considers the convective term, but a velocity expression that assumes constant pressure was used. The results of the simulation show that the convective term is insignificant for the mass loss and temperature profiles of a thick particle. Experiments of biomass spheres were done to further validate the results of the conduction and convection model. The experimental data is compatible with the results of the conduction and convection model.</p>
----------------------------------------------------------------------
In diva2:1684046 abstract is: <p>IEC 60601 electrical safety tests are still used in many medical engineeringdepartments in hospitals for testing equipment in connection with arrival controland repair. This report investigates the possibility of producing a basis for starting toimplement IEC 62353 as an electrical safety test standard instead of IEC 60601.The study focuses on whether Karolinska University Hospital should apply IEC62353 as its electrical safety test standard. Furthermore, the differences between thetwo above standards are discussed. This is done by finding out how the KarolinskaUniversity Hospital responds to the question and the collection of information, aswell as how the IEC 62353 standard is applied in industry.The information presented in the results after contact with several hospitals showstwo different aspects. One is an independent application of IEC 62353 as an electricalsafety test standard, the other is to follow the manufacturer's instructions. Futurestudies that can provide clearer guidance from the manufacturer's point of view arerecommended. </p>


corrected abstract:
<p>IEC 60601 electrical safety tests are still used in many medical engineering departments in hospitals for testing equipment in connection with arrival control and repair. This report investigates the possibility of producing a basis for starting to implement IEC 62353 as an electrical safety test standard instead of IEC 60601.</p><p>The study focuses on whether Karolinska University Hospital should apply IEC 62353 as its electrical safety test standard. Furthermore, the differences between the two above standards are discussed. This is done by finding out how the Karolinska University Hospital responds to the question and the collection of information, as well as how the IEC 62353 standard is applied in industry.</p><p>The information presented in the results after contact with several hospitals shows two different aspects. One is an independent application of IEC 62353 as an electrical safety test standard, the other is to follow the manufacturer's instructions. Future studies that can provide clearer guidance from the manufacturer's point of view are recommended.</p>
----------------------------------------------------------------------
In diva2:1866444 abstract is: <p>This bachelor thesis has been carried out in cooperation with Luvly AB, a light urban vehiclestartup working out of Stockholm. The project’s goal has been to develop a theory regardingcomputerized troubleshooting of the CAN-bus in three domains, the messages, the signals,and the electrical characteristics, which is of importance for testing in commercial applications. The result of this report consists of two parts, a software program applying the messagedomain and a theoretical part explaining how the two other domains can be implemented.These three domains together give an intuitive and complete troubleshooting theory thatcould be automated as well as built into the CAN-bus itself in industrial and vehicular applications.The developed program is available for download on the Royal Institute of Technology’sOneDrive and on MediaFire.</p>

corrected abstract:
<p>This bachelor thesis has been carried out in cooperation with Luvly AB, a light urban vehicle startup working out of Stockholm. The project’s goal has been to develop a theory regarding computerized troubleshooting of the CAN-bus in three domains, the messages, the signals, and the electrical characteristics, which is of importance for testing in commercial applications. The result of this report consists of two parts, a software program applying the message domain and a theoretical part explaining how the two other domains can be implemented.</p><p>These three domains together give an intuitive and complete troubleshooting theory that could be automated as well as built into the CAN-bus itself in industrial and vehicular applications.</p><p>The developed program is available for download on the Royal Institute of Technology’s OneDrive and on MediaFire.</p>
----------------------------------------------------------------------
title: "Effect of nutrient limitation in chemostat cultures on amino acid excretion in Clostridium thermocellum"
==>    "Effect of nutrient limitation in chemostat cultures on amino acid excretion in <em>Clostridium thermocellum</em>"

In diva2:1297420 abstract is: <p>Introduction: Clostridium thermocellum is considered a model organism for consolidated bioprocessing, due to its ability to hydrolyze lignocellulosic biomass more efficiently than many other organisms and to produce ethanol.In order to meet the industrial requirements of ethanol yield and titer, metabolic engineering efforts have been made resulting in a strain that successfully displays increased ethanol yield with reduced amount of some byproducts.However, the ethanol yield in this engineered strain still does not meet the industrial requirements and significant amounts of amino acids are still produced. To attempt to decrease the level of amino acid excretion intended to improve the ethanol yield in C. thermocellum, it is essential to understand its metabolism and how it is affected by different cultivation conditions and mediumcompositions. This study aimed to gain an insight in how carbon- and nitrogenlimitation affect amino acid excretion in C. thermocellum, with the hypothesisthat excess of carbon and nitrogen yields more amino acid excretion. Methods: Mass-balance based calculations of rates and yields were used to analyze the metabolism of a wild-type of C. thermocellum (DSM 1313) grownanaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbonmedium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as the limiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0and 400 RPM shaking at a dilution rate of 0.1 h-1.Conclusions: Considering yields of total amino acids excreted in both limitations, it was hypothesized that C. thermocellum exploited the amino acid excretion to maintain carbon balance around the pyruvate node caused by excess of the carbon. Based on yield of valine excreted in particular, it was hypothesized that amino acid excretion was used to maintain redox balance in the metabolism of C. thermocellum, where malate shunt could play a major role.However, results of the Carbon-limitation did not allow any conclusion of nitrogen excess having an effect on amino acid excretion in C. thermocellum.</p>


corrected abstract:
<p><strong>Introduction:</strong> <em>Clostridium thermocellum</em> is considered a model organism for consolidated bioprocessing, due to its ability to hydrolyze lignocellulosic biomass more efficiently than many other organisms and to produce ethanol. In order to meet the industrial requirements of ethanol yield and titer, metabolic engineering efforts have been made resulting in a strain that successfully displays increased ethanol yield with reduced amount of some byproducts. However, the ethanol yield in this engineered strain still does not meet the industrial requirements and significant amounts of amino acids are still produced. To attempt to decrease the level of amino acid excretion intended to improve the ethanol yield in <em>C. thermocellum</em>, it is essential to understand its metabolism and how it is affected by different cultivation conditions and medium compositions. This study aimed to gain an insight in how carbon- and nitrogen limitation affect amino acid excretion in <em>C. thermocellum</em>, with the hypothesis that excess of carbon and nitrogen yields more amino acid excretion.</p><p><strong>Methods:</strong> Mass-balance based calculations of rates and yields were used to analyze the metabolism of a wild-type of <em>C. thermocellum</em> (DSM 1313) grown anaerobically in carbon- or nitrogen-limiting chemostats. For this, Low-Carbon medium containing, respectively, cellobiose (5 g/L) and urea (0.15 g/L) as the limiting nutrient was used. Both cultivations were performed at 55 °C, pH 7.0 and 400 RPM shaking at a dilution rate of 0.1 h<sup>-1</sup>.</p><p><strong>Conclusions:</strong> Considering yields of total amino acids excreted in both limitations, it was hypothesized that <em>C. thermocellum</em> exploited the amino acid excretion to maintain carbon balance around the pyruvate node caused by excess of the carbon. Based on yield of valine excreted in particular, it was hypothesized that amino acid excretion was used to maintain redox balance in the metabolism of <em>C. thermocellum</em>, where malate shunt could play a major role. However, results of the Carbon-limitation did not allow any conclusion of nitrogen excess having an effect on amino acid excretion in <em>C. thermocellum</em>.</p>
----------------------------------------------------------------------
In diva2:1139906 abstract is: <p>Nowadays surgery simulations are aiming to apply not just visual effects but forcefeedback as well. To carry out force feedback, haptic devices are utilized that are mostlycommercial products for general purposes. Some of the haptic device features are moreimportant than others in case of surgery simulator use. The precision of the output forcemagnitude is one such property. The specifications provided by haptic devicemanufacturers are lacking details on device characteristics, known to cause difficulties inplanning of accurate surgery simulations.This project shows the design of a testbed that is capable of measuring the precision ofoutput forces within the haptic devices’ workspace. With the testbed, a set ofmeasurements can be run on different haptic devices, giving as a result a betterknowledge of the utilized device. This knowledge aids the design of more precise andrealistic surgery simulations.</p>


corrected abstract:
<p>Nowadays surgery simulations are aiming to apply not just visual effects but force feedback as well. To carry out force feedback, haptic devices are utilized that are mostly commercial products for general purposes. Some of the haptic device features are more important than others in case of surgery simulator use. The precision of the output force magnitude is one such property. The specifications provided by haptic device manufacturers are lacking details on device characteristics, known to cause difficulties in planning of accurate surgery simulations.</p><p>This project shows the design of a testbed that is capable of measuring the precision of output forces within the haptic devices’ workspace. With the testbed, a set of measurements can be run on different haptic devices, giving as a result a better knowledge of the utilized device. This knowledge aids the design of more precise and realistic surgery simulations.</p>
----------------------------------------------------------------------
In diva2:1864545 abstract is: <p>In this thesis, the Purdue model and its applicability within network security forindustrial control systems under the framework of Industry 4.0 are analyzed.Through a literature review, the model's structure and function are examined inrelation to the new challenges that have emerged due to increased digitization andintegration of IIoT technologies. The study identifies both strengths and weaknessesin the traditional Purdue model. In the results section, a modified version of thePurdue model is introduced, designed to enhance network security and increase thesystems' ability to handle cyber threats and adapt to technological changes in theindustrial sector. This adaptation has been achieved by incorporating additionalsecurity standards and tools aimed at improving the model's efficiency andrelevance.</p>

corrected abstract:
<p>In this thesis, the Purdue model and its applicability within network security for industrial control systems under the framework of Industry 4.0 are analyzed. Through a literature review, the model's structure and function are examined in relation to the new challenges that have emerged due to increased digitization and integration of IIoT technologies. The study identifies both strengths and weaknesses in the traditional Purdue model. In the results section, a modified version of the Purdue model is introduced, designed to enhance network security and increase the systems' ability to handle cyber threats and adapt to technological changes in the industrial sector. This adaptation has been achieved by incorporating additional security standards and tools aimed at improving the model's efficiency and relevance.</p>
----------------------------------------------------------------------
In diva2:1458899 abstract is: <p>Immune-orchestrating biomaterials that precisely modulate the immune reac-tion to the host could lead the way for improving the implantation outcomein the transplantation field, in comparison to passive biomaterials. The lab-oratory of Dr. Thomas Crouzier has shown hydrogels derived from mucinsare capable of orchestrating the immune response mediated by foreign bodyresponse (FBR), as a result of evading fibrosis. Further, a recent study fromhis group showed sialic acid on mucin hydrogels is essential for the immuno-logical activity of those materials. Mucin glycans transiently activated thendampened macrophages, important orchestrators for material-mediated FBR,in a sialic acid-dependent manner for the majority of cytokines followed. Thematerial properties such as rheological properties, self-healing capacity, andstability, can be governed by the crosslinking chemistry used and have a drasticimpact on the functionalities of the materials. In this project, various cross-linking strategies are applied to tune the hydrogel properties. We show thatthe robust cross-linking formed mucin hydrogels having a 1.5% (wt/v) bettersupported insulin-secreting cells form islet-like organoids, compared to 2.5%mucin hydrogels. We then investigate the self-healing properties of the newmucin hydrogels and their interactions with various cell systems.</p>

Note that the actual abstract some time sets "cross-lining" or "crosslinking".
corrected abstract:
<p>Immune-orchestrating biomaterials that precisely modulate the immune reaction to the host could lead the way for improving the implantation outcome in the transplantation field, in comparison to passive biomaterials. The laboratory of Dr. Thomas Crouzier has shown hydrogels derived from mucins are capable of orchestrating the immune response mediated by foreign body response (FBR), as a result of evading fibrosis. Further, a recent study from his group showed sialic acid on mucin hydrogels is essential for the immunological activity of those materials. Mucin glycans transiently activated then dampened macrophages, important orchestrators for material-mediated FBR, in a sialic acid-dependent manner for the majority of cytokines followed. The material properties such as rheological properties, self-healing capacity, and stability, can be governed by the crosslinking chemistry used and have a drastic impact on the functionalities of the materials. In this project, various cross-linking strategies are applied to tune the hydrogel properties. We show that the robust cross-linking formed mucin hydrogels having a 1.5% (wt/v) better supported insulin-secreting cells form islet-like organoids, compared to 2.5% mucin hydrogels. We then investigate the self-healing properties of the new mucin hydrogels and their interactions with various cell systems.</p>
----------------------------------------------------------------------
In diva2:1223357 abstract is: <p>Dietary fatty acids (FAs) have been extensively studied in terms of preventing cardiovascular disease (CVD) through a healthy diet. Certain FA patterns in plasma lipids have been related to increased risk to develop CVD whereas others seems to be preventative (e.g. high linoleic acid [LA], and low palmitoleic acid [16:1n-7]).However, the mechanism behind how FAs are involved in CVD remains unclear. It is believed to involve inflammation and cholesterol metabolism, conditions in which both FAs and a large variety of cardiovascular(CV) proteins are involved. The aim of this project was to investigate whether there are any linkage between plasma fatty acid patterns related to CVD and CV proteins. A cross-sectional analysis was performed in two independent population-based cohorts. The Prospective Investigation of the Vasculature in Uppsala Seniors(PIVUS) cohort was used as a discovery sample and The Prospective investigation of Obesity, Energy andMetabolism (POEM) cohort in Uppsala was used as a replicate sample. A factor analysis of FAs measured in plasma was conducted in respective cohort to identify fatty acid patterns related to CVD. Linear regressionmodels between FA patterns and 84 CV proteins measured in respective cohort (samples) were performed.The 81 proteins being found in both samples were meta-analyzed. Traditional CVD risk factors (sex, BMI,diabetes, systolic blood pressure, LDL and HDL-cholesterol and smoking) were adjusted for. Two factors(Low-LA-factor, Fatty-fish-Lipid-factor) (eigenvalue&gt;1) built up by the same patterns of FAs in both samples and constituted of FAs previously related to CVD were disclosed. The Low-LA-factor was constituted of highloadings of palmitoleic acid (16:1n-7) and oleic acid (18:1n-9) and low loadings of linoleic acid (LA) (18:2n-6),previously recognized as a CVD risk pattern. The Fatty-fish-lipid-factor was constituted of high loading’s ofeicosapentaenoic acid (EPA) (20:5n-3) and docosahexaenoic acid (DHA) (22:6-n-3), previously recognized as cardio preventative FA pattern. In the linear regression meta-analysis, the Low-LA-factor was significantly positively related to 3 plasma proteins. The Fatty-fish-lipid-factor was significantly related negatively with 16plasma proteins and positively with one plasma protein. The majority of these CV proteins have previously been related to CVD but not connected with plasma fatty acid composition. Thus, these novel results confirmthat there are relationships between dietary FA patterns and proteins involved in CVD that potentially partially mediates the association between certain FA patterns and CVD risk. With further research oncausality between FAs and these proteins as well as potential mechanisms, these links may be used as a basisfor future and more tailored dietary strategies to prevent CVD in high-risk individuals.</p>


corrected abstract:
<p>Dietary fatty acids (FAs) have been extensively studied in terms of preventing cardiovascular disease (CVD) through a healthy diet. Certain FA patterns in plasma lipids have been related to increased risk to develop CVD whereas others seems to be preventative (e.g. high linoleic acid [LA], and low palmitoleic acid [16:1n-7]). However, the mechanism behind how FAs are involved in CVD remains unclear. It is believed to involve inflammation and cholesterol metabolism, conditions in which both FAs and a large variety of cardiovascular (CV) proteins are involved. The aim of this project was to investigate whether there are any linkage between plasma fatty acid patterns related to CVD and CV proteins. A cross-sectional analysis was performed in two independent population-based cohorts. The Prospective Investigation of the Vasculature in Uppsala Seniors (PIVUS) cohort was used as a discovery sample and The Prospective investigation of Obesity, Energy and Metabolism (POEM) cohort in Uppsala was used as a replicate sample. A factor analysis of FAs measured in plasma was conducted in respective cohort to identify fatty acid patterns related to CVD. Linear regression models between FA patterns and 84 CV proteins measured in respective cohort (samples) were performed. The 81 proteins being found in both samples were meta-analyzed. Traditional CVD risk factors (sex, BMI, diabetes, systolic blood pressure, LDL and HDL-cholesterol and smoking) were adjusted for. Two factors (Low-LA-factor, Fatty-fish-Lipid-factor) (eigenvalue &gt; 1) built up by the same patterns of FAs in both samples and constituted of FAs previously related to CVD were disclosed. The Low-LA-factor was constituted of high loadings of palmitoleic acid (16:1n-7) and oleic acid (18:1n-9) and low loadings of linoleic acid (LA) (18:2n-6), previously recognized as a CVD risk pattern. The Fatty-fish-lipid-factor was constituted of high loading’s ofeicosapentaenoic acid (EPA) (20:5n-3) and docosahexaenoic acid (DHA) (22:6-n-3), previously recognized as cardio preventative FA pattern. In the linear regression meta-analysis, the Low-LA-factor was significantly positively related to 3 plasma proteins. The Fatty-fish-lipid-factor was significantly related negatively with 16 plasma proteins and positively with one plasma protein. The majority of these CV proteins have previously been related to CVD but not connected with plasma fatty acid composition. Thus, these novel results confirm that there are relationships between dietary FA patterns and proteins involved in CVD that potentially partially mediates the association between certain FA patterns and CVD risk. With further research on causality between FAs and these proteins as well as potential mechanisms, these links may be used as a basis for future and more tailored dietary strategies to prevent CVD in high-risk individuals.</p>
----------------------------------------------------------------------
In diva2:1536745 abstract is: <p>This study shows that stress is one of the important issues which may have a negative impactboth in physical and mental effects on a nurse's working life. Poor ergonomics of the hospitaland a poor behavior approach of nurses can cause stress. To reduce these stress factors, thisstudy suggests solutions based on gamification that imply activities to create a good workingenvironment and to reduce the stress to ensure the good health and well-being of the nurseswho work different shifts especially in night shifts. To achieve the purpose, qualitative researchis used as a method. For this study, nurses who work in hospitals were interviewed in differentshifts especially in night shifts with some open-ended questions related to stress andgamification. Based on their answers, trying to evaluate and analyze the problem and findings.Based on the findings, a gamification concept was developed to provide recommendations tohandle the stress and to be motivated and engaged to develop the well-being of life.</p>

corrected abstract:
<p>This study shows that stress is one of the important issues which may have a negative impact both in physical and mental effects on a nurse's working life. Poor ergonomics of the hospital and a poor behavior approach of nurses can cause stress. To reduce these stress factors, this study suggests solutions based on gamification that imply activities to create a good working environment and to reduce the stress to ensure the good health and well-being of the nurses who work different shifts especially in night shifts. To achieve the purpose, qualitative research is used as a method. For this study, nurses who work in hospitals were interviewed in different shifts especially in night shifts with some open-ended questions related to stress and gamification. Based on their answers, trying to evaluate and analyze the problem and findings. Based on the findings, a gamification concept was developed to provide recommendations to handle the stress and to be motivated and engaged to develop the well-being of life.</p>
----------------------------------------------------------------------
In diva2:936117 abstract is: <p>This paper presents the possibility to transfer Musical Instrument Digital Interface messages overBluetooth Low Energy. The main problem was to transmit the messages between two computers inless than 10 milliseconds. Anything above 10 milliseconds could be noticed as a delay by the personplaying or listening to the music. A prototype was written which could transfer Musical InstrumentDigital Interface messages over Bluetooth Low Energy between two Linux-computers together with atesting framework which was used to make measurements. The prototype was written in the languageC++ with the BlueZ library. The time it took for one packet to travel back and forth from the computerswas clocked to get an estimation of the time it took for a packet to travel from one computer to theother. The measured results showed that it was possible to reach the desired time of 10 milliseconds.The results can also be used when considering development of other kind of equipment and/or applicationsthat implements the use of Bluetooth Low Energy.</p>

corrected abstract:
<p>This paper presents the possibility to transfer Musical Instrument Digital Interface messages over Bluetooth Low Energy. The main problem was to transmit the messages between two computers in less than 10 milliseconds. Anything above 10 milliseconds could be noticed as a delay by the person playing or listening to the music. A prototype was written which could transfer Musical Instrument Digital Interface messages over Bluetooth Low Energy between two Linux-computers together with a testing framework which was used to make measurements. The prototype was written in the language C++ with the BlueZ library. The time it took for one packet to travel back and forth from the computers was clocked to get an estimation of the time it took for a packet to travel from one computer to the other. The measured results showed that it was possible to reach the desired time of 10 milliseconds. The results can also be used when considering development of other kind of equipment and/or applications that implements the use of Bluetooth Low Energy.</p>
----------------------------------------------------------------------
In diva2:935124 abstract is: <p>Today, a steel roll is cut within the steel industry in a cutting factory, the steel rollsare divided to smaller part bands with poor control of the burr height quality. Samplesis taken manually, the amount of samples is too low to know the quality of thesteel roll, the steel rolls can be divided up to 150 times and the length will be 30 kilometers. A whole resend for one steel roll costs up against a million SEK and has anegative climatic impact. One software prototype for detection of burr heights witha reference line was programmed. The prototype contained one light sensor, two motors,a PC and one prototype construction. Each task in the software was allocatedan own thread. Operating systems, threads and algorithms was performance testedfor measurement of execution times and period times. The result showed that a burrheight detector where possible to implement. The algorithm could detect burrheights that were too large related to its reference line.</p>

Note: The actual abstract has "kilo meters" - with a line break between the two words.

corrected abstract:
<p>Today, a steel roll is cut within the steel industry in a cutting factory, the steel rolls are divided to smaller part bands with poor control of the burr height quality. Samples is taken manually, the amount of samples is too low to know the quality of the steel roll, the steel rolls can be divided up to 150 times and the length will be 30 kilo meters. A whole resend for one steel roll costs up against a million SEK and has a negative climatic impact. One software prototype for detection of burr heights with a reference line was programmed. The prototype contained one light sensor, two motors, a PC and one prototype construction. Each task in the software was allocated an own thread. Operating systems, threads and algorithms was performance tested for measurement of execution times and period times. The result showed that a burr height detector where possible to implement. The algorithm could detect burr heights that were too large related to its reference line.</p>
----------------------------------------------------------------------
In diva2:1881766 abstract is: <p>The purpose of the project is to improve the management of aids, accessories and spare parts forMedicinteknisk apparatur i hemmet (MAH) by replacing existing Word-files with a user-friendlydatabase application. The goal is to facilitate MAH employees in finding the right accessories andspare parts for various aids with the help of an application.</p><p>The application was developed in Visual Studio Code using the C programming language and wasdesigned to be user-friendly with pushable buttons and easy updating of information. A user testwas performed to ensure usability. The test showed positive feedback, and suggestions forimprovements which included adding images and the ability to add multiple articles at once. Thenew application proved to be more structured and user-friendly than the previous solutions.</p><p>A comparison between other existing databases was carried out, which showed that the newsolution was the most effective for MAH's needs. The application has streamlined the work ofMAH's employees and offers a better management of data, with future improvements proposed tofurther increase functionality. </p>


corrected abstract:
<p>The purpose of the project is to improve the management of aids, accessories and spare parts for Medicinteknisk apparatur i hemmet (MAH) by replacing existing Word-files with a user-friendly database application. The goal is to facilitate MAH employees in finding the right accessories and spare parts for various aids with the help of an application.</p><p>The application was developed in Visual Studio Code using the C programming language and was designed to be user-friendly with pushable buttons and easy updating of information. A user test was performed to ensure usability. The test showed positive feedback, and suggestions for improvements which included adding images and the ability to add multiple articles at once. The new application proved to be more structured and user-friendly than the previous solutions.</p><p>A comparison between other existing databases was carried out, which showed that the new solution was the most effective for MAH's needs. The application has streamlined the work of MAH's employees and offers a better management of data, with future improvements proposed to further increase functionality.</p>
----------------------------------------------------------------------
In diva2:852471 abstract is: <p>In transcriptomics, Cap Analysis of Gene Expression (CAGE) has been recognized as a powerful tool to identify locations of transcription to start sites and to investigate gene transcription activity and promoter usage on the genome-wide scale. However, despite the substantial efforts from the FANTOM consortium, CAGE has not yet become the leding method of choice for the gene expressions investigations, with data processing and analysis methods awaiting further developments in order to be easily and commonly applicable as for instance the currently popular RNA-seq method. In this thesis, we aimed at developing such CAGE-oriented methods with the ultimate goal of enabling CAGE studies and broadening their applicability. In particular, wefocused on the computational prediction of the location of active enhancers and on application of CAGE to extending the reference transcriptome for a non-model species for which the referencegenome is not yet known.</p><p>In terms of enchancer prediction, we predicted the genomic locations of the active enhancerstranscribed in the white adipose tissues (WAT) using two methods termed Enhancer Intersecttion (EI) and Enhancer Prediction (EP). Both methods were developed and optimized around a previously published study which documented enhancer properties of transcribed bidirectional capped RNAs that can be measured via CAGE. Following the eliminiation of the false positivesvia a set of designed filters, 5.976 uniquely identified enhancer candidates were obtained andassessed in the biological context of obesity in WAT. In Terms of reference transcriptome, also two methods of extending the reference transcirptome of the red spotted salamnder (Notoph-thalmus viridescens) were developed and tested, i.e. de novo contigs-based assembly (DMA) and mapping assembly (MA). The two methods yielded comparable results of ca. 4% of the reference transcripts being extended with DMA method outperforming slightly the MA methodin terms of the newly added bases. Alongside the reference transcriptome extension, the lowerthan standard amount of TNA for preparing CAGE library were tested, showing that as low as100 ng total RNA could work well for the library preparation as well as the subsequen data processing and analysis.</p><p>Together, this thesis presents methods and their application results that could be viewed as new directions of the applicability of the CAGE technique  in genome-wide gene expression and regulation studies. The limited size of the available testing data, unfortunately, does not allow drawing statistically valid conclusions, yet the results clearly highlight the potential of CAGE to computationally predict the location of enhancer candidates and to extend the reference transcriptome when the reference genome is not known. We hope that this work will increase awareness of CAGE and will direct its future application to investigate gene regulation via enhancers and to address genomic questions in the studies involving non-model species.</p>


w='transcirptome' val={'c': 'transcriptome', 's': 'diva2:852471'}
w='eliminiation' val={'c': 'elimination', 's': 'diva2:852471', 'n': 'no full text'}
w='enchancer' val={'c': 'enhancer', 's': 'diva2:852471', 'n': 'no full text'}
w='leding' val={'c': 'leading', 's': 'diva2:852471', 'n': 'no full text'}
w='subsequen' val={'c': 'subsequent', 's': 'diva2:852471', 'n': 'no full text'}
w='salamnder' val={'c': 'salamander', 's': 'diva2:852471', 'n': 'no full text'}
w='Intersecttion' val={'c': 'Intersection', 's': 'diva2:852471', 'n': 'no full text'}

I assumed the usual italics for species and Latin words.

corrected abstract:
<p>In transcriptomics, Cap Analysis of Gene Expression (CAGE) has been recognized as a powerful tool to identify locations of transcription to start sites and to investigate gene transcription activity and promoter usage on the genome-wide scale. However, despite the substantial efforts from the FANTOM consortium, CAGE has not yet become the leading method of choice for the gene expressions investigations, with data processing and analysis methods awaiting further developments in order to be easily and commonly applicable as for instance the currently popular RNA-seq method. In this thesis, we aimed at developing such CAGE-oriented methods with the ultimate goal of enabling CAGE studies and broadening their applicability. In particular, we focused on the computational prediction of the location of active enhancers and on application of CAGE to extending the reference transcriptome for a non-model species for which the reference genome is not yet known.</p><p>In terms of enhancer prediction, we predicted the genomic locations of the active enhancers transcribed in the white adipose tissues (WAT) using two methods termed Enhancer Intersection (EI) and Enhancer Prediction (EP). Both methods were developed and optimized around a previously published study which documented enhancer properties of transcribed bidirectional capped RNAs that can be measured via CAGE. Following the elimination of the false positives via a set of designed filters, 5.976 uniquely identified enhancer candidates were obtained and assessed in the biological context of obesity in WAT. In Terms of reference transcriptome, also two methods of extending the reference transcriptome of the red spotted salamander (<em>Notoph-thalmus viridescens</em>) were developed and tested, i.e. <em>de novo</em> contigs-based assembly (DMA) and mapping assembly (MA). The two methods yielded comparable results of ca. 4% of the reference transcripts being extended with DMA method outperforming slightly the MA method in terms of the newly added bases. Alongside the reference transcriptome extension, the lower than standard amount of TNA for preparing CAGE library were tested, showing that as low as 100 ng total RNA could work well for the library preparation as well as the subsequent data processing and analysis.</p><p>Together, this thesis presents methods and their application results that could be viewed as new directions of the applicability of the CAGE technique  in genome-wide gene expression and regulation studies. The limited size of the available testing data, unfortunately, does not allow drawing statistically valid conclusions, yet the results clearly highlight the potential of CAGE to computationally predict the location of enhancer candidates and to extend the reference transcriptome when the reference genome is not known. We hope that this work will increase awareness of CAGE and will direct its future application to investigate gene regulation via enhancers and to address genomic questions in the studies involving non-model species.</p>
----------------------------------------------------------------------
In diva2:1044172 abstract is: <p>Several epidemiological, in vitro and animal studies using full ERβ knockout(fERβKO) mice suggest that ERβ can be a possible target for colorectal cancer (CRC) prevention andtreatment but the mechanisms are not fully understood. In the present study, intestine specific ERβknockout (iERβKO) mice lacking ERβ specifically in the intestinal epithelial cells have been comparedto mice expressing ERβ, and fERβKO mice completely lacking ERβ, before and after azoxymethane(AOM)/dextran sodium sulfate (DSS) carcinogenic treatment. Alcian blue staining, which specificallystains goblet cells, immunohistochemistry (IHC) with an antibody against the proliferative markerBromodeoxyuridine (BrdU), quantitative PCR (qPCR) and an enzyme-linked immunosorbent assay(ELISA) for the detection of inflammatory markers have been performed. The percentage ofproliferative cells increased as a result of intestinal loss of ERβ in the AOM/DSS model and thenumber of goblet cells did show a trend of increase for the fERβKO mice. These results indicate thatERβ in colon epithelial cells is important for cell proliferation/cancer progression and ERβ in othercells, e.g. immune cells is important for inflammation.</p>


I assumed the usual italics for Latin words.

corrected abstract:
<p>Several epidemiological, <em>in vitro</em> and animal studies using full ERβ knockout (fERβKO) mice suggest that ERβ can be a possible target for colorectal cancer (CRC) prevention and treatment but the mechanisms are not fully understood. In the present study, intestine specific ERβ knockout (iERβKO) mice lacking ERβ specifically in the intestinal epithelial cells have been compared to mice expressing ERβ, and fERβKO mice completely lacking ERβ, before and after azoxymethane (AOM)/dextran sodium sulfate (DSS) carcinogenic treatment. Alcian blue staining, which specifically stains goblet cells, immunohistochemistry (IHC) with an antibody against the proliferative marker Bromodeoxyuridine (BrdU), quantitative PCR (qPCR) and an enzyme-linked immunosorbent assay (ELISA) for the detection of inflammatory markers have been performed. The percentage of proliferative cells increased as a result of intestinal loss of ERβ in the AOM/DSS model and the number of goblet cells did show a trend of increase for the fERβKO mice. These results indicate that ERβ in colon epithelial cells is important for cell proliferation/cancer progression and ERβ in other cells, e.g. immune cells is important for inflammation.</p>
----------------------------------------------------------------------
In diva2:895264 abstract is: <p>Central Venous Catheters (CVCs) are used to provide safe administration of chemotherapeutic drugsduring breast cancer treatment. This study focused on how the exposure of the medical drugs influences the material properties providing possible alterations of the inner surface of one type of CVCs,the Subcutaneous Venous Access Port (SVAP).The SVAP is completely implanted under the skin and is associated with fewer complications and restrictions in regards to physical activity for the patient compared  to other eves on the market.</p><p>The polyurethane catheters from the SVAP examined in this study were obtained from a previous study. The samples were exposed to chemotherapy treatments, containing the cytostatic drugs FE100C + Taxotere, following the treatment protocol for patients, over a prolonged period of time, i.e. 18 weeks. One reference sample exposed to NaCl solution according to the same procedure was obtained from another study conducted earlier.</p><p>The surface degradation on the inner catheter surface was analyzed by the Field Emission Scanning Electron Microscopy (FE-SEM). An increase in porosity and crazing of the material were observed throughout the duration of the treatment. This is caused by the leaching of additives/particlesfrom the material, resulting in alteration of the mechanical and chemical properties.</p><p>The effect on the mechanical properties was analyzed by tensile test. After exposure to chemotherapy treatment a decrease in strength and increase in ductility of the material were observed. The uncertainty in the characterization method of the dynamic contact anglemeasurements made it difficult to conclude a prominent trend for the changes in hydrophobicityofthe material. The analysis, however, indicated that exposure to chemotherapy treatment affectedthe surfaceroughness.</p><p>The results in this study clearly indicate the degradation of the polyurethane catheters afterlong­term exposure to chemotherapy drugs,however,further research is required to estimate the extent of the material deterioration.</p>


w='FE100C' val={'c': 'Fe<sub>100</sub>', 's': 'diva2:895264', 'n': 'no full text - there seems to be a 4 and 6 potentially in front of the formula'}

Is "eves" supposed to be "CVCs"?


corrected abstract:
<p>Central Venous Catheters (CVCs) are used to provide safe administration of chemotherapeutic drugs during breast cancer treatment. This study focused on how the exposure of the medical drugs influences the material properties providing possible alterations of the inner surface of one type of CVCs, the Subcutaneous Venous Access Port (SVAP). The SVAP is completely implanted under the skin and is associated with fewer complications and restrictions in regards to physical activity for the patient compared  to other eves on the market.</p><p>The polyurethane catheters from the SVAP examined in this study were obtained from a previous study. The samples were exposed to chemotherapy treatments, containing the cytostatic drugs Fe<sub>100</sub> + Taxotere, following the treatment protocol for patients, over a prolonged period of time, i.e. 18 weeks. One reference sample exposed to NaCl solution according to the same procedure was obtained from another study conducted earlier.</p><p>The surface degradation on the inner catheter surface was analyzed by the Field Emission Scanning Electron Microscopy (FE-SEM). An increase in porosity and crazing of the material were observed throughout the duration of the treatment. This is caused by the leaching of additives/particles from the material, resulting in alteration of the mechanical and chemical properties.</p><p>The effect on the mechanical properties was analyzed by tensile test. After exposure to chemotherapy treatment a decrease in strength and increase in ductility of the material were observed. The uncertainty in the characterization method of the dynamic contact angle measurements made it difficult to conclude a prominent trend for the changes in hydrophobicity of the material. The analysis, however, indicated that exposure to chemotherapy treatment affected the surface roughness.</p><p>The results in this study clearly indicate the degradation of the polyurethane catheters after long­term exposure to chemotherapy drugs, however, further research is required to estimate the extent of the material deterioration.</p>
----------------------------------------------------------------------
In diva2:1463898 abstract is: <p>This study investigated the architecture and composition of eight lower limb musclesin typically developing children using diffusion tensor imaging and mDixontechniques, respectively. Moreover, the correlation between intramuscular fat fractionand force generation capacity was studied. It was observed that intramuscularfat fraction differed in the considered muscles. A positive correlation was observedbetween the maximum voluntary contraction and the intramuscular fat fraction ofgastrocnemius, soleus and tibialis anterior in four subjects, implying that maximumvoluntary contraction increases proportionally with intramuscular fat fraction.This outcome disproves the primary hypothesis which states that lower intramuscularfat fraction corresponds to a higher amount of produced force. It wasconcluded that intramuscular fat fractions do not affect force generation capacityin typically developing children.</p>

corrected abstract:
<p>This study investigated the architecture and composition of eight lower limb muscles in typically developing children using diffusion tensor imaging and mDixon techniques, respectively. Moreover, the correlation between intramuscular fat fraction and force generation capacity was studied. It was observed that intramuscular fat fraction differed in the considered muscles. A positive correlation was observed between the maximum voluntary contraction and the intramuscular fat fraction of gastrocnemius, soleus and tibialis anterior in four subjects, implying that maximum voluntary contraction increases proportionally with intramuscular fat fraction. This outcome disproves the primary hypothesis which states that lower intramuscular fat fraction corresponds to a higher amount of produced force. It was concluded that intramuscular fat fractions do not affect force generation capacity in typically developing children.</p>
----------------------------------------------------------------------
In diva2:1217156 abstract is: <p>In commercial vehicles, where the driver overnights with the engine turned off whilestill consuming electricity, it is important to know how much the battery can bedischarged before reliable engine starting is at risk. The vehicle’s ability to crank theengine, i.e. startability, changes with the vehicle’s ambient temperature and thebatteries state of charge. The aim of this project is therefore to test the startability ofa commercial vehicle and its cranking system’s behaviour at different ambienttemperatures and battery state of charge. Physical startability tests were planned andperformed on a commercial vehicle at different temperatures inside a climatechamber. The results of these tests show the torque of the vehicle’s powertrainincreasing with lowering temperature while the cranking system’s performancedecreases. This decrease in the cranking system’s performance is a result of thebattery’s lowering ability to supply power at lower temperatures.</p>


corrected abstract:
<p>In commercial vehicles, where the driver overnights with the engine turned off while still consuming electricity, it is important to know how much the battery can be discharged before reliable engine starting is at risk. The vehicle’s ability to crank the engine, i.e. startability, changes with the vehicle’s ambient temperature and the batteries state of charge. The aim of this project is therefore to test the startability of a commercial vehicle and its cranking system’s behaviour at different ambient temperatures and battery state of charge. Physical startability tests were planned and performed on a commercial vehicle at different temperatures inside a climate chamber. The results of these tests show the torque of the vehicle’s powertrain increasing with lowering temperature while the cranking system’s performance decreases. This decrease in the cranking system’s performance is a result of the battery’s lowering ability to supply power at lower temperatures.</p>
----------------------------------------------------------------------
title: "Production of ganglioside biosynthetic membrane enzymes for biochemical and functional studies: Expression, purification and crystallization optimization of Thermococcus onnurineus Dolicho l-phosphate mannose synthase, Homosapiens and Branchiostoma floridae Glucosylceramide synthase"
==>    "Production of ganglioside biosynthetic membrane enzymes for biochemical and functional studies:
Expression, purification and crystallization optimization of <em>Thermococcus onnurineus</em> Dolichol-phosphate mannose synthase, <em>Homo sapiens</em> and <em>Branchiostoma floridae</em> Glucosylceramide synthase"


In diva2:1219274 abstract is: <p>Glycolipids play important roles in the biology of prokaryotes and eukaryotes, including humans, and although theyare found on the cell-membrane surface of all eukaryotic cells, not much is known about their biosynthesis. The aim ofthis project was to characterize two enzymes: glucosylceramide synthase (GCS) which is involved in the biosynthesisof glycolipids such as gangliosides that are abundant in the membranes of nerve cells; and dolicholphosphate mannosesynthase (DPMS), involved in the synthesis precursor for protein glycosylation. Both GCS and DPMS have been shown play a role in cancer as well as in congenital disorders of glycosylation, and are therefore interesting targets tostudy from a therapeutic perspective.With the goal to identify a suitable expression system for GCS, the genes coding for GCS from lancelet (Branchiostoma floridae) and human (Homo sapiens) were cloned and tested for expression in Escherichia coliBL21(DE3)T1 and C41(DE3) using different vectors. Cloning into three different vectors was successful and initial expression testing was performed. SDS-PAGE analysis confirmed initial expression of proteins. Although the correctsize of the protein could be confirmed by Western blot, no fluorescence of the GFP-fusion protein could be detected.DPMS from Thermococcus onnurineus (ToDP) was expressed in E. coli C41(DE3) and purified by immobilized metal ion affinity chromatography and gel filtration. Crystallization optimization was performed for ToDP produced from the vector pNIC28-Bsa4 and plate-like crystals were obtained. X-ray intensity data analysis indicated that thesecrystals contained lipid rather than protein. Crystallization screening for ToDP produced from the vector pNIC-CTHO construct was successful. Crystallization screening using the commercially available MemGold-HT96 crystallization kit resulted in initial crystallization that yielded protein crystals that diffracted to 10 °A resolution.</p>

w='°A' val={'c': 'Å', 's': 'diva2:1219274', 'n': 'correct in original'}

corrected abstract:
<p>Glycolipids play important roles in the biology of prokaryotes and eukaryotes, including humans, and although they are found on the cell-membrane surface of all eukaryotic cells, not much is known about their biosynthesis. The aim of this project was to characterize two enzymes: glucosylceramide synthase (GCS) which is involved in the biosynthesis of glycolipids such as gangliosides that are abundant in the membranes of nerve cells; and dolicholphosphate mannose synthase (DPMS), involved in the synthesis precursor for protein glycosylation. Both GCS and DPMS have been shown play a role in cancer as well as in congenital disorders of glycosylation, and are therefore interesting targets to study from a therapeutic perspective.</p><p>With the goal to identify a suitable expression system for GCS, the genes coding for GCS from lancelet (<em>Branchiostoma floridae</em>) and human (<em>Homo sapiens</em>) were cloned and tested for expression in <em>Escherichia coli</em> BL21(DE3)T1 and C41(DE3) using different vectors. Cloning into three different vectors was successful and initial expression testing was performed. SDS-PAGE analysis confirmed initial expression of proteins. Although the correct size of the protein could be confirmed by Western blot, no fluorescence of the GFP-fusion protein could be detected.</p><p>DPMS from <em>Thermococcus onnurineus</em> (ToDP) was expressed in <em>E. coli</em> C41(DE3) and purified by immobilized metal ion affinity chromatography and gel filtration. Crystallization optimization was performed for ToDP produced from the vector pNIC28-Bsa4 and plate-like crystals were obtained. X-ray intensity data analysis indicated that these crystals contained lipid rather than protein. Crystallization screening for ToDP produced from the vector pNIC-CTHO construct was successful. Crystallization screening using the commercially available MemGold-HT96 crystallization kit resulted in initial crystallization that yielded protein crystals that diffracted to 10 Å resolution.</p>
----------------------------------------------------------------------
In diva2:1353874 abstract is: <p>Positioning systems have a vital role in securing safe movement of trains. There are many differenttypes of positioning systems. This thesis is about how axle counters, communications-based traincontrol (CBTC) and different kinds of track circuits operate. It also contains an analysis of AC-trackcircuits and axle counters with RAMS-parameters (Reliability, Availability, Maintenance and Safety)as guide points to make a conclusion of what type of system that best suits for Stockholm PublicTransports railway tracks. Through interviews with experienced persons within the railway industryin Stockholm knowledge of pros and cons of different systems was obtained. A fault tree analyses(FTA) was made for axle counters and track circuits to visualize potentially hazardous situations.Failure statistics were produced to show failure frequency for one track with axle counters and onetrack with track circuits. A clear result was not shown but it can be concluded that sources of failurethat are prone to track circuit systems can be avoided using axle counters. What became evident isthat the management need to standardize to a fewer amount of different positioning systems. Itwould make it easier to find available personnel with the required skills for doing maintenance. Thiswould also have a benefit when securing maintenance supplies.</p>

corrected abstract:
<p>Positioning systems have a vital role in securing safe movement of trains. There are many different types of positioning systems. This thesis is about how axle counters, communications-based train control (CBTC) and different kinds of track circuits operate. It also contains an analysis of AC-track circuits and axle counters with RAMS-parameters (Reliability, Availability, Maintenance and Safety) as guide points to make a conclusion of what type of system that best suits for Stockholm Public Transports railway tracks. Through interviews with experienced persons within the railway industry in Stockholm knowledge of pros and cons of different systems was obtained. A fault tree analyses (FTA) was made for axle counters and track circuits to visualize potentially hazardous situations. Failure statistics were produced to show failure frequency for one track with axle counters and one track with track circuits. A clear result was not shown but it can be concluded that sources of failure that are prone to track circuit systems can be avoided using axle counters. What became evident is that the management need to standardize to a fewer amount of different positioning systems. It would make it easier to find available personnel with the required skills for doing maintenance. This would also have a benefit when securing maintenance supplies.</p>
----------------------------------------------------------------------
In diva2:1873924 abstract is: <p>Atopic dermatitis (AD) and other atopic diseases are strongly related to skinbarrier dysfunction, a biomeasure which has limited and unsatisfactory assessmenttechniques. Machine learning (ML) powered electrical impedance spectroscopy(EIS) has been shown to differentiate defective barrier function in mice and adults. Techniques such as principal component analysis (PCA) andsupport vector classifiers (SVC) can be used as ML tools to evaluate EIS measurements.</p><p>EIS measurements taken on unaffected skin of children aged between 4 monthsand 3 years were collected and analysed using the children’s AD status. Measurements were grouped into one of four groups based on this AD status; No AD (No AD was developed up until 2 years of age), Pre AD (measurements takenbefore the onset of AD), AD remission (measurements taken after the onset ofAD) and AD flare (measurements taken during active AD). A SVC model was trained on the raw EIS measurement data to distinguish measurements from twoof the binarized AD status groups; AD flare and No AD. An additional SVC model was trained on the No AD group, distinguishing measurements based onbinarized age groups: measurements taken at 4 months against measurements taken at 3 years of age.</p><p>The AD model tested on AD flare against No AD within the test set yielded AUC of 0.92, with a sensitivity and specificity of 89.29% and 88.89% respectively. When testing on all groups of the test set, Pre AD and AD remission groups had group means between the AD flare and No AD groups. No data bias against age was detected in the model. The results of the age model showed that age could be chronologically identified by the age model.</p><p>The AD model was able to differentiate active AD children from children never experiencing AD symptoms on visually unaffected skin and in turn detecting skin barrier dysfunction. Separate studies would need to be conducted to test the predictive power and external validity of the model. Age is a significant factor to consider when designing ML models using EIS data in children, with proper balance in the training set a data bias within the model can be avoided. EIS is a versatile technique due to its data rich nature.</p><p>Machine learning powered electrical impedance spectroscopy measurements are able to detect skin barrier dysfunction. Age is a significant factor when measuring EIS on children, but can be managed.</p>


corrected abstract:
<p>Atopic dermatitis (AD) and other atopic diseases are strongly related to skin barrier dysfunction, a biomeasure which has limited and unsatisfactory assessment techniques. Machine learning (ML) powered electrical impedance spectroscopy (EIS) has been shown to differentiate defective barrier function in mice and adults. Techniques such as principal component analysis (PCA) and support vector classifiers (SVC) can be used as ML tools to evaluate EIS measurements.</p><p>EIS measurements taken on unaffected skin of children aged between 4 months and 3 years were collected and analysed using the children’s AD status. Measurements were grouped into one of four groups based on this AD status; No AD (No AD was developed up until 2 years of age), Pre AD (measurements taken before the onset of AD), AD remission (measurements taken after the onset of AD) and AD flare (measurements taken during active AD). A SVC model was trained on the raw EIS measurement data to distinguish measurements from two of the binarized AD status groups; AD flare and No AD. An additional SVC model was trained on the No AD group, distinguishing measurements based on binarized age groups: measurements taken at 4 months against measurements taken at 3 years of age.</p><p>The AD model tested on AD flare against No AD within the test set yielded AUC of 0.92, with a sensitivity and specificity of 89.29% and 88.89% respectively. When testing on all groups of the test set, Pre AD and AD remission groups had group means between the AD flare and No AD groups. No data bias against age was detected in the model. The results of the age model showed that age could be chronologically identified by the age model.</p><p>The AD model was able to differentiate active AD children from children never experiencing AD symptoms on visually unaffected skin and in turn detecting skin barrier dysfunction. Separate studies would need to be conducted to test the predictive power and external validity of the model. Age is a significant factor to consider when designing ML models using EIS data in children, with proper balance in the training set a data bias within the model can be avoided. EIS is a versatile technique due to its data rich nature.</p><p>Machine learning powered electrical impedance spectroscopy measurements are able to detect skin barrier dysfunction. Age is a significant factor when measuring EIS on children, but can be managed.</p>
----------------------------------------------------------------------
In diva2:1451599 abstract is: <p>Abstract</p><p>Dating apps are continuously becoming a larger part of the social media market.Like any social media app, dating apps utilize a large amount of personaldata. This thesis analyzes two dating apps and how they handle personal informationfrom a security and privacy standpoint. This was done by conceptualizinga threat model and then validating the threat through penetration testingon both of the apps in an attempt to find security vulnerabilities. This analysisproves that there is a substantial difference in whether or not app developerstake security seriously or not. Itwas found that in one of the two apps analyzed,gaining access to personal data was particularly more trivial than expected, asTLS or other encryption were not implemented and server-side authorizationwas lacking in important app features like the one-to-one user chat.</p><p>Keywords – Penetration testing, ethical hacking, dating apps, Android, reverseengineering, threat modeling, risk rating</p>

corrected abstract:
<p>Dating apps are continuously becoming a larger part of the social media market. Like any social media app, dating apps utilize a large amount of personal data. This thesis analyzes two dating apps and how they handle personal information from a security and privacy standpoint. This was done by conceptualizing a threat model and then validating the threat through penetration testing on both of the apps in an attempt to find security vulnerabilities. This analysis proves that there is a substantial difference in whether or not app developers take security seriously or not. It was found that in one of the two apps analyzed, gaining access to personal data was particularly more trivial than expected, as TLS or other encryption were not implemented and server-side authorization was lacking in important app features like the one-to-one user chat.</p>
----------------------------------------------------------------------
In diva2:1108979 abstract is: <p>This thesis has been carried out on behalf of the consulting company Seavus to re-duce cable usage in their office. Smart hubs (i.e. a central control unit) are made tomake usage of devices more effective.One problem is to decide between Bluetooth and Wi-Fi for communication betweenthe user and the smart hub. The choice may depend upon several factors such asbandwidth (throughput), jitter (variation in delay) and packet loss, which are im-portant parameters for assessing the quality of the communication channel.The HW-platform Raspberry Pi and compatible software was used as a measure-ment tool to test Bluetooth and Wi-Fi in different environments.The result showed that Wi-Fi is best suited for communication systems that requirehigh bandwidth and low jitter, and where high amount of packet loss is tolerable.Bluetooth is best suited for communication systems where low bandwidth and highjitter is tolerable, and minimal packet losses preferred.</p>


corrected abstract:
<p>This thesis has been carried out on behalf of the consulting company Seavus to reduce cable usage in their office. Smart hubs (i.e. a central control unit) are made to make usage of devices more effective.</p><p>One problem is to decide between Bluetooth and Wi-Fi for communication between the user and the smart hub. The choice may depend upon several factors such as bandwidth (throughput), jitter (variation in delay) and packet loss, which are important parameters for assessing the quality of the communication channel.</p><p>The HW-platform Raspberry Pi and compatible software was used as a measurement tool to test Bluetooth and Wi-Fi in different environments.</p><p>The result showed that Wi-Fi is best suited for communication systems that require high bandwidth and low jitter, and where high amount of packet loss is tolerable. Bluetooth is best suited for communication systems where low bandwidth and high jitter is tolerable, and minimal packet losses preferred.</p>
----------------------------------------------------------------------
In diva2:1451552 abstract is: <p>Abstract</p><p>A prototype for a smart shoe has been developed at the Royal Institute ofTechnology. It consisted of an Internet of Things (IoT) device which was builtand integrated with a radio module to wirelessly transmit packets. With thissmart-shoe as a base, this thesis attempts to create a system that has the ability torecognize certain foot gestures, and utilizing them to trigger a radio transmissionwhen faced with a danger to hopefully receive the aid that is necessary. Thedangers could for instance be in the event of abuse, kidnapping or robbery. Inorder to accomplish this, machine learning (ML) models were used to predictwhat activity or gesture was executed. For the radio communication, LoRa(Long Range) was utilized as the modulation technique and enabled wirelesstransmissions.</p><p>A system was developed that can detect two gestures with a high accuracy.Whenever a gesture is performed, the radio module is activated and generatesa transmission to another unit.</p><p>Keywords</p><p>Internet of Things, embedded systems, radio communication, LoRa, machinelearning, foot gestures, recognitioniv</p>

w='recognitioniv' val={'c': 'recognition', 's': 'diva2:1451552', 'n': 'correct in original'}

corrected abstract:
<p>A prototype for a smart shoe has been developed at the Royal Institute of Technology. It consisted of an Internet of Things (IoT) device which was built and integrated with a radio module to wirelessly transmit packets. With this smart-shoe as a base, this thesis attempts to create a system that has the ability to recognize certain foot gestures, and utilizing them to trigger a radio transmission when faced with a danger to hopefully receive the aid that is necessary. The dangers could for instance be in the event of abuse, kidnapping or robbery. In order to accomplish this, machine learning (ML) models were used to predict what activity or gesture was executed. For the radio communication, LoRa (Long Range) was utilized as the modulation technique and enabled wireless transmissions.</p><p>A system was developed that can detect two gestures with a high accuracy. Whenever a gesture is performed, the radio module is activated and generates a transmission to another unit.</p>
----------------------------------------------------------------------
In diva2:1438165 abstract is: <p>Plastics and composites have been a growing industry for decades, and the always growing worldand new technologies demand even greater amounts of polymeric composites for a variety ofapplications. Nowadays the negative environmental aspects of polymer production and plastic waste are known, but despite that crude oil is still the primary material for most polymers. Finding biobased materials with sufficient properties to replace the fully synthetic ones is crucial in sustainable development. This master’s thesis studies both glass fiber and microcrystalline cellulose reinforcedbio-oil based polyamide, how they could be compatibilized, the mechanical properties and applicability for additive manufacturing. Compatibilization is an important aspect when two compounds are mixed to make a composite. A proper compatibilizer will enhance the interfacial adhesion between the reinforcement and matrix, thus increasing the mechanical properties of the material. The glass fiber/polyamide11 composite was compatibilized with vinyltrimethoxysilane, and the microcrystalline cellulose/polyamide11 composite was compatibilized with 4,4'-diphenylmethane diisocyanate. Both composites were analyzed to obtain information about thermal, mechanical, and rheological properties. The surface and fracture morphology are examined, as well.The results indicate that reinforcing resulted to enhanced mechanical properties, even though the desired compatibilization was not acquired in the experiment. The most encouraging result was that the bio-based cellulose reinforcement enhanced mechanical properties, by visual examination thefully bio-based polymeric composite was found to be more ductile than the glass fiber reinforcedone. For future prospect, there are few issues to be addressed and overcome for these materials to bemade suitable for additive manufacturing. The key is finding a compatibilizer that can withstand high processing temperatures repeatedly. Maintaining uniform properties requires proper dispersion of reinforcement, which is achieved by optimizing the manufacturing method. In addition, cellulose is prone to thermal degradation, so the processing temperatures for both reinforcement and matrix should be considered carefully.</p>


corrected abstract:
<p>Plastics and composites have been a growing industry for decades, and the always growing world and new technologies demand even greater amounts of polymeric composites for a variety of applications. Nowadays the negative environmental aspects of polymer production and plastic waste are known, but despite that crude oil is still the primary material for most polymers. Finding biobased materials with sufficient properties to replace the fully synthetic ones is crucial in sustainable development. This master’s thesis studies both glass fiber and microcrystalline cellulose reinforced bio-oil based polyamide, how they could be compatibilized, the mechanical properties and applicability for additive manufacturing.</p><p>Compatibilization is an important aspect when two compounds are mixed to make a composite. A proper compatibilizer will enhance the interfacial adhesion between the reinforcement and matrix, thus increasing the mechanical properties of the material. The glass fiber/polyamide11 composite was compatibilized with vinyltrimethoxysilane, and the microcrystalline cellulose/polyamide11 composite was compatibilized with 4,4'-diphenylmethane diisocyanate. Both composites were analyzed to obtain information about thermal, mechanical, and rheological properties. The surface and fracture morphology are examined, as well.</p><p>The results indicate that reinforcing resulted to enhanced mechanical properties, even though the desired compatibilization was not acquired in the experiment. The most encouraging result was that the bio-based cellulose reinforcement enhanced mechanical properties, by visual examination the fully bio-based polymeric composite was found to be more ductile than the glass fiber reinforced one.</p><p>For future prospect, there are few issues to be addressed and overcome for these materials to be made suitable for additive manufacturing. The key is finding a compatibilizer that can withstand high processing temperatures repeatedly. Maintaining uniform properties requires proper dispersion of reinforcement, which is achieved by optimizing the manufacturing method. In addition, cellulose is prone to thermal degradation, so the processing temperatures for both reinforcement and matrix should be considered carefully.</p>
----------------------------------------------------------------------
In diva2:1801810 abstract is: <p>This Bachelor thesis presents an in-depth investigation into the effects of RogueAccess Point interference within Internet of Things networks. The study focuses onthe impact of rogue APs on the modulation and coding scheme indices, round triptime, and overall network performance. The presence of a rogue AP was found toshift devices from dual-stream to single-stream operation, causing a decrease in themodulation and coding scheme indices and data rates. Additionally, a significantincrease in round trip time was observed, emphasizing the detrimental impact ofrogue AP interference on network latency. The insights gained from this researchcontribute to a greater understanding of the challenges posed by rogue APinterference. This deeper comprehension paves the way for devising effectivestrategies to mitigate these impacts, thereby enhancing the reliability, security, andperformance of IoT networks.</p>


corrected abstract:
<p>This Bachelor thesis presents an in-depth investigation into the effects of Rogue Access Point interference within Internet of Things networks. The study focuses on the impact of rogue APs on the modulation and coding scheme indices, round trip time, and overall network performance. The presence of a rogue AP was found to shift devices from dual-stream to single-stream operation, causing a decrease in the modulation and coding scheme indices and data rates. Additionally, a significant increase in round trip time was observed, emphasizing the detrimental impact of rogue AP interference on network latency. The insights gained from this research contribute to a greater understanding of the challenges posed by rogue AP interference. This deeper comprehension paves the way for devising effective strategies to mitigate these impacts, thereby enhancing the reliability, security, and performance of IoT networks.</p>
----------------------------------------------------------------------
In diva2:1686451 abstract is: <p>When removal of primary tumors is performed by surgery, the surgeon andthe patient would benefit from using intraoperative feedback to evaluate thetumor resection margin. If intraoperative feedback is not provided there isadditional risk of treatment of the tumor. With improvements to the fixationtime of soft tissue, phase-contrast computed tomography (PC-CT) could allow3D intraoperative feedback to the surgeons.The focus of this project has been to develop a method for measuring fixationtime on soft tissue samples and examine ways to decrease the required fixationtime. Furthermore, evaluation of fixatives to achieve image contrast that wouldbe sufficient to accomplish the possibility of intraoperative tumor feedback, inPC-CT images has been performed in this project.</p>

corrected abstract:
<p>When removal of primary tumors is performed by surgery, the surgeon and the patient would benefit from using intraoperative feedback to evaluate the tumor resection margin. If intraoperative feedback is not provided there is additional risk of treatment of the tumor. With improvements to the fixation time of soft tissue, phase-contrast computed tomography (PC-CT) could allow 3D intraoperative feedback to the surgeons.</p><p>The focus of this project has been to develop a method for measuring fixation time on soft tissue samples and examine ways to decrease the required fixation time. Furthermore, evaluation of fixatives to achieve image contrast that would be sufficient to accomplish the possibility of intraoperative tumor feedback, in PC-CT images has been performed in this project.</p>
----------------------------------------------------------------------
In diva2:1080331 abstract is: <p>This diploma work has been carried out on behalf of Stegia AB. Stegia manufacturesand sells linear motors that are tested in a test bench before shipped to thecustomer. Today there is a need for a test bench that collects test results from theperformed tests. The company would like to lower the risk of falsely made tests,and to make the test bench easier for the production staff to use. At this moment,the test of the linear motors is made up of several manual tasks before the test iscompleted. This increases the risk of wrong results because of incorrectly madetests, with no traceability of the results. The goal with this diploma work is to lowerthe risk of incorrect test result by automating the testing process for the productionstaff. The result is presented as a prototype, that should increase the reliability ofthe test result of the motors. The result is then validated and analyzed in this report.</p>

corrected abstract:
<p>This diploma work has been carried out on behalf of Stegia AB. Stegia manufactures and sells linear motors that are tested in a test bench before shipped to the customer. Today there is a need for a test bench that collects test results from the performed tests. The company would like to lower the risk of falsely made tests, and to make the test bench easier for the production staff to use. At this moment, the test of the linear motors is made up of several manual tasks before the test is completed. This increases the risk of wrong results because of incorrectly made tests, with no traceability of the results. The goal with this diploma work is to lower the risk of incorrect test result by automating the testing process for the production staff. The result is presented as a prototype, that should increase the reliability of the test result of the motors. The result is then validated and analyzed in this report.</p>
----------------------------------------------------------------------
In diva2:1738689 abstract is: <p>The Network Operation Center (NOC) at Transtema Network Service monitors andfilters incoming alarms from different types of networks. When a disturbance occursin a network, among the measures taken is to inform customers about it. This processis not automated, which means that people working in the NOC must do it manually.At the beginning of this study, a literature study of previous work has been done inorder to collect data and be able to increase knowledge about how the automation ofthe process for sending information to a customer is carried out. Interviews and existing documentation helped to collect data and to understand the process. Afterthat, a software prototype was developed to automate the process of sending information to a customer about the current state of the network. In addition, pre-automation and post-automation tests of the process were carried out to compare themand analyze the results, which showed that the NOC-department can increase theefficiency and benefit of both the company and the customers with the automationof the process</p>

corrected abstract:
<p>The Network Operation Center (NOC) at Transtema Network Service monitors and filters incoming alarms from different types of networks. When a disturbance occurs in a network, among the measures taken is to inform customers about it. This process is not automated, which means that people working in the NOC must do it manually. At the beginning of this study, a literature study of previous work has been done in order to collect data and be able to increase knowledge about how the automation of the process for sending information to a customer is carried out. Interviews and existing documentation helped to collect data and to understand the process. After that, a software prototype was developed to automate the process of sending information to a customer about the current state of the network. In addition, pre-automation and post-automation tests of the process were carried out to compare them and analyze the results, which showed that the NOC-department can increase the efficiency and benefit of both the company and the customers with the automation of the process.</p>
----------------------------------------------------------------------
In diva2:801742 abstract is: <p>Alzheimer's disease (AD) and Lewy body dementia (DLB) are mong the most common form of dementia. Neurodegenerative diseases affect many people worldwide, and due to the lack in therapeutic targets, as well as biomarkers for diagnostics, there is a need for novel biomarkers for monitoring disease progression and especially early diagnose. A list of disease associated, brain enriched genes served as a starting point for the initiative phase in the search of a protein as a potntial biomarker. The method used was multiplex- immunofluorescence, in which a limitationtoday is the availability of primary antibodies raised in different host species. Here, heatinactivation as elution to eliminate cross-reaction of antibodies was investigated using tyramide signal amplification in an automated stainer. As proof of principle, sequential staining wasperformed on human cortex, with four rabbit antibodies targeting different morphological structures. There was no cross-rection between secondary antibodies and the protocol designed was used throughout the study. Protein distribution pattern was analysed on a tissue microarray(TMA) composed of human temporal cortex tissue obtained from 29 subjects; ten from patientsdiagnosed with AD, ten with DLB and nine controls, two cores from each case. IF was applied on proteins selected from the list provided, to assess how they relate to disease progression and evaluate TMA as approach. TMA was regarded as a useful tool for initiative step in screening for proteins as biomarkers, as long as representative tissue for cores is carefully selected.There are challenges important to address in using the sequential staining method and the TMA, but as a whole, a new approach for qualitative analysis of protein distribtuion was proposed. With thisapproach, the full potential of the antibodies generated within the HPA project, and other primary antibodies derived from the same species, can be combined with desired fluorophore within the same staining protocol, in a high throughput screening for proteins as a biomarker.</p>

w='distribtuion' val={'c': 'distribution', 's': 'diva2:801742', 'n': 'no full text'}
w='cross-rection' val={'c': 'cross-reaction', 's': 'diva2:801742', 'n': 'no full text'}
w='mong' val={'c': 'among', 's': 'diva2:801742', 'n': 'no full text'}
w='potntial' val={'c': 'potential', 's': 'diva2:801742', 'n': 'no full text'}

corrected abstract:
<p>Alzheimer's disease (AD) and Lewy body dementia (DLB) are among the most common form of dementia. Neurodegenerative diseases affect many people worldwide, and due to the lack in therapeutic targets, as well as biomarkers for diagnostics, there is a need for novel biomarkers for monitoring disease progression and especially early diagnose. A list of disease associated, brain enriched genes served as a starting point for the initiative phase in the search of a protein as a potential biomarker. The method used was multiplex- immunofluorescence, in which a limitation today is the availability of primary antibodies raised in different host species. Here, heatin activation as elution to eliminate cross-reaction of antibodies was investigated using tyramide signal amplification in an automated stainer. As proof of principle, sequential staining was performed on human cortex, with four rabbit antibodies targeting different morphological structures. There was no cross-reaction between secondary antibodies and the protocol designed was used throughout the study. Protein distribution pattern was analysed on a tissue microarray (TMA) composed of human temporal cortex tissue obtained from 29 subjects; ten from patients diagnosed with AD, ten with DLB and nine controls, two cores from each case. IF was applied on proteins selected from the list provided, to assess how they relate to disease progression and evaluate TMA as approach. TMA was regarded as a useful tool for initiative step in screening for proteins as biomarkers, as long as representative tissue for cores is carefully selected. There are challenges important to address in using the sequential staining method and the TMA, but as a whole, a new approach for qualitative analysis of protein distribution was proposed. With this approach, the full potential of the antibodies generated within the HPA project, and other primary antibodies derived from the same species, can be combined with desired fluorophore within the same staining protocol, in a high throughput screening for proteins as a biomarker.</p>

----------------------------------------------------------------------
In diva2:854019 abstract is: <p>Glycosyltransferases (GT) are enzymes that synthesise vital complexes consisting of one or more sugar molecule units. A large number of GTs are integral membrane proteins. This means they can be found embedded in the many different membrane structures of cells. One example is the bacterial glycosyltransferase XCGT.</p><p>Few threee-dimensional structures have been solved for membrane GTs because of the technical difficultires that arie in work with integral membrane proteins. As a result there is only limited information available on the interplay of structure and function for membrane GTs. The aim of thisproject is to characterise XCGT biochemically and optimise conditions for increased protein stability and well.ordered, crystallisation. This should pave the road to a successfully determined crystalstructure. This thesis reports the project before publication in a peer-reviewed journal and for this reason some information is not disclosed. Yet, specific protein characteristics and methodological details are presented and supported by theory and experimental data.</p><p>The thesis opens with a theoretical review of the family of glycosyltransferases and methodology for work with membrane proteins. In addition, a background is provided for later discussion of the effect of certaindetergents on protein stability. Choice of detergent, additives and crystallisation format are shown to have critical influence on crystallisation. An outlook on alternative techniques that may improve the chances to solve XCGT's crystal structure forms the closing of the thesis.</p><p>Throughout the project, methods have been refined and results were obtained that will havesignificant importance for the continued efforts to determine the crystal structure of XGCT, and themethodological findings can even be extrapolated to the work with membrane proteins in general.</p>


w='arie' val={'c': 'are', 's': 'diva2:854019', 'n': 'no full text'}
w='difficultires' val={'c': 'difficulties', 's': 'diva2:854019', 'n': 'no full text'}
w='threee-dimensional' val={'c': 'three-dimensional', 's': 'diva2:854019', 'n': 'no full text'}

corrected abstract:
<p>Glycosyltransferases (GT) are enzymes that synthesise vital complexes consisting of one or more sugar molecule units. A large number of GTs are integral membrane proteins. This means they can be found embedded in the many different membrane structures of cells. One example is the bacterial glycosyltransferase XCGT.</p><p>Few three-dimensional structures have been solved for membrane GTs because of the technical difficulties that are in work with integral membrane proteins. As a result there is only limited information available on the interplay of structure and function for membrane GTs. The aim of this project is to characterise XCGT biochemically and optimise conditions for increased protein stability and well. ordered, crystallisation. This should pave the road to a successfully determined crystal structure. This thesis reports the project before publication in a peer-reviewed journal and for this reason some information is not disclosed. Yet, specific protein characteristics and methodological details are presented and supported by theory and experimental data.</p><p>The thesis opens with a theoretical review of the family of glycosyltransferases and methodology for work with membrane proteins. In addition, a background is provided for later discussion of the effect of certain detergents on protein stability. Choice of detergent, additives and crystallisation format are shown to have critical influence on crystallisation. An outlook on alternative techniques that may improve the chances to solve XCGT's crystal structure forms the closing of the thesis.</p><p>Throughout the project, methods have been refined and results were obtained that will have significant importance for the continued efforts to determine the crystal structure of XGCT, and the methodological findings can even be extrapolated to the work with membrane proteins in general.</p>
----------------------------------------------------------------------
In diva2:721188 abstract is: <p>Acid catalyzed liquefaction of brown paper handtowel was performed using a mixture of diethylene glycol and glycerol andaddition of minute amount of p-toluenesulphonic acid as a catalyst. TheLiquefied brown paper was used as a polyhydroxy alcohol during the estersynthesis as large number of hydroxyl groups is available in liquefied brownpaper products. Esterification was performed by using hexanoic acid along withdibutyl tin (IV) oxide as a catalyst. The product was characterized by usingFTIR, SEC and SEM. PVC films were prepared by solution casting. They wereplasticized by the prepared liquefied brown paper ester and by traditionaldiisooctyl phthalate. Comparison of both plasticizers on the basis ofmechanical, thermal properties and plasticizer migration was elaborated.</p>


I think that it should be "dibutyltin (IV) oxide" and not have a space before "tin". See for example: https://www.sigmaaldrich.com/SE/en/product/aldrich/183083

corrected abstract:
<p>Acid catalyzed liquefaction of brown paper hand towel was performed using a mixture of diethylene glycol and glycerol and addition of minute amount of p-toluenesulphonic acid as a catalyst. The Liquefied brown paper was used as a polyhydroxy alcohol during the ester synthesis as large number of hydroxyl groups is available in liquefied brownpaper products. Esterification was performed by using hexanoic acid along with dibutyltin (IV) oxide as a catalyst. The product was characterized by using FTIR, SEC and SEM. PVC films were prepared by solution casting. They were plasticized by the prepared liquefied brown paper ester and by traditional diisooctyl phthalate. Comparison of both plasticizers on the basis of mechanical, thermal properties and plasticizer migration was elaborated.</p>
----------------------------------------------------------------------
In diva2:1763068 abstract is: <p>This report analyzes and tests various methods aimed at distinguishinghuman-generated solutions to tasks and texts from those generated by artificialintelligence. Recently the use of artificial intelligence has seen a significantincrease, especially among students. The purpose of this study is to determinewhether it is currently possible to detect if a college student in electricalengineering is using AI to cheat. In this report, solutions to tasks and textsgenerated by the program ChatGPT are tested using a general methodology andexternal AI-based tools. The research covers the areas of mathematics,programming and written text. The results of the investigation suggest that it is notpossible to detect cheating with the help of an AI in the subjects of mathematicsand programming. In the case of text, cheating by using an AI can be detected tosome extent.</p>

corrected abstract:
<p>This report analyzes and tests various methods aimed at distinguishing human-generated solutions to tasks and texts from those generated by artificial intelligence. Recently the use of artificial intelligence has seen a significant increase, especially among students. The purpose of this study is to determine whether it is currently possible to detect if a college student in electrical engineering is using AI to cheat. In this report, solutions to tasks and texts generated by the program ChatGPT are tested using a general methodology and external AI-based tools. The research covers the areas of mathematics, programming and written text. The results of the investigation suggest that it is not possible to detect cheating with the help of an AI in the subjects of mathematics and programming. In the case of text, cheating by using an AI can be detected to some extent.</p>
----------------------------------------------------------------------
In diva2:1383922 abstract is: <p>The interior of heavy goods vehicles (HGVs) differs from passenger cars. Both the steering wheel and the occupant are positioned differently in a HGV and increases the risk of steering wheel rim impacts. Such impact scenarios are relatively unexplored compared to passenger car safety studies that are more prevalent within the field of injury biomechanics. The idea with using human body models (HBMs) is to complement current crash test dummies with biomechanical data. Furthermore, the biofidelity of a crash dummy for loading similar to a steering wheel rimimpact is relatively unstudied and especially to different rib levels. Therefore, the aim with this thesis was to evaluate HGV occupant thoracic response between THUMS v4.0 and Hybrid III (H3) during steering wheel rim impacts with respect to different rib levels (level 1-2, 3-4, 6-7, 7-8, 9-10) with regards to ribs, aorta, liver, and spleen.</p><p>To the author’s best knowledge, use of local injury risk functions for thoracic injuries is fairly rare compared to the predominant usage of global injury criteria that mainly predicts the most commonthoracic injury risk, i.e. rib fractures. Therefore, local injury criteria using experimental test datahave been developed for the ribs and the organs. The measured parameters were chest deflectionand steering wheel to thorax contact force on a global level, whilst 1st principal Green-Lagrangestrains was assessed for the rib and the organ injury risk. The material models for the liver and</p><p>the spleen were remodelled using an Ogden material model based on experimental stress-strain data to account for hyperelasticity. Rate-dependency was included by iteration of viscoelastic parameters. The contact modelling of the organs was changed from a sliding contact to a tied contact to minimize unrealistic contact separations during impact.</p><p>The results support previous findings that H3 needs additional instrumentation to accurately</p><p>register chest deflection for rib levels beyond its current range, namely at ribs 1-2, 7-8, and 9-10. For THUMS, the chest deflection were within reasonable values for the applied velocities, but there were no definite injury risk. Fact is, the global injury criteria might overpredict the AIS3 injury risk (rib fractures) for rib level 1-2, 7-8, and 9-10. The rib strains could not be correlated with the measured chest deflections. This was explained by the unique localized loading characterized by pure steering wheel rim impact that mainly affected the sternum and the rib cartilage while minimizing rib deformation. The organ strains indicate some risk of rupture where the spleen deforms the most at rib levels 3-4 and 6-7, and the liver and the aorta at rib levels 6-7 and 7-8.</p><p>This study provides a framework for complementing H3 with THUMS for HGV occupant safety</p><p>with emphasis on the importance of using local injury criteria for functional injury prediction,</p><p>i.e. prediction of injury risk using parameters directly related to rib fracture or organ rupture.</p><p>Local injury criteria are thus a powerful safety assessment tool as it is independent on exterior loading such as airbag, steering wheel hub, or seat belt loading. It was noticed that global injury criteria with very localized impacts such as rim impacts have not been studied and will affect rib fracture risk differently than what has been studied using airbag or seat belt restraints. However, improvements are needed to accurately predict thoracic injury risk at a material level by finding more data for the local injury risk functions.</p><p>Conclusively, it is clear that Hybrid III has insufficient instrumentation and is in need of upgrades to register chest deflections at multiple rib levels. Furthermore, the following are needed: better understanding of global injury criteria specific for HGV occupant safety evaluation, more data for age-dependent (ribs) and rate-dependent (organs) injury risk functions, a tiebreak contact with tangential sliding for better organ kinematics during impacts, and improving the biofidelity of the material models using data from tissue level experiments.</p>

corrected abstract:
<p>The interior of heavy goods vehicles (HGVs) differs from passenger cars. Both the steering wheel and the occupant are positioned differently in a HGV and increases the risk of steering wheel rim impacts. Such impact scenarios are relatively unexplored compared to passenger car safety studies that are more prevalent within the field of injury biomechanics. The idea with using human body models (HBMs) is to complement current crash test dummies with biomechanical data. Furthermore, the biofidelity of a crash dummy for loading similar to a steering wheel rim impact is relatively unstudied and especially to different rib levels. Therefore, the aim with this thesis was to evaluate HGV occupant thoracic response between THUMS v4.0 and Hybrid III (H3) during steering wheel rim impacts with respect to different rib levels (level 1-2, 3-4, 6-7, 7-8, 9-10) with regards to ribs, aorta, liver, and spleen.</p><p>To the author’s best knowledge, use of local injury risk functions for thoracic injuries is fairly rare compared to the predominant usage of global injury criteria that mainly predicts the most common thoracic injury risk, i.e. rib fractures. Therefore, local injury criteria using experimental test data have been developed for the ribs and the organs. The measured parameters were chest deflection and steering wheel to thorax contact force on a global level, whilst 1st principal Green-Lagrange strains was assessed for the rib and the organ injury risk. The material models for the liver and the spleen were remodelled using an Ogden material model based on experimental stress-strain data to account for hyperelasticity. Rate-dependency was included by iteration of viscoelastic parameters. The contact modelling of the organs was changed from a sliding contact to a tied contact to minimize unrealistic contact separations during impact.</p><p>The results support previous findings that H3 needs additional instrumentation to accurately register chest deflection for rib levels beyond its current range, namely at ribs 1-2, 7-8, and 9-10. For THUMS, the chest deflection were within reasonable values for the applied velocities, but there were no definite injury risk. Fact is, the global injury criteria might overpredict the AIS3 injury risk (rib fractures) for rib level 1-2, 7-8, and 9-10. The rib strains could not be correlated with the measured chest deflections. This was explained by the unique localized loading characterized by pure steering wheel rim impact that mainly affected the sternum and the rib cartilage while minimizing rib deformation. The organ strains indicate some risk of rupture where the spleen deforms the most at rib levels 3-4 and 6-7, and the liver and the aorta at rib levels 6-7 and 7-8.</p><p>This study provides a framework for complementing H3 with THUMS for HGV occupant safety with emphasis on the importance of using local injury criteria for functional injury prediction, i.e. prediction of injury risk using parameters directly related to rib fracture or organ rupture. Local injury criteria are thus a powerful safety assessment tool as it is independent on exterior loading such as airbag, steering wheel hub, or seat belt loading. It was noticed that global injury criteria with very localized impacts such as rim impacts have not been studied and will affect rib fracture risk differently than what has been studied using airbag or seat belt restraints. However, improvements are needed to accurately predict thoracic injury risk at a material level by finding more data for the local injury risk functions.</p><p>Conclusively, it is clear that Hybrid III has insufficient instrumentation and is in need of upgrades to register chest deflections at multiple rib levels. Furthermore, the following are needed: better understanding of global injury criteria specific for HGV occupant safety evaluation, more data for age-dependent (ribs) and rate-dependent (organs) injury risk functions, a tiebreak contact with tangential sliding for better organ kinematics during impacts, and improving the biofidelity of the material models using data from tissue level experiments.</p>
----------------------------------------------------------------------
In diva2:572065 abstract is: <p>Voice over IP (VoIP) is a relatively new technology that enables voice calls over data networks.With VoIP it is possible to lower expenses, and increase functionality and flexibility. FromSwedish Armed Forces point of view, the security issue is of great importance, why the focus inthis report is on the security aspect of the two most common open-source VoIP-protocols H.323and SIP, some of the most common attacks, and counter-measures for those attacks.Because of the level of complexity with a network running H.323 or SIP, and the fact that it hasyet to stand the same level of trial as of traditional telephony, a VoIP-system includes manyknown security-issues, and probably at present many unknown security flaws.</p><p>The conclusion is that it takes great knowledge and insight about a VoIP-network based onH.323 or SIP to make the network satisfyingly safe as it is today, and is therefore perhaps not asuitable solution for the Swedish Armed Forces today for their more sensitive communications.</p>

corrected abstract:
<p>Voice over IP (VoIP) is a relatively new technology that enables voice calls over data networks. With VoIP it is possible to lower expenses, and increase functionality and flexibility. From Swedish Armed Forces point of view, the security issue is of great importance, why the focus in this report is on the security aspect of the two most common open-source VoIP-protocols H.323 and SIP, some of the most common attacks, and counter-measures for those attacks. Because of the level of complexity with a network running H.323 or SIP, and the fact that it has yet to stand the same level of trial as of traditional telephony, a VoIP-system includes many known security-issues, and probably at present many unknown security flaws.</p><p>The conclusion is that it takes great knowledge and insight about a VoIP-network based on H.323 or SIP to make the network satisfyingly safe as it is today, and is therefore perhaps not a suitable solution for the Swedish Armed Forces today for their more sensitive communications.</p>
----------------------------------------------------------------------
In diva2:1454860 abstract is: <p>Implementing the string method with swarms of trajectories, to model transitions between protein states,is laborious work. As it is also repetitive with small di↵erences in between repetitions, it suits itself wellfor automation. This project’s aim was to produce software, developed in Python3, implementing the GromacsPython API, capable of automating a generalised implementation of the string method with swarmsof trajectories. The final software was able to adapt one collective variable (CV) configuration file and twoprotein configuration files representing start and end states of the protein in preparation of, and subsequentlyrun MD simulations implementing the string method with swarms of trajectories. The resulting software hasgarnered interest from both the TCB-labs at Science for Life Laboratories who wish to use the software forthe implementation of the string method, and the Gromacs development team for further insights into theusability of the API.</p>

corrected abstract:
<p>Implementing the string method with swarms of trajectories, to model transitions between protein states, is laborious work. As it is also repetitive with small di↵erences in between repetitions, it suits itself well for automation. This project’s aim was to produce software, developed in Python3, implementing the Gromacs Python API, capable of automating a generalised implementation of the string method with swarms of trajectories. The final software was able to adapt one collective variable (CV) configuration file and two protein configuration files representing start and end states of the protein in preparation of, and subsequently run MD simulations implementing the string method with swarms of trajectories. The resulting software has garnered interest from both the TCB-labs at Science for Life Laboratories who wish to use the software for the implementation of the string method, and the Gromacs development team for further insights into the usability of the API.</p>
----------------------------------------------------------------------
In diva2:1144116 abstract is: <p>This thesis has been carried out on behalf of KTH the school for medicaltechnology, who saw the need to use diaphanography, an medical imaging methodto detect breastmilk and diseases in women. In this thesis, diaphanography is usedto detect breastmilk in breastfeeding women.Breast engorgement is a problem for women who breastfeed. When the motherproduces more milk than the baby uses, pain occurs. To reduce engorgementdiaphanography can be used to analyze and see if there is any milk produced.Two methods, reflectance and penetration was used to examine the amount ofmilk.The results showed that both methods could detect milk, but only the penetrationcould estimate the amount of milk.</p>


corrected abstract:
<p>This thesis has been carried out on behalf of KTH the school for medical technology, who saw the need to use diaphanography, an medical imaging method to detect breastmilk and diseases in women. In this thesis, diaphanography is used to detect breastmilk in breastfeeding women.</p><p>Breast engorgement is a problem for women who breastfeed. When the mother produces more milk than the baby uses, pain occurs. To reduce engorgement diaphanography can be used to analyze and see if there is any milk produced.</p><p>Two methods, reflectance and penetration was used to examine the amount of milk.</p><p>The results showed that both methods could detect milk, but only the penetration could estimate the amount of milk.</p>
----------------------------------------------------------------------
title: "Analyzing Changes inIntra-OperativeSignals UsingMachine Learning"
==>    "Analyzing Changes in Intra-Operative Signals Using Machine Learning"

In diva2:1874237 abstract is: <p>Non-cardiac surgeries conducted globally each year often lead to cardiac complications,with myocardial injury commonly occurring within 30 days post-surgery. This thesis investigates the correlation between intra-operative signals and myocardial injuryusing machine learning models. The study focuses on analyzing intra-operative STelevation, heart rate, diastolic blood pressure, and pulse pressure variation. Multiplemachine learning models, including decision tree and random forest classifiers, weredeveloped and evaluated using two approaches: the sequence of event times and comprehensive event features.The results indicate that intra-operative physiological signals are valuable predictorsof myocardial injury, with random forest models generally outperforming decision treemodels. However, limited and unbalanced data posed challenges, affecting model performance variability. This research establishes a framework for enhancing predictivemodels and monitoring strategies to improve patient outcomes.</p>

partal corrected: diva2:1874237: <p>Non-cardiac surgeries conducted globally each year often lead to cardiac complications, with myocardial injury commonly occurring within 30 days post-surgery. This thesis investigates the correlation between intra-operative signals and myocardial injury using machine learning models. The study focuses on analyzing intra-operative ST elevation, heart rate, diastolic blood pressure, and pulse pressure variation. Multiple machine learning models, including decision tree and random forest classifiers, were developed and evaluated using two approaches: the sequence of event times and comprehensive event features. The results indicate that intra-operative physiological signals are valuable predictors of myocardial injury, with random forest models generally outperforming decision tree models. However, limited and unbalanced data posed challenges, affecting model performance variability. This research establishes a framework for enhancing predictivemodels and monitoring strategies to improve patient outcomes.</p>

corrected abstract:
<p>Non-cardiac surgeries conducted globally each year often lead to cardiac complications, with myocardial injury commonly occurring within 30 days post-surgery. This thesis investigates the correlation between intra-operative signals and myocardial injury using machine learning models. The study focuses on analyzing intra-operative ST-elevation, heart rate, diastolic blood pressure, and pulse pressure variation. Multiple machine learning models, including decision tree and random forest classifiers, were developed and evaluated using two approaches: the sequence of event times and comprehensive event features.</p><p>The results indicate that intra-operative physiological signals are valuable predictors of myocardial injury, with random forest models generally outperforming decision tree models. However, limited and unbalanced data posed challenges, affecting model performance variability. This research establishes a framework for enhancing predictive models and monitoring strategies to improve patient outcomes.</p>
----------------------------------------------------------------------
title: "Optimeringsanalys av en modern,systemintegrerad operationssal: Jämförelse av tre styrningsfall"
==>    "Optimeringsanalys av en modern, systemintegrerad operationssal: Jämförelse av tre styrningsfall"

In diva2:1389960 abstract is: <p>Healthcare system complexity can be simplified by equipping hospital operatingtheatres with an integrated system solution. The goal is to improve the hospitalworkflow by enabling maneuvering of all the equipment from a common unit.Today there is no data to support the claim that the system offered feature of usingpreset scenes provides any improvements.</p><p>The aim of this study is to demonstrate the benefits of the integrated operatingsystem. This is done by performing a workflow analysis based on three differentcases (with preset scenes, maneuvered through the applications and without use ofthe integrated system) and three different parameters (clicks, steps, and time). Thepurpose is to clarify the, from a user perspective, most beneficial case.</p><p>The measured result shows an advantage when using the system with presetscenes. All of the assumptions in the survey, including the limitations ofquantitative data, are considered in the study.</p><p>The result is validated by the analysis. A combination of the case with preset scenesand a distributed control between the users are therefore the best alternative for anoptimized workflow.</p>

partal corrected: diva2:1389960: <p>Healthcare system complexity can be simplified by equipping hospital operatingtheatres with an integrated system solution. The goal is to improve the hospitalworkflow by enabling maneuvering of all the equipment from a common unit. Today there is no data to support the claim that the system offered feature of using preset scenes provides any improvements.</p><p>The aim of this study is to demonstrate the benefits of the integrated operatingsystem. This is done by performing a workflow analysis based on three different cases (with preset scenes, maneuvered through the applications and without use of the integrated system) and three different parameters (clicks, steps, and time). The purpose is to clarify the, from a user perspective, most beneficial case.</p><p>The measured result shows an advantage when using the system with presetscenes. All of the assumptions in the survey, including the limitations of quantitative data, are considered in the study.</p><p>The result is validated by the analysis. A combin ation of the case with preset scenes and a distributed control between the users are therefore the best alternative for an optimized workflow.</p>

corrected abstract:
<p>Healthcare system complexity can be simplified by equipping hospital operating theatres with an integrated system solution. The goal is to improve the hospital workflow by enabling maneuvering of all the equipment from a common unit. Today there is no data to support the claim that the system offered feature of using preset scenes provides any improvements.</p><p>The aim of this study is to demonstrate the benefits of the integrated operating system. This is done by performing a workflow analysis based on three different cases (with preset scenes, maneuvered through the applications and without use of the integrated system) and three different parameters (clicks, steps, and time). The purpose is to clarify the, from a user perspective, most beneficial case.</p><p>The measured result shows an advantage when using the system with preset scenes. All of the assumptions in the survey, including the limitations of quantitative data, are considered in the study.</p><p>The result is validated by the analysis. A combination of the case with preset scenes and a distributed control between the users are therefore the best alternative for an optimized workflow.</p>
----------------------------------------------------------------------
In diva2:1849768 abstract is: <p>This thesis explored the development of advanced machine learning models to improve autonomous transportation systems. By focusing on the identification and classification of traffic light signals, the work contributes to the safety and efficiency of self-driving vehicles. Areview of models such as the Single Shot MultiBox Detector (SSD), as an object detectionmodel, and InceptionV3 and VGG16, as classification models, was conducted, with particular emphasis on their training and testing processes.The results, in terms of validation accuracy and validation loss, showed that the InceptionV3model performed well across various parameters. This model proved to be robust and adaptable, making it a good choice for the project's goal of accurate and reliable classification oftraffic light signals.On the other hand, the VGG16 model showed varying results. While it performed well undercertain conditions, it proved to be less robust at certain parameter settings, especially at higherbatch sizes, which led to lower validation accuracy and higher validation loss.</p>


corrected abstract:
<p>This thesis explored the development of advanced machine learning models to improve autonomous transportation systems. By focusing on the identification and classification of traffic light signals, the work contributes to the safety and efficiency of self-driving vehicles. A review of models such as the Single Shot MultiBox Detector (SSD), as an object detection model, and InceptionV3 and VGG16, as classification models, was conducted, with particular emphasis on their training and testing processes.</p><p>The results, in terms of validation accuracy and validation loss, showed that the InceptionV3 model performed well across various parameters. This model proved to be robust and adaptable, making it a good choice for the project's goal of accurate and reliable classification of traffic light signals.</p><p>On the other hand, the VGG16 model showed varying results. While it performed well under certain conditions, it proved to be less robust at certain parameter settings, especially at higher batch sizes, which led to lower validation accuracy and higher validation loss.</p>
----------------------------------------------------------------------
In diva2:1217609 abstract is: <p>As a result of the new General Data Protection Regulation (GDPR) in the EU, there arestricter requirements for handling personal data. For the first time, companies risk sanc-tions if they fail to handle personal data properly, giving rise to a wide spectrum of im-pacts. In the IT sector, an analysis must be undertaken to determine which data will beaffected by the introduction of GDPR and how this data can be managed in current IT sys-tems in order to meet the new requirements. Against this backdrop, this study was con-ducted at Primona, a purchasing and electronic trade company located in Stockholm.A proposed solution was developed by studying the GDPR, related works and the resultsfrom the interviews which was conducted in this study. The proposed solution was thentested on a selected part of one of the company´s systems. Furthermore, this study pre-sents an economic analysis to determine the significance of implementing of this solution,which points to a need for such a solution to be prioritized by the company.Overall, the proposed solution proves to have a positive effect with respect to complyingwith GDPR and can be reused with relatively few resources.</p>

w='pre-sents' val={'c': 'presents', 's': 'diva2:1217609', 'n': 'hyphen at end of line in original'}
w='sanc-tions' val={'c': 'sanctions', 's': 'diva2:1217609', 'n': 'hyphen at end of line in original'}

corrected abstract:
<p>As a result of the new General Data Protection Regulation (GDPR) in the EU, there are stricter requirements for handling personal data. For the first time, companies risk sanctions if they fail to handle personal data properly, giving rise to a wide spectrum of impacts. In the IT sector, an analysis must be undertaken to determine which data will be affected by the introduction of GDPR and how this data can be managed in current IT systems in order to meet the new requirements. Against this backdrop, this study was conducted at Primona, a purchasing and electronic trade company located in Stockholm.</p><p>A proposed solution was developed by studying the GDPR, related works and the results from the interviews which was conducted in this study. The proposed solution was then tested on a selected part of one of the company´s systems. Furthermore, this study presents an economic analysis to determine the significance of implementing of this solution, which points to a need for such a solution to be prioritized by the company.</p><p>Overall, the proposed solution proves to have a positive effect with respect to complying with GDPR and can be reused with relatively few resources.</p>
----------------------------------------------------------------------
